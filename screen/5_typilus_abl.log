(base) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ exit
exit
(base) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ conda aacti[K[K[K[Kactivate [1P[C[C[C[C[C[C[C[Cnai[Kturalcc
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python [K[K[K[K[K[K[Kcd onda activate naturalccd [K/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [C[26Ponda activate naturalcc
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cd /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ pytho n train_enhanced.py [K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[Krun_experiments.py --g[K[K-cong[Kfig experiments_model.yml 
============================================================
Typilus ÂèÇÊï∞Ë∞É‰ºòÂÆûÈ™åÁ≥ªÁªü
============================================================
ÈÖçÁΩÆÊñá‰ª∂: experiments_model.yml

ÂÖ± 6 ‰∏™ÂÆûÈ™å:

1. layers_3: ‰∏âÂ±ÇGGNN (Â¢ûÂä†Ê∑±Â∫¶)
2. layers_4: ÂõõÂ±ÇGGNN (Êõ¥Ê∑±ÁΩëÁªú)
3. dropout_0: Êó†Dropout (ÊµãËØïÊòØÂê¶ÈúÄË¶ÅÊ≠£ÂàôÂåñ)
4. dropout_015: Dropout=0.15 (ÈÄÇÂ∫¶Â¢ûÂº∫)
5. dropout_02: Dropout=0.2 (Âº∫Ê≠£ÂàôÂåñ)
6. dropout_03: Dropout=0.3 (ÊûÅÂº∫Ê≠£ÂàôÂåñ)

============================================================

Êåâ Enter ÂºÄÂßãÂÆûÈ™å...

ËøõÂ∫¶: 1/6

============================================================
ÂÆûÈ™å: layers_3 - ‰∏âÂ±ÇGGNN (Â¢ûÂä†Ê∑±Â∫¶)
Êó∂Èó¥: 2025-11-21 12:04:22
============================================================

[32m[2025-11-21 12:04:23]    INFO >> Âä†ËΩΩÈÖçÁΩÆ: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_3/config.yml (train_enhanced.py:666, cli_main())[0m
[32m[2025-11-21 12:04:23]    INFO >> ÂçïGPUËÆ≠ÁªÉ... (train_enhanced.py:694, cli_main())[0m
[32m[2025-11-21 12:04:24]    INFO >> ËÆ≠ÁªÉÊó•ÂøóÂ∞Ü‰øùÂ≠òÂà∞: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_3/logs (train_enhanced.py:561, single_main())[0m
[32m[2025-11-21 12:04:24]    INFO >> [nodes] dictionary: 9999 types (typilus.py:102, setup_task())[0m
[32m[2025-11-21 12:04:24]    INFO >> [edges] dictionary: 0 types (typilus.py:102, setup_task())[0m
[32m[2025-11-21 12:04:24]    INFO >> [supernodes.annotation] dictionary: 99 types (typilus.py:106, setup_task())[0m
[TrainingLogger] Êó•ÂøóÁõÆÂΩï: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_3/logs
[TrainingLogger] ÂéüÂßãËæìÂá∫Â∞Ü‰øùÂ≠òÂà∞: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_3/logs/training_output.log
Traceback (most recent call last):
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py", line 699, in <module>
    cli_main()
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py", line 695, in cli_main
    single_main(args)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py", line 566, in single_main
    model = task.build_model(args)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py", line 259, in build_model
    return models.build_model(args, config, self)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/models/__init__.py", line 37, in build_model
    return MODEL_REGISTRY[args['model']['arch']].build_model(args, config, task)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/models/type_prediction/typilus.py", line 144, in build_model
    encoder = GGNNEncoder(
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/models/type_prediction/typilus.py", line 85, in __init__
    self.ggnns = nn.ModuleList([
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/models/type_prediction/typilus.py", line 88, in <listcomp>
    backward=edge_backward, timesteps=timesteps[i], dropout=dropout)
IndexError: list index out of range

‚úó layers_3 Â§±Ë¥•(1)

ÂÆûÈ™åÂ§±Ë¥•ÔºåÁªßÁª≠? (y/n): n

============================================================
ÂÆûÈ™åÂÆåÊàê
============================================================
‚úó layers_3
============================================================

Ê≠£Âú®ÂàÜÊûêÁªìÊûú...

================================================================================
ÂÆûÈ™åÁªìÊûúÂØπÊØî
================================================================================
ÂÆûÈ™åÂêçÁß∞                      ËΩÆÊï∞       ÊúÄÁªàLoss       ÊúÄ‰Ω≥È™åËØÅLoss       
--------------------------------------------------------------------------------
layers_1                  11       3.4400       3.6130         
embed_128                 11       3.3240       3.6830         
embed_96                  11       3.3540       3.6980         
embed_32                  11       3.6130       3.7080         
lr_1e-3                   4        3.5320       3.7180         
lr_1.5e-3                 4        3.5200       3.7210         
lr_1.25e-3                4        3.5090       3.7340         
lr_7.5e-4                 4        3.5530       3.7560         
lr_5e-4                   4        3.6480       3.7850         
================================================================================
run_experiments.py:282: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.tight_layout()
run_experiments.py:282: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.tight_layout()
run_experiments.py:282: UserWarning: Glyph 23545 (\N{CJK UNIFIED IDEOGRAPH-5BF9}) missing from current font.
  plt.tight_layout()
run_experiments.py:282: UserWarning: Glyph 27604 (\N{CJK UNIFIED IDEOGRAPH-6BD4}) missing from current font.
  plt.tight_layout()
run_experiments.py:282: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.tight_layout()
run_experiments.py:282: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.tight_layout()
run_experiments.py:283: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.savefig(exp_base_dir / 'comparison.png', dpi=120, bbox_inches='tight')
run_experiments.py:283: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.savefig(exp_base_dir / 'comparison.png', dpi=120, bbox_inches='tight')
run_experiments.py:283: UserWarning: Glyph 23545 (\N{CJK UNIFIED IDEOGRAPH-5BF9}) missing from current font.
  plt.savefig(exp_base_dir / 'comparison.png', dpi=120, bbox_inches='tight')
run_experiments.py:283: UserWarning: Glyph 27604 (\N{CJK UNIFIED IDEOGRAPH-6BD4}) missing from current font.
  plt.savefig(exp_base_dir / 'comparison.png', dpi=120, bbox_inches='tight')
run_experiments.py:283: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.savefig(exp_base_dir / 'comparison.png', dpi=120, bbox_inches='tight')
run_experiments.py:283: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.savefig(exp_base_dir / 'comparison.png', dpi=120, bbox_inches='tight')

ÂØπÊØîÂõæÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/comparison.png
Êä•ÂëäÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/report.md

(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python run_experiments.py --config experiments_model.yml [A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ cd /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [Conda activate naturalcc[K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [1Pexit
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cscreen -r lrexp [C[C[Cls[K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ conda activate naturalcc[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ screen[2P -r transformer [A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ conda activate naturalcc[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ screen[8P -r trans [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ s
creen -r trans [A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ c
onda activate naturalcc[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ p
ython analyze_results.py [A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ c
d run/type_prediction/typilus/experiment_tools/[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ p
[22Python analyze_results.py [A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ c
[2Ponda activate naturalcc[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ s
[8Pcreen -r trans [A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ c
onda activate naturalcc[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ s
[2Pcreen -r transformer [A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ c
onda activate naturalcc[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ s
[14Pcreen -lsr lrexp [A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ e
xit[K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ c
onda activate naturalccd /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ p
[39Python run_experiments.py --config experiments_model.yml [A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ 
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cp
ython run_experiments.py --config experiments_model.yml [A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ c
d /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ cd /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [13Ppython run_experiments.py --config experiments_model.yml 
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Kpython run_experiments.py --config experiments_model.yml 
============================================================
Typilus ÂèÇÊï∞Ë∞É‰ºòÂÆûÈ™åÁ≥ªÁªü
============================================================
ÈÖçÁΩÆÊñá‰ª∂: experiments_model.yml

ÂÖ± 4 ‰∏™ÂÆûÈ™å:

1. dropout_0: Êó†Dropout (ÊµãËØïÊòØÂê¶ÈúÄË¶ÅÊ≠£ÂàôÂåñ)
2. dropout_015: Dropout=0.15 (ÈÄÇÂ∫¶Â¢ûÂº∫)
3. dropout_02: Dropout=0.2 (Âº∫Ê≠£ÂàôÂåñ)
4. dropout_03: Dropout=0.3 (ÊûÅÂº∫Ê≠£ÂàôÂåñ)

============================================================

Êåâ Enter ÂºÄÂßãÂÆûÈ™å...

ËøõÂ∫¶: 1/4

============================================================
ÂÆûÈ™å: dropout_0 - Êó†Dropout (ÊµãËØïÊòØÂê¶ÈúÄË¶ÅÊ≠£ÂàôÂåñ)
Êó∂Èó¥: 2025-11-21 14:13:59
============================================================

[32m[2025-11-21 14:14:01]    INFO >> Âä†ËΩΩÈÖçÁΩÆ: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/config.yml (train_enhanced.py:666, cli_main())[0m
[32m[2025-11-21 14:14:01]    INFO >> ÂçïGPUËÆ≠ÁªÉ... (train_enhanced.py:694, cli_main())[0m
[32m[2025-11-21 14:14:01]    INFO >> ËÆ≠ÁªÉÊó•ÂøóÂ∞Ü‰øùÂ≠òÂà∞: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs (train_enhanced.py:561, single_main())[0m
[32m[2025-11-21 14:14:01]    INFO >> [nodes] dictionary: 9999 types (typilus.py:102, setup_task())[0m
[32m[2025-11-21 14:14:01]    INFO >> [edges] dictionary: 0 types (typilus.py:102, setup_task())[0m
[32m[2025-11-21 14:14:01]    INFO >> [supernodes.annotation] dictionary: 99 types (typilus.py:106, setup_task())[0m
[32m[2025-11-21 14:14:10]    INFO >> Typilus(
  (encoder): GGNNEncoder(
    (node_embedding): Embedding(9999, 64, padding_idx=0)
    (node_layer): Sequential(
      (0): Dropout(p=0.0, inplace=False)
      (1): Linear(in_features=64, out_features=64, bias=False)
      (2): Dropout(p=0.0, inplace=False)
    )
    (ggnns): ModuleList(
      (0): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=64, out_features=64, bias=False)
          (OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (NEXT): Linear(in_features=64, out_features=64, bias=False)
          (SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
          (_CHILD): Linear(in_features=64, out_features=64, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT): Linear(in_features=64, out_features=64, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (_COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (_RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
        )
        (rnn_cell): GRUCell(64, 64)
      )
      (1): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=64, out_features=64, bias=False)
          (OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (NEXT): Linear(in_features=64, out_features=64, bias=False)
          (SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
          (_CHILD): Linear(in_features=64, out_features=64, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT): Linear(in_features=64, out_features=64, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (_COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (_RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
        )
        (rnn_cell): GRUCell(128, 64)
      )
    )
  )
  (decoder): DenseDecoder(
    (cls_layers): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=False)
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=64, out_features=99, bias=True)
    )
  )
) (train_enhanced.py:568, single_main())[0m
[32m[2025-11-21 14:14:10]    INFO >> Ê®°Âûã: typilus, ÊçüÂ§±ÂáΩÊï∞: TypilusCriterion (train_enhanced.py:569, single_main())[0m
[32m[2025-11-21 14:14:10]    INFO >> Ê®°ÂûãÂèÇÊï∞: 847843 (ÂèØËÆ≠ÁªÉ: 847843) (train_enhanced.py:570, single_main())[0m
[32m[2025-11-21 14:14:10]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 14:14:10]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 80579 MB ; used memory = 1340 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 14:14:10]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 14:14:10]    INFO >> ‰ΩøÁî® 1 ‰∏™GPUËÆ≠ÁªÉ (train_enhanced.py:576, single_main())[0m
[32m[2025-11-21 14:14:10]    INFO >> no existing checkpoint found /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-21 14:14:10]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[32m[2025-11-21 14:15:18]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-21 14:15:18] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-21 14:15:26]    INFO >> epoch 001:     50 / 1539 loss=5.627, wps=5212.9, ups=7.21, wpb=720, bsz=720, num_updates=50, lr=0.0004, gnorm=1.175, clip=0, train_wall=7, gb_free=74.4, wall=72 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:15:28] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 15.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 73.64 GiB is allocated by PyTorch, and 4.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:15:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:15:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:15:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 2         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75351 MiB |  75411 MiB |   1663 GiB |   1589 GiB |
|       from large pool |  74995 MiB |  75055 MiB |   1652 GiB |   1578 GiB |
|       from small pool |    356 MiB |    357 MiB |     11 GiB |     10 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75351 MiB |  75411 MiB |   1663 GiB |   1589 GiB |
|       from large pool |  74995 MiB |  75055 MiB |   1652 GiB |   1578 GiB |
|       from small pool |    356 MiB |    357 MiB |     11 GiB |     10 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75205 MiB |  75264 MiB |   1658 GiB |   1584 GiB |
|       from large pool |  74850 MiB |  74910 MiB |   1647 GiB |   1573 GiB |
|       from small pool |    354 MiB |    355 MiB |     11 GiB |     10 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80492 MiB |  80492 MiB |  91734 MiB |  11242 MiB |
|       from large pool |  80098 MiB |  80120 MiB |  91336 MiB |  11238 MiB |
|       from small pool |    394 MiB |    394 MiB |    398 MiB |      4 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5080 MiB |   5613 MiB |    793 GiB |    788 GiB |
|       from large pool |   5042 MiB |   5588 MiB |    780 GiB |    775 GiB |
|       from small pool |     37 MiB |     38 MiB |     13 GiB |     13 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6682    |    6685    |  125203    |  118521    |
|       from large pool |     828    |     829    |   49276    |   48448    |
|       from small pool |    5854    |    5857    |   75927    |   70073    |
|---------------------------------------------------------------------------|
| Active allocs         |    6682    |    6685    |  125203    |  118521    |
|       from large pool |     828    |     829    |   49276    |   48448    |
|       from small pool |    5854    |    5857    |   75927    |   70073    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     760    |     789    |     990    |     230    |
|       from large pool |     563    |     607    |     791    |     228    |
|       from small pool |     197    |     197    |     199    |       2    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     676    |     676    |   76683    |   76007    |
|       from large pool |     322    |     322    |   37136    |   36814    |
|       from small pool |     354    |     354    |   39547    |   39193    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:15:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:15:28] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:15:33]    INFO >> epoch 001:    101 / 1539 loss=4.072, wps=4430.9, ups=7.26, wpb=610.7, bsz=610.7, num_updates=100, lr=0.0004, gnorm=1.535, clip=0, train_wall=6, gb_free=75.7, wall=79 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:15:41]    INFO >> epoch 001:    151 / 1539 loss=3.445, wps=5573.4, ups=6.82, wpb=816.7, bsz=816.7, num_updates=150, lr=0.0004, gnorm=1.517, clip=0, train_wall=7, gb_free=74.3, wall=86 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:15:47]    INFO >> epoch 001:    201 / 1539 loss=3.506, wps=4967.4, ups=7.74, wpb=641.7, bsz=641.7, num_updates=200, lr=0.0004, gnorm=1.52, clip=0, train_wall=6, gb_free=74.9, wall=93 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:15:54]    INFO >> epoch 001:    251 / 1539 loss=3.436, wps=4788.9, ups=7.51, wpb=637.9, bsz=637.9, num_updates=250, lr=0.0004, gnorm=1.675, clip=0, train_wall=6, gb_free=71.7, wall=99 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:16:03]    INFO >> epoch 001:    301 / 1539 loss=3.446, wps=5293.7, ups=6.73, wpb=786.5, bsz=786.5, num_updates=300, lr=0.0004, gnorm=1.617, clip=0, train_wall=7, gb_free=73.9, wall=107 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:16:09]    INFO >> epoch 001:    351 / 1539 loss=3.272, wps=5087.8, ups=7.58, wpb=671.6, bsz=671.6, num_updates=350, lr=0.0004, gnorm=1.667, clip=0, train_wall=6, gb_free=72.6, wall=114 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:16:16]    INFO >> epoch 001:    401 / 1539 loss=3.261, wps=5867.2, ups=6.89, wpb=851.7, bsz=851.7, num_updates=400, lr=0.0004, gnorm=1.966, clip=0, train_wall=7, gb_free=73.4, wall=121 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:16:26] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 17.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 77.91 GiB is allocated by PyTorch, and 711.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:16:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:16:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:16:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 4         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79716 MiB |  79776 MiB |  11725 GiB |  11647 GiB |
|       from large pool |  79575 MiB |  79635 MiB |  11661 GiB |  11583 GiB |
|       from small pool |    141 MiB |    142 MiB |     64 GiB |     63 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79716 MiB |  79776 MiB |  11725 GiB |  11647 GiB |
|       from large pool |  79575 MiB |  79635 MiB |  11661 GiB |  11583 GiB |
|       from small pool |    141 MiB |    142 MiB |     64 GiB |     63 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79601 MiB |  79660 MiB |  11695 GiB |  11617 GiB |
|       from large pool |  79459 MiB |  79519 MiB |  11631 GiB |  11553 GiB |
|       from small pool |    141 MiB |    142 MiB |     64 GiB |     63 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80488 MiB |  80490 MiB | 166570 MiB |  86082 MiB |
|       from large pool |  80342 MiB |  80342 MiB | 166046 MiB |  85704 MiB |
|       from small pool |    146 MiB |    394 MiB |    524 MiB |    378 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory | 728130 KiB |   2495 MiB |   5498 GiB |   5498 GiB |
|       from large pool | 723709 KiB |   2491 MiB |   5424 GiB |   5423 GiB |
|       from small pool |   4421 KiB |     17 MiB |     74 GiB |     74 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2876    |    2879    |     781 K  |     778 K  |
|       from large pool |     496    |     497    |     357 K  |     357 K  |
|       from small pool |    2380    |    2383    |     423 K  |     421 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2876    |    2879    |     781 K  |     778 K  |
|       from large pool |     496    |     497    |     357 K  |     357 K  |
|       from small pool |    2380    |    2383    |     423 K  |     421 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     373    |     759    |    1313    |     940    |
|       from large pool |     300    |     562    |    1051    |     751    |
|       from small pool |      73    |     197    |     262    |     189    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     142    |     144    |  485024    |  484882    |
|       from large pool |      74    |      75    |  277574    |  277500    |
|       from small pool |      68    |      70    |  207450    |  207382    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:16:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:16:26] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:16:30]    INFO >> epoch 001:    452 / 1539 loss=3.263, wps=2371.9, ups=3.76, wpb=631.6, bsz=631.6, num_updates=450, lr=0.0004, gnorm=1.689, clip=0, train_wall=6, gb_free=72, wall=134 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:16:41]    INFO >> epoch 001:    502 / 1539 loss=3.49, wps=3940.4, ups=5.3, wpb=743.1, bsz=743.1, num_updates=500, lr=0.0004, gnorm=1.686, clip=0, train_wall=9, gb_free=72.7, wall=144 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:16:47]    INFO >> epoch 001:    552 / 1539 loss=3.211, wps=4780.7, ups=7.28, wpb=657, bsz=657, num_updates=550, lr=0.0004, gnorm=1.876, clip=0, train_wall=6, gb_free=65.9, wall=150 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:16:54]    INFO >> epoch 001:    602 / 1539 loss=3.306, wps=4840.3, ups=7.24, wpb=668.6, bsz=668.6, num_updates=600, lr=0.0004, gnorm=1.98, clip=0, train_wall=6, gb_free=73.4, wall=157 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:17:01]    INFO >> epoch 001:    652 / 1539 loss=3.335, wps=4979.4, ups=6.99, wpb=712.2, bsz=712.2, num_updates=650, lr=0.0004, gnorm=1.717, clip=0, train_wall=6, gb_free=73.7, wall=164 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:17:11]    INFO >> epoch 001:    702 / 1539 loss=3.21, wps=4376.2, ups=6.51, wpb=672.5, bsz=672.5, num_updates=700, lr=0.0004, gnorm=1.756, clip=0, train_wall=7, gb_free=74.2, wall=172 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:17:18]    INFO >> epoch 001:    752 / 1539 loss=3.083, wps=5299.6, ups=7, wpb=757.4, bsz=757.4, num_updates=750, lr=0.0004, gnorm=1.677, clip=0, train_wall=6, gb_free=73.8, wall=179 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:17:24]    INFO >> epoch 001:    802 / 1539 loss=3.411, wps=5413.2, ups=7.46, wpb=725.2, bsz=725.2, num_updates=800, lr=0.0004, gnorm=1.95, clip=0, train_wall=6, gb_free=73.5, wall=186 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:17:32]    INFO >> epoch 001:    852 / 1539 loss=3.369, wps=4419, ups=6.89, wpb=641.1, bsz=641.1, num_updates=850, lr=0.0004, gnorm=1.838, clip=0, train_wall=7, gb_free=71.9, wall=193 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:17:40]    INFO >> epoch 001:    902 / 1539 loss=3.265, wps=4907.3, ups=7.43, wpb=660.1, bsz=660.1, num_updates=900, lr=0.0004, gnorm=1.747, clip=0, train_wall=6, gb_free=72.3, wall=200 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:17:47]    INFO >> epoch 001:    952 / 1539 loss=3.246, wps=5118.8, ups=7.18, wpb=713.2, bsz=713.2, num_updates=950, lr=0.0004, gnorm=1.753, clip=0, train_wall=6, gb_free=72, wall=207 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:17:49] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 376.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 237.25 MiB is free. Including non-PyTorch memory, this process has 78.88 GiB memory in use. Of the allocated memory 77.19 GiB is allocated by PyTorch, and 1.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:17:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:17:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:17:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 3            |        cudaMalloc retries: 7         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  66231 MiB |  79042 MiB |  26729 GiB |  26664 GiB |
|       from large pool |  66222 MiB |  79033 MiB |  26597 GiB |  26532 GiB |
|       from small pool |      8 MiB |     18 MiB |    131 GiB |    131 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  66231 MiB |  79042 MiB |  26729 GiB |  26664 GiB |
|       from large pool |  66222 MiB |  79033 MiB |  26597 GiB |  26532 GiB |
|       from small pool |      8 MiB |     18 MiB |    131 GiB |    131 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  66225 MiB |  79032 MiB |  26678 GiB |  26613 GiB |
|       from large pool |  66216 MiB |  79023 MiB |  26546 GiB |  26482 GiB |
|       from small pool |      8 MiB |     18 MiB |    131 GiB |    131 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80268 MiB |  80268 MiB | 285424 MiB | 205156 MiB |
|       from large pool |  80246 MiB |  80246 MiB | 284826 MiB | 204580 MiB |
|       from small pool |     22 MiB |     96 MiB |    598 MiB |    576 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2472 MiB |   5499 MiB |  19910 GiB |  19907 GiB |
|       from large pool |   2459 MiB |   5485 MiB |  19758 GiB |  19755 GiB |
|       from small pool |     13 MiB |     17 MiB |    151 GiB |    151 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     543    |     585    |    1675 K  |    1674 K  |
|       from large pool |     260    |     302    |     802 K  |     802 K  |
|       from small pool |     283    |     354    |     872 K  |     872 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     543    |     585    |    1675 K  |    1674 K  |
|       from large pool |     260    |     302    |     802 K  |     802 K  |
|       from small pool |     283    |     354    |     872 K  |     872 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     115    |     169    |    1487    |    1372    |
|       from large pool |     104    |     121    |    1188    |    1084    |
|       from small pool |      11    |      48    |     299    |     288    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     115    |     120    |     969 K  |     969 K  |
|       from large pool |      93    |      98    |     561 K  |     561 K  |
|       from small pool |      22    |      43    |     407 K  |     407 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:17:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:17:49] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:17:55]    INFO >> epoch 001:   1003 / 1539 loss=3.272, wps=3872.8, ups=5.96, wpb=649.4, bsz=649.4, num_updates=1000, lr=0.0004, gnorm=1.879, clip=0, train_wall=6, gb_free=72.3, wall=215 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:18:02]    INFO >> epoch 001:   1053 / 1539 loss=3.154, wps=5622.6, ups=6.84, wpb=822.6, bsz=822.6, num_updates=1050, lr=0.0004, gnorm=1.94, clip=0, train_wall=7, gb_free=68.3, wall=223 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:18:10] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.88 GiB. GPU 2 has a total capacity of 79.14 GiB of which 519.25 MiB is free. Including non-PyTorch memory, this process has 78.61 GiB memory in use. Of the allocated memory 72.51 GiB is allocated by PyTorch, and 5.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:18:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:18:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:18:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 4            |        cudaMalloc retries: 9         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  63155 MiB |  75126 MiB |  30822 GiB |  30760 GiB |
|       from large pool |  63144 MiB |  75115 MiB |  30668 GiB |  30607 GiB |
|       from small pool |     11 MiB |     15 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  63155 MiB |  75126 MiB |  30822 GiB |  30760 GiB |
|       from large pool |  63144 MiB |  75115 MiB |  30668 GiB |  30607 GiB |
|       from small pool |     11 MiB |     15 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  63147 MiB |  75114 MiB |  30766 GiB |  30704 GiB |
|       from large pool |  63136 MiB |  75103 MiB |  30612 GiB |  30551 GiB |
|       from small pool |     11 MiB |     15 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79986 MiB |  80256 MiB | 305464 MiB | 225478 MiB |
|       from large pool |  79964 MiB |  80234 MiB | 304684 MiB | 224720 MiB |
|       from small pool |     22 MiB |    204 MiB |    780 MiB |    758 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   8174 MiB |  10475 MiB |  24343 GiB |  24335 GiB |
|       from large pool |   8163 MiB |  10464 MiB |  24166 GiB |  24158 GiB |
|       from small pool |     10 MiB |     17 MiB |    177 GiB |    177 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     513    |     555    |    1930 K  |    1930 K  |
|       from large pool |     230    |     272    |     918 K  |     918 K  |
|       from small pool |     283    |     354    |    1012 K  |    1011 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     513    |     555    |    1930 K  |    1930 K  |
|       from large pool |     230    |     272    |     918 K  |     918 K  |
|       from small pool |     283    |     354    |    1012 K  |    1011 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      98    |     204    |    1597    |    1499    |
|       from large pool |      87    |     102    |    1207    |    1120    |
|       from small pool |      11    |     102    |     390    |     379    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     105    |     108    |    1110 K  |    1110 K  |
|       from large pool |      83    |      86    |     633 K  |     633 K  |
|       from small pool |      22    |      42    |     477 K  |     477 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:18:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:18:10] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:18:11]    INFO >> epoch 001:   1104 / 1539 loss=3.099, wps=5478.3, ups=5.89, wpb=929.6, bsz=929.6, num_updates=1100, lr=0.0004, gnorm=2, clip=0, train_wall=7, gb_free=73, wall=231 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:18:19]    INFO >> epoch 001:   1154 / 1539 loss=3.466, wps=5088.7, ups=7.35, wpb=692.6, bsz=692.6, num_updates=1150, lr=0.0004, gnorm=1.917, clip=0, train_wall=6, gb_free=73.3, wall=238 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:18:26]    INFO >> epoch 001:   1204 / 1539 loss=3.14, wps=5060, ups=7.46, wpb=678.4, bsz=678.4, num_updates=1200, lr=0.0004, gnorm=1.843, clip=0, train_wall=6, gb_free=71.4, wall=245 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:18:33]    INFO >> epoch 001:   1254 / 1539 loss=3.356, wps=5280.2, ups=7.22, wpb=730.9, bsz=730.9, num_updates=1250, lr=0.0004, gnorm=1.915, clip=0, train_wall=6, gb_free=71.5, wall=252 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:18:40]    INFO >> epoch 001:   1304 / 1539 loss=3.375, wps=5186.5, ups=7.05, wpb=735.4, bsz=735.4, num_updates=1300, lr=0.0004, gnorm=1.744, clip=0, train_wall=6, gb_free=74.4, wall=259 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:18:48]    INFO >> epoch 001:   1354 / 1539 loss=3.274, wps=4723.4, ups=7.19, wpb=657.2, bsz=657.2, num_updates=1350, lr=0.0004, gnorm=1.933, clip=0, train_wall=6, gb_free=73.5, wall=266 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:18:55]    INFO >> epoch 001:   1404 / 1539 loss=3.293, wps=5009.9, ups=7.03, wpb=712.5, bsz=712.5, num_updates=1400, lr=0.0004, gnorm=2.03, clip=0, train_wall=6, gb_free=73.4, wall=273 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:19:02]    INFO >> epoch 001:   1454 / 1539 loss=3.243, wps=5160.3, ups=7.35, wpb=702.2, bsz=702.2, num_updates=1450, lr=0.0004, gnorm=1.947, clip=0, train_wall=6, gb_free=71.8, wall=280 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:19:09]    INFO >> epoch 001:   1504 / 1539 loss=3.285, wps=4936.8, ups=7.09, wpb=695.9, bsz=695.9, num_updates=1500, lr=0.0004, gnorm=2.194, clip=0, train_wall=6, gb_free=71, wall=287 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:19:14]    INFO >> epoch 001 | loss 3.398 | wps 4836 | ups 6.78 | wpb 712.7 | bsz 712.7 | num_updates 1535 | lr 0.0004 | gnorm 1.798 | clip 0 | train_wall 196 | gb_free 76.7 | wall 291 (progress_bar.py:267, print())[0m
[33m[2025-11-21 14:19:14] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:19:27]    INFO >> epoch 001 | valid on 'valid' subset | loss 3.481 | wps 11507.2 | wpb 5412.5 | bsz 5412.5 | num_updates 1535 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
[32m[2025-11-21 14:19:28]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 14:19:28]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_best.pt (epoch 1 @ 1535 updates, score 3.481) (writing took 0.024328 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 14:19:28] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-21 14:19:30]    INFO >> epoch 002:     15 / 1539 loss=3.248, wps=1774.4, ups=2.43, wpb=731.3, bsz=731.3, num_updates=1550, lr=0.0004, gnorm=1.987, clip=0, train_wall=6, gb_free=74.5, wall=307 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:19:36]    INFO >> epoch 002:     65 / 1539 loss=3.309, wps=4951.3, ups=7.52, wpb=658.8, bsz=658.8, num_updates=1600, lr=0.0004, gnorm=1.779, clip=0, train_wall=6, gb_free=73.5, wall=314 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:19:43]    INFO >> epoch 002:    115 / 1539 loss=3.27, wps=5015.6, ups=7.02, wpb=714.6, bsz=714.6, num_updates=1650, lr=0.0004, gnorm=1.91, clip=0, train_wall=7, gb_free=66, wall=321 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:19:51]    INFO >> epoch 002:    165 / 1539 loss=3.119, wps=5113.7, ups=6.92, wpb=738.8, bsz=738.8, num_updates=1700, lr=0.0004, gnorm=1.655, clip=0, train_wall=7, gb_free=73.8, wall=328 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:20:00]    INFO >> epoch 002:    215 / 1539 loss=3.377, wps=4966.8, ups=7.05, wpb=704.2, bsz=704.2, num_updates=1750, lr=0.0004, gnorm=2.095, clip=0, train_wall=7, gb_free=71.4, wall=335 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:20:08]    INFO >> epoch 002:    265 / 1539 loss=2.884, wps=5454.5, ups=6.24, wpb=873.5, bsz=873.5, num_updates=1800, lr=0.0004, gnorm=1.743, clip=0, train_wall=8, gb_free=74.8, wall=343 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:20:15]    INFO >> epoch 002:    315 / 1539 loss=3.288, wps=4672.2, ups=7.24, wpb=645.1, bsz=645.1, num_updates=1850, lr=0.0004, gnorm=1.863, clip=0, train_wall=7, gb_free=72.1, wall=350 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:20:22]    INFO >> epoch 002:    365 / 1539 loss=3.341, wps=4741.7, ups=7.23, wpb=655.8, bsz=655.8, num_updates=1900, lr=0.0004, gnorm=2.025, clip=0, train_wall=7, gb_free=74.1, wall=357 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:20:31]    INFO >> epoch 002:    415 / 1539 loss=3.297, wps=4705.8, ups=6.6, wpb=713.2, bsz=713.2, num_updates=1950, lr=0.0004, gnorm=1.666, clip=0, train_wall=7, gb_free=76.3, wall=365 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:20:32] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.46 GiB. GPU 2 has a total capacity of 79.14 GiB of which 991.25 MiB is free. Including non-PyTorch memory, this process has 78.15 GiB memory in use. Of the allocated memory 75.54 GiB is allocated by PyTorch, and 2.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:20:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:20:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:20:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 5            |        cudaMalloc retries: 10        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  66230 MiB |  77540 MiB |  58600 GiB |  58535 GiB |
|       from large pool |  66221 MiB |  77531 MiB |  58297 GiB |  58232 GiB |
|       from small pool |      8 MiB |     21 MiB |    303 GiB |    303 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  66230 MiB |  77540 MiB |  58600 GiB |  58535 GiB |
|       from large pool |  66221 MiB |  77531 MiB |  58297 GiB |  58232 GiB |
|       from small pool |      8 MiB |     21 MiB |    303 GiB |    303 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  66225 MiB |  77534 MiB |  58506 GiB |  58441 GiB |
|       from large pool |  66216 MiB |  77525 MiB |  58203 GiB |  58139 GiB |
|       from small pool |      8 MiB |     21 MiB |    302 GiB |    302 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79514 MiB |  79696 MiB | 313830 MiB | 234316 MiB |
|       from large pool |  79492 MiB |  79492 MiB | 312868 MiB | 233376 MiB |
|       from small pool |     22 MiB |    204 MiB |    962 MiB |    940 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5099 MiB |   6843 MiB |  53186 GiB |  53181 GiB |
|       from large pool |   5086 MiB |   6830 MiB |  52843 GiB |  52838 GiB |
|       from small pool |     13 MiB |     17 MiB |    343 GiB |    343 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     545    |     586    |    3667 K  |    3666 K  |
|       from large pool |     260    |     301    |    1671 K  |    1671 K  |
|       from small pool |     285    |     355    |    1996 K  |    1995 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     545    |     586    |    3667 K  |    3666 K  |
|       from large pool |     260    |     301    |    1671 K  |    1671 K  |
|       from small pool |     285    |     355    |    1996 K  |    1995 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      91    |     182    |    1691    |    1600    |
|       from large pool |      80    |      80    |    1210    |    1130    |
|       from small pool |      11    |     102    |     481    |     470    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      94    |      94    |    2069 K  |    2069 K  |
|       from large pool |      73    |      73    |    1102 K  |    1102 K  |
|       from small pool |      21    |      44    |     966 K  |     966 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:20:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:20:32] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:20:39]    INFO >> epoch 002:    466 / 1539 loss=3.206, wps=4598.7, ups=6.3, wpb=729.4, bsz=729.4, num_updates=2000, lr=0.0004, gnorm=1.974, clip=0, train_wall=7, gb_free=71.8, wall=373 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:20:46]    INFO >> epoch 002:    516 / 1539 loss=3.434, wps=4874.5, ups=6.88, wpb=708.4, bsz=708.4, num_updates=2050, lr=0.0004, gnorm=2.013, clip=0, train_wall=7, gb_free=71.4, wall=380 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:20:54]    INFO >> epoch 002:    566 / 1539 loss=3.321, wps=4333.2, ups=6.96, wpb=622.3, bsz=622.3, num_updates=2100, lr=0.0004, gnorm=1.854, clip=0, train_wall=7, gb_free=74.1, wall=387 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:21:02]    INFO >> epoch 002:    616 / 1539 loss=3.32, wps=5738.6, ups=6.59, wpb=871.1, bsz=871.1, num_updates=2150, lr=0.0004, gnorm=2.003, clip=0, train_wall=7, gb_free=68.9, wall=395 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:21:10]    INFO >> epoch 002:    666 / 1539 loss=3.173, wps=5054.8, ups=6.54, wpb=772.9, bsz=772.9, num_updates=2200, lr=0.0004, gnorm=1.873, clip=0, train_wall=7, gb_free=68.7, wall=402 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:21:17]    INFO >> epoch 002:    716 / 1539 loss=3.165, wps=4961.1, ups=7, wpb=708.6, bsz=708.6, num_updates=2250, lr=0.0004, gnorm=1.659, clip=0, train_wall=7, gb_free=73.2, wall=409 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:21:24]    INFO >> epoch 002:    766 / 1539 loss=3.175, wps=4577.6, ups=6.88, wpb=665.2, bsz=665.2, num_updates=2300, lr=0.0004, gnorm=1.742, clip=0, train_wall=7, gb_free=75.8, wall=417 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:21:32]    INFO >> epoch 002:    816 / 1539 loss=3.397, wps=4564.4, ups=6.96, wpb=656, bsz=656, num_updates=2350, lr=0.0004, gnorm=1.847, clip=0, train_wall=7, gb_free=72.3, wall=424 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:21:41]    INFO >> epoch 002:    866 / 1539 loss=3.431, wps=4674.7, ups=6.49, wpb=719.8, bsz=719.8, num_updates=2400, lr=0.0004, gnorm=2.08, clip=0, train_wall=7, gb_free=75.8, wall=432 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:21:48]    INFO >> epoch 002:    916 / 1539 loss=3.231, wps=4550.3, ups=6.78, wpb=671.5, bsz=671.5, num_updates=2450, lr=0.0004, gnorm=1.864, clip=0, train_wall=7, gb_free=73.2, wall=439 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:21:55]    INFO >> epoch 002:    966 / 1539 loss=3.198, wps=4408, ups=6.91, wpb=637.9, bsz=637.9, num_updates=2500, lr=0.0004, gnorm=1.746, clip=0, train_wall=7, gb_free=69.4, wall=446 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:21:56] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 5.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.05 GiB is allocated by PyTorch, and 1.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:21:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:21:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:21:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 6            |        cudaMalloc retries: 11        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78842 MiB |  78902 MiB |  73881 GiB |  73804 GiB |
|       from large pool |  78450 MiB |  78510 MiB |  73502 GiB |  73425 GiB |
|       from small pool |    392 MiB |    393 MiB |    379 GiB |    379 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78842 MiB |  78902 MiB |  73881 GiB |  73804 GiB |
|       from large pool |  78450 MiB |  78510 MiB |  73502 GiB |  73425 GiB |
|       from small pool |    392 MiB |    393 MiB |    379 GiB |    379 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78748 MiB |  78807 MiB |  73766 GiB |  73689 GiB |
|       from large pool |  78358 MiB |  78417 MiB |  73387 GiB |  73310 GiB |
|       from small pool |    390 MiB |    391 MiB |    379 GiB |    378 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80500 MiB |  80502 MiB | 323002 MiB | 242502 MiB |
|       from large pool |  80068 MiB |  80068 MiB | 321628 MiB | 241560 MiB |
|       from small pool |    432 MiB |    434 MiB |   1374 MiB |    942 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1597 MiB |   5160 MiB |  70444 GiB |  70442 GiB |
|       from large pool |   1557 MiB |   5155 MiB |  70012 GiB |  70011 GiB |
|       from small pool |     39 MiB |     41 MiB |    431 GiB |    431 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7333    |    7336    |    4635 K  |    4628 K  |
|       from large pool |     887    |     888    |    2133 K  |    2132 K  |
|       from small pool |    6446    |    6449    |    2501 K  |    2495 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7333    |    7336    |    4635 K  |    4628 K  |
|       from large pool |     887    |     888    |    2133 K  |    2132 K  |
|       from small pool |    6446    |    6449    |    2501 K  |    2495 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     439    |     440    |    2043    |    1604    |
|       from large pool |     223    |     223    |    1356    |    1133    |
|       from small pool |     216    |     217    |     687    |     471    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     465    |     466    |    2590 K  |    2589 K  |
|       from large pool |      75    |      75    |    1385 K  |    1385 K  |
|       from small pool |     390    |     391    |    1204 K  |    1203 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:21:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:21:56] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 14:22:01] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 49.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 2.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:22:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:22:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:22:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 7            |        cudaMalloc retries: 13        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78272 MiB |  78332 MiB |  74786 GiB |  74710 GiB |
|       from large pool |  78144 MiB |  78204 MiB |  74401 GiB |  74325 GiB |
|       from small pool |    127 MiB |    128 MiB |    384 GiB |    384 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78272 MiB |  78332 MiB |  74786 GiB |  74710 GiB |
|       from large pool |  78144 MiB |  78204 MiB |  74401 GiB |  74325 GiB |
|       from small pool |    127 MiB |    128 MiB |    384 GiB |    384 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78218 MiB |  78278 MiB |  74669 GiB |  74593 GiB |
|       from large pool |  78091 MiB |  78150 MiB |  74285 GiB |  74209 GiB |
|       from small pool |    127 MiB |    128 MiB |    384 GiB |    384 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80456 MiB |  80458 MiB | 329240 MiB | 248784 MiB |
|       from large pool |  80324 MiB |  80324 MiB | 327754 MiB | 247430 MiB |
|       from small pool |    132 MiB |    432 MiB |   1486 MiB |   1354 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2123 MiB |   9172 MiB |  71231 GiB |  71229 GiB |
|       from large pool |   2119 MiB |   9165 MiB |  70793 GiB |  70791 GiB |
|       from small pool |      4 MiB |     19 MiB |    438 GiB |    438 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2625    |    2628    |    4695 K  |    4692 K  |
|       from large pool |     473    |     474    |    2159 K  |    2158 K  |
|       from small pool |    2152    |    2155    |    2536 K  |    2534 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2625    |    2628    |    4695 K  |    4692 K  |
|       from large pool |     473    |     474    |    2159 K  |    2158 K  |
|       from small pool |    2152    |    2155    |    2536 K  |    2534 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     289    |     438    |    2178    |    1889    |
|       from large pool |     223    |     223    |    1435    |    1212    |
|       from small pool |      66    |     216    |     743    |     677    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     193    |     194    |    2627 K  |    2627 K  |
|       from large pool |     130    |     134    |    1403 K  |    1403 K  |
|       from small pool |      63    |      64    |    1223 K  |    1223 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:22:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:22:01] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:22:04]    INFO >> epoch 002:   1018 / 1539 loss=3.245, wps=4027.2, ups=5.85, wpb=687.9, bsz=687.9, num_updates=2550, lr=0.0004, gnorm=1.984, clip=0, train_wall=7, gb_free=73.1, wall=455 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:22:13]    INFO >> epoch 002:   1068 / 1539 loss=3.086, wps=4995.5, ups=6.54, wpb=763.7, bsz=763.7, num_updates=2600, lr=0.0004, gnorm=1.661, clip=0, train_wall=7, gb_free=73.7, wall=462 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:22:21]    INFO >> epoch 002:   1118 / 1539 loss=3.077, wps=5125.8, ups=6.47, wpb=791.9, bsz=791.9, num_updates=2650, lr=0.0004, gnorm=1.811, clip=0, train_wall=7, gb_free=69.4, wall=470 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:22:28]    INFO >> epoch 002:   1168 / 1539 loss=3.408, wps=5113.2, ups=6.64, wpb=770.1, bsz=770.1, num_updates=2700, lr=0.0004, gnorm=2.11, clip=0, train_wall=7, gb_free=72.5, wall=478 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:22:36]    INFO >> epoch 002:   1218 / 1539 loss=3.354, wps=4760.2, ups=6.7, wpb=710, bsz=710, num_updates=2750, lr=0.0004, gnorm=1.965, clip=0, train_wall=7, gb_free=71.1, wall=485 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:22:39] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 121.25 MiB is free. Including non-PyTorch memory, this process has 79.00 GiB memory in use. Of the allocated memory 76.03 GiB is allocated by PyTorch, and 2.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:22:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:22:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:22:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 8            |        cudaMalloc retries: 15        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77368 MiB |  77855 MiB |  81624 GiB |  81548 GiB |
|       from large pool |  77357 MiB |  77844 MiB |  81204 GiB |  81129 GiB |
|       from small pool |     11 MiB |     14 MiB |    419 GiB |    419 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77368 MiB |  77855 MiB |  81624 GiB |  81548 GiB |
|       from large pool |  77357 MiB |  77844 MiB |  81204 GiB |  81129 GiB |
|       from small pool |     11 MiB |     14 MiB |    419 GiB |    419 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77354 MiB |  77841 MiB |  81497 GiB |  81422 GiB |
|       from large pool |  77342 MiB |  77830 MiB |  81078 GiB |  81003 GiB |
|       from small pool |     11 MiB |     14 MiB |    418 GiB |    418 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80384 MiB |  80482 MiB | 338664 MiB | 258280 MiB |
|       from large pool |  80362 MiB |  80362 MiB | 337092 MiB | 256730 MiB |
|       from small pool |     22 MiB |    218 MiB |   1572 MiB |   1550 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2745 MiB |   6422 MiB |  78054 GiB |  78052 GiB |
|       from large pool |   2734 MiB |   6411 MiB |  77576 GiB |  77573 GiB |
|       from small pool |     10 MiB |     17 MiB |    478 GiB |    478 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     564    |     568    |    5121 K  |    5120 K  |
|       from large pool |     279    |     283    |    2358 K  |    2358 K  |
|       from small pool |     285    |     356    |    2762 K  |    2762 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     564    |     568    |    5121 K  |    5120 K  |
|       from large pool |     279    |     283    |    2358 K  |    2358 K  |
|       from small pool |     285    |     356    |    2762 K  |    2762 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      88    |     331    |    2230    |    2142    |
|       from large pool |      77    |     222    |    1444    |    1367    |
|       from small pool |      11    |     109    |     786    |     775    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      98    |      99    |    2867 K  |    2867 K  |
|       from large pool |      75    |      76    |    1533 K  |    1533 K  |
|       from small pool |      23    |      49    |    1334 K  |    1334 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:22:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:22:39] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:22:45]    INFO >> epoch 002:   1269 / 1539 loss=3.188, wps=4624.8, ups=6.04, wpb=765.3, bsz=765.3, num_updates=2800, lr=0.0004, gnorm=1.733, clip=0, train_wall=7, gb_free=70.7, wall=493 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:22:52]    INFO >> epoch 002:   1319 / 1539 loss=3.163, wps=4735.1, ups=7.16, wpb=661.5, bsz=661.5, num_updates=2850, lr=0.0004, gnorm=1.675, clip=0, train_wall=7, gb_free=75.1, wall=500 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:22:59]    INFO >> epoch 002:   1369 / 1539 loss=3.271, wps=4994, ups=6.86, wpb=728.5, bsz=728.5, num_updates=2900, lr=0.0004, gnorm=1.842, clip=0, train_wall=7, gb_free=70.7, wall=508 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:23:07]    INFO >> epoch 002:   1419 / 1539 loss=3.207, wps=4570, ups=6.99, wpb=653.6, bsz=653.6, num_updates=2950, lr=0.0004, gnorm=1.682, clip=0, train_wall=7, gb_free=65.1, wall=515 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:23:16]    INFO >> epoch 002:   1469 / 1539 loss=3.294, wps=4365.3, ups=6.21, wpb=702.5, bsz=702.5, num_updates=3000, lr=0.0004, gnorm=1.686, clip=0, train_wall=7, gb_free=70.5, wall=523 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:23:24]    INFO >> epoch 002:   1519 / 1539 loss=3.269, wps=4452.7, ups=6.61, wpb=673.3, bsz=673.3, num_updates=3050, lr=0.0004, gnorm=1.835, clip=0, train_wall=7, gb_free=74.3, wall=530 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:23:27]    INFO >> epoch 002 | loss 3.248 | wps 4518.4 | ups 6.34 | wpb 712.7 | bsz 712.7 | num_updates 3070 | lr 0.0004 | gnorm 1.847 | clip 0 | train_wall 213 | gb_free 72.6 | wall 534 (progress_bar.py:267, print())[0m
[33m[2025-11-21 14:23:27] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:23:40]    INFO >> epoch 002 | valid on 'valid' subset | loss 3.466 | wps 11475.4 | wpb 5412.5 | bsz 5412.5 | num_updates 3070 | best_loss 3.481 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 14:23:40]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 14:23:40]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_last.pt (epoch 2 @ 3070 updates, score 3.466) (writing took 0.017383 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 14:23:40] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:23:45]    INFO >> epoch 003:     30 / 1539 loss=3.286, wps=1622.1, ups=2.38, wpb=681.1, bsz=681.1, num_updates=3100, lr=0.000392, gnorm=1.839, clip=0, train_wall=7, gb_free=70.9, wall=551 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:23:53]    INFO >> epoch 003:     80 / 1539 loss=3.371, wps=5162.8, ups=6.68, wpb=772.8, bsz=772.8, num_updates=3150, lr=0.000392, gnorm=1.733, clip=0, train_wall=7, gb_free=73.6, wall=559 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:23:56] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 938.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 41.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 77.37 GiB is allocated by PyTorch, and 1.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:23:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:23:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:23:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 9            |        cudaMalloc retries: 17        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73478 MiB |  79415 MiB |  96974 GiB |  96902 GiB |
|       from large pool |  73469 MiB |  79406 MiB |  96467 GiB |  96395 GiB |
|       from small pool |      8 MiB |     19 MiB |    506 GiB |    506 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73478 MiB |  79415 MiB |  96974 GiB |  96902 GiB |
|       from large pool |  73469 MiB |  79406 MiB |  96467 GiB |  96395 GiB |
|       from small pool |      8 MiB |     19 MiB |    506 GiB |    506 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73471 MiB |  79407 MiB |  96828 GiB |  96756 GiB |
|       from large pool |  73462 MiB |  79398 MiB |  96322 GiB |  96250 GiB |
|       from small pool |      8 MiB |     19 MiB |    506 GiB |    506 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80464 MiB |  80464 MiB | 339136 MiB | 258672 MiB |
|       from large pool |  80440 MiB |  80440 MiB | 337468 MiB | 257028 MiB |
|       from small pool |     24 MiB |    118 MiB |   1668 MiB |   1644 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4257 MiB |   5673 MiB |  93086 GiB |  93082 GiB |
|       from large pool |   4242 MiB |   5658 MiB |  92513 GiB |  92509 GiB |
|       from small pool |     15 MiB |     21 MiB |    572 GiB |    572 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     571    |     589    |    6061 K  |    6060 K  |
|       from large pool |     286    |     304    |    2725 K  |    2725 K  |
|       from small pool |     285    |     356    |    3335 K  |    3335 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     571    |     589    |    6061 K  |    6060 K  |
|       from large pool |     286    |     304    |    2725 K  |    2725 K  |
|       from small pool |     285    |     356    |    3335 K  |    3335 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      88    |     135    |    2279    |    2191    |
|       from large pool |      76    |      76    |    1445    |    1369    |
|       from small pool |      12    |      59    |     834    |     822    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      99    |      99    |    3402 K  |    3402 K  |
|       from large pool |      78    |      78    |    1755 K  |    1755 K  |
|       from small pool |      21    |      44    |    1646 K  |    1646 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:23:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:23:56] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:24:01]    INFO >> epoch 003:    131 / 1539 loss=3.282, wps=5229.4, ups=6.26, wpb=835, bsz=835, num_updates=3200, lr=0.000392, gnorm=1.931, clip=0, train_wall=7, gb_free=72, wall=567 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:24:09]    INFO >> epoch 003:    181 / 1539 loss=3.386, wps=4599.7, ups=7, wpb=656.8, bsz=656.8, num_updates=3250, lr=0.000392, gnorm=1.819, clip=0, train_wall=7, gb_free=73.2, wall=574 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:24:13] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.88 GiB. GPU 2 has a total capacity of 79.14 GiB of which 929.25 MiB is free. Including non-PyTorch memory, this process has 78.21 GiB memory in use. Of the allocated memory 72.51 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:24:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:24:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:24:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 10           |        cudaMalloc retries: 18        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  63159 MiB |  75129 MiB | 100307 GiB | 100246 GiB |
|       from large pool |  63148 MiB |  75118 MiB |  99782 GiB |  99721 GiB |
|       from small pool |     11 MiB |     21 MiB |    525 GiB |    525 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  63159 MiB |  75129 MiB | 100307 GiB | 100246 GiB |
|       from large pool |  63148 MiB |  75118 MiB |  99782 GiB |  99721 GiB |
|       from small pool |     11 MiB |     21 MiB |    525 GiB |    525 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  63147 MiB |  75114 MiB | 100157 GiB | 100095 GiB |
|       from large pool |  63136 MiB |  75103 MiB |  99632 GiB |  99571 GiB |
|       from small pool |     11 MiB |     21 MiB |    524 GiB |    524 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79576 MiB |  79818 MiB | 341218 MiB | 261642 MiB |
|       from large pool |  79554 MiB |  79614 MiB | 339370 MiB | 259816 MiB |
|       from small pool |     22 MiB |    204 MiB |   1848 MiB |   1826 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   8788 MiB |  10848 MiB |  97046 GiB |  97037 GiB |
|       from large pool |   8777 MiB |  10837 MiB |  96452 GiB |  96443 GiB |
|       from small pool |     10 MiB |     21 MiB |    594 GiB |    594 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     515    |     557    |    6276 K  |    6275 K  |
|       from large pool |     230    |     272    |    2822 K  |    2822 K  |
|       from small pool |     285    |     356    |    3453 K  |    3453 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     515    |     557    |    6276 K  |    6275 K  |
|       from large pool |     230    |     272    |    2822 K  |    2822 K  |
|       from small pool |     285    |     356    |    3453 K  |    3453 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      86    |     178    |    2370    |    2284    |
|       from large pool |      75    |      76    |    1446    |    1371    |
|       from small pool |      11    |     102    |     924    |     913    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      97    |     100    |    3519 K  |    3519 K  |
|       from large pool |      76    |      79    |    1813 K  |    1812 K  |
|       from small pool |      21    |      47    |    1706 K  |    1706 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:24:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:24:13] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:24:17]    INFO >> epoch 003:    232 / 1539 loss=2.879, wps=4630.8, ups=6.04, wpb=767.2, bsz=767.2, num_updates=3300, lr=0.000392, gnorm=1.638, clip=0, train_wall=7, gb_free=74.1, wall=582 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:24:25]    INFO >> epoch 003:    282 / 1539 loss=3.195, wps=5151.2, ups=6.85, wpb=751.9, bsz=751.9, num_updates=3350, lr=0.000392, gnorm=1.763, clip=0, train_wall=7, gb_free=70.8, wall=590 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:24:33]    INFO >> epoch 003:    332 / 1539 loss=3.453, wps=4932.6, ups=6.24, wpb=790.4, bsz=790.4, num_updates=3400, lr=0.000392, gnorm=1.912, clip=0, train_wall=8, gb_free=73.9, wall=598 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:24:41]    INFO >> epoch 003:    382 / 1539 loss=3.248, wps=4421.9, ups=6.42, wpb=688.4, bsz=688.4, num_updates=3450, lr=0.000392, gnorm=1.807, clip=0, train_wall=7, gb_free=72.7, wall=605 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:24:49]    INFO >> epoch 003:    432 / 1539 loss=3.251, wps=4443.2, ups=6.75, wpb=657.8, bsz=657.8, num_updates=3500, lr=0.000392, gnorm=1.693, clip=0, train_wall=7, gb_free=66.5, wall=613 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:24:58]    INFO >> epoch 003:    482 / 1539 loss=3.095, wps=4916.8, ups=6.52, wpb=754, bsz=754, num_updates=3550, lr=0.000392, gnorm=1.833, clip=0, train_wall=7, gb_free=73.2, wall=620 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:25:05]    INFO >> epoch 003:    532 / 1539 loss=3.326, wps=5166.8, ups=6.71, wpb=770.5, bsz=770.5, num_updates=3600, lr=0.000392, gnorm=2.21, clip=0, train_wall=7, gb_free=73.9, wall=628 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:25:13]    INFO >> epoch 003:    582 / 1539 loss=3.182, wps=4742.2, ups=6.59, wpb=720.1, bsz=720.1, num_updates=3650, lr=0.000392, gnorm=1.6, clip=0, train_wall=7, gb_free=71.7, wall=636 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:25:20]    INFO >> epoch 003:    632 / 1539 loss=3.315, wps=4839.2, ups=7.17, wpb=674.5, bsz=674.5, num_updates=3700, lr=0.000392, gnorm=1.805, clip=0, train_wall=7, gb_free=66.7, wall=642 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:25:27]    INFO >> epoch 003:    682 / 1539 loss=3.297, wps=5017, ups=6.63, wpb=756.5, bsz=756.5, num_updates=3750, lr=0.000392, gnorm=1.706, clip=0, train_wall=7, gb_free=75.2, wall=650 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:25:37]    INFO >> epoch 003:    732 / 1539 loss=3.02, wps=4911.7, ups=6.06, wpb=810.3, bsz=810.3, num_updates=3800, lr=0.000392, gnorm=1.776, clip=0, train_wall=8, gb_free=74, wall=658 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:25:44]    INFO >> epoch 003:    782 / 1539 loss=2.907, wps=5233.6, ups=6.58, wpb=795, bsz=795, num_updates=3850, lr=0.000392, gnorm=1.572, clip=0, train_wall=7, gb_free=73.8, wall=666 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:25:45] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 35.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 77.28 GiB is allocated by PyTorch, and 1.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:25:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:25:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:25:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 11           |        cudaMalloc retries: 20        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79072 MiB |  79132 MiB | 116837 GiB | 116759 GiB |
|       from large pool |  78937 MiB |  78997 MiB | 116228 GiB | 116151 GiB |
|       from small pool |    135 MiB |    136 MiB |    608 GiB |    608 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79072 MiB |  79132 MiB | 116837 GiB | 116759 GiB |
|       from large pool |  78937 MiB |  78997 MiB | 116228 GiB | 116151 GiB |
|       from small pool |    135 MiB |    136 MiB |    608 GiB |    608 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78999 MiB |  79059 MiB | 116664 GiB | 116587 GiB |
|       from large pool |  78864 MiB |  78924 MiB | 116056 GiB | 115979 GiB |
|       from small pool |    135 MiB |    136 MiB |    607 GiB |    607 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80470 MiB |  80472 MiB | 349842 MiB | 269372 MiB |
|       from large pool |  80330 MiB |  80330 MiB | 347774 MiB | 267444 MiB |
|       from small pool |    140 MiB |    238 MiB |   2068 MiB |   1928 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1337 MiB |   5589 MiB | 116735 GiB | 116734 GiB |
|       from large pool |   1332 MiB |   5580 MiB | 116044 GiB | 116043 GiB |
|       from small pool |      4 MiB |     19 MiB |    690 GiB |    690 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2768    |    2771    |    7313 K  |    7310 K  |
|       from large pool |     486    |     487    |    3311 K  |    3310 K  |
|       from small pool |    2282    |    2285    |    4001 K  |    3999 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2768    |    2771    |    7313 K  |    7310 K  |
|       from large pool |     486    |     487    |    3311 K  |    3310 K  |
|       from small pool |    2282    |    2285    |    4001 K  |    3999 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     267    |     314    |    2606    |    2339    |
|       from large pool |     197    |     197    |    1572    |    1375    |
|       from small pool |      70    |     119    |    1034    |     964    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     145    |     146    |    4075 K  |    4075 K  |
|       from large pool |      77    |      83    |    2103 K  |    2103 K  |
|       from small pool |      68    |      69    |    1972 K  |    1972 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:25:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:25:45] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:25:52]    INFO >> epoch 003:    833 / 1539 loss=3.417, wps=4663.7, ups=6.34, wpb=736, bsz=736, num_updates=3900, lr=0.000392, gnorm=1.868, clip=0, train_wall=7, gb_free=73.2, wall=674 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:26:00]    INFO >> epoch 003:    883 / 1539 loss=3.337, wps=4810.5, ups=6.71, wpb=717, bsz=717, num_updates=3950, lr=0.000392, gnorm=1.833, clip=0, train_wall=7, gb_free=72.7, wall=681 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:26:07]    INFO >> epoch 003:    933 / 1539 loss=3.133, wps=4765.7, ups=6.53, wpb=729.5, bsz=729.5, num_updates=4000, lr=0.000392, gnorm=1.706, clip=0, train_wall=7, gb_free=67.6, wall=689 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:26:16]    INFO >> epoch 003:    983 / 1539 loss=3.313, wps=3987.9, ups=5.83, wpb=684.3, bsz=684.3, num_updates=4050, lr=0.000392, gnorm=1.768, clip=0, train_wall=8, gb_free=71.7, wall=697 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:26:23]    INFO >> epoch 003:   1033 / 1539 loss=3.368, wps=4457.3, ups=7, wpb=636.3, bsz=636.3, num_updates=4100, lr=0.000392, gnorm=1.711, clip=0, train_wall=7, gb_free=68.5, wall=705 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:26:30]    INFO >> epoch 003:   1083 / 1539 loss=3.235, wps=4780.7, ups=7.09, wpb=673.9, bsz=673.9, num_updates=4150, lr=0.000392, gnorm=2.017, clip=0, train_wall=7, gb_free=73.1, wall=712 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:26:40] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 37.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 76.52 GiB is allocated by PyTorch, and 2.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:26:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:26:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:26:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 12           |        cudaMalloc retries: 22        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78293 MiB |  78353 MiB | 126431 GiB | 126355 GiB |
|       from large pool |  77907 MiB |  77967 MiB | 125775 GiB | 125699 GiB |
|       from small pool |    385 MiB |    386 MiB |    656 GiB |    655 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78293 MiB |  78353 MiB | 126431 GiB | 126355 GiB |
|       from large pool |  77907 MiB |  77967 MiB | 125775 GiB | 125699 GiB |
|       from small pool |    385 MiB |    386 MiB |    656 GiB |    655 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78087 MiB |  78147 MiB | 126244 GiB | 126168 GiB |
|       from large pool |  77704 MiB |  77763 MiB | 125589 GiB | 125513 GiB |
|       from small pool |    383 MiB |    384 MiB |    655 GiB |    654 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80468 MiB |  80504 MiB | 366268 MiB | 285800 MiB |
|       from large pool |  80042 MiB |  80270 MiB | 363914 MiB | 283872 MiB |
|       from small pool |    426 MiB |    426 MiB |   2354 MiB |   1928 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2114 MiB |   5523 MiB | 125970 GiB | 125968 GiB |
|       from large pool |   2074 MiB |   5518 MiB | 125225 GiB | 125223 GiB |
|       from small pool |     40 MiB |     41 MiB |    745 GiB |    745 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7212    |    7215    |    7911 K  |    7904 K  |
|       from large pool |     876    |     877    |    3597 K  |    3596 K  |
|       from small pool |    6336    |    6339    |    4314 K  |    4307 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7212    |    7215    |    7911 K  |    7904 K  |
|       from large pool |     876    |     877    |    3597 K  |    3596 K  |
|       from small pool |    6336    |    6339    |    4314 K  |    4307 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     672    |     672    |    3018    |    2346    |
|       from large pool |     459    |     459    |    1841    |    1382    |
|       from small pool |     213    |     213    |    1177    |     964    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     485    |     485    |    4407 K  |    4406 K  |
|       from large pool |     101    |     101    |    2289 K  |    2289 K  |
|       from small pool |     384    |     384    |    2117 K  |    2117 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:26:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:26:40] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:26:41]    INFO >> epoch 003:   1134 / 1539 loss=3.187, wps=4366, ups=6.38, wpb=684.8, bsz=684.8, num_updates=4200, lr=0.000392, gnorm=1.807, clip=0, train_wall=7, gb_free=73.2, wall=719 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:26:48]    INFO >> epoch 003:   1184 / 1539 loss=3.212, wps=4585.9, ups=6.81, wpb=673.6, bsz=673.6, num_updates=4250, lr=0.000392, gnorm=1.707, clip=0, train_wall=7, gb_free=73.6, wall=727 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:26:56]    INFO >> epoch 003:   1234 / 1539 loss=3.356, wps=4749.6, ups=6.61, wpb=718.1, bsz=718.1, num_updates=4300, lr=0.000392, gnorm=2.167, clip=0, train_wall=7, gb_free=70.7, wall=734 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:27:03]    INFO >> epoch 003:   1284 / 1539 loss=3.075, wps=4421.2, ups=7.13, wpb=619.7, bsz=619.7, num_updates=4350, lr=0.000392, gnorm=1.682, clip=0, train_wall=7, gb_free=73.7, wall=741 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:27:11]    INFO >> epoch 003:   1334 / 1539 loss=3.202, wps=4986.8, ups=7.04, wpb=708, bsz=708, num_updates=4400, lr=0.000392, gnorm=2.092, clip=0, train_wall=7, gb_free=72.3, wall=748 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:27:18]    INFO >> epoch 003:   1384 / 1539 loss=3.219, wps=4985.9, ups=7.2, wpb=692.7, bsz=692.7, num_updates=4450, lr=0.000392, gnorm=1.674, clip=0, train_wall=7, gb_free=74.4, wall=755 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:27:25]    INFO >> epoch 003:   1434 / 1539 loss=3.298, wps=4702.2, ups=7.22, wpb=651.7, bsz=651.7, num_updates=4500, lr=0.000392, gnorm=1.803, clip=0, train_wall=7, gb_free=72.3, wall=762 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:27:32]    INFO >> epoch 003:   1484 / 1539 loss=3.172, wps=4382.1, ups=6.65, wpb=658.9, bsz=658.9, num_updates=4550, lr=0.000392, gnorm=1.783, clip=0, train_wall=7, gb_free=72.6, wall=770 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:27:40]    INFO >> epoch 003:   1534 / 1539 loss=3.407, wps=4675.8, ups=7.05, wpb=662.9, bsz=662.9, num_updates=4600, lr=0.000392, gnorm=1.927, clip=0, train_wall=7, gb_free=72.2, wall=777 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:27:40]    INFO >> epoch 003 | loss 3.235 | wps 4482.5 | ups 6.29 | wpb 712.7 | bsz 712.7 | num_updates 4605 | lr 0.000392 | gnorm 1.807 | clip 0 | train_wall 214 | gb_free 74.5 | wall 778 (progress_bar.py:267, print())[0m
[33m[2025-11-21 14:27:40] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:27:55]    INFO >> epoch 003 | valid on 'valid' subset | loss 3.479 | wps 11260.7 | wpb 5412.5 | bsz 5412.5 | num_updates 4605 | best_loss 3.481 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 14:27:55]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 14:27:55]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_last.pt (epoch 3 @ 4605 updates, score 3.479) (writing took 0.013638 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 14:27:55] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:28:02]    INFO >> epoch 004:     45 / 1539 loss=3.161, wps=1763.5, ups=2.35, wpb=751.7, bsz=751.7, num_updates=4650, lr=0.000376, gnorm=1.937, clip=0, train_wall=7, gb_free=72.9, wall=798 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:28:10]    INFO >> epoch 004:     95 / 1539 loss=3.206, wps=4974.4, ups=6.58, wpb=756, bsz=756, num_updates=4700, lr=0.000376, gnorm=1.753, clip=0, train_wall=7, gb_free=68.1, wall=806 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:28:21]    INFO >> epoch 004:    145 / 1539 loss=2.93, wps=5310.1, ups=5.06, wpb=1048.6, bsz=1048.6, num_updates=4750, lr=0.000376, gnorm=1.962, clip=0, train_wall=9, gb_free=75.2, wall=816 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:28:28]    INFO >> epoch 004:    195 / 1539 loss=3.074, wps=5001.7, ups=7.13, wpb=701.3, bsz=701.3, num_updates=4800, lr=0.000376, gnorm=1.723, clip=0, train_wall=7, gb_free=72, wall=823 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:28:36]    INFO >> epoch 004:    245 / 1539 loss=3.353, wps=4946.7, ups=6.65, wpb=743.6, bsz=743.6, num_updates=4850, lr=0.000376, gnorm=1.996, clip=0, train_wall=7, gb_free=73.7, wall=830 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:28:43]    INFO >> epoch 004:    295 / 1539 loss=3.24, wps=4644.4, ups=7.17, wpb=647.4, bsz=647.4, num_updates=4900, lr=0.000376, gnorm=1.761, clip=0, train_wall=7, gb_free=74.8, wall=837 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:28:50]    INFO >> epoch 004:    345 / 1539 loss=3.12, wps=4788.4, ups=6.97, wpb=687.4, bsz=687.4, num_updates=4950, lr=0.000376, gnorm=1.669, clip=0, train_wall=7, gb_free=70.1, wall=844 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:28:57]    INFO >> epoch 004:    395 / 1539 loss=3.122, wps=5141.1, ups=6.75, wpb=761.1, bsz=761.1, num_updates=5000, lr=0.000376, gnorm=1.728, clip=0, train_wall=7, gb_free=68.1, wall=852 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:29:05]    INFO >> epoch 004:    445 / 1539 loss=3.428, wps=3952.6, ups=6.27, wpb=630.5, bsz=630.5, num_updates=5050, lr=0.000376, gnorm=1.83, clip=0, train_wall=7, gb_free=71.6, wall=860 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:29:12]    INFO >> epoch 004:    495 / 1539 loss=3.13, wps=4923.1, ups=6.84, wpb=720.1, bsz=720.1, num_updates=5100, lr=0.000376, gnorm=1.638, clip=0, train_wall=7, gb_free=74.1, wall=867 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:29:20]    INFO >> epoch 004:    545 / 1539 loss=3.281, wps=4674.9, ups=6.66, wpb=701.6, bsz=701.6, num_updates=5150, lr=0.000376, gnorm=1.892, clip=0, train_wall=7, gb_free=73.6, wall=875 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:29:27]    INFO >> epoch 004:    595 / 1539 loss=3.194, wps=4168.3, ups=6.62, wpb=629.8, bsz=629.8, num_updates=5200, lr=0.000376, gnorm=1.815, clip=0, train_wall=7, gb_free=71.2, wall=882 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:29:29] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 25.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.75 GiB is allocated by PyTorch, and 1.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:29:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:29:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:29:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 13           |        cudaMalloc retries: 24        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78527 MiB |  78587 MiB | 158536 GiB | 158459 GiB |
|       from large pool |  78397 MiB |  78457 MiB | 157709 GiB | 157632 GiB |
|       from small pool |    130 MiB |    131 MiB |    827 GiB |    827 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78527 MiB |  78587 MiB | 158536 GiB | 158459 GiB |
|       from large pool |  78397 MiB |  78457 MiB | 157709 GiB | 157632 GiB |
|       from small pool |    130 MiB |    131 MiB |    827 GiB |    827 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78458 MiB |  78518 MiB | 158303 GiB | 158227 GiB |
|       from large pool |  78329 MiB |  78388 MiB | 157477 GiB | 157401 GiB |
|       from small pool |    129 MiB |    130 MiB |    826 GiB |    826 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80480 MiB |  80482 MiB | 386756 MiB | 306276 MiB |
|       from large pool |  80346 MiB |  80346 MiB | 384288 MiB | 303942 MiB |
|       from small pool |    134 MiB |    426 MiB |   2468 MiB |   2334 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1892 MiB |   9014 MiB | 155328 GiB | 155326 GiB |
|       from large pool |   1888 MiB |   9007 MiB | 154391 GiB | 154389 GiB |
|       from small pool |      3 MiB |     21 MiB |    937 GiB |    937 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2669    |    2672    |    9920 K  |    9917 K  |
|       from large pool |     477    |     478    |    4476 K  |    4476 K  |
|       from small pool |    2192    |    2195    |    5443 K  |    5441 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2669    |    2672    |    9920 K  |    9917 K  |
|       from large pool |     477    |     478    |    4476 K  |    4476 K  |
|       from small pool |    2192    |    2195    |    5443 K  |    5441 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     316    |     671    |    3196    |    2880    |
|       from large pool |     249    |     458    |    1962    |    1713    |
|       from small pool |      67    |     213    |    1234    |    1167    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     191    |     192    |    5544 K  |    5544 K  |
|       from large pool |     125    |     125    |    2861 K  |    2861 K  |
|       from small pool |      66    |      67    |    2682 K  |    2682 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:29:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:29:29] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:29:36]    INFO >> epoch 004:    646 / 1539 loss=3.25, wps=4037.4, ups=6.02, wpb=671.1, bsz=671.1, num_updates=5250, lr=0.000376, gnorm=1.703, clip=0, train_wall=7, gb_free=69, wall=890 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:29:44] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 938.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 41.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 77.37 GiB is allocated by PyTorch, and 1.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:29:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:29:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:29:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 14           |        cudaMalloc retries: 26        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73478 MiB |  79417 MiB | 160997 GiB | 160925 GiB |
|       from large pool |  73469 MiB |  79408 MiB | 160159 GiB | 160087 GiB |
|       from small pool |      8 MiB |     24 MiB |    838 GiB |    838 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73478 MiB |  79417 MiB | 160997 GiB | 160925 GiB |
|       from large pool |  73469 MiB |  79408 MiB | 160159 GiB | 160087 GiB |
|       from small pool |      8 MiB |     24 MiB |    838 GiB |    838 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73471 MiB |  79407 MiB | 160761 GiB | 160689 GiB |
|       from large pool |  73462 MiB |  79398 MiB | 159924 GiB | 159852 GiB |
|       from small pool |      8 MiB |     24 MiB |    837 GiB |    837 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80464 MiB |  80464 MiB | 442508 MiB | 362044 MiB |
|       from large pool |  80440 MiB |  80440 MiB | 440040 MiB | 359600 MiB |
|       from small pool |     24 MiB |    134 MiB |   2468 MiB |   2444 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3505 MiB |   5366 MiB | 157616 GiB | 157613 GiB |
|       from large pool |   3490 MiB |   5351 MiB | 156666 GiB | 156663 GiB |
|       from small pool |     15 MiB |     23 MiB |    949 GiB |    949 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     571    |     589    |   10069 K  |   10068 K  |
|       from large pool |     286    |     304    |    4552 K  |    4551 K  |
|       from small pool |     285    |     356    |    5517 K  |    5516 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     571    |     589    |   10069 K  |   10068 K  |
|       from large pool |     286    |     304    |    4552 K  |    4551 K  |
|       from small pool |     285    |     356    |    5517 K  |    5516 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     131    |     315    |    3254    |    3123    |
|       from large pool |     119    |     248    |    2020    |    1901    |
|       from small pool |      12    |      67    |    1234    |    1222    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     139    |     139    |    5625 K  |    5625 K  |
|       from large pool |     115    |     115    |    2910 K  |    2910 K  |
|       from small pool |      24    |      48    |    2714 K  |    2714 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:29:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:29:44] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:29:45]    INFO >> epoch 004:    697 / 1539 loss=3.29, wps=3481.9, ups=5.47, wpb=636.6, bsz=636.6, num_updates=5300, lr=0.000376, gnorm=1.785, clip=0, train_wall=7, gb_free=68, wall=900 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:29:53]    INFO >> epoch 004:    747 / 1539 loss=3.111, wps=4005.9, ups=5.95, wpb=673.1, bsz=673.1, num_updates=5350, lr=0.000376, gnorm=1.529, clip=0, train_wall=8, gb_free=73.1, wall=908 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:30:01]    INFO >> epoch 004:    797 / 1539 loss=3.186, wps=4788.3, ups=6.74, wpb=710.1, bsz=710.1, num_updates=5400, lr=0.000376, gnorm=1.577, clip=0, train_wall=7, gb_free=72.4, wall=915 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:30:08]    INFO >> epoch 004:    847 / 1539 loss=3.256, wps=5061.2, ups=6.77, wpb=747.5, bsz=747.5, num_updates=5450, lr=0.000376, gnorm=1.644, clip=0, train_wall=7, gb_free=63.9, wall=923 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:30:16]    INFO >> epoch 004:    897 / 1539 loss=3.229, wps=4956, ups=6.37, wpb=778.3, bsz=778.3, num_updates=5500, lr=0.000376, gnorm=1.904, clip=0, train_wall=7, gb_free=70.4, wall=931 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:30:23]    INFO >> epoch 004:    947 / 1539 loss=3.117, wps=4587.6, ups=7.09, wpb=646.7, bsz=646.7, num_updates=5550, lr=0.000376, gnorm=1.636, clip=0, train_wall=7, gb_free=67.8, wall=938 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:30:36]    INFO >> epoch 004:    997 / 1539 loss=3.494, wps=4805.6, ups=6.28, wpb=764.9, bsz=764.9, num_updates=5600, lr=0.000376, gnorm=1.803, clip=0, train_wall=8, gb_free=75.6, wall=946 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:30:44]    INFO >> epoch 004:   1047 / 1539 loss=3.374, wps=4361.9, ups=6.94, wpb=628.6, bsz=628.6, num_updates=5650, lr=0.000376, gnorm=1.863, clip=0, train_wall=7, gb_free=72.6, wall=953 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:30:51]    INFO >> epoch 004:   1097 / 1539 loss=3.115, wps=4948.4, ups=6.78, wpb=729.6, bsz=729.6, num_updates=5700, lr=0.000376, gnorm=1.789, clip=0, train_wall=7, gb_free=71.3, wall=960 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:30:58]    INFO >> epoch 004:   1147 / 1539 loss=3.307, wps=4638.4, ups=6.86, wpb=676.4, bsz=676.4, num_updates=5750, lr=0.000376, gnorm=1.675, clip=0, train_wall=7, gb_free=57, wall=968 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:31:07]    INFO >> epoch 004:   1197 / 1539 loss=3.323, wps=4938.6, ups=6.54, wpb=754.7, bsz=754.7, num_updates=5800, lr=0.000376, gnorm=1.719, clip=0, train_wall=7, gb_free=75.7, wall=975 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:31:14]    INFO >> epoch 004:   1247 / 1539 loss=3.163, wps=4501.7, ups=7.17, wpb=627.8, bsz=627.8, num_updates=5850, lr=0.000376, gnorm=1.532, clip=0, train_wall=7, gb_free=75.7, wall=982 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:31:22]    INFO >> epoch 004:   1297 / 1539 loss=3.244, wps=4174.1, ups=6.48, wpb=644.6, bsz=644.6, num_updates=5900, lr=0.000376, gnorm=1.777, clip=0, train_wall=7, gb_free=74.7, wall=990 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:31:30]    INFO >> epoch 004:   1347 / 1539 loss=3.18, wps=5164, ups=6.45, wpb=801.1, bsz=801.1, num_updates=5950, lr=0.000376, gnorm=1.908, clip=0, train_wall=7, gb_free=67.1, wall=998 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:31:37] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.88 GiB. GPU 2 has a total capacity of 79.14 GiB of which 711.25 MiB is free. Including non-PyTorch memory, this process has 78.42 GiB memory in use. Of the allocated memory 72.51 GiB is allocated by PyTorch, and 5.41 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:31:37] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:31:37] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:31:37] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 15           |        cudaMalloc retries: 28        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  63159 MiB |  75129 MiB | 180473 GiB | 180411 GiB |
|       from large pool |  63148 MiB |  75118 MiB | 179542 GiB | 179480 GiB |
|       from small pool |     11 MiB |     13 MiB |    931 GiB |    931 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  63159 MiB |  75129 MiB | 180473 GiB | 180411 GiB |
|       from large pool |  63148 MiB |  75118 MiB | 179542 GiB | 179480 GiB |
|       from small pool |     11 MiB |     13 MiB |    931 GiB |    931 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  63147 MiB |  75114 MiB | 180209 GiB | 180147 GiB |
|       from large pool |  63136 MiB |  75103 MiB | 179279 GiB | 179217 GiB |
|       from small pool |     11 MiB |     13 MiB |    930 GiB |    930 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79794 MiB |  79794 MiB | 456346 MiB | 376552 MiB |
|       from large pool |  79772 MiB |  79772 MiB | 453664 MiB | 373892 MiB |
|       from small pool |     22 MiB |    238 MiB |   2682 MiB |   2660 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   8518 MiB |  11894 MiB | 178616 GiB | 178607 GiB |
|       from large pool |   8507 MiB |  11883 MiB | 177559 GiB | 177551 GiB |
|       from small pool |     10 MiB |     17 MiB |   1056 GiB |   1056 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     515    |     557    |   11266 K  |   11265 K  |
|       from large pool |     230    |     272    |    5136 K  |    5136 K  |
|       from small pool |     285    |     336    |    6130 K  |    6129 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     515    |     557    |   11266 K  |   11265 K  |
|       from large pool |     230    |     272    |    5136 K  |    5136 K  |
|       from small pool |     285    |     336    |    6130 K  |    6129 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      99    |     235    |    3373    |    3274    |
|       from large pool |      88    |     116    |    2032    |    1944    |
|       from small pool |      11    |     119    |    1341    |    1330    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     106    |     107    |    6265 K  |    6265 K  |
|       from large pool |      81    |      82    |    3270 K  |    3270 K  |
|       from small pool |      25    |      38    |    2995 K  |    2995 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:31:37] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:31:37] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:31:40]    INFO >> epoch 004:   1398 / 1539 loss=3.219, wps=4583.6, ups=5.79, wpb=791.5, bsz=791.5, num_updates=6000, lr=0.000376, gnorm=1.912, clip=0, train_wall=7, gb_free=71.8, wall=1006 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:31:47]    INFO >> epoch 004:   1448 / 1539 loss=3.351, wps=4999.8, ups=7.02, wpb=711.7, bsz=711.7, num_updates=6050, lr=0.000376, gnorm=1.803, clip=0, train_wall=7, gb_free=73.7, wall=1013 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:31:54]    INFO >> epoch 004:   1498 / 1539 loss=3.386, wps=4667.3, ups=6.84, wpb=682, bsz=682, num_updates=6100, lr=0.000376, gnorm=1.961, clip=0, train_wall=7, gb_free=74.3, wall=1021 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:31:56] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 17.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 76.99 GiB is allocated by PyTorch, and 1.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:31:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:31:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:31:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 16           |        cudaMalloc retries: 29        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78779 MiB |  78839 MiB | 183746 GiB | 183669 GiB |
|       from large pool |  78387 MiB |  78447 MiB | 182796 GiB | 182719 GiB |
|       from small pool |    391 MiB |    392 MiB |    950 GiB |    950 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78779 MiB |  78839 MiB | 183746 GiB | 183669 GiB |
|       from large pool |  78387 MiB |  78447 MiB | 182796 GiB | 182719 GiB |
|       from small pool |    391 MiB |    392 MiB |    950 GiB |    950 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78688 MiB |  78747 MiB | 183478 GiB | 183401 GiB |
|       from large pool |  78298 MiB |  78358 MiB | 182529 GiB | 182452 GiB |
|       from small pool |    389 MiB |    390 MiB |    949 GiB |    948 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80488 MiB |  80490 MiB | 465158 MiB | 384670 MiB |
|       from large pool |  80056 MiB |  80056 MiB | 462064 MiB | 382008 MiB |
|       from small pool |    432 MiB |    434 MiB |   3094 MiB |   2662 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1648 MiB |   5018 MiB | 182378 GiB | 182376 GiB |
|       from large pool |   1608 MiB |   5012 MiB | 181299 GiB | 181298 GiB |
|       from small pool |     40 MiB |     41 MiB |   1078 GiB |   1078 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7322    |    7325    |   11491 K  |   11483 K  |
|       from large pool |     886    |     887    |    5235 K  |    5234 K  |
|       from small pool |    6436    |    6439    |    6255 K  |    6249 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7322    |    7325    |   11491 K  |   11483 K  |
|       from large pool |     886    |     887    |    5235 K  |    5234 K  |
|       from small pool |    6436    |    6439    |    6255 K  |    6249 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     436    |     437    |    3719    |    3283    |
|       from large pool |     220    |     220    |    2172    |    1952    |
|       from small pool |     216    |     217    |    1547    |    1331    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     466    |     467    |    6386 K  |    6385 K  |
|       from large pool |      76    |      76    |    3329 K  |    3329 K  |
|       from small pool |     390    |     391    |    3056 K  |    3056 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:31:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:31:56] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:32:00]    INFO >> epoch 004 | loss 3.227 | wps 4390.1 | ups 6.16 | wpb 712.7 | bsz 712.7 | num_updates 6140 | lr 0.000376 | gnorm 1.77 | clip 0 | train_wall 217 | gb_free 70.3 | wall 1027 (progress_bar.py:267, print())[0m
[33m[2025-11-21 14:32:00] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:32:15]    INFO >> epoch 004 | valid on 'valid' subset | loss 3.486 | wps 10873.6 | wpb 5412.5 | bsz 5412.5 | num_updates 6140 | best_loss 3.486 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 14:32:16]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 14:32:16]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_best.pt (epoch 4 @ 6140 updates, score 3.486) (writing took 0.018010 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 14:32:16] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:32:17]    INFO >> epoch 005:     10 / 1539 loss=3.447, wps=1474.5, ups=2.29, wpb=643.2, bsz=643.2, num_updates=6150, lr=0.000354, gnorm=1.573, clip=0, train_wall=7, gb_free=72.3, wall=1043 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:32:25]    INFO >> epoch 005:     60 / 1539 loss=3.174, wps=4465, ups=6.34, wpb=704.5, bsz=704.5, num_updates=6200, lr=0.000354, gnorm=1.6, clip=0, train_wall=7, gb_free=73.5, wall=1050 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:32:33]    INFO >> epoch 005:    110 / 1539 loss=3.241, wps=4716.4, ups=6.69, wpb=705.4, bsz=705.4, num_updates=6250, lr=0.000354, gnorm=1.723, clip=0, train_wall=7, gb_free=71.7, wall=1058 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:32:40]    INFO >> epoch 005:    160 / 1539 loss=3.215, wps=4478.2, ups=7, wpb=639.6, bsz=639.6, num_updates=6300, lr=0.000354, gnorm=1.674, clip=0, train_wall=7, gb_free=74.1, wall=1065 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:32:48]    INFO >> epoch 005:    210 / 1539 loss=3.124, wps=4561.2, ups=6.79, wpb=671.6, bsz=671.6, num_updates=6350, lr=0.000354, gnorm=1.6, clip=0, train_wall=7, gb_free=71.2, wall=1072 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:32:57]    INFO >> epoch 005:    260 / 1539 loss=3.358, wps=5419.6, ups=6.05, wpb=895.2, bsz=895.2, num_updates=6400, lr=0.000354, gnorm=1.79, clip=0, train_wall=8, gb_free=68.1, wall=1081 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:33:04]    INFO >> epoch 005:    310 / 1539 loss=3.149, wps=4808.5, ups=6.82, wpb=704.8, bsz=704.8, num_updates=6450, lr=0.000354, gnorm=1.685, clip=0, train_wall=7, gb_free=72, wall=1088 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:33:12]    INFO >> epoch 005:    360 / 1539 loss=2.798, wps=5052.1, ups=6.54, wpb=772.8, bsz=772.8, num_updates=6500, lr=0.000354, gnorm=1.684, clip=0, train_wall=7, gb_free=69.7, wall=1096 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:33:18] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 27.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.15 GiB is allocated by PyTorch, and 2.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:33:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:33:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:33:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 17           |        cudaMalloc retries: 30        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77914 MiB |  77974 MiB | 199835 GiB | 199759 GiB |
|       from large pool |  77531 MiB |  77591 MiB | 198785 GiB | 198709 GiB |
|       from small pool |    383 MiB |    384 MiB |   1050 GiB |   1050 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77914 MiB |  77974 MiB | 199835 GiB | 199759 GiB |
|       from large pool |  77531 MiB |  77591 MiB | 198785 GiB | 198709 GiB |
|       from small pool |    383 MiB |    384 MiB |   1050 GiB |   1050 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77847 MiB |  77907 MiB | 199543 GiB | 199467 GiB |
|       from large pool |  77466 MiB |  77526 MiB | 198494 GiB | 198418 GiB |
|       from small pool |    380 MiB |    382 MiB |   1048 GiB |   1048 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80478 MiB |  80488 MiB | 465218 MiB | 384740 MiB |
|       from large pool |  80056 MiB |  80056 MiB | 462124 MiB | 382068 MiB |
|       from small pool |    422 MiB |    432 MiB |   3094 MiB |   2672 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2503 MiB |   6459 MiB | 195877 GiB | 195875 GiB |
|       from large pool |   2464 MiB |   6453 MiB | 194688 GiB | 194686 GiB |
|       from small pool |     38 MiB |     40 MiB |   1189 GiB |   1189 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7168    |    7171    |   12551 K  |   12543 K  |
|       from large pool |     872    |     873    |    5636 K  |    5636 K  |
|       from small pool |    6296    |    6299    |    6914 K  |    6907 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7168    |    7171    |   12551 K  |   12543 K  |
|       from large pool |     872    |     873    |    5636 K  |    5636 K  |
|       from small pool |    6296    |    6299    |    6914 K  |    6907 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     431    |     436    |    3720    |    3289    |
|       from large pool |     220    |     220    |    2173    |    1953    |
|       from small pool |     211    |     216    |    1547    |    1336    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     493    |     494    |    7015 K  |    7015 K  |
|       from large pool |     111    |     111    |    3594 K  |    3594 K  |
|       from small pool |     382    |     383    |    3421 K  |    3421 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:33:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:33:18] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:33:19]    INFO >> epoch 005:    411 / 1539 loss=3.297, wps=4546.5, ups=6.67, wpb=681.6, bsz=681.6, num_updates=6550, lr=0.000354, gnorm=1.739, clip=0, train_wall=7, gb_free=74, wall=1103 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:33:28]    INFO >> epoch 005:    461 / 1539 loss=3.283, wps=4206.3, ups=6.92, wpb=608, bsz=608, num_updates=6600, lr=0.000354, gnorm=1.7, clip=0, train_wall=7, gb_free=74.8, wall=1110 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:33:40]    INFO >> epoch 005:    511 / 1539 loss=3.1, wps=3054.2, ups=4.18, wpb=730.5, bsz=730.5, num_updates=6650, lr=0.000354, gnorm=1.599, clip=0, train_wall=12, gb_free=70.1, wall=1122 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:33:47]    INFO >> epoch 005:    561 / 1539 loss=3.375, wps=4156, ups=6.7, wpb=620.7, bsz=620.7, num_updates=6700, lr=0.000354, gnorm=1.673, clip=0, train_wall=7, gb_free=72.7, wall=1130 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:33:52] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 17.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 76.55 GiB is allocated by PyTorch, and 2.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:33:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:33:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:33:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 18           |        cudaMalloc retries: 32        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78329 MiB |  78389 MiB | 204932 GiB | 204855 GiB |
|       from large pool |  78200 MiB |  78260 MiB | 203857 GiB | 203781 GiB |
|       from small pool |    128 MiB |    129 MiB |   1074 GiB |   1074 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78329 MiB |  78389 MiB | 204932 GiB | 204855 GiB |
|       from large pool |  78200 MiB |  78260 MiB | 203857 GiB | 203781 GiB |
|       from small pool |    128 MiB |    129 MiB |   1074 GiB |   1074 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78278 MiB |  78338 MiB | 204632 GiB | 204556 GiB |
|       from large pool |  78150 MiB |  78210 MiB | 203559 GiB | 203483 GiB |
|       from small pool |    127 MiB |    129 MiB |   1072 GiB |   1072 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80488 MiB |  80490 MiB | 465582 MiB | 385094 MiB |
|       from large pool |  80356 MiB |  80356 MiB | 462484 MiB | 382128 MiB |
|       from small pool |    132 MiB |    422 MiB |   3098 MiB |   2966 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2098 MiB |   8462 MiB | 200686 GiB | 200684 GiB |
|       from large pool |   2095 MiB |   8453 MiB | 199469 GiB | 199467 GiB |
|       from small pool |      3 MiB |     19 MiB |   1216 GiB |   1216 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2636    |    2639    |   12862 K  |   12859 K  |
|       from large pool |     474    |     475    |    5790 K  |    5790 K  |
|       from small pool |    2162    |    2165    |    7071 K  |    7069 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2636    |    2639    |   12862 K  |   12859 K  |
|       from large pool |     474    |     475    |    5790 K  |    5790 K  |
|       from small pool |    2162    |    2165    |    7071 K  |    7069 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     291    |     431    |    3728    |    3437    |
|       from large pool |     225    |     225    |    2179    |    1954    |
|       from small pool |      66    |     211    |    1549    |    1483    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     204    |     205    |    7188 K  |    7188 K  |
|       from large pool |     140    |     141    |    3694 K  |    3694 K  |
|       from small pool |      64    |      65    |    3493 K  |    3493 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:33:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:33:52] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:33:57]    INFO >> epoch 005:    612 / 1539 loss=3.321, wps=4505.6, ups=5.96, wpb=756, bsz=756, num_updates=6750, lr=0.000354, gnorm=1.707, clip=0, train_wall=7, gb_free=70.3, wall=1138 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:34:04]    INFO >> epoch 005:    662 / 1539 loss=3.341, wps=4941.8, ups=6.71, wpb=736.8, bsz=736.8, num_updates=6800, lr=0.000354, gnorm=1.6, clip=0, train_wall=7, gb_free=74.3, wall=1146 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:34:12]    INFO >> epoch 005:    712 / 1539 loss=3.36, wps=4667.9, ups=6.4, wpb=729, bsz=729, num_updates=6850, lr=0.000354, gnorm=2.002, clip=0, train_wall=7, gb_free=71.6, wall=1153 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:34:20]    INFO >> epoch 005:    762 / 1539 loss=3.25, wps=4888.6, ups=6.82, wpb=717.1, bsz=717.1, num_updates=6900, lr=0.000354, gnorm=1.747, clip=0, train_wall=7, gb_free=73.1, wall=1161 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:34:29]    INFO >> epoch 005:    812 / 1539 loss=3.294, wps=4158, ups=6.17, wpb=673.7, bsz=673.7, num_updates=6950, lr=0.000354, gnorm=1.945, clip=0, train_wall=8, gb_free=69.9, wall=1169 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:34:36]    INFO >> epoch 005:    862 / 1539 loss=3.024, wps=4574.7, ups=6.82, wpb=670.9, bsz=670.9, num_updates=7000, lr=0.000354, gnorm=1.588, clip=0, train_wall=7, gb_free=66.7, wall=1176 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:34:44]    INFO >> epoch 005:    912 / 1539 loss=3.222, wps=4405.4, ups=6.6, wpb=667, bsz=667, num_updates=7050, lr=0.000354, gnorm=1.77, clip=0, train_wall=7, gb_free=71.9, wall=1184 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:34:50] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.79 GiB is free. Including non-PyTorch memory, this process has 77.33 GiB memory in use. Of the allocated memory 71.77 GiB is allocated by PyTorch, and 5.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:34:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:34:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:34:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 19           |        cudaMalloc retries: 34        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73095 MiB |  73493 MiB | 214939 GiB | 214868 GiB |
|       from large pool |  73086 MiB |  73484 MiB | 213819 GiB | 213747 GiB |
|       from small pool |      8 MiB |     24 MiB |   1120 GiB |   1120 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73095 MiB |  73493 MiB | 214939 GiB | 214868 GiB |
|       from large pool |  73086 MiB |  73484 MiB | 213819 GiB | 213747 GiB |
|       from small pool |      8 MiB |     24 MiB |   1120 GiB |   1120 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73086 MiB |  73484 MiB | 214624 GiB | 214553 GiB |
|       from large pool |  73078 MiB |  73475 MiB | 213506 GiB | 213434 GiB |
|       from small pool |      8 MiB |     24 MiB |   1118 GiB |   1118 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78676 MiB |  80428 MiB | 469248 MiB | 390572 MiB |
|       from large pool |  78652 MiB |  80296 MiB | 466150 MiB | 387498 MiB |
|       from small pool |     24 MiB |    132 MiB |   3098 MiB |   3074 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5580 MiB |   7715 MiB | 210005 GiB | 210000 GiB |
|       from large pool |   5565 MiB |   7700 MiB | 208736 GiB | 208730 GiB |
|       from small pool |     15 MiB |     21 MiB |   1269 GiB |   1269 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     571    |     577    |   13466 K  |   13465 K  |
|       from large pool |     286    |     292    |    6090 K  |    6090 K  |
|       from small pool |     285    |     356    |    7375 K  |    7375 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     571    |     577    |   13466 K  |   13465 K  |
|       from large pool |     286    |     292    |    6090 K  |    6090 K  |
|       from small pool |     285    |     356    |    7375 K  |    7375 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     153    |     290    |    3730    |    3577    |
|       from large pool |     141    |     224    |    2181    |    2040    |
|       from small pool |      12    |      66    |    1549    |    1537    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     158    |     158    |    7520 K  |    7520 K  |
|       from large pool |     136    |     136    |    3889 K  |    3889 K  |
|       from small pool |      22    |      46    |    3630 K  |    3630 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:34:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:34:50] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:34:52]    INFO >> epoch 005:    963 / 1539 loss=3.182, wps=4170, ups=6.16, wpb=676.6, bsz=676.6, num_updates=7100, lr=0.000354, gnorm=1.806, clip=0, train_wall=7, gb_free=70.6, wall=1192 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:35:02]    INFO >> epoch 005:   1013 / 1539 loss=2.993, wps=5459.5, ups=5.72, wpb=954.2, bsz=954.2, num_updates=7150, lr=0.000354, gnorm=1.782, clip=0, train_wall=8, gb_free=70.5, wall=1201 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:35:10]    INFO >> epoch 005:   1063 / 1539 loss=3.007, wps=4942, ups=6.25, wpb=791.1, bsz=791.1, num_updates=7200, lr=0.000354, gnorm=1.713, clip=0, train_wall=8, gb_free=65, wall=1209 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:35:17]    INFO >> epoch 005:   1113 / 1539 loss=3.317, wps=4353.2, ups=6.95, wpb=626.1, bsz=626.1, num_updates=7250, lr=0.000354, gnorm=1.677, clip=0, train_wall=7, gb_free=74.3, wall=1216 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:35:19] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.09 GiB is free. Including non-PyTorch memory, this process has 78.03 GiB memory in use. Of the allocated memory 71.51 GiB is allocated by PyTorch, and 6.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:35:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:35:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:35:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 20           |        cudaMalloc retries: 36        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72703 MiB |  73225 MiB | 220144 GiB | 220073 GiB |
|       from large pool |  72692 MiB |  73214 MiB | 218996 GiB | 218925 GiB |
|       from small pool |     11 MiB |     21 MiB |   1147 GiB |   1147 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72703 MiB |  73225 MiB | 220144 GiB | 220073 GiB |
|       from large pool |  72692 MiB |  73214 MiB | 218996 GiB | 218925 GiB |
|       from small pool |     11 MiB |     21 MiB |   1147 GiB |   1147 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72691 MiB |  73213 MiB | 219821 GiB | 219750 GiB |
|       from large pool |  72680 MiB |  73202 MiB | 218675 GiB | 218604 GiB |
|       from small pool |     11 MiB |     21 MiB |   1145 GiB |   1145 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79388 MiB |  80316 MiB | 473754 MiB | 394366 MiB |
|       from large pool |  79364 MiB |  80098 MiB | 470462 MiB | 391098 MiB |
|       from small pool |     24 MiB |    218 MiB |   3292 MiB |   3268 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6684 MiB |  10454 MiB | 215209 GiB | 215202 GiB |
|       from large pool |   6671 MiB |  10441 MiB | 213908 GiB | 213901 GiB |
|       from small pool |     12 MiB |     21 MiB |   1300 GiB |   1300 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     544    |     551    |   13790 K  |   13789 K  |
|       from large pool |     259    |     266    |    6239 K  |    6239 K  |
|       from small pool |     285    |     356    |    7551 K  |    7550 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     544    |     551    |   13790 K  |   13789 K  |
|       from large pool |     259    |     266    |    6239 K  |    6239 K  |
|       from small pool |     285    |     356    |    7551 K  |    7550 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      99    |     253    |    3833    |    3734    |
|       from large pool |      87    |     144    |    2187    |    2100    |
|       from small pool |      12    |     109    |    1646    |    1634    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     103    |     103    |    7700 K  |    7700 K  |
|       from large pool |      79    |      79    |    3983 K  |    3983 K  |
|       from small pool |      24    |      46    |    3716 K  |    3716 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:35:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:35:19] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:35:26]    INFO >> epoch 005:   1164 / 1539 loss=3.313, wps=4581.8, ups=5.87, wpb=780.4, bsz=780.4, num_updates=7300, lr=0.000354, gnorm=1.917, clip=0, train_wall=7, gb_free=69.8, wall=1224 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:35:33]    INFO >> epoch 005:   1214 / 1539 loss=3.223, wps=4351.3, ups=6.62, wpb=657.7, bsz=657.7, num_updates=7350, lr=0.000354, gnorm=1.907, clip=0, train_wall=7, gb_free=68.9, wall=1232 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:35:41]    INFO >> epoch 005:   1264 / 1539 loss=3.22, wps=4681.6, ups=7.37, wpb=635.1, bsz=635.1, num_updates=7400, lr=0.000354, gnorm=1.626, clip=0, train_wall=6, gb_free=73.6, wall=1239 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:35:49]    INFO >> epoch 005:   1314 / 1539 loss=3.248, wps=4353.6, ups=6.68, wpb=651.5, bsz=651.5, num_updates=7450, lr=0.000354, gnorm=1.746, clip=0, train_wall=7, gb_free=70.2, wall=1246 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:35:56]    INFO >> epoch 005:   1364 / 1539 loss=3.208, wps=4895.5, ups=6.75, wpb=725.6, bsz=725.6, num_updates=7500, lr=0.000354, gnorm=1.548, clip=0, train_wall=7, gb_free=74.8, wall=1254 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:36:04]    INFO >> epoch 005:   1414 / 1539 loss=3.354, wps=4311.9, ups=6.84, wpb=630.1, bsz=630.1, num_updates=7550, lr=0.000354, gnorm=1.889, clip=0, train_wall=7, gb_free=74.4, wall=1261 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:36:11]    INFO >> epoch 005:   1464 / 1539 loss=3.267, wps=4887.7, ups=6.4, wpb=763.7, bsz=763.7, num_updates=7600, lr=0.000354, gnorm=1.887, clip=0, train_wall=7, gb_free=49.4, wall=1269 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:36:19]    INFO >> epoch 005:   1514 / 1539 loss=3.193, wps=5044.2, ups=6.28, wpb=802.7, bsz=802.7, num_updates=7650, lr=0.000354, gnorm=2.017, clip=0, train_wall=8, gb_free=55.2, wall=1277 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:36:23]    INFO >> epoch 005 | loss 3.217 | wps 4309 | ups 6.05 | wpb 712.7 | bsz 712.7 | num_updates 7675 | lr 0.000354 | gnorm 1.743 | clip 0 | train_wall 223 | gb_free 63.9 | wall 1281 (progress_bar.py:267, print())[0m
[33m[2025-11-21 14:36:23] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:36:37]    INFO >> epoch 005 | valid on 'valid' subset | loss 3.489 | wps 11214.5 | wpb 5412.5 | bsz 5412.5 | num_updates 7675 | best_loss 3.489 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 14:36:37]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 14:36:37]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_best.pt (epoch 5 @ 7675 updates, score 3.489) (writing took 0.023158 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 14:36:37] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:36:44]    INFO >> epoch 006:     25 / 1539 loss=3.286, wps=1656.3, ups=2.34, wpb=708.8, bsz=708.8, num_updates=7700, lr=0.000327, gnorm=1.69, clip=0, train_wall=7, gb_free=75.1, wall=1298 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:36:52]    INFO >> epoch 006:     75 / 1539 loss=3.335, wps=5371.4, ups=6.22, wpb=863.7, bsz=863.7, num_updates=7750, lr=0.000327, gnorm=2.011, clip=0, train_wall=8, gb_free=75.3, wall=1306 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:36:59]    INFO >> epoch 006:    125 / 1539 loss=3.316, wps=4673.3, ups=6.92, wpb=675.3, bsz=675.3, num_updates=7800, lr=0.000327, gnorm=1.737, clip=0, train_wall=7, gb_free=70.5, wall=1313 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:37:06]    INFO >> epoch 006:    175 / 1539 loss=3.149, wps=4647.2, ups=6.75, wpb=689, bsz=689, num_updates=7850, lr=0.000327, gnorm=1.581, clip=0, train_wall=7, gb_free=74.4, wall=1321 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:37:08] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.09 GiB is free. Including non-PyTorch memory, this process has 78.02 GiB memory in use. Of the allocated memory 71.51 GiB is allocated by PyTorch, and 6.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:37:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:37:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:37:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 21           |        cudaMalloc retries: 37        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72702 MiB |  73224 MiB | 241121 GiB | 241050 GiB |
|       from large pool |  72691 MiB |  73213 MiB | 239858 GiB | 239787 GiB |
|       from small pool |     11 MiB |     15 MiB |   1263 GiB |   1263 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72702 MiB |  73224 MiB | 241121 GiB | 241050 GiB |
|       from large pool |  72691 MiB |  73213 MiB | 239858 GiB | 239787 GiB |
|       from small pool |     11 MiB |     15 MiB |   1263 GiB |   1263 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72691 MiB |  73213 MiB | 240771 GiB | 240700 GiB |
|       from large pool |  72680 MiB |  73202 MiB | 239510 GiB | 239439 GiB |
|       from small pool |     11 MiB |     15 MiB |   1261 GiB |   1261 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79386 MiB |  79460 MiB | 473826 MiB | 394440 MiB |
|       from large pool |  79364 MiB |  79364 MiB | 470462 MiB | 391098 MiB |
|       from small pool |     22 MiB |     96 MiB |   3364 MiB |   3342 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6683 MiB |  10904 MiB | 236510 GiB | 236503 GiB |
|       from large pool |   6672 MiB |  10893 MiB | 235081 GiB | 235075 GiB |
|       from small pool |     10 MiB |     17 MiB |   1428 GiB |   1428 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     544    |     551    |   15087 K  |   15087 K  |
|       from large pool |     259    |     266    |    6778 K  |    6777 K  |
|       from small pool |     285    |     356    |    8309 K  |    8309 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     544    |     551    |   15087 K  |   15087 K  |
|       from large pool |     259    |     266    |    6778 K  |    6777 K  |
|       from small pool |     285    |     356    |    8309 K  |    8309 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      98    |     135    |    3869    |    3771    |
|       from large pool |      87    |      87    |    2187    |    2100    |
|       from small pool |      11    |      48    |    1682    |    1671    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     106    |     106    |    8424 K  |    8424 K  |
|       from large pool |      82    |      82    |    4310 K  |    4310 K  |
|       from small pool |      24    |      43    |    4114 K  |    4114 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:37:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:37:08] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 14:37:14] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 45.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 76.88 GiB is allocated by PyTorch, and 1.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:37:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:37:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:37:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 22           |        cudaMalloc retries: 38        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78661 MiB |  78721 MiB | 242225 GiB | 242149 GiB |
|       from large pool |  78529 MiB |  78589 MiB | 240957 GiB | 240880 GiB |
|       from small pool |    132 MiB |    133 MiB |   1268 GiB |   1268 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78661 MiB |  78721 MiB | 242225 GiB | 242149 GiB |
|       from large pool |  78529 MiB |  78589 MiB | 240957 GiB | 240880 GiB |
|       from small pool |    132 MiB |    133 MiB |   1268 GiB |   1268 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78639 MiB |  78698 MiB | 241874 GiB | 241797 GiB |
|       from large pool |  78507 MiB |  78567 MiB | 240607 GiB | 240531 GiB |
|       from small pool |    131 MiB |    132 MiB |   1266 GiB |   1266 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80460 MiB |  80462 MiB | 474902 MiB | 394442 MiB |
|       from large pool |  80324 MiB |  80324 MiB | 471422 MiB | 391098 MiB |
|       from small pool |    136 MiB |    138 MiB |   3480 MiB |   3344 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1738 MiB |   7080 MiB | 237731 GiB | 237730 GiB |
|       from large pool |   1734 MiB |   7073 MiB | 236297 GiB | 236295 GiB |
|       from small pool |      3 MiB |     17 MiB |   1434 GiB |   1434 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2702    |    2705    |   15153 K  |   15150 K  |
|       from large pool |     480    |     481    |    6806 K  |    6806 K  |
|       from small pool |    2222    |    2225    |    8346 K  |    8343 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2702    |    2705    |   15153 K  |   15150 K  |
|       from large pool |     480    |     481    |    6806 K  |    6806 K  |
|       from small pool |    2222    |    2225    |    8346 K  |    8343 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     171    |     172    |    3943    |    3772    |
|       from large pool |     103    |     103    |    2203    |    2100    |
|       from small pool |      68    |      69    |    1740    |    1672    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     147    |     149    |    8460 K  |    8460 K  |
|       from large pool |      84    |      85    |    4327 K  |    4327 K  |
|       from small pool |      63    |      65    |    4132 K  |    4132 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:37:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:37:14] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:37:17]    INFO >> epoch 006:    227 / 1539 loss=3.237, wps=3966.4, ups=5.51, wpb=719.2, bsz=719.2, num_updates=7900, lr=0.000327, gnorm=1.644, clip=0, train_wall=7, gb_free=71.6, wall=1330 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:37:24]    INFO >> epoch 006:    277 / 1539 loss=3.167, wps=4520.9, ups=7.23, wpb=625.7, bsz=625.7, num_updates=7950, lr=0.000327, gnorm=1.679, clip=0, train_wall=7, gb_free=74.7, wall=1337 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:37:31]    INFO >> epoch 006:    327 / 1539 loss=2.869, wps=5368, ups=6.46, wpb=830.5, bsz=830.5, num_updates=8000, lr=0.000327, gnorm=1.771, clip=0, train_wall=7, gb_free=70.5, wall=1344 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:37:39]    INFO >> epoch 006:    377 / 1539 loss=3.219, wps=4391.8, ups=6.68, wpb=657.6, bsz=657.6, num_updates=8050, lr=0.000327, gnorm=1.735, clip=0, train_wall=7, gb_free=71.3, wall=1352 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:37:46]    INFO >> epoch 006:    427 / 1539 loss=3.204, wps=5212, ups=6.75, wpb=772.1, bsz=772.1, num_updates=8100, lr=0.000327, gnorm=2.025, clip=0, train_wall=7, gb_free=75.1, wall=1359 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:37:50] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.46 GiB. GPU 2 has a total capacity of 79.14 GiB of which 219.25 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 75.54 GiB is allocated by PyTorch, and 2.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:37:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:37:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:37:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 23           |        cudaMalloc retries: 39        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  66232 MiB |  77542 MiB | 248293 GiB | 248228 GiB |
|       from large pool |  66223 MiB |  77534 MiB | 246994 GiB | 246929 GiB |
|       from small pool |      8 MiB |     21 MiB |   1298 GiB |   1298 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  66232 MiB |  77542 MiB | 248293 GiB | 248228 GiB |
|       from large pool |  66223 MiB |  77534 MiB | 246994 GiB | 246929 GiB |
|       from small pool |      8 MiB |     21 MiB |   1298 GiB |   1298 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  66225 MiB |  77534 MiB | 247933 GiB | 247868 GiB |
|       from large pool |  66216 MiB |  77525 MiB | 246636 GiB | 246572 GiB |
|       from small pool |      8 MiB |     21 MiB |   1296 GiB |   1296 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80286 MiB |  80468 MiB | 474970 MiB | 394684 MiB |
|       from large pool |  80264 MiB |  80264 MiB | 471422 MiB | 391158 MiB |
|       from small pool |     22 MiB |    204 MiB |   3548 MiB |   3526 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5869 MiB |   7612 MiB | 244383 GiB | 244378 GiB |
|       from large pool |   5856 MiB |   7599 MiB | 242914 GiB | 242908 GiB |
|       from small pool |     13 MiB |     23 MiB |   1469 GiB |   1469 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     545    |     586    |   15537 K  |   15537 K  |
|       from large pool |     260    |     301    |    6990 K  |    6990 K  |
|       from small pool |     285    |     356    |    8547 K  |    8546 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     545    |     586    |   15537 K  |   15537 K  |
|       from large pool |     260    |     301    |    6990 K  |    6990 K  |
|       from small pool |     285    |     356    |    8547 K  |    8546 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     113    |     204    |    3977    |    3864    |
|       from large pool |     102    |     102    |    2203    |    2101    |
|       from small pool |      11    |     102    |    1774    |    1763    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     117    |     118    |    8669 K  |    8669 K  |
|       from large pool |      96    |      97    |    4441 K  |    4441 K  |
|       from small pool |      21    |      48    |    4228 K  |    4228 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:37:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:37:50] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:37:55]    INFO >> epoch 006:    478 / 1539 loss=3.233, wps=4449.8, ups=6.3, wpb=705.8, bsz=705.8, num_updates=8150, lr=0.000327, gnorm=1.765, clip=0, train_wall=7, gb_free=65.3, wall=1367 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:38:03]    INFO >> epoch 006:    528 / 1539 loss=3.104, wps=4413.2, ups=6.24, wpb=707.6, bsz=707.6, num_updates=8200, lr=0.000327, gnorm=1.711, clip=0, train_wall=8, gb_free=72.1, wall=1375 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:38:11]    INFO >> epoch 006:    578 / 1539 loss=3.368, wps=5178.9, ups=6.49, wpb=797.6, bsz=797.6, num_updates=8250, lr=0.000327, gnorm=1.687, clip=0, train_wall=7, gb_free=74.2, wall=1383 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:38:18]    INFO >> epoch 006:    628 / 1539 loss=3.234, wps=4671, ups=7.24, wpb=644.9, bsz=644.9, num_updates=8300, lr=0.000327, gnorm=1.667, clip=0, train_wall=6, gb_free=70.7, wall=1390 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:38:26]    INFO >> epoch 006:    678 / 1539 loss=3.327, wps=4825.8, ups=6.61, wpb=730.2, bsz=730.2, num_updates=8350, lr=0.000327, gnorm=1.614, clip=0, train_wall=7, gb_free=73.7, wall=1397 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:38:33]    INFO >> epoch 006:    728 / 1539 loss=3.147, wps=4321.4, ups=7.01, wpb=616.1, bsz=616.1, num_updates=8400, lr=0.000327, gnorm=1.676, clip=0, train_wall=7, gb_free=62.4, wall=1405 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:38:34] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 17.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 76.69 GiB is allocated by PyTorch, and 1.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:38:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:38:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:38:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 24           |        cudaMalloc retries: 40        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78474 MiB |  78534 MiB | 256294 GiB | 256218 GiB |
|       from large pool |  78085 MiB |  78145 MiB | 254952 GiB | 254876 GiB |
|       from small pool |    388 MiB |    389 MiB |   1342 GiB |   1341 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78474 MiB |  78534 MiB | 256294 GiB | 256218 GiB |
|       from large pool |  78085 MiB |  78145 MiB | 254952 GiB | 254876 GiB |
|       from small pool |    388 MiB |    389 MiB |   1342 GiB |   1341 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78388 MiB |  78447 MiB | 255923 GiB | 255846 GiB |
|       from large pool |  78001 MiB |  78061 MiB | 254582 GiB | 254506 GiB |
|       from small pool |    386 MiB |    387 MiB |   1340 GiB |   1339 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80488 MiB |  80490 MiB | 483358 MiB | 402870 MiB |
|       from large pool |  80060 MiB |  80060 MiB | 479402 MiB | 399342 MiB |
|       from small pool |    428 MiB |    430 MiB |   3956 MiB |   3528 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1953 MiB |   5223 MiB | 253170 GiB | 253168 GiB |
|       from large pool |   1914 MiB |   5219 MiB | 251650 GiB | 251648 GiB |
|       from small pool |     39 MiB |     41 MiB |   1519 GiB |   1519 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7267    |    7270    |   16070 K  |   16063 K  |
|       from large pool |     881    |     882    |    7236 K  |    7235 K  |
|       from small pool |    6386    |    6389    |    8834 K  |    8827 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7267    |    7270    |   16070 K  |   16063 K  |
|       from large pool |     881    |     882    |    7236 K  |    7235 K  |
|       from small pool |    6386    |    6389    |    8834 K  |    8827 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     446    |     447    |    4314    |    3868    |
|       from large pool |     232    |     232    |    2336    |    2104    |
|       from small pool |     214    |     215    |    1978    |    1764    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     481    |     482    |    8958 K  |    8957 K  |
|       from large pool |      94    |      94    |    4592 K  |    4592 K  |
|       from small pool |     387    |     388    |    4365 K  |    4365 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:38:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:38:34] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:38:40]    INFO >> epoch 006:    779 / 1539 loss=3.162, wps=4252.8, ups=6.62, wpb=642.5, bsz=642.5, num_updates=8450, lr=0.000327, gnorm=1.521, clip=0, train_wall=7, gb_free=74.2, wall=1412 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:38:48]    INFO >> epoch 006:    829 / 1539 loss=3.259, wps=4438.6, ups=6.76, wpb=656.8, bsz=656.8, num_updates=8500, lr=0.000327, gnorm=1.754, clip=0, train_wall=7, gb_free=66.9, wall=1420 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:38:58]    INFO >> epoch 006:    879 / 1539 loss=3.272, wps=4884.4, ups=6.7, wpb=729.6, bsz=729.6, num_updates=8550, lr=0.000327, gnorm=1.569, clip=0, train_wall=7, gb_free=70.4, wall=1427 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:39:06]    INFO >> epoch 006:    929 / 1539 loss=3.339, wps=5005.5, ups=6.06, wpb=826.1, bsz=826.1, num_updates=8600, lr=0.000327, gnorm=1.909, clip=0, train_wall=8, gb_free=73.2, wall=1435 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:39:14]    INFO >> epoch 006:    979 / 1539 loss=3.209, wps=4645.8, ups=6.63, wpb=701.1, bsz=701.1, num_updates=8650, lr=0.000327, gnorm=1.801, clip=0, train_wall=7, gb_free=75.2, wall=1443 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:39:22]    INFO >> epoch 006:   1029 / 1539 loss=3.133, wps=4402, ups=5.78, wpb=761.1, bsz=761.1, num_updates=8700, lr=0.000327, gnorm=1.575, clip=0, train_wall=8, gb_free=17.4, wall=1451 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:39:30]    INFO >> epoch 006:   1079 / 1539 loss=3.36, wps=4855.8, ups=6.58, wpb=737.8, bsz=737.8, num_updates=8750, lr=0.000327, gnorm=2.054, clip=0, train_wall=7, gb_free=76.9, wall=1459 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:39:38]    INFO >> epoch 006:   1129 / 1539 loss=3.084, wps=4605.9, ups=6.09, wpb=755.9, bsz=755.9, num_updates=8800, lr=0.000327, gnorm=1.78, clip=0, train_wall=8, gb_free=73.1, wall=1467 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:39:46]    INFO >> epoch 006:   1179 / 1539 loss=3.376, wps=4889.2, ups=6.7, wpb=729.8, bsz=729.8, num_updates=8850, lr=0.000327, gnorm=1.93, clip=0, train_wall=7, gb_free=75.9, wall=1475 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:39:53]    INFO >> epoch 006:   1229 / 1539 loss=3.31, wps=4306.4, ups=6.35, wpb=678, bsz=678, num_updates=8900, lr=0.000327, gnorm=1.717, clip=0, train_wall=7, gb_free=73.1, wall=1483 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:40:01]    INFO >> epoch 006:   1279 / 1539 loss=3.339, wps=4411.7, ups=6.74, wpb=654.3, bsz=654.3, num_updates=8950, lr=0.000327, gnorm=1.749, clip=0, train_wall=7, gb_free=63.2, wall=1490 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:40:11]    INFO >> epoch 006:   1329 / 1539 loss=3.225, wps=4826.6, ups=6.6, wpb=731.2, bsz=731.2, num_updates=9000, lr=0.000327, gnorm=1.839, clip=0, train_wall=7, gb_free=73.2, wall=1498 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:40:18]    INFO >> epoch 006:   1379 / 1539 loss=3.104, wps=4312.3, ups=7.2, wpb=598.8, bsz=598.8, num_updates=9050, lr=0.000327, gnorm=1.684, clip=0, train_wall=7, gb_free=68, wall=1505 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:40:26]    INFO >> epoch 006:   1429 / 1539 loss=3.21, wps=4991.3, ups=6.5, wpb=767.4, bsz=767.4, num_updates=9100, lr=0.000327, gnorm=1.938, clip=0, train_wall=7, gb_free=70, wall=1512 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:40:34]    INFO >> epoch 006:   1479 / 1539 loss=3.015, wps=4616.3, ups=6.41, wpb=719.9, bsz=719.9, num_updates=9150, lr=0.000327, gnorm=1.675, clip=0, train_wall=7, gb_free=52, wall=1520 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:40:42]    INFO >> epoch 006:   1529 / 1539 loss=3.049, wps=4791.5, ups=7.12, wpb=673, bsz=673, num_updates=9200, lr=0.000327, gnorm=1.612, clip=0, train_wall=7, gb_free=71.4, wall=1527 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:40:43]    INFO >> epoch 006 | loss 3.212 | wps 4413.6 | ups 6.19 | wpb 712.7 | bsz 712.7 | num_updates 9210 | lr 0.000327 | gnorm 1.745 | clip 0 | train_wall 218 | gb_free 72.4 | wall 1529 (progress_bar.py:267, print())[0m
[33m[2025-11-21 14:40:43] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:40:57]    INFO >> epoch 006 | valid on 'valid' subset | loss 3.497 | wps 11058.7 | wpb 5412.5 | bsz 5412.5 | num_updates 9210 | best_loss 3.497 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 14:40:57]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 14:40:57]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_best.pt (epoch 6 @ 9210 updates, score 3.497) (writing took 0.018779 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 14:40:57] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:41:04]    INFO >> epoch 007:     40 / 1539 loss=3.141, wps=1584.4, ups=2.25, wpb=703.3, bsz=703.3, num_updates=9250, lr=0.000295, gnorm=1.909, clip=0, train_wall=7, gb_free=73.7, wall=1549 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:41:06] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 2.05 GiB is free. Including non-PyTorch memory, this process has 77.07 GiB memory in use. Of the allocated memory 71.77 GiB is allocated by PyTorch, and 4.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:41:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:41:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:41:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 25           |        cudaMalloc retries: 42        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73095 MiB |  73493 MiB | 284602 GiB | 284530 GiB |
|       from large pool |  73087 MiB |  73484 MiB | 283109 GiB | 283038 GiB |
|       from small pool |      8 MiB |     25 MiB |   1492 GiB |   1492 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73095 MiB |  73493 MiB | 284602 GiB | 284530 GiB |
|       from large pool |  73087 MiB |  73484 MiB | 283109 GiB | 283038 GiB |
|       from small pool |      8 MiB |     25 MiB |   1492 GiB |   1492 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73086 MiB |  73484 MiB | 284189 GiB | 284118 GiB |
|       from large pool |  73078 MiB |  73475 MiB | 282699 GiB | 282628 GiB |
|       from small pool |      8 MiB |     25 MiB |   1489 GiB |   1489 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78408 MiB |  80428 MiB | 487024 MiB | 408616 MiB |
|       from large pool |  78386 MiB |  80000 MiB | 483068 MiB | 404682 MiB |
|       from small pool |     22 MiB |    428 MiB |   3956 MiB |   3934 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5312 MiB |   7818 MiB | 277623 GiB | 277618 GiB |
|       from large pool |   5298 MiB |   7805 MiB | 275936 GiB | 275931 GiB |
|       from small pool |     13 MiB |     27 MiB |   1686 GiB |   1686 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     571    |     577    |   17806 K  |   17805 K  |
|       from large pool |     286    |     292    |    7987 K  |    7987 K  |
|       from small pool |     285    |     356    |    9818 K  |    9818 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     571    |     577    |   17806 K  |   17805 K  |
|       from large pool |     286    |     292    |    7987 K  |    7987 K  |
|       from small pool |     285    |     356    |    9818 K  |    9818 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     156    |     445    |    4316    |    4160    |
|       from large pool |     145    |     231    |    2338    |    2193    |
|       from small pool |      11    |     214    |    1978    |    1967    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     158    |     159    |    9949 K  |    9949 K  |
|       from large pool |     137    |     138    |    5085 K  |    5085 K  |
|       from small pool |      21    |      53    |    4863 K  |    4863 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:41:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:41:06] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:41:13]    INFO >> epoch 007:     91 / 1539 loss=3.198, wps=4367.6, ups=6.5, wpb=672, bsz=672, num_updates=9300, lr=0.000295, gnorm=1.593, clip=0, train_wall=7, gb_free=72.4, wall=1557 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:41:21]    INFO >> epoch 007:    141 / 1539 loss=3.264, wps=4451.8, ups=6.69, wpb=665.8, bsz=665.8, num_updates=9350, lr=0.000295, gnorm=1.709, clip=0, train_wall=7, gb_free=69.5, wall=1564 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:41:28]    INFO >> epoch 007:    191 / 1539 loss=3.138, wps=5600.1, ups=6.4, wpb=874.9, bsz=874.9, num_updates=9400, lr=0.000295, gnorm=1.789, clip=0, train_wall=7, gb_free=71.6, wall=1572 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:41:36]    INFO >> epoch 007:    241 / 1539 loss=3.324, wps=5003.8, ups=6.26, wpb=798.9, bsz=798.9, num_updates=9450, lr=0.000295, gnorm=1.695, clip=0, train_wall=8, gb_free=74.5, wall=1580 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:41:45]    INFO >> epoch 007:    291 / 1539 loss=3.206, wps=4732.4, ups=6.7, wpb=706.1, bsz=706.1, num_updates=9500, lr=0.000295, gnorm=1.651, clip=0, train_wall=7, gb_free=69.1, wall=1588 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:41:51] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 3.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 76.77 GiB is allocated by PyTorch, and 1.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:41:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:41:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:41:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 26           |        cudaMalloc retries: 44        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78552 MiB |  78612 MiB | 292352 GiB | 292275 GiB |
|       from large pool |  78421 MiB |  78481 MiB | 290820 GiB | 290744 GiB |
|       from small pool |    130 MiB |    132 MiB |   1531 GiB |   1531 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78552 MiB |  78612 MiB | 292352 GiB | 292275 GiB |
|       from large pool |  78421 MiB |  78481 MiB | 290820 GiB | 290744 GiB |
|       from small pool |    130 MiB |    132 MiB |   1531 GiB |   1531 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78519 MiB |  78578 MiB | 291927 GiB | 291850 GiB |
|       from large pool |  78388 MiB |  78448 MiB | 290398 GiB | 290321 GiB |
|       from small pool |    130 MiB |    131 MiB |   1529 GiB |   1528 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80502 MiB |  80502 MiB | 489202 MiB | 408700 MiB |
|       from large pool |  80366 MiB |  80366 MiB | 485048 MiB | 404682 MiB |
|       from small pool |    136 MiB |    218 MiB |   4154 MiB |   4018 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1889 MiB |   8627 MiB | 285302 GiB | 285300 GiB |
|       from large pool |   1884 MiB |   8616 MiB | 283570 GiB | 283568 GiB |
|       from small pool |      5 MiB |     23 MiB |   1731 GiB |   1731 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2680    |    2683    |   18295 K  |   18292 K  |
|       from large pool |     478    |     479    |    8218 K  |    8218 K  |
|       from small pool |    2202    |    2205    |   10076 K  |   10074 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2680    |    2683    |   18295 K  |   18292 K  |
|       from large pool |     478    |     479    |    8218 K  |    8218 K  |
|       from small pool |    2202    |    2205    |   10076 K  |   10074 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     246    |     285    |    4448    |    4202    |
|       from large pool |     178    |     178    |    2371    |    2193    |
|       from small pool |      68    |     109    |    2077    |    2009    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     194    |     194    |   10220 K  |   10220 K  |
|       from large pool |     130    |     134    |    5234 K  |    5234 K  |
|       from small pool |      64    |      64    |    4985 K  |    4985 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:41:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:41:51] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:41:54]    INFO >> epoch 007:    342 / 1539 loss=3.248, wps=3999.7, ups=5.82, wpb=686.9, bsz=686.9, num_updates=9550, lr=0.000295, gnorm=1.791, clip=0, train_wall=7, gb_free=70.9, wall=1596 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:42:01]    INFO >> epoch 007:    392 / 1539 loss=3.273, wps=4149.6, ups=6.96, wpb=596.2, bsz=596.2, num_updates=9600, lr=0.000295, gnorm=1.66, clip=0, train_wall=7, gb_free=75.1, wall=1603 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:42:09]    INFO >> epoch 007:    442 / 1539 loss=3.063, wps=4365.6, ups=6.39, wpb=683.5, bsz=683.5, num_updates=9650, lr=0.000295, gnorm=1.754, clip=0, train_wall=7, gb_free=76.5, wall=1611 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:42:14] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.29 GiB is free. Including non-PyTorch memory, this process has 77.82 GiB memory in use. Of the allocated memory 71.51 GiB is allocated by PyTorch, and 5.81 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:42:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:42:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:42:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 27           |        cudaMalloc retries: 46        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72704 MiB |  73227 MiB | 296484 GiB | 296413 GiB |
|       from large pool |  72693 MiB |  73216 MiB | 294935 GiB | 294864 GiB |
|       from small pool |     11 MiB |     18 MiB |   1549 GiB |   1549 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72704 MiB |  73227 MiB | 296484 GiB | 296413 GiB |
|       from large pool |  72693 MiB |  73216 MiB | 294935 GiB | 294864 GiB |
|       from small pool |     11 MiB |     18 MiB |   1549 GiB |   1549 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72691 MiB |  73213 MiB | 296053 GiB | 295982 GiB |
|       from large pool |  72680 MiB |  73202 MiB | 294507 GiB | 294436 GiB |
|       from small pool |     11 MiB |     18 MiB |   1546 GiB |   1546 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79180 MiB |  80442 MiB | 493514 MiB | 414334 MiB |
|       from large pool |  79158 MiB |  80306 MiB | 489360 MiB | 410202 MiB |
|       from small pool |     22 MiB |    136 MiB |   4154 MiB |   4132 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6475 MiB |  10245 MiB | 289217 GiB | 289211 GiB |
|       from large pool |   6464 MiB |  10234 MiB | 287465 GiB | 287459 GiB |
|       from small pool |     10 MiB |     19 MiB |   1751 GiB |   1751 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     544    |     551    |   18533 K  |   18532 K  |
|       from large pool |     259    |     266    |    8338 K  |    8338 K  |
|       from small pool |     285    |     356    |   10194 K  |   10193 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     544    |     551    |   18533 K  |   18532 K  |
|       from large pool |     259    |     266    |    8338 K  |    8338 K  |
|       from small pool |     285    |     356    |   10194 K  |   10193 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     103    |     245    |    4454    |    4351    |
|       from large pool |      92    |     177    |    2377    |    2285    |
|       from small pool |      11    |      68    |    2077    |    2066    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     102    |     103    |   10351 K  |   10351 K  |
|       from large pool |      81    |      82    |    5313 K  |    5313 K  |
|       from small pool |      21    |      43    |    5037 K  |    5037 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:42:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:42:14] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:42:18]    INFO >> epoch 007:    493 / 1539 loss=3.331, wps=4414.5, ups=6.01, wpb=734.8, bsz=734.8, num_updates=9700, lr=0.000295, gnorm=1.772, clip=0, train_wall=7, gb_free=66.5, wall=1620 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:42:26]    INFO >> epoch 007:    543 / 1539 loss=3.265, wps=4455.9, ups=7.01, wpb=635.3, bsz=635.3, num_updates=9750, lr=0.000295, gnorm=1.72, clip=0, train_wall=7, gb_free=73, wall=1627 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:42:33]    INFO >> epoch 007:    593 / 1539 loss=3.319, wps=4660.8, ups=6.86, wpb=679.2, bsz=679.2, num_updates=9800, lr=0.000295, gnorm=1.879, clip=0, train_wall=7, gb_free=72.7, wall=1634 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:42:41]    INFO >> epoch 007:    643 / 1539 loss=3.16, wps=4521, ups=6.53, wpb=692.4, bsz=692.4, num_updates=9850, lr=0.000295, gnorm=1.925, clip=0, train_wall=7, gb_free=74.7, wall=1642 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:42:48]    INFO >> epoch 007:    693 / 1539 loss=3.147, wps=5454.1, ups=6.45, wpb=845.2, bsz=845.2, num_updates=9900, lr=0.000295, gnorm=1.749, clip=0, train_wall=7, gb_free=71.4, wall=1649 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:42:57]    INFO >> epoch 007:    743 / 1539 loss=3.226, wps=4812.4, ups=6.72, wpb=715.8, bsz=715.8, num_updates=9950, lr=0.000295, gnorm=1.81, clip=0, train_wall=7, gb_free=74.2, wall=1657 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:43:04]    INFO >> epoch 007:    793 / 1539 loss=3.28, wps=4494.5, ups=6.88, wpb=653.1, bsz=653.1, num_updates=10000, lr=0.000295, gnorm=1.685, clip=0, train_wall=7, gb_free=75, wall=1664 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:43:12]    INFO >> epoch 007:    843 / 1539 loss=3.121, wps=4741.9, ups=6.7, wpb=707.7, bsz=707.7, num_updates=10050, lr=0.000295, gnorm=1.745, clip=0, train_wall=7, gb_free=71.9, wall=1672 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:43:19]    INFO >> epoch 007:    893 / 1539 loss=3.206, wps=4579.8, ups=6.55, wpb=699.7, bsz=699.7, num_updates=10100, lr=0.000295, gnorm=1.732, clip=0, train_wall=7, gb_free=72.4, wall=1679 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:43:28]    INFO >> epoch 007:    943 / 1539 loss=3.288, wps=4703.2, ups=6.73, wpb=699.1, bsz=699.1, num_updates=10150, lr=0.000295, gnorm=1.781, clip=0, train_wall=7, gb_free=50.1, wall=1687 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:43:36]    INFO >> epoch 007:    993 / 1539 loss=3.262, wps=4701.7, ups=6.46, wpb=727.9, bsz=727.9, num_updates=10200, lr=0.000295, gnorm=1.762, clip=0, train_wall=7, gb_free=68.8, wall=1694 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:43:43]    INFO >> epoch 007:   1043 / 1539 loss=3.312, wps=4816.2, ups=6.72, wpb=716.6, bsz=716.6, num_updates=10250, lr=0.000295, gnorm=1.898, clip=0, train_wall=7, gb_free=67.5, wall=1702 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:43:51]    INFO >> epoch 007:   1093 / 1539 loss=2.987, wps=4408.3, ups=6.5, wpb=677.8, bsz=677.8, num_updates=10300, lr=0.000295, gnorm=1.741, clip=0, train_wall=7, gb_free=69.1, wall=1710 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:43:59]    INFO >> epoch 007:   1143 / 1539 loss=3.241, wps=4931.2, ups=6.17, wpb=799.6, bsz=799.6, num_updates=10350, lr=0.000295, gnorm=1.673, clip=0, train_wall=8, gb_free=73.4, wall=1718 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:44:07]    INFO >> epoch 007:   1193 / 1539 loss=3.235, wps=5129.6, ups=6.33, wpb=809.8, bsz=809.8, num_updates=10400, lr=0.000295, gnorm=1.858, clip=0, train_wall=7, gb_free=72, wall=1726 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:44:14]    INFO >> epoch 007:   1243 / 1539 loss=3.224, wps=4776.4, ups=6.9, wpb=691.8, bsz=691.8, num_updates=10450, lr=0.000295, gnorm=1.623, clip=0, train_wall=7, gb_free=74.5, wall=1733 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:44:22]    INFO >> epoch 007:   1293 / 1539 loss=3.225, wps=4329, ups=6.82, wpb=634.9, bsz=634.9, num_updates=10500, lr=0.000295, gnorm=1.696, clip=0, train_wall=7, gb_free=73.8, wall=1740 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:44:29]    INFO >> epoch 007:   1343 / 1539 loss=3.239, wps=4859.4, ups=6.94, wpb=700.4, bsz=700.4, num_updates=10550, lr=0.000295, gnorm=1.797, clip=0, train_wall=7, gb_free=73.3, wall=1747 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:44:39]    INFO >> epoch 007:   1393 / 1539 loss=3.087, wps=4570.8, ups=6.58, wpb=694.2, bsz=694.2, num_updates=10600, lr=0.000295, gnorm=1.594, clip=0, train_wall=7, gb_free=72.3, wall=1755 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:44:45] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 17.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 76.86 GiB is allocated by PyTorch, and 1.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:44:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:44:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:44:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 28           |        cudaMalloc retries: 47        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78648 MiB |  78708 MiB | 322775 GiB | 322698 GiB |
|       from large pool |  78258 MiB |  78318 MiB | 321095 GiB | 321019 GiB |
|       from small pool |    390 MiB |    392 MiB |   1679 GiB |   1679 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78648 MiB |  78708 MiB | 322775 GiB | 322698 GiB |
|       from large pool |  78258 MiB |  78318 MiB | 321095 GiB | 321019 GiB |
|       from small pool |    390 MiB |    392 MiB |   1679 GiB |   1679 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78628 MiB |  78687 MiB | 322309 GiB | 322232 GiB |
|       from large pool |  78239 MiB |  78298 MiB | 320631 GiB | 320555 GiB |
|       from small pool |    388 MiB |    390 MiB |   1677 GiB |   1676 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80488 MiB |  80490 MiB | 494824 MiB | 414336 MiB |
|       from large pool |  80058 MiB |  80058 MiB | 490260 MiB | 410202 MiB |
|       from small pool |    430 MiB |    432 MiB |   4564 MiB |   4134 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1779 MiB |   4616 MiB | 319343 GiB | 319341 GiB |
|       from large pool |   1739 MiB |   4612 MiB | 317440 GiB | 317438 GiB |
|       from small pool |     39 MiB |     41 MiB |   1902 GiB |   1902 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7311    |    7314    |   20195 K  |   20188 K  |
|       from large pool |     885    |     886    |    9137 K  |    9136 K  |
|       from small pool |    6426    |    6429    |   11057 K  |   11051 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7311    |    7314    |   20195 K  |   20188 K  |
|       from large pool |     885    |     886    |    9137 K  |    9136 K  |
|       from small pool |    6426    |    6429    |   11057 K  |   11051 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     322    |     323    |    4674    |    4352    |
|       from large pool |     107    |     107    |    2392    |    2285    |
|       from small pool |     215    |     216    |    2282    |    2067    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     476    |     478    |   11234 K  |   11233 K  |
|       from large pool |      88    |      88    |    5794 K  |    5794 K  |
|       from small pool |     388    |     390    |    5439 K  |    5439 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:44:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:44:45] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:44:47]    INFO >> epoch 007:   1444 / 1539 loss=3.265, wps=4411.3, ups=6.15, wpb=716.9, bsz=716.9, num_updates=10650, lr=0.000295, gnorm=1.729, clip=0, train_wall=7, gb_free=68.6, wall=1763 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:44:55]    INFO >> epoch 007:   1494 / 1539 loss=3.314, wps=4968, ups=6.98, wpb=711.5, bsz=711.5, num_updates=10700, lr=0.000295, gnorm=1.61, clip=0, train_wall=7, gb_free=67.1, wall=1770 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:45:02]    INFO >> epoch 007 | loss 3.207 | wps 4395.5 | ups 6.17 | wpb 712.7 | bsz 712.7 | num_updates 10745 | lr 0.000295 | gnorm 1.736 | clip 0 | train_wall 218 | gb_free 70.4 | wall 1777 (progress_bar.py:267, print())[0m
[33m[2025-11-21 14:45:02] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:45:16]    INFO >> epoch 007 | valid on 'valid' subset | loss 3.489 | wps 11465.4 | wpb 5412.5 | bsz 5412.5 | num_updates 10745 | best_loss 3.497 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 14:45:17]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 14:45:17]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_last.pt (epoch 7 @ 10745 updates, score 3.489) (writing took 0.014930 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 14:45:17] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:45:17]    INFO >> epoch 008:      5 / 1539 loss=2.872, wps=1691.2, ups=2.33, wpb=726.4, bsz=726.4, num_updates=10750, lr=0.000262, gnorm=1.471, clip=0, train_wall=7, gb_free=71.6, wall=1792 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:45:25]    INFO >> epoch 008:     55 / 1539 loss=2.916, wps=4823.7, ups=6.46, wpb=746.9, bsz=746.9, num_updates=10800, lr=0.000262, gnorm=1.524, clip=0, train_wall=7, gb_free=74.3, wall=1799 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:45:33]    INFO >> epoch 008:    105 / 1539 loss=3.216, wps=4853.1, ups=6.35, wpb=764.8, bsz=764.8, num_updates=10850, lr=0.000262, gnorm=1.741, clip=0, train_wall=7, gb_free=72.1, wall=1807 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:45:40]    INFO >> epoch 008:    155 / 1539 loss=3.172, wps=4390.9, ups=6.72, wpb=653.2, bsz=653.2, num_updates=10900, lr=0.000262, gnorm=1.465, clip=0, train_wall=7, gb_free=66.3, wall=1815 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:45:50]    INFO >> epoch 008:    205 / 1539 loss=3.016, wps=5123.4, ups=6.14, wpb=834, bsz=834, num_updates=10950, lr=0.000262, gnorm=1.759, clip=0, train_wall=8, gb_free=74.6, wall=1823 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:45:57]    INFO >> epoch 008:    255 / 1539 loss=3.171, wps=4746.8, ups=6.71, wpb=707.5, bsz=707.5, num_updates=11000, lr=0.000262, gnorm=1.899, clip=0, train_wall=7, gb_free=76.3, wall=1830 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:46:05]    INFO >> epoch 008:    305 / 1539 loss=3.394, wps=4695.2, ups=6.5, wpb=722.6, bsz=722.6, num_updates=11050, lr=0.000262, gnorm=1.785, clip=0, train_wall=7, gb_free=72.4, wall=1838 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:46:13]    INFO >> epoch 008:    355 / 1539 loss=3.227, wps=4177.1, ups=6.21, wpb=672.6, bsz=672.6, num_updates=11100, lr=0.000262, gnorm=1.803, clip=0, train_wall=8, gb_free=74.1, wall=1846 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:46:22]    INFO >> epoch 008:    405 / 1539 loss=3.225, wps=4635.5, ups=6.61, wpb=701, bsz=701, num_updates=11150, lr=0.000262, gnorm=1.633, clip=0, train_wall=7, gb_free=69, wall=1854 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:46:30]    INFO >> epoch 008:    455 / 1539 loss=3.389, wps=5603.6, ups=6.38, wpb=878.7, bsz=878.7, num_updates=11200, lr=0.000262, gnorm=1.874, clip=0, train_wall=7, gb_free=73.4, wall=1862 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:46:37]    INFO >> epoch 008:    505 / 1539 loss=3.239, wps=5083.2, ups=6.76, wpb=751.6, bsz=751.6, num_updates=11250, lr=0.000262, gnorm=1.943, clip=0, train_wall=7, gb_free=69.2, wall=1869 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:46:45]    INFO >> epoch 008:    555 / 1539 loss=3.257, wps=4354, ups=6.75, wpb=645.3, bsz=645.3, num_updates=11300, lr=0.000262, gnorm=1.497, clip=0, train_wall=7, gb_free=72.3, wall=1876 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:46:54]    INFO >> epoch 008:    605 / 1539 loss=3.345, wps=4508.6, ups=6.58, wpb=685.6, bsz=685.6, num_updates=11350, lr=0.000262, gnorm=1.659, clip=0, train_wall=7, gb_free=73.9, wall=1884 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:47:00]    INFO >> epoch 008:    655 / 1539 loss=3.244, wps=4627.6, ups=7.28, wpb=635.9, bsz=635.9, num_updates=11400, lr=0.000262, gnorm=1.74, clip=0, train_wall=6, gb_free=74.2, wall=1891 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:47:08]    INFO >> epoch 008:    705 / 1539 loss=3.211, wps=4664.5, ups=6.79, wpb=686.9, bsz=686.9, num_updates=11450, lr=0.000262, gnorm=1.761, clip=0, train_wall=7, gb_free=71.7, wall=1898 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:47:15]    INFO >> epoch 008:    755 / 1539 loss=3.253, wps=4840.9, ups=6.68, wpb=725.2, bsz=725.2, num_updates=11500, lr=0.000262, gnorm=1.789, clip=0, train_wall=7, gb_free=71.5, wall=1906 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:47:24]    INFO >> epoch 008:    805 / 1539 loss=3.128, wps=4659.8, ups=6.68, wpb=697.6, bsz=697.6, num_updates=11550, lr=0.000262, gnorm=1.817, clip=0, train_wall=7, gb_free=74.7, wall=1913 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:47:31]    INFO >> epoch 008:    855 / 1539 loss=3.157, wps=4335.5, ups=7.23, wpb=599.6, bsz=599.6, num_updates=11600, lr=0.000262, gnorm=1.516, clip=0, train_wall=7, gb_free=72.6, wall=1920 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:47:39]    INFO >> epoch 008:    905 / 1539 loss=3.088, wps=5062.9, ups=6.22, wpb=814.2, bsz=814.2, num_updates=11650, lr=0.000262, gnorm=1.728, clip=0, train_wall=8, gb_free=72, wall=1928 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:47:46]    INFO >> epoch 008:    955 / 1539 loss=3.162, wps=4703, ups=6.98, wpb=674.2, bsz=674.2, num_updates=11700, lr=0.000262, gnorm=1.625, clip=0, train_wall=7, gb_free=70.8, wall=1935 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:47:51] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 11.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 76.93 GiB is allocated by PyTorch, and 1.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:47:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:47:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:47:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 29           |        cudaMalloc retries: 49        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78718 MiB |  78778 MiB | 357267 GiB | 357190 GiB |
|       from large pool |  78585 MiB |  78645 MiB | 355398 GiB | 355322 GiB |
|       from small pool |    132 MiB |    133 MiB |   1868 GiB |   1868 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78718 MiB |  78778 MiB | 357267 GiB | 357190 GiB |
|       from large pool |  78585 MiB |  78645 MiB | 355398 GiB | 355322 GiB |
|       from small pool |    132 MiB |    133 MiB |   1868 GiB |   1868 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78699 MiB |  78758 MiB | 356751 GiB | 356674 GiB |
|       from large pool |  78567 MiB |  78626 MiB | 354885 GiB | 354808 GiB |
|       from small pool |    132 MiB |    133 MiB |   1865 GiB |   1865 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80494 MiB |  80496 MiB | 495188 MiB | 414694 MiB |
|       from large pool |  80358 MiB |  80358 MiB | 490620 MiB | 410262 MiB |
|       from small pool |    136 MiB |    430 MiB |   4568 MiB |   4432 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1715 MiB |   7712 MiB | 354758 GiB | 354756 GiB |
|       from large pool |   1712 MiB |   7705 MiB | 352642 GiB | 352641 GiB |
|       from small pool |      3 MiB |     19 MiB |   2115 GiB |   2115 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2713    |    2716    |   22401 K  |   22398 K  |
|       from large pool |     481    |     482    |   10099 K  |   10098 K  |
|       from small pool |    2232    |    2235    |   12302 K  |   12300 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2713    |    2716    |   22401 K  |   22398 K  |
|       from large pool |     481    |     482    |   10099 K  |   10098 K  |
|       from small pool |    2232    |    2235    |   12302 K  |   12300 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     180    |     322    |    4682    |    4502    |
|       from large pool |     112    |     112    |    2398    |    2286    |
|       from small pool |      68    |     215    |    2284    |    2216    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     159    |     161    |   12459 K  |   12459 K  |
|       from large pool |      94    |      95    |    6388 K  |    6388 K  |
|       from small pool |      65    |      67    |    6071 K  |    6071 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:47:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:47:51] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:47:54]    INFO >> epoch 008:   1006 / 1539 loss=3.189, wps=3814.4, ups=6.47, wpb=589.5, bsz=589.5, num_updates=11750, lr=0.000262, gnorm=1.562, clip=0, train_wall=7, gb_free=71.6, wall=1943 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:48:04]    INFO >> epoch 008:   1056 / 1539 loss=3.067, wps=4831.1, ups=5.76, wpb=839.3, bsz=839.3, num_updates=11800, lr=0.000262, gnorm=1.512, clip=0, train_wall=8, gb_free=72.6, wall=1952 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:48:11]    INFO >> epoch 008:   1106 / 1539 loss=3.163, wps=4598.2, ups=6.9, wpb=666.3, bsz=666.3, num_updates=11850, lr=0.000262, gnorm=1.779, clip=0, train_wall=7, gb_free=70.5, wall=1959 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:48:18]    INFO >> epoch 008:   1156 / 1539 loss=3.355, wps=3900.8, ups=7.04, wpb=553.7, bsz=553.7, num_updates=11900, lr=0.000262, gnorm=1.727, clip=0, train_wall=7, gb_free=72.8, wall=1966 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:48:21] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.23 GiB is free. Including non-PyTorch memory, this process has 77.88 GiB memory in use. Of the allocated memory 71.51 GiB is allocated by PyTorch, and 5.87 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:48:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:48:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:48:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 30           |        cudaMalloc retries: 50        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72704 MiB |  73226 MiB | 362358 GiB | 362287 GiB |
|       from large pool |  72693 MiB |  73215 MiB | 360466 GiB | 360395 GiB |
|       from small pool |     11 MiB |     19 MiB |   1891 GiB |   1891 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72704 MiB |  73226 MiB | 362358 GiB | 362287 GiB |
|       from large pool |  72693 MiB |  73215 MiB | 360466 GiB | 360395 GiB |
|       from small pool |     11 MiB |     19 MiB |   1891 GiB |   1891 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72691 MiB |  73213 MiB | 361835 GiB | 361764 GiB |
|       from large pool |  72680 MiB |  73202 MiB | 359946 GiB | 359875 GiB |
|       from small pool |     11 MiB |     19 MiB |   1888 GiB |   1888 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79242 MiB |  80434 MiB | 495188 MiB | 415946 MiB |
|       from large pool |  79218 MiB |  80298 MiB | 490620 MiB | 411402 MiB |
|       from small pool |     24 MiB |    136 MiB |   4568 MiB |   4544 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6537 MiB |   9102 MiB | 360117 GiB | 360111 GiB |
|       from large pool |   6524 MiB |   9089 MiB | 357975 GiB | 357969 GiB |
|       from small pool |     12 MiB |     21 MiB |   2142 GiB |   2142 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     544    |     551    |   22706 K  |   22705 K  |
|       from large pool |     259    |     266    |   10250 K  |   10250 K  |
|       from small pool |     285    |     348    |   12455 K  |   12454 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     544    |     551    |   22706 K  |   22705 K  |
|       from large pool |     259    |     266    |   10250 K  |   10250 K  |
|       from small pool |     285    |     348    |   12455 K  |   12454 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     105    |     179    |    4682    |    4577    |
|       from large pool |      93    |     111    |    2398    |    2305    |
|       from small pool |      12    |      68    |    2284    |    2272    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     112    |     112    |   12621 K  |   12621 K  |
|       from large pool |      84    |      84    |    6482 K  |    6482 K  |
|       from small pool |      28    |      43    |    6139 K  |    6139 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:48:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:48:21] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:48:27]    INFO >> epoch 008:   1207 / 1539 loss=3.194, wps=4274.8, ups=5.97, wpb=716.6, bsz=716.6, num_updates=11950, lr=0.000262, gnorm=1.685, clip=0, train_wall=7, gb_free=70.3, wall=1974 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:48:34] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 17.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 76.75 GiB is allocated by PyTorch, and 1.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:48:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:48:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:48:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 31           |        cudaMalloc retries: 51        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78528 MiB |  78588 MiB | 364513 GiB | 364437 GiB |
|       from large pool |  78138 MiB |  78198 MiB | 362609 GiB | 362532 GiB |
|       from small pool |    389 MiB |    390 MiB |   1904 GiB |   1904 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78528 MiB |  78588 MiB | 364513 GiB | 364437 GiB |
|       from large pool |  78138 MiB |  78198 MiB | 362609 GiB | 362532 GiB |
|       from small pool |    389 MiB |    390 MiB |   1904 GiB |   1904 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78508 MiB |  78567 MiB | 363988 GiB | 363911 GiB |
|       from large pool |  78120 MiB |  78179 MiB | 362086 GiB | 362010 GiB |
|       from small pool |    387 MiB |    388 MiB |   1901 GiB |   1901 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80488 MiB |  80490 MiB | 496436 MiB | 415948 MiB |
|       from large pool |  80058 MiB |  80058 MiB | 491460 MiB | 411402 MiB |
|       from small pool |    430 MiB |    432 MiB |   4976 MiB |   4546 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1899 MiB |   4678 MiB | 362552 GiB | 362550 GiB |
|       from large pool |   1859 MiB |   4672 MiB | 360395 GiB | 360393 GiB |
|       from small pool |     40 MiB |     41 MiB |   2156 GiB |   2156 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7289    |    7292    |   22848 K  |   22840 K  |
|       from large pool |     883    |     884    |   10309 K  |   10308 K  |
|       from small pool |    6406    |    6409    |   12538 K  |   12532 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7289    |    7292    |   22848 K  |   22840 K  |
|       from large pool |     883    |     884    |   10309 K  |   10308 K  |
|       from small pool |    6406    |    6409    |   12538 K  |   12532 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     322    |     323    |    4900    |    4578    |
|       from large pool |     107    |     107    |    2412    |    2305    |
|       from small pool |     215    |     216    |    2488    |    2273    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     480    |     480    |   12699 K  |   12699 K  |
|       from large pool |      91    |      91    |    6517 K  |    6517 K  |
|       from small pool |     389    |     389    |    6182 K  |    6181 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:48:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:48:34] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:48:37]    INFO >> epoch 008:   1258 / 1539 loss=3.382, wps=5020.3, ups=5.92, wpb=848.4, bsz=848.4, num_updates=12000, lr=0.000262, gnorm=2.071, clip=0, train_wall=7, gb_free=70.3, wall=1983 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:48:44]    INFO >> epoch 008:   1308 / 1539 loss=3.163, wps=4773.1, ups=6.83, wpb=698.5, bsz=698.5, num_updates=12050, lr=0.000262, gnorm=1.727, clip=0, train_wall=7, gb_free=71.5, wall=1990 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:48:51]    INFO >> epoch 008:   1358 / 1539 loss=3.065, wps=4432.9, ups=6.59, wpb=672.3, bsz=672.3, num_updates=12100, lr=0.000262, gnorm=1.723, clip=0, train_wall=7, gb_free=67.6, wall=1998 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:48:52] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.46 GiB. GPU 2 has a total capacity of 79.14 GiB of which 485.25 MiB is free. Including non-PyTorch memory, this process has 78.64 GiB memory in use. Of the allocated memory 75.54 GiB is allocated by PyTorch, and 2.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:48:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:48:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:48:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 32           |        cudaMalloc retries: 52        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  66232 MiB |  77542 MiB | 367951 GiB | 367886 GiB |
|       from large pool |  66223 MiB |  77533 MiB | 366031 GiB | 365966 GiB |
|       from small pool |      8 MiB |     18 MiB |   1920 GiB |   1920 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  66232 MiB |  77542 MiB | 367951 GiB | 367886 GiB |
|       from large pool |  66223 MiB |  77533 MiB | 366031 GiB | 365966 GiB |
|       from small pool |      8 MiB |     18 MiB |   1920 GiB |   1920 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  66225 MiB |  77534 MiB | 367421 GiB | 367356 GiB |
|       from large pool |  66216 MiB |  77525 MiB | 365503 GiB | 365439 GiB |
|       from small pool |      8 MiB |     18 MiB |   1917 GiB |   1917 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80020 MiB |  80428 MiB | 496436 MiB | 416416 MiB |
|       from large pool |  79998 MiB |  79998 MiB | 491460 MiB | 411462 MiB |
|       from small pool |     22 MiB |    430 MiB |   4976 MiB |   4954 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5603 MiB |   7347 MiB | 366252 GiB | 366247 GiB |
|       from large pool |   5590 MiB |   7333 MiB | 364077 GiB | 364072 GiB |
|       from small pool |     13 MiB |     21 MiB |   2175 GiB |   2175 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     545    |     586    |   23048 K  |   23047 K  |
|       from large pool |     260    |     301    |   10408 K  |   10407 K  |
|       from small pool |     285    |     347    |   12640 K  |   12639 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     545    |     586    |   23048 K  |   23047 K  |
|       from large pool |     260    |     301    |   10408 K  |   10407 K  |
|       from small pool |     285    |     347    |   12640 K  |   12639 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     117    |     321    |    4900    |    4783    |
|       from large pool |     106    |     106    |    2412    |    2306    |
|       from small pool |      11    |     215    |    2488    |    2477    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     123    |     128    |   12807 K  |   12807 K  |
|       from large pool |     100    |     105    |    6578 K  |    6578 K  |
|       from small pool |      23    |      45    |    6229 K  |    6229 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:48:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:48:52] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:49:00]    INFO >> epoch 008:   1409 / 1539 loss=3.136, wps=4889.9, ups=5.92, wpb=825.5, bsz=825.5, num_updates=12150, lr=0.000262, gnorm=1.77, clip=0, train_wall=7, gb_free=69.5, wall=2006 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:49:10]    INFO >> epoch 008:   1459 / 1539 loss=3.337, wps=4330.7, ups=6.04, wpb=716.4, bsz=716.4, num_updates=12200, lr=0.000262, gnorm=1.822, clip=0, train_wall=8, gb_free=66.7, wall=2014 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:49:17]    INFO >> epoch 008:   1509 / 1539 loss=3.217, wps=4472.4, ups=6.77, wpb=660.9, bsz=660.9, num_updates=12250, lr=0.000262, gnorm=1.798, clip=0, train_wall=7, gb_free=73.3, wall=2022 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:49:22]    INFO >> epoch 008 | loss 3.198 | wps 4393 | ups 6.16 | wpb 712.7 | bsz 712.7 | num_updates 12280 | lr 0.000262 | gnorm 1.724 | clip 0 | train_wall 219 | gb_free 74.9 | wall 2026 (progress_bar.py:267, print())[0m
[33m[2025-11-21 14:49:22] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:49:35]    INFO >> epoch 008 | valid on 'valid' subset | loss 3.524 | wps 11223.6 | wpb 5412.5 | bsz 5412.5 | num_updates 12280 | best_loss 3.524 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 14:49:35]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 14:49:35]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_best.pt (epoch 8 @ 12280 updates, score 3.524) (writing took 0.019832 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 14:49:35] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:49:39]    INFO >> epoch 009:     20 / 1539 loss=3.179, wps=1580.2, ups=2.36, wpb=669.4, bsz=669.4, num_updates=12300, lr=0.000227, gnorm=1.807, clip=0, train_wall=7, gb_free=73.2, wall=2043 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:49:47]    INFO >> epoch 009:     70 / 1539 loss=3.232, wps=4470, ups=6.72, wpb=664.8, bsz=664.8, num_updates=12350, lr=0.000227, gnorm=1.74, clip=0, train_wall=7, gb_free=73.2, wall=2050 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:49:54]    INFO >> epoch 009:    120 / 1539 loss=3.18, wps=4781.7, ups=6.94, wpb=688.8, bsz=688.8, num_updates=12400, lr=0.000227, gnorm=1.636, clip=0, train_wall=7, gb_free=75.3, wall=2058 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:50:02]    INFO >> epoch 009:    170 / 1539 loss=3.201, wps=5366, ups=6.44, wpb=833.1, bsz=833.1, num_updates=12450, lr=0.000227, gnorm=1.961, clip=0, train_wall=7, gb_free=71.9, wall=2065 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:50:09]    INFO >> epoch 009:    220 / 1539 loss=3.252, wps=4886.6, ups=7.01, wpb=696.6, bsz=696.6, num_updates=12500, lr=0.000227, gnorm=1.77, clip=0, train_wall=7, gb_free=70.4, wall=2073 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:50:17]    INFO >> epoch 009:    270 / 1539 loss=3.19, wps=4973, ups=7, wpb=710.9, bsz=710.9, num_updates=12550, lr=0.000227, gnorm=1.671, clip=0, train_wall=7, gb_free=75, wall=2080 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:50:25]    INFO >> epoch 009:    320 / 1539 loss=3.254, wps=4937.6, ups=6.41, wpb=770.7, bsz=770.7, num_updates=12600, lr=0.000227, gnorm=1.856, clip=0, train_wall=7, gb_free=69.4, wall=2088 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:50:34]    INFO >> epoch 009:    370 / 1539 loss=3.306, wps=4313.1, ups=6.04, wpb=714.7, bsz=714.7, num_updates=12650, lr=0.000227, gnorm=1.693, clip=0, train_wall=8, gb_free=71.8, wall=2096 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:50:35] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.88 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.34 GiB is free. Including non-PyTorch memory, this process has 77.78 GiB memory in use. Of the allocated memory 72.51 GiB is allocated by PyTorch, and 4.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:50:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:50:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:50:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 33           |        cudaMalloc retries: 53        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  63159 MiB |  75129 MiB | 387779 GiB | 387717 GiB |
|       from large pool |  63148 MiB |  75118 MiB | 385748 GiB | 385687 GiB |
|       from small pool |     11 MiB |     16 MiB |   2030 GiB |   2030 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  63159 MiB |  75129 MiB | 387779 GiB | 387717 GiB |
|       from large pool |  63148 MiB |  75118 MiB | 385748 GiB | 385687 GiB |
|       from small pool |     11 MiB |     16 MiB |   2030 GiB |   2030 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  63147 MiB |  75114 MiB | 387221 GiB | 387160 GiB |
|       from large pool |  63136 MiB |  75103 MiB | 385194 GiB | 385132 GiB |
|       from small pool |     11 MiB |     16 MiB |   2027 GiB |   2027 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79132 MiB |  80068 MiB | 504668 MiB | 425536 MiB |
|       from large pool |  79110 MiB |  79950 MiB | 499596 MiB | 420486 MiB |
|       from small pool |     22 MiB |    118 MiB |   5072 MiB |   5050 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   9758 MiB |   9758 MiB | 385321 GiB | 385311 GiB |
|       from large pool |   9747 MiB |   9747 MiB | 383025 GiB | 383016 GiB |
|       from small pool |     10 MiB |     19 MiB |   2295 GiB |   2295 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     515    |     557    |   24267 K  |   24266 K  |
|       from large pool |     230    |     272    |   10906 K  |   10906 K  |
|       from small pool |     285    |     342    |   13360 K  |   13360 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     515    |     557    |   24267 K  |   24266 K  |
|       from large pool |     230    |     272    |   10906 K  |   10906 K  |
|       from small pool |     285    |     342    |   13360 K  |   13360 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     108    |     170    |    4956    |    4848    |
|       from large pool |      97    |     111    |    2420    |    2323    |
|       from small pool |      11    |      59    |    2536    |    2525    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     109    |     109    |   13490 K  |   13490 K  |
|       from large pool |      87    |      87    |    6887 K  |    6887 K  |
|       from small pool |      22    |      38    |    6602 K  |    6602 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:50:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:50:35] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:50:42]    INFO >> epoch 009:    421 / 1539 loss=3.239, wps=4319.7, ups=6.05, wpb=714.3, bsz=714.3, num_updates=12700, lr=0.000227, gnorm=1.803, clip=0, train_wall=7, gb_free=73.9, wall=2104 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:50:51]    INFO >> epoch 009:    471 / 1539 loss=3.043, wps=4651.4, ups=6.72, wpb=691.9, bsz=691.9, num_updates=12750, lr=0.000227, gnorm=1.544, clip=0, train_wall=7, gb_free=74.7, wall=2112 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:50:58]    INFO >> epoch 009:    521 / 1539 loss=2.936, wps=4901.9, ups=6.46, wpb=759.2, bsz=759.2, num_updates=12800, lr=0.000227, gnorm=1.591, clip=0, train_wall=7, gb_free=73.7, wall=2119 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:51:06]    INFO >> epoch 009:    571 / 1539 loss=3.332, wps=4765.6, ups=6.33, wpb=752.6, bsz=752.6, num_updates=12850, lr=0.000227, gnorm=1.946, clip=0, train_wall=7, gb_free=72.3, wall=2127 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:51:07] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 31.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.05 GiB is allocated by PyTorch, and 1.54 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:51:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:51:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:51:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 34           |        cudaMalloc retries: 54        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78834 MiB |  78894 MiB | 393520 GiB | 393443 GiB |
|       from large pool |  78700 MiB |  78760 MiB | 391462 GiB | 391386 GiB |
|       from small pool |    133 MiB |    134 MiB |   2057 GiB |   2057 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78834 MiB |  78894 MiB | 393520 GiB | 393443 GiB |
|       from large pool |  78700 MiB |  78760 MiB | 391462 GiB | 391386 GiB |
|       from small pool |    133 MiB |    134 MiB |   2057 GiB |   2057 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78759 MiB |  78819 MiB | 392955 GiB | 392878 GiB |
|       from large pool |  78626 MiB |  78686 MiB | 390901 GiB | 390824 GiB |
|       from small pool |    132 MiB |    133 MiB |   2054 GiB |   2054 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80474 MiB |  80474 MiB | 512224 MiB | 431750 MiB |
|       from large pool |  80336 MiB |  80336 MiB | 507036 MiB | 426700 MiB |
|       from small pool |    138 MiB |    138 MiB |   5188 MiB |   5050 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1579 MiB |   6069 MiB | 391807 GiB | 391806 GiB |
|       from large pool |   1575 MiB |   6062 MiB | 389480 GiB | 389478 GiB |
|       from small pool |      4 MiB |     17 MiB |   2327 GiB |   2327 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2724    |    2727    |   24616 K  |   24613 K  |
|       from large pool |     482    |     483    |   11075 K  |   11075 K  |
|       from small pool |    2242    |    2245    |   13541 K  |   13538 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2724    |    2727    |   24616 K  |   24613 K  |
|       from large pool |     482    |     483    |   11075 K  |   11075 K  |
|       from small pool |    2242    |    2245    |   13541 K  |   13538 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     283    |     283    |    5138    |    4855    |
|       from large pool |     214    |     214    |    2544    |    2330    |
|       from small pool |      69    |      69    |    2594    |    2525    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     150    |     152    |   13675 K  |   13675 K  |
|       from large pool |      86    |      89    |    6989 K  |    6989 K  |
|       from small pool |      64    |      66    |    6686 K  |    6685 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:51:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:51:07] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:51:14]    INFO >> epoch 009:    622 / 1539 loss=3.247, wps=4242, ups=6.06, wpb=699.8, bsz=699.8, num_updates=12900, lr=0.000227, gnorm=1.758, clip=0, train_wall=7, gb_free=71.1, wall=2135 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:51:22]    INFO >> epoch 009:    672 / 1539 loss=3.27, wps=4422.3, ups=6.88, wpb=643.2, bsz=643.2, num_updates=12950, lr=0.000227, gnorm=1.668, clip=0, train_wall=7, gb_free=64.2, wall=2143 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:51:29]    INFO >> epoch 009:    722 / 1539 loss=3.135, wps=4744.9, ups=6.82, wpb=695.7, bsz=695.7, num_updates=13000, lr=0.000227, gnorm=1.749, clip=0, train_wall=7, gb_free=73.6, wall=2150 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:51:37]    INFO >> epoch 009:    772 / 1539 loss=3.345, wps=4435.8, ups=6.55, wpb=676.8, bsz=676.8, num_updates=13050, lr=0.000227, gnorm=1.822, clip=0, train_wall=7, gb_free=72.8, wall=2158 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:51:45] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.08 GiB is free. Including non-PyTorch memory, this process has 78.03 GiB memory in use. Of the allocated memory 71.77 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:51:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:51:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:51:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 35           |        cudaMalloc retries: 56        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73095 MiB |  73493 MiB | 400254 GiB | 400182 GiB |
|       from large pool |  73086 MiB |  73484 MiB | 398165 GiB | 398093 GiB |
|       from small pool |      8 MiB |     18 MiB |   2088 GiB |   2088 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73095 MiB |  73493 MiB | 400254 GiB | 400182 GiB |
|       from large pool |  73086 MiB |  73484 MiB | 398165 GiB | 398093 GiB |
|       from small pool |      8 MiB |     18 MiB |   2088 GiB |   2088 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73086 MiB |  73484 MiB | 399678 GiB | 399607 GiB |
|       from large pool |  73078 MiB |  73475 MiB | 397593 GiB | 397521 GiB |
|       from small pool |      8 MiB |     18 MiB |   2085 GiB |   2085 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79396 MiB |  80414 MiB | 514952 MiB | 435556 MiB |
|       from large pool |  79374 MiB |  80276 MiB | 509764 MiB | 430390 MiB |
|       from small pool |     22 MiB |    138 MiB |   5188 MiB |   5166 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6180 MiB |   8177 MiB | 398079 GiB | 398073 GiB |
|       from large pool |   6167 MiB |   8163 MiB | 395716 GiB | 395710 GiB |
|       from small pool |     13 MiB |     17 MiB |   2363 GiB |   2363 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     571    |     577    |   25023 K  |   25022 K  |
|       from large pool |     286    |     292    |   11277 K  |   11277 K  |
|       from small pool |     285    |     348    |   13745 K  |   13745 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     571    |     577    |   25023 K  |   25022 K  |
|       from large pool |     286    |     292    |   11277 K  |   11277 K  |
|       from small pool |     285    |     348    |   13745 K  |   13745 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     168    |     282    |    5139    |    4971    |
|       from large pool |     157    |     213    |    2545    |    2388    |
|       from small pool |      11    |      69    |    2594    |    2583    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     169    |     171    |   13899 K  |   13898 K  |
|       from large pool |     148    |     150    |    7121 K  |    7121 K  |
|       from small pool |      21    |      40    |    6777 K  |    6777 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:51:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:51:45] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:51:45]    INFO >> epoch 009:    823 / 1539 loss=3.195, wps=4477.7, ups=6.11, wpb=732.7, bsz=732.7, num_updates=13100, lr=0.000227, gnorm=1.846, clip=0, train_wall=7, gb_free=74, wall=2166 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:51:55]    INFO >> epoch 009:    873 / 1539 loss=3.235, wps=4606.4, ups=7.14, wpb=644.7, bsz=644.7, num_updates=13150, lr=0.000227, gnorm=1.578, clip=0, train_wall=7, gb_free=69.6, wall=2173 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:52:02]    INFO >> epoch 009:    923 / 1539 loss=3.336, wps=4363.5, ups=7.05, wpb=618.7, bsz=618.7, num_updates=13200, lr=0.000227, gnorm=1.785, clip=0, train_wall=7, gb_free=67.4, wall=2180 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:52:11]    INFO >> epoch 009:    973 / 1539 loss=3.099, wps=4801.8, ups=5.56, wpb=863.2, bsz=863.2, num_updates=13250, lr=0.000227, gnorm=1.68, clip=0, train_wall=8, gb_free=70.1, wall=2189 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:52:19]    INFO >> epoch 009:   1023 / 1539 loss=3.122, wps=4567.7, ups=6.37, wpb=717.2, bsz=717.2, num_updates=13300, lr=0.000227, gnorm=1.731, clip=0, train_wall=7, gb_free=70.1, wall=2197 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:52:27]    INFO >> epoch 009:   1073 / 1539 loss=3.237, wps=4893.1, ups=6.7, wpb=730.3, bsz=730.3, num_updates=13350, lr=0.000227, gnorm=1.655, clip=0, train_wall=7, gb_free=69.1, wall=2204 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:52:35]    INFO >> epoch 009:   1123 / 1539 loss=3.261, wps=4762.9, ups=6.3, wpb=756.5, bsz=756.5, num_updates=13400, lr=0.000227, gnorm=1.78, clip=0, train_wall=7, gb_free=71.7, wall=2212 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:52:43]    INFO >> epoch 009:   1173 / 1539 loss=3.201, wps=4509.7, ups=6.74, wpb=669.3, bsz=669.3, num_updates=13450, lr=0.000227, gnorm=1.678, clip=0, train_wall=7, gb_free=73.6, wall=2220 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:52:51]    INFO >> epoch 009:   1223 / 1539 loss=3.01, wps=5113.4, ups=6.05, wpb=844.6, bsz=844.6, num_updates=13500, lr=0.000227, gnorm=1.679, clip=0, train_wall=8, gb_free=74.2, wall=2228 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:53:00]    INFO >> epoch 009:   1273 / 1539 loss=3.155, wps=5122.9, ups=6.49, wpb=789.1, bsz=789.1, num_updates=13550, lr=0.000227, gnorm=1.749, clip=0, train_wall=7, gb_free=72.1, wall=2236 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:53:08]    INFO >> epoch 009:   1323 / 1539 loss=3.326, wps=4505.2, ups=6.29, wpb=716.7, bsz=716.7, num_updates=13600, lr=0.000227, gnorm=1.925, clip=0, train_wall=7, gb_free=74.7, wall=2243 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:53:16]    INFO >> epoch 009:   1373 / 1539 loss=2.831, wps=4777.5, ups=6.4, wpb=746, bsz=746, num_updates=13650, lr=0.000227, gnorm=1.504, clip=0, train_wall=7, gb_free=72.4, wall=2251 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:53:22] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 47.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 76.30 GiB is allocated by PyTorch, and 2.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:53:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:53:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:53:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 36           |        cudaMalloc retries: 57        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78068 MiB |  78128 MiB | 416972 GiB | 416895 GiB |
|       from large pool |  77683 MiB |  77743 MiB | 414794 GiB | 414718 GiB |
|       from small pool |    384 MiB |    386 MiB |   2177 GiB |   2177 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78068 MiB |  78128 MiB | 416972 GiB | 416895 GiB |
|       from large pool |  77683 MiB |  77743 MiB | 414794 GiB | 414718 GiB |
|       from small pool |    384 MiB |    386 MiB |   2177 GiB |   2177 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78027 MiB |  78087 MiB | 416371 GiB | 416295 GiB |
|       from large pool |  77644 MiB |  77704 MiB | 414197 GiB | 414121 GiB |
|       from small pool |    382 MiB |    383 MiB |   2174 GiB |   2173 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80458 MiB |  80460 MiB | 516136 MiB | 435678 MiB |
|       from large pool |  80034 MiB |  80034 MiB | 510544 MiB | 430510 MiB |
|       from small pool |    424 MiB |    426 MiB |   5592 MiB |   5168 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2329 MiB |   5159 MiB | 414342 GiB | 414340 GiB |
|       from large pool |   2290 MiB |   5153 MiB | 411876 GiB | 411873 GiB |
|       from small pool |     39 MiB |     41 MiB |   2466 GiB |   2466 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7201    |    7204    |   26111 K  |   26104 K  |
|       from large pool |     875    |     876    |   11781 K  |   11780 K  |
|       from small pool |    6326    |    6329    |   14330 K  |   14324 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7201    |    7204    |   26111 K  |   26104 K  |
|       from large pool |     875    |     876    |   11781 K  |   11780 K  |
|       from small pool |    6326    |    6329    |   14330 K  |   14324 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     380    |     381    |    5354    |    4974    |
|       from large pool |     168    |     168    |    2558    |    2390    |
|       from small pool |     212    |     213    |    2796    |    2584    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     494    |     495    |   14505 K  |   14505 K  |
|       from large pool |     109    |     109    |    7442 K  |    7441 K  |
|       from small pool |     385    |     386    |    7063 K  |    7063 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:53:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:53:22] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:53:24]    INFO >> epoch 009:   1424 / 1539 loss=3.178, wps=4354.1, ups=6.42, wpb=678.2, bsz=678.2, num_updates=13700, lr=0.000227, gnorm=1.644, clip=0, train_wall=7, gb_free=70.4, wall=2259 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:53:31]    INFO >> epoch 009:   1474 / 1539 loss=3.238, wps=4307, ups=6.8, wpb=633.1, bsz=633.1, num_updates=13750, lr=0.000227, gnorm=1.787, clip=0, train_wall=7, gb_free=70.7, wall=2266 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:53:40]    INFO >> epoch 009:   1524 / 1539 loss=3.211, wps=4289, ups=6.88, wpb=623.5, bsz=623.5, num_updates=13800, lr=0.000227, gnorm=1.829, clip=0, train_wall=7, gb_free=62.4, wall=2274 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:53:42]    INFO >> epoch 009 | loss 3.193 | wps 4382.6 | ups 6.15 | wpb 712.7 | bsz 712.7 | num_updates 13815 | lr 0.000227 | gnorm 1.74 | clip 0 | train_wall 219 | gb_free 74.3 | wall 2276 (progress_bar.py:267, print())[0m
[33m[2025-11-21 14:53:42] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:53:55]    INFO >> epoch 009 | valid on 'valid' subset | loss 3.499 | wps 11151.6 | wpb 5412.5 | bsz 5412.5 | num_updates 13815 | best_loss 3.524 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 14:53:56]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 14:53:56]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_last.pt (epoch 9 @ 13815 updates, score 3.499) (writing took 0.014414 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 14:53:56] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:54:02]    INFO >> epoch 010:     35 / 1539 loss=3.047, wps=1559.8, ups=2.22, wpb=703.3, bsz=703.3, num_updates=13850, lr=0.000193, gnorm=1.805, clip=0, train_wall=8, gb_free=74.2, wall=2296 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:54:11]    INFO >> epoch 010:     85 / 1539 loss=3.192, wps=4770.8, ups=6.7, wpb=711.9, bsz=711.9, num_updates=13900, lr=0.000193, gnorm=1.978, clip=0, train_wall=7, gb_free=69.9, wall=2304 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:54:18]    INFO >> epoch 010:    135 / 1539 loss=3.167, wps=4366.7, ups=6.9, wpb=632.6, bsz=632.6, num_updates=13950, lr=0.000193, gnorm=1.686, clip=0, train_wall=7, gb_free=72.3, wall=2311 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:54:26]    INFO >> epoch 010:    185 / 1539 loss=3.255, wps=4553.5, ups=6.08, wpb=748.6, bsz=748.6, num_updates=14000, lr=0.000193, gnorm=1.9, clip=0, train_wall=8, gb_free=70, wall=2319 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:54:35]    INFO >> epoch 010:    235 / 1539 loss=3.015, wps=5249.6, ups=6.08, wpb=862.7, bsz=862.7, num_updates=14050, lr=0.000193, gnorm=1.753, clip=0, train_wall=8, gb_free=32.2, wall=2327 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:54:43]    INFO >> epoch 010:    285 / 1539 loss=3.229, wps=4452.1, ups=6.57, wpb=677.9, bsz=677.9, num_updates=14100, lr=0.000193, gnorm=1.696, clip=0, train_wall=7, gb_free=73.6, wall=2335 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:54:50] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.29 GiB is free. Including non-PyTorch memory, this process has 77.83 GiB memory in use. Of the allocated memory 71.77 GiB is allocated by PyTorch, and 5.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:54:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:54:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:54:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 37           |        cudaMalloc retries: 58        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73095 MiB |  73493 MiB | 433846 GiB | 433774 GiB |
|       from large pool |  73086 MiB |  73484 MiB | 431573 GiB | 431501 GiB |
|       from small pool |      8 MiB |     13 MiB |   2273 GiB |   2273 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73095 MiB |  73493 MiB | 433846 GiB | 433774 GiB |
|       from large pool |  73086 MiB |  73484 MiB | 431573 GiB | 431501 GiB |
|       from small pool |      8 MiB |     13 MiB |   2273 GiB |   2273 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73086 MiB |  73484 MiB | 433221 GiB | 433150 GiB |
|       from large pool |  73078 MiB |  73475 MiB | 430951 GiB | 430880 GiB |
|       from small pool |      8 MiB |     13 MiB |   2269 GiB |   2269 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79186 MiB |  80398 MiB | 516136 MiB | 436950 MiB |
|       from large pool |  79164 MiB |  79974 MiB | 510544 MiB | 431380 MiB |
|       from small pool |     22 MiB |    424 MiB |   5592 MiB |   5570 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5970 MiB |   7721 MiB | 428325 GiB | 428319 GiB |
|       from large pool |   5957 MiB |   7708 MiB | 425753 GiB | 425747 GiB |
|       from small pool |     13 MiB |     17 MiB |   2571 GiB |   2571 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     571    |     577    |   27152 K  |   27152 K  |
|       from large pool |     286    |     292    |   12192 K  |   12192 K  |
|       from small pool |     285    |     342    |   14959 K  |   14959 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     571    |     577    |   27152 K  |   27152 K  |
|       from large pool |     286    |     292    |   12192 K  |   12192 K  |
|       from small pool |     285    |     342    |   14959 K  |   14959 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     168    |     379    |    5354    |    5186    |
|       from large pool |     157    |     167    |    2558    |    2401    |
|       from small pool |      11    |     212    |    2796    |    2785    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     168    |     170    |   15107 K  |   15107 K  |
|       from large pool |     145    |     147    |    7710 K  |    7710 K  |
|       from small pool |      23    |      42    |    7397 K  |    7397 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:54:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:54:50] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:54:51]    INFO >> epoch 010:    336 / 1539 loss=3.125, wps=4519.7, ups=6.44, wpb=702.1, bsz=702.1, num_updates=14150, lr=0.000193, gnorm=1.791, clip=0, train_wall=7, gb_free=71.5, wall=2343 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:54:59]    INFO >> epoch 010:    386 / 1539 loss=3.261, wps=4598.9, ups=6.39, wpb=719.9, bsz=719.9, num_updates=14200, lr=0.000193, gnorm=1.748, clip=0, train_wall=7, gb_free=71.5, wall=2351 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:55:07]    INFO >> epoch 010:    436 / 1539 loss=3.127, wps=4247.9, ups=6.7, wpb=634.1, bsz=634.1, num_updates=14250, lr=0.000193, gnorm=1.539, clip=0, train_wall=7, gb_free=73.6, wall=2358 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:55:16]    INFO >> epoch 010:    486 / 1539 loss=3.178, wps=4244.1, ups=6.33, wpb=670.3, bsz=670.3, num_updates=14300, lr=0.000193, gnorm=1.725, clip=0, train_wall=7, gb_free=62.2, wall=2366 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:55:23]    INFO >> epoch 010:    536 / 1539 loss=3.093, wps=4600.3, ups=7.14, wpb=644.7, bsz=644.7, num_updates=14350, lr=0.000193, gnorm=1.62, clip=0, train_wall=7, gb_free=74.1, wall=2373 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:55:30]    INFO >> epoch 010:    586 / 1539 loss=3.308, wps=4287.5, ups=6.52, wpb=657.2, bsz=657.2, num_updates=14400, lr=0.000193, gnorm=1.743, clip=0, train_wall=7, gb_free=72.5, wall=2381 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:55:38]    INFO >> epoch 010:    636 / 1539 loss=3.23, wps=4926.7, ups=6.81, wpb=724, bsz=724, num_updates=14450, lr=0.000193, gnorm=1.696, clip=0, train_wall=7, gb_free=70.3, wall=2388 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:55:45]    INFO >> epoch 010:    686 / 1539 loss=3.217, wps=4858.4, ups=6.64, wpb=731.6, bsz=731.6, num_updates=14500, lr=0.000193, gnorm=1.67, clip=0, train_wall=7, gb_free=73.7, wall=2396 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:55:50] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 15.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 76.36 GiB is allocated by PyTorch, and 2.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:55:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:55:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:55:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 38           |        cudaMalloc retries: 59        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78131 MiB |  78191 MiB | 444284 GiB | 444208 GiB |
|       from large pool |  77746 MiB |  77806 MiB | 441959 GiB | 441883 GiB |
|       from small pool |    385 MiB |    386 MiB |   2324 GiB |   2324 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78131 MiB |  78191 MiB | 444284 GiB | 444208 GiB |
|       from large pool |  77746 MiB |  77806 MiB | 441959 GiB | 441883 GiB |
|       from small pool |    385 MiB |    386 MiB |   2324 GiB |   2324 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78087 MiB |  78147 MiB | 443645 GiB | 443569 GiB |
|       from large pool |  77704 MiB |  77763 MiB | 441323 GiB | 441248 GiB |
|       from small pool |    383 MiB |    384 MiB |   2321 GiB |   2320 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80490 MiB |  80490 MiB | 517560 MiB | 437070 MiB |
|       from large pool |  80064 MiB |  80064 MiB | 511564 MiB | 431500 MiB |
|       from small pool |    426 MiB |    426 MiB |   5996 MiB |   5570 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2298 MiB |   5190 MiB | 438571 GiB | 438569 GiB |
|       from large pool |   2257 MiB |   5185 MiB | 435940 GiB | 435938 GiB |
|       from small pool |     40 MiB |     41 MiB |   2631 GiB |   2631 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7212    |    7215    |   27814 K  |   27807 K  |
|       from large pool |     876    |     877    |   12512 K  |   12511 K  |
|       from small pool |    6336    |    6339    |   15301 K  |   15295 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7212    |    7215    |   27814 K  |   27807 K  |
|       from large pool |     876    |     877    |   12512 K  |   12511 K  |
|       from small pool |    6336    |    6339    |   15301 K  |   15295 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     385    |     385    |    5573    |    5188    |
|       from large pool |     172    |     172    |    2575    |    2403    |
|       from small pool |     213    |     213    |    2998    |    2785    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     493    |     494    |   15471 K  |   15471 K  |
|       from large pool |     108    |     108    |    7916 K  |    7916 K  |
|       from small pool |     385    |     386    |    7555 K  |    7554 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:55:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:55:50] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:55:54]    INFO >> epoch 010:    737 / 1539 loss=3.248, wps=4121.9, ups=6.55, wpb=629.4, bsz=629.4, num_updates=14550, lr=0.000193, gnorm=1.703, clip=0, train_wall=7, gb_free=72.2, wall=2403 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:56:02]    INFO >> epoch 010:    787 / 1539 loss=3.114, wps=4626.3, ups=6.33, wpb=730.4, bsz=730.4, num_updates=14600, lr=0.000193, gnorm=1.744, clip=0, train_wall=7, gb_free=74.5, wall=2411 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:56:10]    INFO >> epoch 010:    837 / 1539 loss=3.313, wps=4391.2, ups=6.51, wpb=674.5, bsz=674.5, num_updates=14650, lr=0.000193, gnorm=1.836, clip=0, train_wall=7, gb_free=69.8, wall=2419 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:56:18]    INFO >> epoch 010:    887 / 1539 loss=3.088, wps=5167.8, ups=6.07, wpb=850.8, bsz=850.8, num_updates=14700, lr=0.000193, gnorm=1.881, clip=0, train_wall=8, gb_free=50.1, wall=2427 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:56:27]    INFO >> epoch 010:    937 / 1539 loss=3.146, wps=4838.6, ups=6.75, wpb=716.4, bsz=716.4, num_updates=14750, lr=0.000193, gnorm=1.858, clip=0, train_wall=7, gb_free=70.5, wall=2434 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:56:34]    INFO >> epoch 010:    987 / 1539 loss=3.318, wps=4753.4, ups=6.89, wpb=689.5, bsz=689.5, num_updates=14800, lr=0.000193, gnorm=1.5, clip=0, train_wall=7, gb_free=69, wall=2442 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:56:41]    INFO >> epoch 010:   1037 / 1539 loss=3.299, wps=4607.2, ups=6.84, wpb=673.2, bsz=673.2, num_updates=14850, lr=0.000193, gnorm=1.677, clip=0, train_wall=7, gb_free=70.9, wall=2449 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:56:49]    INFO >> epoch 010:   1087 / 1539 loss=3.135, wps=5082.4, ups=6.36, wpb=798.6, bsz=798.6, num_updates=14900, lr=0.000193, gnorm=1.811, clip=0, train_wall=7, gb_free=74.2, wall=2457 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:56:57]    INFO >> epoch 010:   1137 / 1539 loss=3.137, wps=4104.8, ups=6.72, wpb=611, bsz=611, num_updates=14950, lr=0.000193, gnorm=1.566, clip=0, train_wall=7, gb_free=71.5, wall=2464 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:57:04] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 7.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 76.64 GiB is allocated by PyTorch, and 1.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:57:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:57:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:57:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 39           |        cudaMalloc retries: 61        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78422 MiB |  78482 MiB | 457329 GiB | 457252 GiB |
|       from large pool |  78292 MiB |  78352 MiB | 454939 GiB | 454862 GiB |
|       from small pool |    129 MiB |    130 MiB |   2390 GiB |   2389 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78422 MiB |  78482 MiB | 457329 GiB | 457252 GiB |
|       from large pool |  78292 MiB |  78352 MiB | 454939 GiB | 454862 GiB |
|       from small pool |    129 MiB |    130 MiB |   2390 GiB |   2389 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78398 MiB |  78458 MiB | 456669 GiB | 456592 GiB |
|       from large pool |  78269 MiB |  78329 MiB | 454282 GiB | 454206 GiB |
|       from small pool |    129 MiB |    130 MiB |   2386 GiB |   2386 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80498 MiB |  80500 MiB | 517926 MiB | 437428 MiB |
|       from large pool |  80364 MiB |  80364 MiB | 511924 MiB | 431560 MiB |
|       from small pool |    134 MiB |    426 MiB |   6002 MiB |   5868 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2015 MiB |   9719 MiB | 451014 GiB | 451012 GiB |
|       from large pool |   2011 MiB |   9712 MiB | 448308 GiB | 448306 GiB |
|       from small pool |      4 MiB |     17 MiB |   2706 GiB |   2706 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2658    |    2661    |   28636 K  |   28634 K  |
|       from large pool |     476    |     477    |   12905 K  |   12905 K  |
|       from small pool |    2182    |    2185    |   15731 K  |   15729 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2658    |    2661    |   28636 K  |   28634 K  |
|       from large pool |     476    |     477    |   12905 K  |   12905 K  |
|       from small pool |    2182    |    2185    |   15731 K  |   15729 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     244    |     385    |    5582    |    5338    |
|       from large pool |     177    |     177    |    2581    |    2404    |
|       from small pool |      67    |     213    |    3001    |    2934    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     214    |     215    |   15931 K  |   15931 K  |
|       from large pool |     148    |     151    |    8171 K  |    8171 K  |
|       from small pool |      66    |      67    |    7760 K  |    7760 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:57:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:57:04] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:57:06]    INFO >> epoch 010:   1188 / 1539 loss=3.326, wps=4491.4, ups=5.71, wpb=786.7, bsz=786.7, num_updates=15000, lr=0.000193, gnorm=1.754, clip=0, train_wall=8, gb_free=72.7, wall=2473 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:57:13]    INFO >> epoch 010:   1238 / 1539 loss=3.27, wps=4966.3, ups=6.26, wpb=793.2, bsz=793.2, num_updates=15050, lr=0.000193, gnorm=1.828, clip=0, train_wall=8, gb_free=72.7, wall=2481 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:57:22]    INFO >> epoch 010:   1288 / 1539 loss=2.97, wps=4694.4, ups=6.14, wpb=764.7, bsz=764.7, num_updates=15100, lr=0.000193, gnorm=1.714, clip=0, train_wall=8, gb_free=73.6, wall=2489 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:57:32]    INFO >> epoch 010:   1338 / 1539 loss=3.249, wps=4413.2, ups=6.56, wpb=672.9, bsz=672.9, num_updates=15150, lr=0.000193, gnorm=1.839, clip=0, train_wall=7, gb_free=72.5, wall=2497 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:57:40]    INFO >> epoch 010:   1388 / 1539 loss=3.257, wps=4861.4, ups=6.51, wpb=747.1, bsz=747.1, num_updates=15200, lr=0.000193, gnorm=1.902, clip=0, train_wall=7, gb_free=70, wall=2504 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:57:47]    INFO >> epoch 010:   1438 / 1539 loss=3.167, wps=4655.1, ups=6.79, wpb=685.8, bsz=685.8, num_updates=15250, lr=0.000193, gnorm=1.564, clip=0, train_wall=7, gb_free=74.5, wall=2512 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:57:53] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 239.25 MiB is free. Including non-PyTorch memory, this process has 78.88 GiB memory in use. Of the allocated memory 76.03 GiB is allocated by PyTorch, and 2.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:57:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:57:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:57:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 40           |        cudaMalloc retries: 64        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77368 MiB |  77855 MiB | 465578 GiB | 465503 GiB |
|       from large pool |  77357 MiB |  77844 MiB | 463148 GiB | 463073 GiB |
|       from small pool |     11 MiB |     24 MiB |   2430 GiB |   2430 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77368 MiB |  77855 MiB | 465578 GiB | 465503 GiB |
|       from large pool |  77357 MiB |  77844 MiB | 463148 GiB | 463073 GiB |
|       from small pool |     11 MiB |     24 MiB |   2430 GiB |   2430 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77354 MiB |  77841 MiB | 464907 GiB | 464831 GiB |
|       from large pool |  77342 MiB |  77830 MiB | 462480 GiB | 462404 GiB |
|       from small pool |     11 MiB |     24 MiB |   2426 GiB |   2426 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80266 MiB |  80326 MiB | 550316 MiB | 470050 MiB |
|       from large pool |  80244 MiB |  80304 MiB | 544244 MiB | 464000 MiB |
|       from small pool |     22 MiB |    204 MiB |   6072 MiB |   6050 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2897 MiB |   7189 MiB | 459479 GiB | 459476 GiB |
|       from large pool |   2886 MiB |   7178 MiB | 456726 GiB | 456723 GiB |
|       from small pool |     10 MiB |     23 MiB |   2753 GiB |   2753 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     564    |     568    |   29148 K  |   29147 K  |
|       from large pool |     279    |     283    |   13152 K  |   13151 K  |
|       from small pool |     285    |     356    |   15996 K  |   15996 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     564    |     568    |   29148 K  |   29147 K  |
|       from large pool |     279    |     283    |   13152 K  |   13151 K  |
|       from small pool |     285    |     356    |   15996 K  |   15996 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     122    |     237    |    5647    |    5525    |
|       from large pool |     111    |     135    |    2611    |    2500    |
|       from small pool |      11    |     102    |    3036    |    3025    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     122    |     122    |   16212 K  |   16211 K  |
|       from large pool |     100    |     100    |    8326 K  |    8326 K  |
|       from small pool |      22    |      48    |    7885 K  |    7885 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:57:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:57:53] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:57:55]    INFO >> epoch 010:   1489 / 1539 loss=3.465, wps=4180.7, ups=6.27, wpb=667.2, bsz=667.2, num_updates=15300, lr=0.000193, gnorm=1.945, clip=0, train_wall=7, gb_free=68.9, wall=2520 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:58:03]    INFO >> epoch 010:   1539 / 1539 loss=2.99, wps=4760.7, ups=6.42, wpb=741.8, bsz=741.8, num_updates=15350, lr=0.000193, gnorm=1.878, clip=0, train_wall=7, gb_free=71, wall=2528 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:58:03]    INFO >> epoch 010 | loss 3.188 | wps 4349.7 | ups 6.1 | wpb 712.7 | bsz 712.7 | num_updates 15350 | lr 0.000193 | gnorm 1.751 | clip 0 | train_wall 221 | gb_free 71 | wall 2528 (progress_bar.py:267, print())[0m
[33m[2025-11-21 14:58:03] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:58:17]    INFO >> epoch 010 | valid on 'valid' subset | loss 3.489 | wps 10321.5 | wpb 5412.5 | bsz 5412.5 | num_updates 15350 | best_loss 3.524 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 14:58:18]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 14:58:18]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_last.pt (epoch 10 @ 15350 updates, score 3.489) (writing took 0.020486 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 14:58:18] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:58:25]    INFO >> epoch 011:     50 / 1539 loss=3.248, wps=1459.6, ups=2.22, wpb=658.6, bsz=658.6, num_updates=15400, lr=0.000161, gnorm=1.753, clip=0, train_wall=7, gb_free=72, wall=2550 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:58:36]    INFO >> epoch 011:    100 / 1539 loss=3.278, wps=4658.9, ups=6.39, wpb=728.7, bsz=728.7, num_updates=15450, lr=0.000161, gnorm=1.594, clip=0, train_wall=7, gb_free=72.2, wall=2558 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:58:43]    INFO >> epoch 011:    150 / 1539 loss=3.213, wps=4780.1, ups=7.13, wpb=670.8, bsz=670.8, num_updates=15500, lr=0.000161, gnorm=1.748, clip=0, train_wall=7, gb_free=71.9, wall=2565 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:58:51]    INFO >> epoch 011:    200 / 1539 loss=2.83, wps=4942, ups=6.47, wpb=764, bsz=764, num_updates=15550, lr=0.000161, gnorm=1.7, clip=0, train_wall=7, gb_free=73, wall=2573 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:58:58]    INFO >> epoch 011:    250 / 1539 loss=3.197, wps=5013.6, ups=6.84, wpb=733, bsz=733, num_updates=15600, lr=0.000161, gnorm=1.627, clip=0, train_wall=7, gb_free=70.3, wall=2580 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:59:05]    INFO >> epoch 011:    300 / 1539 loss=3.261, wps=4191.2, ups=6.74, wpb=621.7, bsz=621.7, num_updates=15650, lr=0.000161, gnorm=1.531, clip=0, train_wall=7, gb_free=73.2, wall=2587 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:59:15]    INFO >> epoch 011:    350 / 1539 loss=3.156, wps=4721.9, ups=6.31, wpb=748.4, bsz=748.4, num_updates=15700, lr=0.000161, gnorm=1.942, clip=0, train_wall=7, gb_free=76.1, wall=2595 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:59:22]    INFO >> epoch 011:    400 / 1539 loss=3.285, wps=4634.9, ups=6.83, wpb=678.8, bsz=678.8, num_updates=15750, lr=0.000161, gnorm=1.822, clip=0, train_wall=7, gb_free=72, wall=2603 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:59:29]    INFO >> epoch 011:    450 / 1539 loss=3.362, wps=4321.4, ups=6.96, wpb=620.6, bsz=620.6, num_updates=15800, lr=0.000161, gnorm=1.882, clip=0, train_wall=7, gb_free=72.8, wall=2610 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:59:37]    INFO >> epoch 011:    500 / 1539 loss=3.291, wps=3663.3, ups=6.01, wpb=609.6, bsz=609.6, num_updates=15850, lr=0.000161, gnorm=1.668, clip=0, train_wall=8, gb_free=72.7, wall=2618 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:59:45]    INFO >> epoch 011:    550 / 1539 loss=3.306, wps=4400.9, ups=6.79, wpb=648.3, bsz=648.3, num_updates=15900, lr=0.000161, gnorm=1.749, clip=0, train_wall=7, gb_free=73.4, wall=2626 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:59:46] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 19.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 76.64 GiB is allocated by PyTorch, and 1.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:59:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:59:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:59:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 41           |        cudaMalloc retries: 66        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78417 MiB |  78477 MiB | 486641 GiB | 486565 GiB |
|       from large pool |  78030 MiB |  78090 MiB | 484093 GiB | 484016 GiB |
|       from small pool |    387 MiB |    388 MiB |   2548 GiB |   2548 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78417 MiB |  78477 MiB | 486641 GiB | 486565 GiB |
|       from large pool |  78030 MiB |  78090 MiB | 484093 GiB | 484016 GiB |
|       from small pool |    387 MiB |    388 MiB |   2548 GiB |   2548 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78267 MiB |  78327 MiB | 485939 GiB | 485863 GiB |
|       from large pool |  77882 MiB |  77942 MiB | 483395 GiB | 483318 GiB |
|       from small pool |    385 MiB |    386 MiB |   2544 GiB |   2544 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80486 MiB |  80504 MiB | 565602 MiB | 485116 MiB |
|       from large pool |  80058 MiB |  80244 MiB | 559124 MiB | 479066 MiB |
|       from small pool |    428 MiB |    428 MiB |   6478 MiB |   6050 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2008 MiB |   5588 MiB | 480894 GiB | 480892 GiB |
|       from large pool |   1967 MiB |   5584 MiB | 478010 GiB | 478009 GiB |
|       from small pool |     40 MiB |     41 MiB |   2883 GiB |   2883 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7245    |    7248    |   30483 K  |   30476 K  |
|       from large pool |     879    |     880    |   13705 K  |   13704 K  |
|       from small pool |    6366    |    6369    |   16778 K  |   16771 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7245    |    7248    |   30483 K  |   30476 K  |
|       from large pool |     879    |     880    |   13705 K  |   13704 K  |
|       from small pool |    6366    |    6369    |   16778 K  |   16771 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     566    |     566    |    6098    |    5532    |
|       from large pool |     352    |     352    |    2859    |    2507    |
|       from small pool |     214    |     214    |    3239    |    3025    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     489    |     489    |   16949 K  |   16949 K  |
|       from large pool |     104    |     104    |    8661 K  |    8661 K  |
|       from small pool |     385    |     385    |    8288 K  |    8288 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:59:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:59:46] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:59:53]    INFO >> epoch 011:    601 / 1539 loss=3.216, wps=4084.7, ups=5.87, wpb=696, bsz=696, num_updates=15950, lr=0.000161, gnorm=1.659, clip=0, train_wall=7, gb_free=71.6, wall=2634 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:00:01]    INFO >> epoch 011:    651 / 1539 loss=3.316, wps=5058.6, ups=6.23, wpb=812, bsz=812, num_updates=16000, lr=0.000161, gnorm=1.619, clip=0, train_wall=8, gb_free=69.8, wall=2642 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:00:09]    INFO >> epoch 011:    701 / 1539 loss=3.057, wps=4415.9, ups=6.71, wpb=657.7, bsz=657.7, num_updates=16050, lr=0.000161, gnorm=1.868, clip=0, train_wall=7, gb_free=75.1, wall=2650 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:00:20]    INFO >> epoch 011:    751 / 1539 loss=3.199, wps=4311.2, ups=5.85, wpb=736.4, bsz=736.4, num_updates=16100, lr=0.000161, gnorm=1.644, clip=0, train_wall=8, gb_free=70.8, wall=2658 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:00:26] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 301.25 MiB is free. Including non-PyTorch memory, this process has 78.82 GiB memory in use. Of the allocated memory 76.03 GiB is allocated by PyTorch, and 2.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:00:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:00:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:00:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 42           |        cudaMalloc retries: 68        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77369 MiB |  77856 MiB | 493259 GiB | 493183 GiB |
|       from large pool |  77357 MiB |  77845 MiB | 490678 GiB | 490603 GiB |
|       from small pool |     11 MiB |     21 MiB |   2580 GiB |   2580 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77369 MiB |  77856 MiB | 493259 GiB | 493183 GiB |
|       from large pool |  77357 MiB |  77845 MiB | 490678 GiB | 490603 GiB |
|       from small pool |     11 MiB |     21 MiB |   2580 GiB |   2580 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77354 MiB |  77841 MiB | 492547 GiB | 492471 GiB |
|       from large pool |  77342 MiB |  77830 MiB | 489971 GiB | 489895 GiB |
|       from small pool |     11 MiB |     21 MiB |   2576 GiB |   2576 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80204 MiB |  80426 MiB | 580666 MiB | 500462 MiB |
|       from large pool |  80182 MiB |  80242 MiB | 574188 MiB | 494006 MiB |
|       from small pool |     22 MiB |    428 MiB |   6478 MiB |   6456 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2834 MiB |   4860 MiB | 487011 GiB | 487008 GiB |
|       from large pool |   2824 MiB |   4849 MiB | 484090 GiB | 484088 GiB |
|       from small pool |     10 MiB |     17 MiB |   2920 GiB |   2920 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     564    |     568    |   30891 K  |   30890 K  |
|       from large pool |     279    |     283    |   13901 K  |   13901 K  |
|       from small pool |     285    |     356    |   16989 K  |   16989 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     564    |     568    |   30891 K  |   30890 K  |
|       from large pool |     279    |     283    |   13901 K  |   13901 K  |
|       from small pool |     285    |     356    |   16989 K  |   16989 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     126    |     565    |    6110    |    5984    |
|       from large pool |     115    |     351    |    2871    |    2756    |
|       from small pool |      11    |     214    |    3239    |    3228    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     127    |     127    |   17179 K  |   17179 K  |
|       from large pool |     104    |     104    |    8789 K  |    8789 K  |
|       from small pool |      23    |      49    |    8389 K  |    8389 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:00:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:00:26] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:00:28]    INFO >> epoch 011:    802 / 1539 loss=3.268, wps=3768.2, ups=6.45, wpb=584.6, bsz=584.6, num_updates=16150, lr=0.000161, gnorm=1.721, clip=0, train_wall=6, gb_free=75.4, wall=2666 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:00:35]    INFO >> epoch 011:    852 / 1539 loss=3.155, wps=4382.6, ups=6.71, wpb=653.4, bsz=653.4, num_updates=16200, lr=0.000161, gnorm=1.65, clip=0, train_wall=7, gb_free=75.3, wall=2673 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:00:43]    INFO >> epoch 011:    902 / 1539 loss=3.206, wps=5316, ups=6.75, wpb=787.2, bsz=787.2, num_updates=16250, lr=0.000161, gnorm=1.757, clip=0, train_wall=7, gb_free=73, wall=2681 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:00:50]    INFO >> epoch 011:    952 / 1539 loss=3.18, wps=4727.6, ups=6.43, wpb=735.8, bsz=735.8, num_updates=16300, lr=0.000161, gnorm=1.84, clip=0, train_wall=7, gb_free=72, wall=2688 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:00:58]    INFO >> epoch 011:   1002 / 1539 loss=3.071, wps=4809.9, ups=6.52, wpb=737.4, bsz=737.4, num_updates=16350, lr=0.000161, gnorm=1.806, clip=0, train_wall=7, gb_free=71.3, wall=2696 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:01:06]    INFO >> epoch 011:   1052 / 1539 loss=3.034, wps=4542, ups=6.49, wpb=699.4, bsz=699.4, num_updates=16400, lr=0.000161, gnorm=1.577, clip=0, train_wall=7, gb_free=71.5, wall=2704 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:01:11] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 3.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.16 GiB is allocated by PyTorch, and 1.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:01:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:01:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:01:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 43           |        cudaMalloc retries: 69        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78954 MiB |  79014 MiB | 501348 GiB | 501271 GiB |
|       from large pool |  78819 MiB |  78879 MiB | 498727 GiB | 498650 GiB |
|       from small pool |    135 MiB |    136 MiB |   2620 GiB |   2620 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78954 MiB |  79014 MiB | 501348 GiB | 501271 GiB |
|       from large pool |  78819 MiB |  78879 MiB | 498727 GiB | 498650 GiB |
|       from small pool |    135 MiB |    136 MiB |   2620 GiB |   2620 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78939 MiB |  78999 MiB | 500625 GiB | 500548 GiB |
|       from large pool |  78805 MiB |  78864 MiB | 498009 GiB | 497932 GiB |
|       from small pool |    134 MiB |    135 MiB |   2616 GiB |   2616 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80502 MiB |  80502 MiB | 580964 MiB | 500462 MiB |
|       from large pool |  80362 MiB |  80362 MiB | 574368 MiB | 494006 MiB |
|       from small pool |    140 MiB |    140 MiB |   6596 MiB |   6456 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1487 MiB |   6604 MiB | 496218 GiB | 496217 GiB |
|       from large pool |   1482 MiB |   6595 MiB | 493251 GiB | 493250 GiB |
|       from small pool |      4 MiB |     21 MiB |   2967 GiB |   2967 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2757    |    2760    |   31395 K  |   31392 K  |
|       from large pool |     485    |     486    |   14142 K  |   14142 K  |
|       from small pool |    2272    |    2275    |   17252 K  |   17250 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2757    |    2760    |   31395 K  |   31392 K  |
|       from large pool |     485    |     486    |   14142 K  |   14142 K  |
|       from small pool |    2272    |    2275    |   17252 K  |   17250 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     188    |     188    |    6172    |    5984    |
|       from large pool |     118    |     118    |    2874    |    2756    |
|       from small pool |      70    |      70    |    3298    |    3228    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     172    |     172    |   17446 K  |   17446 K  |
|       from large pool |     103    |     103    |    8934 K  |    8933 K  |
|       from small pool |      69    |      69    |    8512 K  |    8512 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:01:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:01:11] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:01:15]    INFO >> epoch 011:   1103 / 1539 loss=3.157, wps=4367, ups=5.68, wpb=768.3, bsz=768.3, num_updates=16450, lr=0.000161, gnorm=1.795, clip=0, train_wall=8, gb_free=71.6, wall=2713 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:01:26]    INFO >> epoch 011:   1153 / 1539 loss=2.96, wps=5464.4, ups=6.03, wpb=906.2, bsz=906.2, num_updates=16500, lr=0.000161, gnorm=1.86, clip=0, train_wall=8, gb_free=74.5, wall=2721 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:01:33]    INFO >> epoch 011:   1203 / 1539 loss=3.14, wps=3850.1, ups=6.59, wpb=584.3, bsz=584.3, num_updates=16550, lr=0.000161, gnorm=1.759, clip=0, train_wall=7, gb_free=76.3, wall=2729 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:01:42]    INFO >> epoch 011:   1253 / 1539 loss=3.147, wps=4819.1, ups=5.72, wpb=842.2, bsz=842.2, num_updates=16600, lr=0.000161, gnorm=1.759, clip=0, train_wall=8, gb_free=67.1, wall=2737 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:01:50]    INFO >> epoch 011:   1303 / 1539 loss=2.951, wps=4824.8, ups=6.45, wpb=748.1, bsz=748.1, num_updates=16650, lr=0.000161, gnorm=1.772, clip=0, train_wall=7, gb_free=73.8, wall=2745 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:01:53] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 938.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 505.25 MiB is free. Including non-PyTorch memory, this process has 78.62 GiB memory in use. Of the allocated memory 77.38 GiB is allocated by PyTorch, and 760.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:01:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:01:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:01:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 44           |        cudaMalloc retries: 72        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73487 MiB |  79427 MiB | 508033 GiB | 507961 GiB |
|       from large pool |  73479 MiB |  79418 MiB | 505379 GiB | 505307 GiB |
|       from small pool |      8 MiB |     23 MiB |   2654 GiB |   2654 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73487 MiB |  79427 MiB | 508033 GiB | 507961 GiB |
|       from large pool |  73479 MiB |  79418 MiB | 505379 GiB | 505307 GiB |
|       from small pool |      8 MiB |     23 MiB |   2654 GiB |   2654 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73471 MiB |  79407 MiB | 507301 GiB | 507229 GiB |
|       from large pool |  73462 MiB |  79398 MiB | 504651 GiB | 504579 GiB |
|       from small pool |      8 MiB |     23 MiB |   2649 GiB |   2649 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80000 MiB |  80188 MiB | 681082 MiB | 601082 MiB |
|       from large pool |  79978 MiB |  80166 MiB | 674422 MiB | 594444 MiB |
|       from small pool |     22 MiB |    204 MiB |   6660 MiB |   6638 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2844 MiB |   3827 MiB | 503660 GiB | 503658 GiB |
|       from large pool |   2830 MiB |   3814 MiB | 500655 GiB | 500652 GiB |
|       from small pool |     13 MiB |     21 MiB |   3005 GiB |   3005 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     571    |     589    |   31808 K  |   31808 K  |
|       from large pool |     286    |     304    |   14337 K  |   14337 K  |
|       from small pool |     285    |     356    |   17471 K  |   17470 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     571    |     589    |   31808 K  |   31808 K  |
|       from large pool |     286    |     304    |   14337 K  |   14337 K  |
|       from small pool |     285    |     356    |   17471 K  |   17470 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     144    |     214    |    6312    |    6168    |
|       from large pool |     133    |     134    |    2982    |    2849    |
|       from small pool |      11    |     102    |    3330    |    3319    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     145    |     146    |   17669 K  |   17669 K  |
|       from large pool |     123    |     124    |    9051 K  |    9051 K  |
|       from small pool |      22    |      48    |    8617 K  |    8617 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:01:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:01:53] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:02:01]    INFO >> epoch 011:   1354 / 1539 loss=3.317, wps=3278, ups=5.12, wpb=640.8, bsz=640.8, num_updates=16700, lr=0.000161, gnorm=1.705, clip=0, train_wall=7, gb_free=68.6, wall=2755 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:02:09]    INFO >> epoch 011:   1404 / 1539 loss=3.307, wps=5185.4, ups=6.18, wpb=838.7, bsz=838.7, num_updates=16750, lr=0.000161, gnorm=2.151, clip=0, train_wall=8, gb_free=71.9, wall=2763 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:02:16]    INFO >> epoch 011:   1454 / 1539 loss=3.35, wps=4422.1, ups=6.6, wpb=669.9, bsz=669.9, num_updates=16800, lr=0.000161, gnorm=1.7, clip=0, train_wall=7, gb_free=70.9, wall=2770 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:02:25]    INFO >> epoch 011:   1504 / 1539 loss=3.118, wps=4865.7, ups=5.94, wpb=818.8, bsz=818.8, num_updates=16850, lr=0.000161, gnorm=1.608, clip=0, train_wall=8, gb_free=70.6, wall=2779 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:02:31]    INFO >> epoch 011 | loss 3.182 | wps 4263.6 | ups 5.98 | wpb 712.7 | bsz 712.7 | num_updates 16885 | lr 0.000161 | gnorm 1.742 | clip 0 | train_wall 223 | gb_free 74.1 | wall 2784 (progress_bar.py:267, print())[0m
[33m[2025-11-21 15:02:31] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:02:47]    INFO >> epoch 011 | valid on 'valid' subset | loss 3.498 | wps 9988.2 | wpb 5412.5 | bsz 5412.5 | num_updates 16885 | best_loss 3.524 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 15:02:47]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 15:02:47]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_last.pt (epoch 11 @ 16885 updates, score 3.498) (writing took 0.018486 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 15:02:47] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:02:49]    INFO >> epoch 012:     15 / 1539 loss=3.253, wps=1403.7, ups=2.18, wpb=644.6, bsz=644.6, num_updates=16900, lr=0.000132, gnorm=1.823, clip=0, train_wall=7, gb_free=66.2, wall=2802 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:02:58]    INFO >> epoch 012:     65 / 1539 loss=3.351, wps=3950.1, ups=5.67, wpb=696.4, bsz=696.4, num_updates=16950, lr=0.000132, gnorm=1.838, clip=0, train_wall=8, gb_free=73.1, wall=2811 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:03:07]    INFO >> epoch 012:    115 / 1539 loss=3.168, wps=4461.5, ups=6.82, wpb=654.1, bsz=654.1, num_updates=17000, lr=0.000132, gnorm=1.718, clip=0, train_wall=7, gb_free=73.4, wall=2818 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:03:15]    INFO >> epoch 012:    165 / 1539 loss=2.945, wps=4566, ups=6.34, wpb=720.2, bsz=720.2, num_updates=17050, lr=0.000132, gnorm=1.704, clip=0, train_wall=7, gb_free=72.6, wall=2826 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:03:18] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 43.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 76.61 GiB is allocated by PyTorch, and 1.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:03:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:03:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:03:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 45           |        cudaMalloc retries: 74        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78388 MiB |  78448 MiB | 524014 GiB | 523938 GiB |
|       from large pool |  78259 MiB |  78319 MiB | 521266 GiB | 521190 GiB |
|       from small pool |    128 MiB |    130 MiB |   2748 GiB |   2747 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78388 MiB |  78448 MiB | 524014 GiB | 523938 GiB |
|       from large pool |  78259 MiB |  78319 MiB | 521266 GiB | 521190 GiB |
|       from small pool |    128 MiB |    130 MiB |   2748 GiB |   2747 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78338 MiB |  78398 MiB | 523260 GiB | 523183 GiB |
|       from large pool |  78210 MiB |  78269 MiB | 520516 GiB | 520440 GiB |
|       from small pool |    128 MiB |    129 MiB |   2743 GiB |   2743 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80462 MiB |  80488 MiB | 685300 MiB | 604838 MiB |
|       from large pool |  80330 MiB |  80330 MiB | 678442 MiB | 598112 MiB |
|       from small pool |    132 MiB |    218 MiB |   6858 MiB |   6726 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2013 MiB |   6548 MiB | 518619 GiB | 518617 GiB |
|       from large pool |   2010 MiB |   6539 MiB | 515511 GiB | 515509 GiB |
|       from small pool |      3 MiB |     23 MiB |   3108 GiB |   3108 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2647    |    2650    |   32805 K  |   32802 K  |
|       from large pool |     475    |     476    |   14721 K  |   14720 K  |
|       from small pool |    2172    |    2175    |   18084 K  |   18082 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2647    |    2650    |   32805 K  |   32802 K  |
|       from large pool |     475    |     476    |   14721 K  |   14720 K  |
|       from small pool |    2172    |    2175    |   18084 K  |   18082 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     260    |     302    |    6478    |    6218    |
|       from large pool |     194    |     194    |    3049    |    2855    |
|       from small pool |      66    |     109    |    3429    |    3363    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     191    |     194    |   18244 K  |   18244 K  |
|       from large pool |     129    |     132    |    9296 K  |    9296 K  |
|       from small pool |      62    |      65    |    8948 K  |    8948 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:03:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:03:18] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:03:23]    INFO >> epoch 012:    216 / 1539 loss=3.203, wps=4137.2, ups=6.14, wpb=673.7, bsz=673.7, num_updates=17100, lr=0.000132, gnorm=1.804, clip=0, train_wall=7, gb_free=72.6, wall=2834 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:03:30]    INFO >> epoch 012:    266 / 1539 loss=3.293, wps=4404.2, ups=6.43, wpb=684.7, bsz=684.7, num_updates=17150, lr=0.000132, gnorm=1.738, clip=0, train_wall=7, gb_free=73.2, wall=2842 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:03:38]    INFO >> epoch 012:    316 / 1539 loss=3.37, wps=4951, ups=6.84, wpb=724.2, bsz=724.2, num_updates=17200, lr=0.000132, gnorm=1.771, clip=0, train_wall=7, gb_free=72.9, wall=2849 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:03:39] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.88 GiB. GPU 2 has a total capacity of 79.14 GiB of which 393.25 MiB is free. Including non-PyTorch memory, this process has 78.73 GiB memory in use. Of the allocated memory 72.51 GiB is allocated by PyTorch, and 5.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:03:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:03:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:03:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 46           |        cudaMalloc retries: 76        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  63157 MiB |  75128 MiB | 527654 GiB | 527592 GiB |
|       from large pool |  63146 MiB |  75117 MiB | 524889 GiB | 524827 GiB |
|       from small pool |     11 MiB |     24 MiB |   2765 GiB |   2765 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  63157 MiB |  75128 MiB | 527654 GiB | 527592 GiB |
|       from large pool |  63146 MiB |  75117 MiB | 524889 GiB | 524827 GiB |
|       from small pool |     11 MiB |     24 MiB |   2765 GiB |   2765 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  63147 MiB |  75114 MiB | 526893 GiB | 526832 GiB |
|       from large pool |  63136 MiB |  75103 MiB | 524133 GiB | 524071 GiB |
|       from small pool |     11 MiB |     24 MiB |   2760 GiB |   2760 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80112 MiB |  80402 MiB | 707780 MiB | 627668 MiB |
|       from large pool |  80086 MiB |  80270 MiB | 700922 MiB | 620836 MiB |
|       from small pool |     26 MiB |    132 MiB |   6858 MiB |   6832 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   8838 MiB |  10224 MiB | 521882 GiB | 521874 GiB |
|       from large pool |   8823 MiB |  10209 MiB | 518755 GiB | 518746 GiB |
|       from small pool |     14 MiB |     23 MiB |   3127 GiB |   3127 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     515    |     557    |   33032 K  |   33032 K  |
|       from large pool |     230    |     272    |   14835 K  |   14834 K  |
|       from small pool |     285    |     356    |   18197 K  |   18197 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     515    |     557    |   33032 K  |   33032 K  |
|       from large pool |     230    |     272    |   14835 K  |   14834 K  |
|       from small pool |     285    |     356    |   18197 K  |   18197 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      96    |     259    |    6498    |    6402    |
|       from large pool |      83    |     193    |    3069    |    2986    |
|       from small pool |      13    |      66    |    3429    |    3416    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     101    |     104    |   18371 K  |   18370 K  |
|       from large pool |      78    |      81    |    9372 K  |    9372 K  |
|       from small pool |      23    |      48    |    8998 K  |    8998 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:03:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:03:39] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:03:46]    INFO >> epoch 012:    367 / 1539 loss=3.154, wps=3998.2, ups=5.83, wpb=686.2, bsz=686.2, num_updates=17250, lr=0.000132, gnorm=1.713, clip=0, train_wall=7, gb_free=75.1, wall=2858 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:03:54]    INFO >> epoch 012:    417 / 1539 loss=3.26, wps=4179.8, ups=6.89, wpb=606.7, bsz=606.7, num_updates=17300, lr=0.000132, gnorm=1.798, clip=0, train_wall=7, gb_free=72.9, wall=2865 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:04:02]    INFO >> epoch 012:    467 / 1539 loss=3.311, wps=4513.6, ups=6.14, wpb=735.6, bsz=735.6, num_updates=17350, lr=0.000132, gnorm=1.962, clip=0, train_wall=8, gb_free=74, wall=2873 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:04:12]    INFO >> epoch 012:    517 / 1539 loss=3.2, wps=4824.7, ups=6.45, wpb=747.7, bsz=747.7, num_updates=17400, lr=0.000132, gnorm=1.864, clip=0, train_wall=7, gb_free=73.2, wall=2881 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:04:21]    INFO >> epoch 012:    567 / 1539 loss=3.082, wps=5035.8, ups=5.97, wpb=843.2, bsz=843.2, num_updates=17450, lr=0.000132, gnorm=1.641, clip=0, train_wall=8, gb_free=71.7, wall=2889 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:04:28]    INFO >> epoch 012:    617 / 1539 loss=3.157, wps=4962.7, ups=6.53, wpb=759.8, bsz=759.8, num_updates=17500, lr=0.000132, gnorm=1.869, clip=0, train_wall=7, gb_free=69, wall=2897 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:04:40] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 3.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.10 GiB is allocated by PyTorch, and 1.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:04:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:04:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:04:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 47           |        cudaMalloc retries: 77        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78889 MiB |  79049 MiB | 537027 GiB | 536950 GiB |
|       from large pool |  78498 MiB |  78657 MiB | 534212 GiB | 534135 GiB |
|       from small pool |    391 MiB |    394 MiB |   2815 GiB |   2814 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78889 MiB |  79049 MiB | 537027 GiB | 536950 GiB |
|       from large pool |  78498 MiB |  78657 MiB | 534212 GiB | 534135 GiB |
|       from small pool |    391 MiB |    394 MiB |   2815 GiB |   2814 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78803 MiB |  78960 MiB | 536254 GiB | 536177 GiB |
|       from large pool |  78414 MiB |  78570 MiB | 533443 GiB | 533366 GiB |
|       from small pool |    389 MiB |    392 MiB |   2810 GiB |   2810 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80502 MiB |  80504 MiB | 716288 MiB | 635786 MiB |
|       from large pool |  80070 MiB |  80070 MiB | 709022 MiB | 628952 MiB |
|       from small pool |    432 MiB |    434 MiB |   7266 MiB |   6834 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1552 MiB |   4153 MiB | 532736 GiB | 532735 GiB |
|       from large pool |   1511 MiB |   4123 MiB | 529551 GiB | 529550 GiB |
|       from small pool |     40 MiB |     41 MiB |   3185 GiB |   3185 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7325    |    7358    |   33645 K  |   33638 K  |
|       from large pool |     887    |     890    |   15116 K  |   15115 K  |
|       from small pool |    6438    |    6469    |   18529 K  |   18522 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7325    |    7358    |   33645 K  |   33638 K  |
|       from large pool |     887    |     890    |   15116 K  |   15115 K  |
|       from small pool |    6438    |    6469    |   18529 K  |   18522 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     426    |     427    |    6837    |    6411    |
|       from large pool |     210    |     210    |    3204    |    2994    |
|       from small pool |     216    |     217    |    3633    |    3417    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     472    |     473    |   18702 K  |   18701 K  |
|       from large pool |      78    |      78    |    9542 K  |    9542 K  |
|       from small pool |     394    |     396    |    9159 K  |    9158 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:04:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:04:40] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:04:42]    INFO >> epoch 012:    668 / 1539 loss=2.94, wps=3004.1, ups=3.6, wpb=834.8, bsz=834.8, num_updates=17550, lr=0.000132, gnorm=1.896, clip=0, train_wall=8, gb_free=72.9, wall=2911 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:04:51]    INFO >> epoch 012:    718 / 1539 loss=3.088, wps=4967.4, ups=6.57, wpb=756.5, bsz=756.5, num_updates=17600, lr=0.000132, gnorm=1.68, clip=0, train_wall=7, gb_free=70.5, wall=2918 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:04:59]    INFO >> epoch 012:    768 / 1539 loss=2.886, wps=4993.3, ups=6.09, wpb=819.3, bsz=819.3, num_updates=17650, lr=0.000132, gnorm=1.896, clip=0, train_wall=8, gb_free=77, wall=2927 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:05:07]    INFO >> epoch 012:    818 / 1539 loss=3.284, wps=4436.6, ups=6.7, wpb=662.3, bsz=662.3, num_updates=17700, lr=0.000132, gnorm=1.768, clip=0, train_wall=7, gb_free=71.1, wall=2934 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:05:15]    INFO >> epoch 012:    868 / 1539 loss=3.272, wps=4807.4, ups=6.26, wpb=767.6, bsz=767.6, num_updates=17750, lr=0.000132, gnorm=1.799, clip=0, train_wall=7, gb_free=64.5, wall=2942 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:05:24]    INFO >> epoch 012:    918 / 1539 loss=3.251, wps=4866.1, ups=6, wpb=811.1, bsz=811.1, num_updates=17800, lr=0.000132, gnorm=1.729, clip=0, train_wall=8, gb_free=68.5, wall=2950 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:05:32]    INFO >> epoch 012:    968 / 1539 loss=3.249, wps=4594.9, ups=6.61, wpb=694.9, bsz=694.9, num_updates=17850, lr=0.000132, gnorm=1.714, clip=0, train_wall=7, gb_free=73.8, wall=2958 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:05:40]    INFO >> epoch 012:   1018 / 1539 loss=3.257, wps=4112.8, ups=6.21, wpb=662.6, bsz=662.6, num_updates=17900, lr=0.000132, gnorm=1.778, clip=0, train_wall=8, gb_free=67, wall=2966 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:05:48]    INFO >> epoch 012:   1068 / 1539 loss=3.215, wps=4297.8, ups=6.38, wpb=673.9, bsz=673.9, num_updates=17950, lr=0.000132, gnorm=1.714, clip=0, train_wall=7, gb_free=72, wall=2974 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:05:57]    INFO >> epoch 012:   1118 / 1539 loss=3.211, wps=3908.2, ups=6.56, wpb=595.8, bsz=595.8, num_updates=18000, lr=0.000132, gnorm=1.54, clip=0, train_wall=7, gb_free=76.2, wall=2981 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:05:58] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.78 GiB is free. Including non-PyTorch memory, this process has 77.34 GiB memory in use. Of the allocated memory 71.77 GiB is allocated by PyTorch, and 5.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:05:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:05:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:05:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 48           |        cudaMalloc retries: 79        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73097 MiB |  73495 MiB | 550158 GiB | 550087 GiB |
|       from large pool |  73088 MiB |  73486 MiB | 547277 GiB | 547206 GiB |
|       from small pool |      8 MiB |     13 MiB |   2880 GiB |   2880 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73097 MiB |  73495 MiB | 550158 GiB | 550087 GiB |
|       from large pool |  73088 MiB |  73486 MiB | 547277 GiB | 547206 GiB |
|       from small pool |      8 MiB |     13 MiB |   2880 GiB |   2880 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73086 MiB |  73484 MiB | 549365 GiB | 549294 GiB |
|       from large pool |  73078 MiB |  73475 MiB | 546489 GiB | 546417 GiB |
|       from small pool |      8 MiB |     13 MiB |   2876 GiB |   2876 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78682 MiB |  80442 MiB | 719016 MiB | 640334 MiB |
|       from large pool |  78658 MiB |  80010 MiB | 711750 MiB | 633092 MiB |
|       from small pool |     24 MiB |    432 MiB |   7266 MiB |   7242 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5464 MiB |   6691 MiB | 545298 GiB | 545292 GiB |
|       from large pool |   5449 MiB |   6676 MiB | 542037 GiB | 542032 GiB |
|       from small pool |     15 MiB |     19 MiB |   3260 GiB |   3260 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     568    |     574    |   34463 K  |   34463 K  |
|       from large pool |     286    |     292    |   15506 K  |   15506 K  |
|       from small pool |     282    |     356    |   18956 K  |   18956 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     568    |     574    |   34463 K  |   34463 K  |
|       from large pool |     286    |     292    |   15506 K  |   15506 K  |
|       from small pool |     282    |     356    |   18956 K  |   18956 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     154    |     425    |    6838    |    6684    |
|       from large pool |     142    |     209    |    3205    |    3063    |
|       from small pool |      12    |     216    |    3633    |    3621    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     155    |     157    |   19161 K  |   19161 K  |
|       from large pool |     131    |     133    |    9797 K  |    9797 K  |
|       from small pool |      24    |      52    |    9363 K  |    9363 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:05:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:05:58] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:06:05]    INFO >> epoch 012:   1169 / 1539 loss=3.415, wps=3858.3, ups=5.81, wpb=664.4, bsz=664.4, num_updates=18050, lr=0.000132, gnorm=2.004, clip=0, train_wall=7, gb_free=74.2, wall=2990 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:06:14]    INFO >> epoch 012:   1219 / 1539 loss=3.21, wps=4936.6, ups=6.1, wpb=809.2, bsz=809.2, num_updates=18100, lr=0.000132, gnorm=1.748, clip=0, train_wall=8, gb_free=68.6, wall=2998 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:06:23]    INFO >> epoch 012:   1269 / 1539 loss=3.14, wps=4522.4, ups=5.62, wpb=804.1, bsz=804.1, num_updates=18150, lr=0.000132, gnorm=1.711, clip=0, train_wall=8, gb_free=63.9, wall=3007 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:06:31]    INFO >> epoch 012:   1319 / 1539 loss=3.246, wps=4799.1, ups=6.9, wpb=695.5, bsz=695.5, num_updates=18200, lr=0.000132, gnorm=1.729, clip=0, train_wall=7, gb_free=72.5, wall=3014 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:06:39]    INFO >> epoch 012:   1369 / 1539 loss=3.184, wps=4075.5, ups=6.26, wpb=651.4, bsz=651.4, num_updates=18250, lr=0.000132, gnorm=1.619, clip=0, train_wall=8, gb_free=75, wall=3022 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:06:47]    INFO >> epoch 012:   1419 / 1539 loss=3.053, wps=4161.7, ups=6.59, wpb=631.7, bsz=631.7, num_updates=18300, lr=0.000132, gnorm=1.576, clip=0, train_wall=7, gb_free=61.2, wall=3030 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:06:54]    INFO >> epoch 012:   1469 / 1539 loss=2.917, wps=4619.8, ups=6.58, wpb=701.7, bsz=701.7, num_updates=18350, lr=0.000132, gnorm=1.572, clip=0, train_wall=7, gb_free=69.3, wall=3038 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:07:03]    INFO >> epoch 012:   1519 / 1539 loss=3.219, wps=4593.6, ups=6.43, wpb=714.6, bsz=714.6, num_updates=18400, lr=0.000132, gnorm=1.663, clip=0, train_wall=7, gb_free=62.3, wall=3045 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:07:06]    INFO >> epoch 012 | loss 3.177 | wps 4144.6 | ups 5.82 | wpb 712.7 | bsz 712.7 | num_updates 18420 | lr 0.000132 | gnorm 1.75 | clip 0 | train_wall 226 | gb_free 72.7 | wall 3048 (progress_bar.py:267, print())[0m
[33m[2025-11-21 15:07:06] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:07:22]    INFO >> epoch 012 | valid on 'valid' subset | loss 3.507 | wps 9785.7 | wpb 5412.5 | bsz 5412.5 | num_updates 18420 | best_loss 3.524 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 15:07:22]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 15:07:22]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_last.pt (epoch 12 @ 18420 updates, score 3.507) (writing took 0.013904 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 15:07:22] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:07:27]    INFO >> epoch 013:     30 / 1539 loss=3.082, wps=1527.9, ups=2.08, wpb=734.4, bsz=734.4, num_updates=18450, lr=0.000105, gnorm=1.693, clip=0, train_wall=8, gb_free=72.1, wall=3069 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:07:36]    INFO >> epoch 013:     80 / 1539 loss=3.083, wps=4957.7, ups=6.54, wpb=757.6, bsz=757.6, num_updates=18500, lr=0.000105, gnorm=1.654, clip=0, train_wall=7, gb_free=75.7, wall=3077 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:07:44]    INFO >> epoch 013:    130 / 1539 loss=3.17, wps=4199.7, ups=6.23, wpb=674.4, bsz=674.4, num_updates=18550, lr=0.000105, gnorm=1.64, clip=0, train_wall=8, gb_free=71, wall=3085 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:07:52]    INFO >> epoch 013:    180 / 1539 loss=2.884, wps=4708.9, ups=6.26, wpb=751.9, bsz=751.9, num_updates=18600, lr=0.000105, gnorm=1.628, clip=0, train_wall=7, gb_free=71.2, wall=3093 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:08:01]    INFO >> epoch 013:    230 / 1539 loss=3.213, wps=4295.8, ups=6.14, wpb=699.6, bsz=699.6, num_updates=18650, lr=0.000105, gnorm=1.771, clip=0, train_wall=8, gb_free=70.4, wall=3101 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:08:08]    INFO >> epoch 013:    280 / 1539 loss=3.12, wps=4340.3, ups=6.7, wpb=647.6, bsz=647.6, num_updates=18700, lr=0.000105, gnorm=1.608, clip=0, train_wall=7, gb_free=74.1, wall=3109 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:08:16]    INFO >> epoch 013:    330 / 1539 loss=3.215, wps=4406.5, ups=6.14, wpb=717.6, bsz=717.6, num_updates=18750, lr=0.000105, gnorm=1.6, clip=0, train_wall=8, gb_free=71.1, wall=3117 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:08:24]    INFO >> epoch 013:    380 / 1539 loss=3.339, wps=4136.1, ups=6.4, wpb=646.4, bsz=646.4, num_updates=18800, lr=0.000105, gnorm=1.86, clip=0, train_wall=7, gb_free=71.7, wall=3125 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:08:32]    INFO >> epoch 013:    430 / 1539 loss=3.245, wps=4404, ups=6.12, wpb=719.3, bsz=719.3, num_updates=18850, lr=0.000105, gnorm=1.669, clip=0, train_wall=8, gb_free=70.7, wall=3133 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:08:43]    INFO >> epoch 013:    480 / 1539 loss=3.243, wps=4626.3, ups=6.44, wpb=718.7, bsz=718.7, num_updates=18900, lr=0.000105, gnorm=1.66, clip=0, train_wall=7, gb_free=72.7, wall=3141 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:08:49] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.90 GiB is free. Including non-PyTorch memory, this process has 77.22 GiB memory in use. Of the allocated memory 71.77 GiB is allocated by PyTorch, and 4.95 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:08:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:08:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:08:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 49           |        cudaMalloc retries: 80        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73097 MiB |  73495 MiB | 580485 GiB | 580414 GiB |
|       from large pool |  73088 MiB |  73486 MiB | 577447 GiB | 577375 GiB |
|       from small pool |      8 MiB |     18 MiB |   3038 GiB |   3038 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73097 MiB |  73495 MiB | 580485 GiB | 580414 GiB |
|       from large pool |  73088 MiB |  73486 MiB | 577447 GiB | 577375 GiB |
|       from small pool |      8 MiB |     18 MiB |   3038 GiB |   3038 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73086 MiB |  73484 MiB | 579647 GiB | 579575 GiB |
|       from large pool |  73078 MiB |  73475 MiB | 576613 GiB | 576542 GiB |
|       from small pool |      8 MiB |     18 MiB |   3033 GiB |   3033 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78560 MiB |  78656 MiB | 719110 MiB | 640550 MiB |
|       from large pool |  78538 MiB |  78538 MiB | 711750 MiB | 633212 MiB |
|       from small pool |     22 MiB |    118 MiB |   7360 MiB |   7338 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5342 MiB |   6689 MiB | 573422 GiB | 573416 GiB |
|       from large pool |   5329 MiB |   6676 MiB | 569986 GiB | 569981 GiB |
|       from small pool |     13 MiB |     17 MiB |   3435 GiB |   3435 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     571    |     577    |   36318 K  |   36318 K  |
|       from large pool |     286    |     292    |   16323 K  |   16322 K  |
|       from small pool |     285    |     348    |   19995 K  |   19995 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     571    |     577    |   36318 K  |   36318 K  |
|       from large pool |     286    |     292    |   16323 K  |   16322 K  |
|       from small pool |     285    |     348    |   19995 K  |   19995 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     151    |     199    |    6885    |    6734    |
|       from large pool |     140    |     140    |    3205    |    3065    |
|       from small pool |      11    |      59    |    3680    |    3669    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     153    |     155    |   20215 K  |   20215 K  |
|       from large pool |     130    |     132    |   10323 K  |   10323 K  |
|       from small pool |      23    |      43    |    9892 K  |    9892 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:08:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:08:49] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:08:52]    INFO >> epoch 013:    531 / 1539 loss=3.357, wps=4219.7, ups=5.47, wpb=771.3, bsz=771.3, num_updates=18950, lr=0.000105, gnorm=1.858, clip=0, train_wall=8, gb_free=71.5, wall=3150 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:09:00]    INFO >> epoch 013:    581 / 1539 loss=3.212, wps=3732.3, ups=6.15, wpb=606.6, bsz=606.6, num_updates=19000, lr=0.000105, gnorm=1.779, clip=0, train_wall=8, gb_free=72.9, wall=3158 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:09:06] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 43.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 76.24 GiB is allocated by PyTorch, and 2.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:09:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:09:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:09:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 50           |        cudaMalloc retries: 81        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78006 MiB |  78066 MiB | 583418 GiB | 583342 GiB |
|       from large pool |  77622 MiB |  77682 MiB | 580363 GiB | 580287 GiB |
|       from small pool |    384 MiB |    385 MiB |   3054 GiB |   3054 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78006 MiB |  78066 MiB | 583418 GiB | 583342 GiB |
|       from large pool |  77622 MiB |  77682 MiB | 580363 GiB | 580287 GiB |
|       from small pool |    384 MiB |    385 MiB |   3054 GiB |   3054 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77967 MiB |  78027 MiB | 582576 GiB | 582499 GiB |
|       from large pool |  77585 MiB |  77644 MiB | 579525 GiB | 579450 GiB |
|       from small pool |    382 MiB |    383 MiB |   3050 GiB |   3049 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80462 MiB |  80462 MiB | 721132 MiB | 640670 MiB |
|       from large pool |  80038 MiB |  80038 MiB | 713370 MiB | 633332 MiB |
|       from small pool |    424 MiB |    424 MiB |   7762 MiB |   7338 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2395 MiB |   5326 MiB | 576331 GiB | 576329 GiB |
|       from large pool |   2355 MiB |   5322 MiB | 572877 GiB | 572874 GiB |
|       from small pool |     39 MiB |     40 MiB |   3454 GiB |   3454 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7190    |    7193    |   36513 K  |   36506 K  |
|       from large pool |     874    |     875    |   16409 K  |   16408 K  |
|       from small pool |    6316    |    6319    |   20104 K  |   20097 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7190    |    7193    |   36513 K  |   36506 K  |
|       from large pool |     874    |     875    |   16409 K  |   16408 K  |
|       from small pool |    6316    |    6319    |   20104 K  |   20097 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     377    |     377    |    7113    |    6736    |
|       from large pool |     165    |     165    |    3232    |    3067    |
|       from small pool |     212    |     212    |    3881    |    3669    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     494    |     495    |   20325 K  |   20325 K  |
|       from large pool |     112    |     112    |   10379 K  |   10379 K  |
|       from small pool |     382    |     383    |    9946 K  |    9945 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:09:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:09:06] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:09:08]    INFO >> epoch 013:    632 / 1539 loss=3.003, wps=4212.3, ups=5.99, wpb=703.6, bsz=703.6, num_updates=19050, lr=0.000105, gnorm=1.759, clip=0, train_wall=7, gb_free=72.3, wall=3166 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:09:19]    INFO >> epoch 013:    682 / 1539 loss=3.106, wps=3859, ups=5.56, wpb=694.4, bsz=694.4, num_updates=19100, lr=0.000105, gnorm=1.871, clip=0, train_wall=8, gb_free=66.3, wall=3175 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:09:26]    INFO >> epoch 013:    732 / 1539 loss=3.359, wps=4242.8, ups=6.4, wpb=662.6, bsz=662.6, num_updates=19150, lr=0.000105, gnorm=1.812, clip=0, train_wall=7, gb_free=73.3, wall=3183 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:09:34]    INFO >> epoch 013:    782 / 1539 loss=3.189, wps=4292.2, ups=6.91, wpb=621.6, bsz=621.6, num_updates=19200, lr=0.000105, gnorm=1.624, clip=0, train_wall=7, gb_free=72.7, wall=3190 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:09:41]    INFO >> epoch 013:    832 / 1539 loss=3.29, wps=4417.9, ups=6.53, wpb=676.3, bsz=676.3, num_updates=19250, lr=0.000105, gnorm=1.711, clip=0, train_wall=7, gb_free=71.2, wall=3198 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:09:51]    INFO >> epoch 013:    882 / 1539 loss=3.084, wps=4556.1, ups=5.86, wpb=777.1, bsz=777.1, num_updates=19300, lr=0.000105, gnorm=1.719, clip=0, train_wall=8, gb_free=75.5, wall=3206 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:09:59]    INFO >> epoch 013:    932 / 1539 loss=3.174, wps=4372.2, ups=6.37, wpb=686.3, bsz=686.3, num_updates=19350, lr=0.000105, gnorm=1.699, clip=0, train_wall=7, gb_free=74.1, wall=3214 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:10:07]    INFO >> epoch 013:    982 / 1539 loss=3.056, wps=4492.3, ups=6.61, wpb=679.4, bsz=679.4, num_updates=19400, lr=0.000105, gnorm=1.821, clip=0, train_wall=7, gb_free=73.5, wall=3222 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:10:14]    INFO >> epoch 013:   1032 / 1539 loss=3.242, wps=4943.6, ups=6.66, wpb=742.2, bsz=742.2, num_updates=19450, lr=0.000105, gnorm=1.953, clip=0, train_wall=7, gb_free=72.8, wall=3229 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:10:24]    INFO >> epoch 013:   1082 / 1539 loss=3.109, wps=4158.4, ups=5.93, wpb=700.9, bsz=700.9, num_updates=19500, lr=0.000105, gnorm=1.833, clip=0, train_wall=8, gb_free=72.1, wall=3238 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:10:32]    INFO >> epoch 013:   1132 / 1539 loss=3.461, wps=4681.7, ups=5.85, wpb=799.8, bsz=799.8, num_updates=19550, lr=0.000105, gnorm=1.855, clip=0, train_wall=8, gb_free=75.5, wall=3246 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:10:41]    INFO >> epoch 013:   1182 / 1539 loss=3.052, wps=4642.9, ups=5.97, wpb=777.1, bsz=777.1, num_updates=19600, lr=0.000105, gnorm=1.613, clip=0, train_wall=8, gb_free=72.1, wall=3255 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:10:48]    INFO >> epoch 013:   1232 / 1539 loss=3.183, wps=4610.7, ups=6.48, wpb=711.5, bsz=711.5, num_updates=19650, lr=0.000105, gnorm=1.638, clip=0, train_wall=7, gb_free=67.1, wall=3262 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:10:50] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.17 GiB. GPU 2 has a total capacity of 79.14 GiB of which 57.25 MiB is free. Including non-PyTorch memory, this process has 79.06 GiB memory in use. Of the allocated memory 74.62 GiB is allocated by PyTorch, and 3.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:10:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:10:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:10:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 51           |        cudaMalloc retries: 83        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  70623 MiB |  76889 MiB | 600528 GiB | 600459 GiB |
|       from large pool |  70612 MiB |  76878 MiB | 597389 GiB | 597320 GiB |
|       from small pool |     11 MiB |     20 MiB |   3138 GiB |   3138 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  70623 MiB |  76889 MiB | 600528 GiB | 600459 GiB |
|       from large pool |  70612 MiB |  76878 MiB | 597389 GiB | 597320 GiB |
|       from small pool |     11 MiB |     20 MiB |   3138 GiB |   3138 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  70612 MiB |  76876 MiB | 599659 GiB | 599590 GiB |
|       from large pool |  70600 MiB |  76865 MiB | 596525 GiB | 596456 GiB |
|       from small pool |     11 MiB |     20 MiB |   3133 GiB |   3133 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80448 MiB |  80448 MiB | 726858 MiB | 646410 MiB |
|       from large pool |  80424 MiB |  80424 MiB | 719096 MiB | 638672 MiB |
|       from small pool |     24 MiB |    424 MiB |   7762 MiB |   7738 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6000 MiB |   9535 MiB | 592400 GiB | 592394 GiB |
|       from large pool |   5987 MiB |   9522 MiB | 588848 GiB | 588842 GiB |
|       from small pool |     12 MiB |     21 MiB |   3551 GiB |   3551 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     541    |     559    |   37581 K  |   37580 K  |
|       from large pool |     256    |     274    |   16925 K  |   16925 K  |
|       from small pool |     285    |     356    |   20655 K  |   20655 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     541    |     559    |   37581 K  |   37580 K  |
|       from large pool |     256    |     274    |   16925 K  |   16925 K  |
|       from small pool |     285    |     356    |   20655 K  |   20655 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      91    |     376    |    7116    |    7025    |
|       from large pool |      79    |     164    |    3235    |    3156    |
|       from small pool |      12    |     212    |    3881    |    3869    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     105    |     105    |   20919 K  |   20919 K  |
|       from large pool |      79    |      79    |   10716 K  |   10716 K  |
|       from small pool |      26    |      41    |   10203 K  |   10203 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:10:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:10:50] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:10:58]    INFO >> epoch 013:   1283 / 1539 loss=3.315, wps=3878.1, ups=6.02, wpb=644.6, bsz=644.6, num_updates=19700, lr=0.000105, gnorm=1.97, clip=0, train_wall=7, gb_free=75.8, wall=3271 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:11:06]    INFO >> epoch 013:   1333 / 1539 loss=3.244, wps=4410.6, ups=6.42, wpb=687, bsz=687, num_updates=19750, lr=0.000105, gnorm=1.742, clip=0, train_wall=7, gb_free=73.8, wall=3278 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:11:14]    INFO >> epoch 013:   1383 / 1539 loss=3.172, wps=5046, ups=6.16, wpb=819.3, bsz=819.3, num_updates=19800, lr=0.000105, gnorm=2.025, clip=0, train_wall=8, gb_free=72.6, wall=3287 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:11:22]    INFO >> epoch 013:   1433 / 1539 loss=3.3, wps=4477.6, ups=6.56, wpb=682.8, bsz=682.8, num_updates=19850, lr=0.000105, gnorm=1.694, clip=0, train_wall=7, gb_free=73.2, wall=3294 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:11:31]    INFO >> epoch 013:   1483 / 1539 loss=3.018, wps=4247.1, ups=6.24, wpb=681, bsz=681, num_updates=19900, lr=0.000105, gnorm=1.754, clip=0, train_wall=8, gb_free=72.7, wall=3302 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:11:32] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 45.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 77.31 GiB is allocated by PyTorch, and 1.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:11:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:11:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:11:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 52           |        cudaMalloc retries: 84        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79103 MiB |  79163 MiB | 607258 GiB | 607180 GiB |
|       from large pool |  78966 MiB |  79026 MiB | 604085 GiB | 604008 GiB |
|       from small pool |    136 MiB |    137 MiB |   3172 GiB |   3172 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79103 MiB |  79163 MiB | 607258 GiB | 607180 GiB |
|       from large pool |  78966 MiB |  79026 MiB | 604085 GiB | 604008 GiB |
|       from small pool |    136 MiB |    137 MiB |   3172 GiB |   3172 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79060 MiB |  79119 MiB | 606379 GiB | 606302 GiB |
|       from large pool |  78924 MiB |  78983 MiB | 603211 GiB | 603134 GiB |
|       from small pool |    135 MiB |    136 MiB |   3167 GiB |   3167 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80460 MiB |  80462 MiB | 730696 MiB | 650236 MiB |
|       from large pool |  80320 MiB |  80320 MiB | 722816 MiB | 642496 MiB |
|       from small pool |    140 MiB |    142 MiB |   7880 MiB |   7740 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1296 MiB |   7046 MiB | 600203 GiB | 600202 GiB |
|       from large pool |   1293 MiB |   7039 MiB | 596612 GiB | 596611 GiB |
|       from small pool |      3 MiB |     17 MiB |   3590 GiB |   3590 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2779    |    2782    |   38014 K  |   38011 K  |
|       from large pool |     487    |     488    |   17132 K  |   17132 K  |
|       from small pool |    2292    |    2295    |   20881 K  |   20879 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2779    |    2782    |   38014 K  |   38011 K  |
|       from large pool |     487    |     488    |   17132 K  |   17132 K  |
|       from small pool |    2292    |    2295    |   20881 K  |   20879 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     209    |     210    |    7237    |    7028    |
|       from large pool |     139    |     139    |    3297    |    3158    |
|       from small pool |      70    |      71    |    3940    |    3870    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     153    |     154    |   21151 K  |   21151 K  |
|       from large pool |      88    |      92    |   10841 K  |   10841 K  |
|       from small pool |      65    |      66    |   10310 K  |   10310 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:11:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:11:32] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:11:40]    INFO >> epoch 013:   1534 / 1539 loss=2.875, wps=4553, ups=5.34, wpb=852.5, bsz=852.5, num_updates=19950, lr=0.000105, gnorm=1.806, clip=0, train_wall=8, gb_free=69.5, wall=3312 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:11:41]    INFO >> epoch 013 | loss 3.175 | wps 4137 | ups 5.8 | wpb 712.7 | bsz 712.7 | num_updates 19955 | lr 0.000105 | gnorm 1.757 | clip 0 | train_wall 231 | gb_free 71.6 | wall 3313 (progress_bar.py:267, print())[0m
[33m[2025-11-21 15:11:41] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:11:57]    INFO >> epoch 013 | valid on 'valid' subset | loss 3.501 | wps 9747.5 | wpb 5412.5 | bsz 5412.5 | num_updates 19955 | best_loss 3.524 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 15:11:57]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 15:11:57]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_last.pt (epoch 13 @ 19955 updates, score 3.501) (writing took 0.014492 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 15:11:57] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:12:06]    INFO >> epoch 014:     45 / 1539 loss=3.215, wps=1521.6, ups=2.07, wpb=736.4, bsz=736.4, num_updates=20000, lr=8.3e-05, gnorm=1.719, clip=0, train_wall=8, gb_free=67.2, wall=3336 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:12:14]    INFO >> epoch 014:     95 / 1539 loss=3.358, wps=4304.9, ups=6.33, wpb=679.6, bsz=679.6, num_updates=20050, lr=8.3e-05, gnorm=1.752, clip=0, train_wall=7, gb_free=70.9, wall=3344 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:12:22]    INFO >> epoch 014:    145 / 1539 loss=3.04, wps=4508.6, ups=6.14, wpb=734.8, bsz=734.8, num_updates=20100, lr=8.3e-05, gnorm=1.794, clip=0, train_wall=8, gb_free=68.5, wall=3352 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:12:29]    INFO >> epoch 014:    195 / 1539 loss=3.331, wps=4521.7, ups=6.88, wpb=657.2, bsz=657.2, num_updates=20150, lr=8.3e-05, gnorm=1.62, clip=0, train_wall=7, gb_free=70.9, wall=3359 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:12:40]    INFO >> epoch 014:    245 / 1539 loss=2.808, wps=5075.9, ups=5.61, wpb=904.5, bsz=904.5, num_updates=20200, lr=8.3e-05, gnorm=1.637, clip=0, train_wall=8, gb_free=73, wall=3368 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:12:47]    INFO >> epoch 014:    295 / 1539 loss=3.109, wps=4120, ups=6.29, wpb=654.7, bsz=654.7, num_updates=20250, lr=8.3e-05, gnorm=1.475, clip=0, train_wall=7, gb_free=74.1, wall=3376 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:12:49] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 47.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 76.54 GiB is allocated by PyTorch, and 2.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:12:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:12:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:12:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 53           |        cudaMalloc retries: 86        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78320 MiB |  78380 MiB | 621751 GiB | 621674 GiB |
|       from large pool |  77934 MiB |  77994 MiB | 618486 GiB | 618409 GiB |
|       from small pool |    386 MiB |    387 MiB |   3265 GiB |   3264 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78320 MiB |  78380 MiB | 621751 GiB | 621674 GiB |
|       from large pool |  77934 MiB |  77994 MiB | 618486 GiB | 618409 GiB |
|       from small pool |    386 MiB |    387 MiB |   3265 GiB |   3264 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78147 MiB |  78207 MiB | 620851 GiB | 620775 GiB |
|       from large pool |  77763 MiB |  77823 MiB | 617591 GiB | 617515 GiB |
|       from small pool |    384 MiB |    385 MiB |   3260 GiB |   3259 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80458 MiB |  80504 MiB | 747124 MiB | 666666 MiB |
|       from large pool |  80032 MiB |  80260 MiB | 738956 MiB | 658924 MiB |
|       from small pool |    426 MiB |    428 MiB |   8168 MiB |   7742 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2077 MiB |   6219 MiB | 612937 GiB | 612935 GiB |
|       from large pool |   2037 MiB |   6216 MiB | 609245 GiB | 609243 GiB |
|       from small pool |     39 MiB |     41 MiB |   3692 GiB |   3692 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7223    |    7226    |   38963 K  |   38956 K  |
|       from large pool |     877    |     878    |   17475 K  |   17474 K  |
|       from small pool |    6346    |    6349    |   21487 K  |   21481 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7223    |    7226    |   38963 K  |   38956 K  |
|       from large pool |     877    |     878    |   17475 K  |   17474 K  |
|       from small pool |    6346    |    6349    |   21487 K  |   21481 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     614    |     615    |    7650    |    7036    |
|       from large pool |     401    |     401    |    3566    |    3165    |
|       from small pool |     213    |     214    |    4084    |    3871    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     491    |     491    |   21710 K  |   21709 K  |
|       from large pool |     106    |     106    |   11061 K  |   11061 K  |
|       from small pool |     385    |     385    |   10648 K  |   10647 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:12:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:12:49] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:12:56]    INFO >> epoch 014:    346 / 1539 loss=3.15, wps=3710.5, ups=5.57, wpb=666.2, bsz=666.2, num_updates=20300, lr=8.3e-05, gnorm=1.55, clip=0, train_wall=8, gb_free=74.4, wall=3385 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:13:06]    INFO >> epoch 014:    396 / 1539 loss=3.119, wps=4004.7, ups=5.43, wpb=736.9, bsz=736.9, num_updates=20350, lr=8.3e-05, gnorm=1.742, clip=0, train_wall=9, gb_free=17.4, wall=3394 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:13:15]    INFO >> epoch 014:    446 / 1539 loss=3.305, wps=4212.1, ups=6.52, wpb=646, bsz=646, num_updates=20400, lr=8.3e-05, gnorm=1.982, clip=0, train_wall=7, gb_free=74.1, wall=3402 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:13:23]    INFO >> epoch 014:    496 / 1539 loss=3.092, wps=4190.5, ups=6.33, wpb=661.7, bsz=661.7, num_updates=20450, lr=8.3e-05, gnorm=1.75, clip=0, train_wall=7, gb_free=73.1, wall=3410 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:13:30] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 27.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.93 GiB is allocated by PyTorch, and 1.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:13:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:13:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:13:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 54           |        cudaMalloc retries: 88        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78717 MiB |  78777 MiB | 628367 GiB | 628290 GiB |
|       from large pool |  78585 MiB |  78645 MiB | 625071 GiB | 624994 GiB |
|       from small pool |    132 MiB |    133 MiB |   3295 GiB |   3295 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78717 MiB |  78777 MiB | 628367 GiB | 628290 GiB |
|       from large pool |  78585 MiB |  78645 MiB | 625071 GiB | 624994 GiB |
|       from small pool |    132 MiB |    133 MiB |   3295 GiB |   3295 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78639 MiB |  78698 MiB | 627457 GiB | 627380 GiB |
|       from large pool |  78507 MiB |  78567 MiB | 624167 GiB | 624090 GiB |
|       from small pool |    131 MiB |    132 MiB |   3290 GiB |   3290 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80478 MiB |  80480 MiB | 764348 MiB | 683870 MiB |
|       from large pool |  80342 MiB |  80342 MiB | 756066 MiB | 675724 MiB |
|       from small pool |    136 MiB |    138 MiB |   8282 MiB |   8146 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1700 MiB |   8075 MiB | 619242 GiB | 619241 GiB |
|       from large pool |   1696 MiB |   8066 MiB | 615516 GiB | 615514 GiB |
|       from small pool |      3 MiB |     19 MiB |   3726 GiB |   3726 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2702    |    2705    |   39360 K  |   39358 K  |
|       from large pool |     480    |     481    |   17672 K  |   17671 K  |
|       from small pool |    2222    |    2225    |   21688 K  |   21686 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2702    |    2705    |   39360 K  |   39358 K  |
|       from large pool |     480    |     481    |   17672 K  |   17671 K  |
|       from small pool |    2222    |    2225    |   21688 K  |   21686 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     325    |     326    |    7843    |    7518    |
|       from large pool |     257    |     257    |    3702    |    3445    |
|       from small pool |      68    |      69    |    4141    |    4073    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     181    |     182    |   21927 K  |   21927 K  |
|       from large pool |     117    |     120    |   11188 K  |   11188 K  |
|       from small pool |      64    |      65    |   10739 K  |   10739 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:13:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:13:30] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:13:32]    INFO >> epoch 014:    547 / 1539 loss=3.404, wps=4388.6, ups=5.17, wpb=848.8, bsz=848.8, num_updates=20500, lr=8.3e-05, gnorm=1.9, clip=0, train_wall=9, gb_free=50.1, wall=3419 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:13:40]    INFO >> epoch 014:    597 / 1539 loss=3.194, wps=4765.2, ups=6.19, wpb=770.4, bsz=770.4, num_updates=20550, lr=8.3e-05, gnorm=1.625, clip=0, train_wall=8, gb_free=73.2, wall=3427 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:13:50]    INFO >> epoch 014:    647 / 1539 loss=3.241, wps=4344.1, ups=6.25, wpb=695.2, bsz=695.2, num_updates=20600, lr=8.3e-05, gnorm=1.775, clip=0, train_wall=8, gb_free=73.4, wall=3435 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:13:58]    INFO >> epoch 014:    697 / 1539 loss=3.073, wps=4348.3, ups=6.01, wpb=723.9, bsz=723.9, num_updates=20650, lr=8.3e-05, gnorm=1.59, clip=0, train_wall=8, gb_free=74, wall=3444 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:14:06]    INFO >> epoch 014:    747 / 1539 loss=3.078, wps=4273.9, ups=6.31, wpb=677.7, bsz=677.7, num_updates=20700, lr=8.3e-05, gnorm=1.625, clip=0, train_wall=7, gb_free=71.6, wall=3452 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:14:14]    INFO >> epoch 014:    797 / 1539 loss=3.227, wps=4331.6, ups=6.33, wpb=684.6, bsz=684.6, num_updates=20750, lr=8.3e-05, gnorm=1.906, clip=0, train_wall=7, gb_free=74.7, wall=3460 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:14:24]    INFO >> epoch 014:    847 / 1539 loss=3.259, wps=4062.9, ups=5.81, wpb=698.9, bsz=698.9, num_updates=20800, lr=8.3e-05, gnorm=1.814, clip=0, train_wall=8, gb_free=70.9, wall=3468 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:14:32]    INFO >> epoch 014:    897 / 1539 loss=3.133, wps=4595.7, ups=6.26, wpb=733.7, bsz=733.7, num_updates=20850, lr=8.3e-05, gnorm=1.96, clip=0, train_wall=7, gb_free=58.1, wall=3476 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:14:40]    INFO >> epoch 014:    947 / 1539 loss=3.131, wps=4463, ups=6.25, wpb=713.7, bsz=713.7, num_updates=20900, lr=8.3e-05, gnorm=1.631, clip=0, train_wall=7, gb_free=71.3, wall=3484 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:14:47]    INFO >> epoch 014:    997 / 1539 loss=3.1, wps=3971.1, ups=6.55, wpb=606.5, bsz=606.5, num_updates=20950, lr=8.3e-05, gnorm=1.645, clip=0, train_wall=7, gb_free=70.6, wall=3492 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:14:55] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.46 GiB. GPU 2 has a total capacity of 79.14 GiB of which 137.25 MiB is free. Including non-PyTorch memory, this process has 78.98 GiB memory in use. Of the allocated memory 75.55 GiB is allocated by PyTorch, and 2.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:14:55] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:14:55] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:14:55] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 55           |        cudaMalloc retries: 91        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  66234 MiB |  77546 MiB | 642296 GiB | 642231 GiB |
|       from large pool |  66225 MiB |  77537 MiB | 638934 GiB | 638869 GiB |
|       from small pool |      8 MiB |     16 MiB |   3362 GiB |   3362 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  66234 MiB |  77546 MiB | 642296 GiB | 642231 GiB |
|       from large pool |  66225 MiB |  77537 MiB | 638934 GiB | 638869 GiB |
|       from small pool |      8 MiB |     16 MiB |   3362 GiB |   3362 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  66225 MiB |  77534 MiB | 641365 GiB | 641301 GiB |
|       from large pool |  66216 MiB |  77525 MiB | 638008 GiB | 637944 GiB |
|       from small pool |      8 MiB |     16 MiB |   3356 GiB |   3356 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80368 MiB |  80368 MiB |    775 GiB | 713678 MiB |
|       from large pool |  80344 MiB |  80344 MiB |    767 GiB | 705318 MiB |
|       from small pool |     24 MiB |    238 MiB |      8 GiB |   8360 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5641 MiB |   7443 MiB | 632330 GiB | 632325 GiB |
|       from large pool |   5626 MiB |   7428 MiB | 628528 GiB | 628522 GiB |
|       from small pool |     15 MiB |     21 MiB |   3802 GiB |   3802 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     545    |     586    |   40218 K  |   40218 K  |
|       from large pool |     260    |     301    |   18088 K  |   18088 K  |
|       from small pool |     285    |     342    |   22129 K  |   22129 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     545    |     586    |   40218 K  |   40218 K  |
|       from large pool |     260    |     301    |   18088 K  |   18088 K  |
|       from small pool |     285    |     342    |   22129 K  |   22129 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     147    |     376    |    7906    |    7759    |
|       from large pool |     135    |     257    |    3714    |    3579    |
|       from small pool |      12    |     119    |    4192    |    4180    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     151    |     153    |   22403 K  |   22403 K  |
|       from large pool |     127    |     129    |   11460 K  |   11460 K  |
|       from small pool |      24    |      43    |   10943 K  |   10942 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:14:55] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:14:55] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:14:56]    INFO >> epoch 014:   1048 / 1539 loss=3.261, wps=3642.4, ups=5.48, wpb=664.4, bsz=664.4, num_updates=21000, lr=8.3e-05, gnorm=1.728, clip=0, train_wall=8, gb_free=69.9, wall=3501 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:15:05]    INFO >> epoch 014:   1098 / 1539 loss=3.105, wps=4916.9, ups=6.01, wpb=818.7, bsz=818.7, num_updates=21050, lr=8.3e-05, gnorm=1.818, clip=0, train_wall=8, gb_free=73.5, wall=3509 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:15:06] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.17 GiB. GPU 2 has a total capacity of 79.14 GiB of which 801.25 MiB is free. Including non-PyTorch memory, this process has 78.33 GiB memory in use. Of the allocated memory 74.62 GiB is allocated by PyTorch, and 3.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:15:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:15:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:15:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 56           |        cudaMalloc retries: 93        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  70626 MiB |  76892 MiB | 644235 GiB | 644166 GiB |
|       from large pool |  70615 MiB |  76881 MiB | 640864 GiB | 640795 GiB |
|       from small pool |     11 MiB |     23 MiB |   3371 GiB |   3371 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  70626 MiB |  76892 MiB | 644235 GiB | 644166 GiB |
|       from large pool |  70615 MiB |  76881 MiB | 640864 GiB | 640795 GiB |
|       from small pool |     11 MiB |     23 MiB |   3371 GiB |   3371 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  70612 MiB |  76876 MiB | 643302 GiB | 643233 GiB |
|       from large pool |  70600 MiB |  76865 MiB | 639935 GiB | 639866 GiB |
|       from small pool |     11 MiB |     23 MiB |   3366 GiB |   3366 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79704 MiB |  79704 MiB |    786 GiB | 725432 MiB |
|       from large pool |  79680 MiB |  79680 MiB |    777 GiB | 716978 MiB |
|       from small pool |     24 MiB |    118 MiB |      8 GiB |   8454 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5253 MiB |   9024 MiB | 634272 GiB | 634267 GiB |
|       from large pool |   5240 MiB |   9011 MiB | 630459 GiB | 630454 GiB |
|       from small pool |     12 MiB |     21 MiB |   3812 GiB |   3812 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     541    |     559    |   40332 K  |   40331 K  |
|       from large pool |     256    |     274    |   18141 K  |   18141 K  |
|       from small pool |     285    |     356    |   22190 K  |   22190 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     541    |     559    |   40332 K  |   40331 K  |
|       from large pool |     256    |     274    |   18141 K  |   18141 K  |
|       from small pool |     285    |     356    |   22190 K  |   22190 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      96    |     194    |    7961    |    7865    |
|       from large pool |      84    |     135    |    3722    |    3638    |
|       from small pool |      12    |      59    |    4239    |    4227    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     100    |     100    |   22465 K  |   22465 K  |
|       from large pool |      75    |      75    |   11493 K  |   11493 K  |
|       from small pool |      25    |      47    |   10972 K  |   10972 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:15:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:15:06] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:15:13]    INFO >> epoch 014:   1149 / 1539 loss=3.081, wps=3920.5, ups=5.76, wpb=680.4, bsz=680.4, num_updates=21100, lr=8.3e-05, gnorm=1.662, clip=0, train_wall=7, gb_free=74.1, wall=3518 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:15:22]    INFO >> epoch 014:   1199 / 1539 loss=3.295, wps=4130.6, ups=6.24, wpb=662.1, bsz=662.1, num_updates=21150, lr=8.3e-05, gnorm=1.655, clip=0, train_wall=8, gb_free=74.7, wall=3526 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:15:32]    INFO >> epoch 014:   1249 / 1539 loss=3.18, wps=3855.5, ups=6.6, wpb=584.3, bsz=584.3, num_updates=21200, lr=8.3e-05, gnorm=1.537, clip=0, train_wall=7, gb_free=73.9, wall=3534 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:15:39]    INFO >> epoch 014:   1299 / 1539 loss=3.266, wps=4088.6, ups=6.65, wpb=614.8, bsz=614.8, num_updates=21250, lr=8.3e-05, gnorm=1.799, clip=0, train_wall=7, gb_free=73.9, wall=3541 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:15:47]    INFO >> epoch 014:   1349 / 1539 loss=3.201, wps=4409.1, ups=6.23, wpb=707.7, bsz=707.7, num_updates=21300, lr=8.3e-05, gnorm=1.908, clip=0, train_wall=8, gb_free=72.3, wall=3549 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:15:55]    INFO >> epoch 014:   1399 / 1539 loss=3.04, wps=4491.5, ups=6.15, wpb=729.9, bsz=729.9, num_updates=21350, lr=8.3e-05, gnorm=1.787, clip=0, train_wall=8, gb_free=72.5, wall=3557 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:16:06]    INFO >> epoch 014:   1449 / 1539 loss=3.081, wps=5067.2, ups=5.63, wpb=899.8, bsz=899.8, num_updates=21400, lr=8.3e-05, gnorm=2.111, clip=0, train_wall=8, gb_free=72.2, wall=3566 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:16:14]    INFO >> epoch 014:   1499 / 1539 loss=3.29, wps=4300.7, ups=6.31, wpb=681.8, bsz=681.8, num_updates=21450, lr=8.3e-05, gnorm=1.842, clip=0, train_wall=7, gb_free=71.2, wall=3574 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:16:20]    INFO >> epoch 014 | loss 3.171 | wps 4079.2 | ups 5.72 | wpb 712.7 | bsz 712.7 | num_updates 21490 | lr 8.3e-05 | gnorm 1.754 | clip 0 | train_wall 234 | gb_free 71.7 | wall 3581 (progress_bar.py:267, print())[0m
[33m[2025-11-21 15:16:20] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:16:39]    INFO >> epoch 014 | valid on 'valid' subset | loss 3.498 | wps 8901.9 | wpb 5412.5 | bsz 5412.5 | num_updates 21490 | best_loss 3.524 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 15:16:39]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 15:16:39]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_last.pt (epoch 14 @ 21490 updates, score 3.498) (writing took 0.014620 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 15:16:39] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:16:40]    INFO >> epoch 015:     10 / 1539 loss=3.287, wps=1522.8, ups=1.96, wpb=777.5, bsz=777.5, num_updates=21500, lr=6.4e-05, gnorm=2.092, clip=0, train_wall=8, gb_free=72.5, wall=3600 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:16:48]    INFO >> epoch 015:     60 / 1539 loss=3.18, wps=4093.9, ups=6.49, wpb=631.1, bsz=631.1, num_updates=21550, lr=6.4e-05, gnorm=1.726, clip=0, train_wall=7, gb_free=74.3, wall=3607 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:16:56]    INFO >> epoch 015:    110 / 1539 loss=3.102, wps=4440.2, ups=6.18, wpb=718.1, bsz=718.1, num_updates=21600, lr=6.4e-05, gnorm=1.626, clip=0, train_wall=8, gb_free=73.6, wall=3615 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:17:06]    INFO >> epoch 015:    160 / 1539 loss=3.241, wps=4149.2, ups=6.26, wpb=663.1, bsz=663.1, num_updates=21650, lr=6.4e-05, gnorm=1.669, clip=0, train_wall=8, gb_free=71.2, wall=3623 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:17:13]    INFO >> epoch 015:    210 / 1539 loss=3.111, wps=4092.1, ups=6.34, wpb=645.9, bsz=645.9, num_updates=21700, lr=6.4e-05, gnorm=1.59, clip=0, train_wall=7, gb_free=73.9, wall=3631 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:17:23]    INFO >> epoch 015:    260 / 1539 loss=3.174, wps=5593.1, ups=5.37, wpb=1041.2, bsz=1041.2, num_updates=21750, lr=6.4e-05, gnorm=2.024, clip=0, train_wall=9, gb_free=67.7, wall=3641 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:17:31]    INFO >> epoch 015:    310 / 1539 loss=3.195, wps=4531.2, ups=6.44, wpb=703.4, bsz=703.4, num_updates=21800, lr=6.4e-05, gnorm=1.67, clip=0, train_wall=7, gb_free=71.5, wall=3648 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:17:33] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 7.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.38 GiB is allocated by PyTorch, and 1.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:17:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:17:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:17:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 57           |        cudaMalloc retries: 95        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79172 MiB |  79232 MiB | 669457 GiB | 669380 GiB |
|       from large pool |  79035 MiB |  79095 MiB | 665943 GiB | 665866 GiB |
|       from small pool |    136 MiB |    138 MiB |   3514 GiB |   3514 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79172 MiB |  79232 MiB | 669457 GiB | 669380 GiB |
|       from large pool |  79035 MiB |  79095 MiB | 665943 GiB | 665866 GiB |
|       from small pool |    136 MiB |    138 MiB |   3514 GiB |   3514 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79120 MiB |  79179 MiB | 668490 GiB | 668413 GiB |
|       from large pool |  78983 MiB |  79043 MiB | 664982 GiB | 664905 GiB |
|       from small pool |    136 MiB |    137 MiB |   3508 GiB |   3508 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80498 MiB |  80498 MiB |    790 GiB | 729354 MiB |
|       from large pool |  80356 MiB |  80356 MiB |    782 GiB | 720802 MiB |
|       from small pool |    142 MiB |    238 MiB |      8 GiB |   8552 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1265 MiB |   6300 MiB | 660601 GiB | 660600 GiB |
|       from large pool |   1260 MiB |   6293 MiB | 656631 GiB | 656629 GiB |
|       from small pool |      5 MiB |     19 MiB |   3970 GiB |   3970 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2790    |    2793    |   41940 K  |   41938 K  |
|       from large pool |     488    |     489    |   18815 K  |   18814 K  |
|       from small pool |    2302    |    2305    |   23125 K  |   23123 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2790    |    2793    |   41940 K  |   41938 K  |
|       from large pool |     488    |     489    |   18815 K  |   18814 K  |
|       from small pool |    2302    |    2305    |   23125 K  |   23123 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     228    |     274    |    8144    |    7916    |
|       from large pool |     157    |     157    |    3797    |    3640    |
|       from small pool |      71    |     119    |    4347    |    4276    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     150    |     150    |   23357 K  |   23357 K  |
|       from large pool |      83    |      86    |   11901 K  |   11901 K  |
|       from small pool |      67    |      67    |   11455 K  |   11455 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:17:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:17:33] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:17:41]    INFO >> epoch 015:    361 / 1539 loss=3.156, wps=4067.4, ups=5.49, wpb=740.3, bsz=740.3, num_updates=21850, lr=6.4e-05, gnorm=1.717, clip=0, train_wall=8, gb_free=69.3, wall=3657 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:17:49]    INFO >> epoch 015:    411 / 1539 loss=3.292, wps=4285, ups=6.53, wpb=656, bsz=656, num_updates=21900, lr=6.4e-05, gnorm=1.83, clip=0, train_wall=7, gb_free=72.7, wall=3665 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:17:56]    INFO >> epoch 015:    461 / 1539 loss=3.227, wps=4270.6, ups=6.58, wpb=648.7, bsz=648.7, num_updates=21950, lr=6.4e-05, gnorm=1.697, clip=0, train_wall=7, gb_free=73.4, wall=3673 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:18:05]    INFO >> epoch 015:    511 / 1539 loss=2.869, wps=4962.7, ups=5.6, wpb=885.8, bsz=885.8, num_updates=22000, lr=6.4e-05, gnorm=1.866, clip=0, train_wall=8, gb_free=69.8, wall=3682 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:18:15]    INFO >> epoch 015:    561 / 1539 loss=3.125, wps=4594.8, ups=6.05, wpb=759.1, bsz=759.1, num_updates=22050, lr=6.4e-05, gnorm=1.66, clip=0, train_wall=8, gb_free=75.1, wall=3690 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:18:17] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.88 GiB. GPU 2 has a total capacity of 79.14 GiB of which 823.25 MiB is free. Including non-PyTorch memory, this process has 78.31 GiB memory in use. Of the allocated memory 72.51 GiB is allocated by PyTorch, and 5.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:18:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:18:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:18:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 58           |        cudaMalloc retries: 97        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  63158 MiB |  75128 MiB | 676614 GiB | 676552 GiB |
|       from large pool |  63147 MiB |  75117 MiB | 673064 GiB | 673002 GiB |
|       from small pool |     11 MiB |     13 MiB |   3549 GiB |   3549 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  63158 MiB |  75128 MiB | 676614 GiB | 676552 GiB |
|       from large pool |  63147 MiB |  75117 MiB | 673064 GiB | 673002 GiB |
|       from small pool |     11 MiB |     13 MiB |   3549 GiB |   3549 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  63147 MiB |  75114 MiB | 675636 GiB | 675575 GiB |
|       from large pool |  63136 MiB |  75103 MiB | 672092 GiB | 672031 GiB |
|       from small pool |     11 MiB |     13 MiB |   3544 GiB |   3544 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79682 MiB |  80500 MiB |    794 GiB | 734036 MiB |
|       from large pool |  79660 MiB |  80296 MiB |    786 GiB | 725302 MiB |
|       from small pool |     22 MiB |    204 MiB |      8 GiB |   8734 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   9991 MiB |   9991 MiB | 667497 GiB | 667487 GiB |
|       from large pool |   9980 MiB |   9980 MiB | 663485 GiB | 663475 GiB |
|       from small pool |     10 MiB |     17 MiB |   4011 GiB |   4011 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     515    |     557    |   42383 K  |   42383 K  |
|       from large pool |     230    |     272    |   19026 K  |   19025 K  |
|       from small pool |     285    |     342    |   23357 K  |   23357 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     515    |     557    |   42383 K  |   42383 K  |
|       from large pool |     230    |     272    |   19026 K  |   19025 K  |
|       from small pool |     285    |     342    |   23357 K  |   23357 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      95    |     258    |    8177    |    8082    |
|       from large pool |      84    |     156    |    3799    |    3715    |
|       from small pool |      11    |     102    |    4378    |    4367    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     101    |     102    |   23608 K  |   23608 K  |
|       from large pool |      78    |      79    |   12039 K  |   12039 K  |
|       from small pool |      23    |      42    |   11568 K  |   11568 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:18:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:18:17] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:18:23]    INFO >> epoch 015:    612 / 1539 loss=3.193, wps=4196.3, ups=6.07, wpb=691.8, bsz=691.8, num_updates=22100, lr=6.4e-05, gnorm=1.775, clip=0, train_wall=7, gb_free=72.8, wall=3698 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:18:32]    INFO >> epoch 015:    662 / 1539 loss=3.143, wps=4230.6, ups=5.77, wpb=733.6, bsz=733.6, num_updates=22150, lr=6.4e-05, gnorm=1.933, clip=0, train_wall=8, gb_free=73.4, wall=3707 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:18:40]    INFO >> epoch 015:    712 / 1539 loss=3.274, wps=4567.4, ups=6.17, wpb=740.6, bsz=740.6, num_updates=22200, lr=6.4e-05, gnorm=1.98, clip=0, train_wall=8, gb_free=74.4, wall=3715 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:18:50]    INFO >> epoch 015:    762 / 1539 loss=2.944, wps=3964.2, ups=5.68, wpb=698.4, bsz=698.4, num_updates=22250, lr=6.4e-05, gnorm=1.585, clip=0, train_wall=8, gb_free=72.2, wall=3724 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:18:58]    INFO >> epoch 015:    812 / 1539 loss=3.213, wps=4313.6, ups=6.25, wpb=690.6, bsz=690.6, num_updates=22300, lr=6.4e-05, gnorm=1.77, clip=0, train_wall=8, gb_free=72.8, wall=3732 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:19:03] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 938.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 23.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.37 GiB is allocated by PyTorch, and 1.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:19:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:19:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:19:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 59           |        cudaMalloc retries: 99        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73481 MiB |  79417 MiB | 684297 GiB | 684225 GiB |
|       from large pool |  73472 MiB |  79408 MiB | 680712 GiB | 680641 GiB |
|       from small pool |      8 MiB |     12 MiB |   3584 GiB |   3584 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73481 MiB |  79417 MiB | 684297 GiB | 684225 GiB |
|       from large pool |  73472 MiB |  79408 MiB | 680712 GiB | 680641 GiB |
|       from small pool |      8 MiB |     12 MiB |   3584 GiB |   3584 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73471 MiB |  79407 MiB | 683309 GiB | 683238 GiB |
|       from large pool |  73462 MiB |  79398 MiB | 679731 GiB | 679659 GiB |
|       from small pool |      8 MiB |     12 MiB |   3578 GiB |   3578 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80482 MiB |  80482 MiB |    801 GiB | 740628 MiB |
|       from large pool |  80460 MiB |  80460 MiB |    793 GiB | 731834 MiB |
|       from small pool |     22 MiB |     82 MiB |      8 GiB |   8794 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4272 MiB |   4272 MiB | 676190 GiB | 676185 GiB |
|       from large pool |   4259 MiB |   4259 MiB | 672138 GiB | 672134 GiB |
|       from small pool |     13 MiB |     17 MiB |   4051 GiB |   4051 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     571    |     589    |   42836 K  |   42836 K  |
|       from large pool |     286    |     304    |   19250 K  |   19250 K  |
|       from small pool |     285    |     342    |   23586 K  |   23585 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     571    |     589    |   42836 K  |   42836 K  |
|       from large pool |     286    |     304    |   19250 K  |   19250 K  |
|       from small pool |     285    |     342    |   23586 K  |   23585 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      96    |     125    |    8211    |    8115    |
|       from large pool |      85    |      85    |    3803    |    3718    |
|       from small pool |      11    |      41    |    4408    |    4397    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     110    |     110    |   23846 K  |   23846 K  |
|       from large pool |      89    |      89    |   12174 K  |   12174 K  |
|       from small pool |      21    |      39    |   11672 K  |   11672 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:19:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:19:03] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:19:06]    INFO >> epoch 015:    863 / 1539 loss=3.252, wps=3930, ups=5.9, wpb=666.5, bsz=666.5, num_updates=22350, lr=6.4e-05, gnorm=1.666, clip=0, train_wall=7, gb_free=68.9, wall=3740 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:19:15]    INFO >> epoch 015:    913 / 1539 loss=3.185, wps=4556.9, ups=6.03, wpb=755.6, bsz=755.6, num_updates=22400, lr=6.4e-05, gnorm=1.901, clip=0, train_wall=8, gb_free=63.2, wall=3748 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:19:24]    INFO >> epoch 015:    963 / 1539 loss=3.219, wps=4589.2, ups=6.12, wpb=750.1, bsz=750.1, num_updates=22450, lr=6.4e-05, gnorm=1.669, clip=0, train_wall=8, gb_free=72.4, wall=3757 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:19:32]    INFO >> epoch 015:   1013 / 1539 loss=3.122, wps=4374.7, ups=6.69, wpb=653.6, bsz=653.6, num_updates=22500, lr=6.4e-05, gnorm=1.712, clip=0, train_wall=7, gb_free=73.1, wall=3764 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:19:40]    INFO >> epoch 015:   1063 / 1539 loss=3.12, wps=3745.5, ups=6.32, wpb=593, bsz=593, num_updates=22550, lr=6.4e-05, gnorm=1.637, clip=0, train_wall=7, gb_free=73, wall=3772 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:19:48]    INFO >> epoch 015:   1113 / 1539 loss=3.189, wps=4231.1, ups=6.14, wpb=689.6, bsz=689.6, num_updates=22600, lr=6.4e-05, gnorm=1.728, clip=0, train_wall=8, gb_free=69.7, wall=3780 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:19:57]    INFO >> epoch 015:   1163 / 1539 loss=3.224, wps=3799.6, ups=6.33, wpb=600.6, bsz=600.6, num_updates=22650, lr=6.4e-05, gnorm=1.746, clip=0, train_wall=7, gb_free=68.7, wall=3788 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:20:05]    INFO >> epoch 015:   1213 / 1539 loss=3.274, wps=4351.1, ups=6.16, wpb=706.3, bsz=706.3, num_updates=22700, lr=6.4e-05, gnorm=1.855, clip=0, train_wall=8, gb_free=63.2, wall=3796 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:20:13]    INFO >> epoch 015:   1263 / 1539 loss=3.211, wps=5052.4, ups=6.4, wpb=789.9, bsz=789.9, num_updates=22750, lr=6.4e-05, gnorm=1.811, clip=0, train_wall=7, gb_free=72.3, wall=3804 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:20:21]    INFO >> epoch 015:   1313 / 1539 loss=3.122, wps=4939.5, ups=5.98, wpb=826.5, bsz=826.5, num_updates=22800, lr=6.4e-05, gnorm=1.972, clip=0, train_wall=8, gb_free=74.7, wall=3812 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:20:22] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 1.25 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 76.99 GiB is allocated by PyTorch, and 1.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:20:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:20:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:20:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 60           |        cudaMalloc retries: 103       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78782 MiB |  78842 MiB | 697328 GiB | 697251 GiB |
|       from large pool |  78390 MiB |  78450 MiB | 693679 GiB | 693602 GiB |
|       from small pool |    392 MiB |    393 MiB |   3649 GiB |   3648 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78782 MiB |  78842 MiB | 697328 GiB | 697251 GiB |
|       from large pool |  78390 MiB |  78450 MiB | 693679 GiB | 693602 GiB |
|       from small pool |    392 MiB |    393 MiB |   3649 GiB |   3648 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78748 MiB |  78807 MiB | 696323 GiB | 696246 GiB |
|       from large pool |  78358 MiB |  78417 MiB | 692679 GiB | 692603 GiB |
|       from small pool |    390 MiB |    391 MiB |   3643 GiB |   3643 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80504 MiB |  80504 MiB |    804 GiB | 743420 MiB |
|       from large pool |  80072 MiB |  80072 MiB |    795 GiB | 734622 MiB |
|       from small pool |    432 MiB |    434 MiB |      9 GiB |   8798 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1661 MiB |   4274 MiB | 691152 GiB | 691150 GiB |
|       from large pool |   1621 MiB |   4237 MiB | 687025 GiB | 687024 GiB |
|       from small pool |     39 MiB |     41 MiB |   4126 GiB |   4126 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7333    |    7336    |   43662 K  |   43655 K  |
|       from large pool |     887    |     888    |   19646 K  |   19645 K  |
|       from small pool |    6446    |    6449    |   24016 K  |   24009 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7333    |    7336    |   43662 K  |   43655 K  |
|       from large pool |     887    |     888    |   19646 K  |   19645 K  |
|       from small pool |    6446    |    6449    |   24016 K  |   24009 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     339    |     339    |    8458    |    8119    |
|       from large pool |     123    |     123    |    3843    |    3720    |
|       from small pool |     216    |     217    |    4615    |    4399    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     471    |     472    |   24283 K  |   24283 K  |
|       from large pool |      80    |      80    |   12413 K  |   12413 K  |
|       from small pool |     391    |     392    |   11870 K  |   11870 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:20:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:20:22] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:20:32]    INFO >> epoch 015:   1364 / 1539 loss=3.189, wps=3706.6, ups=5.45, wpb=679.8, bsz=679.8, num_updates=22850, lr=6.4e-05, gnorm=1.707, clip=0, train_wall=8, gb_free=72.1, wall=3822 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:20:40]    INFO >> epoch 015:   1414 / 1539 loss=3.098, wps=4671.4, ups=6, wpb=778.1, bsz=778.1, num_updates=22900, lr=6.4e-05, gnorm=1.667, clip=0, train_wall=8, gb_free=74.1, wall=3830 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:20:48]    INFO >> epoch 015:   1464 / 1539 loss=3.106, wps=4441.3, ups=6.66, wpb=666.5, bsz=666.5, num_updates=22950, lr=6.4e-05, gnorm=1.719, clip=0, train_wall=7, gb_free=75.7, wall=3837 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:20:55]    INFO >> epoch 015:   1514 / 1539 loss=3.393, wps=4192.2, ups=6.41, wpb=653.7, bsz=653.7, num_updates=23000, lr=6.4e-05, gnorm=1.778, clip=0, train_wall=7, gb_free=71, wall=3845 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:21:01]    INFO >> epoch 015 | loss 3.168 | wps 4077.4 | ups 5.72 | wpb 712.7 | bsz 712.7 | num_updates 23025 | lr 6.4e-05 | gnorm 1.757 | clip 0 | train_wall 233 | gb_free 72.3 | wall 3849 (progress_bar.py:267, print())[0m
[33m[2025-11-21 15:21:01] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:21:17]    INFO >> epoch 015 | valid on 'valid' subset | loss 3.5 | wps 9143.4 | wpb 5412.5 | bsz 5412.5 | num_updates 23025 | best_loss 3.524 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 15:21:18]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 15:21:18]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_last.pt (epoch 15 @ 23025 updates, score 3.5) (writing took 0.014762 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 15:21:18] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:21:22]    INFO >> epoch 016:     25 / 1539 loss=3.229, wps=1421.6, ups=2.02, wpb=703.6, bsz=703.6, num_updates=23050, lr=4.8e-05, gnorm=1.88, clip=0, train_wall=7, gb_free=73.3, wall=3870 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:21:29]    INFO >> epoch 016:     75 / 1539 loss=3.28, wps=4457.7, ups=6.5, wpb=686.3, bsz=686.3, num_updates=23100, lr=4.8e-05, gnorm=1.772, clip=0, train_wall=7, gb_free=71.5, wall=3878 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:21:39]    INFO >> epoch 016:    125 / 1539 loss=3.242, wps=3792.4, ups=6.3, wpb=602, bsz=602, num_updates=23150, lr=4.8e-05, gnorm=1.605, clip=0, train_wall=7, gb_free=76.3, wall=3886 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:21:46]    INFO >> epoch 016:    175 / 1539 loss=3.14, wps=3797.9, ups=6.31, wpb=602.2, bsz=602.2, num_updates=23200, lr=4.8e-05, gnorm=1.596, clip=0, train_wall=7, gb_free=70.8, wall=3893 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:21:55]    INFO >> epoch 016:    225 / 1539 loss=3.008, wps=4856.9, ups=6.16, wpb=788.5, bsz=788.5, num_updates=23250, lr=4.8e-05, gnorm=1.867, clip=0, train_wall=8, gb_free=65.6, wall=3902 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:22:03]    INFO >> epoch 016:    275 / 1539 loss=3.223, wps=4203.2, ups=5.68, wpb=739.8, bsz=739.8, num_updates=23300, lr=4.8e-05, gnorm=1.755, clip=0, train_wall=8, gb_free=71.7, wall=3910 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:22:12]    INFO >> epoch 016:    325 / 1539 loss=3.296, wps=4548.6, ups=6.54, wpb=695.3, bsz=695.3, num_updates=23350, lr=4.8e-05, gnorm=1.774, clip=0, train_wall=7, gb_free=75, wall=3918 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:22:20]    INFO >> epoch 016:    375 / 1539 loss=3.27, wps=4380.1, ups=6.31, wpb=693.6, bsz=693.6, num_updates=23400, lr=4.8e-05, gnorm=1.946, clip=0, train_wall=7, gb_free=70.1, wall=3926 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:22:24] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.46 GiB. GPU 2 has a total capacity of 79.14 GiB of which 471.25 MiB is free. Including non-PyTorch memory, this process has 78.66 GiB memory in use. Of the allocated memory 75.55 GiB is allocated by PyTorch, and 2.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:22:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:22:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:22:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 61           |        cudaMalloc retries: 104       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  66236 MiB |  77546 MiB | 718499 GiB | 718434 GiB |
|       from large pool |  66227 MiB |  77538 MiB | 714735 GiB | 714670 GiB |
|       from small pool |      8 MiB |     13 MiB |   3763 GiB |   3763 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  66236 MiB |  77546 MiB | 718499 GiB | 718434 GiB |
|       from large pool |  66227 MiB |  77538 MiB | 714735 GiB | 714670 GiB |
|       from small pool |      8 MiB |     13 MiB |   3763 GiB |   3763 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  66225 MiB |  77534 MiB | 717463 GiB | 717398 GiB |
|       from large pool |  66216 MiB |  77525 MiB | 713705 GiB | 713640 GiB |
|       from small pool |      8 MiB |     13 MiB |   3757 GiB |   3757 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80034 MiB |  80444 MiB |    804 GiB | 743890 MiB |
|       from large pool |  80012 MiB |  80012 MiB |    795 GiB | 734682 MiB |
|       from small pool |     22 MiB |    432 MiB |      9 GiB |   9208 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5707 MiB |   7356 MiB | 710433 GiB | 710428 GiB |
|       from large pool |   5694 MiB |   7343 MiB | 706181 GiB | 706176 GiB |
|       from small pool |     13 MiB |     17 MiB |   4252 GiB |   4252 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     545    |     586    |   44961 K  |   44960 K  |
|       from large pool |     260    |     301    |   20193 K  |   20193 K  |
|       from small pool |     285    |     356    |   24768 K  |   24767 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     545    |     586    |   44961 K  |   44960 K  |
|       from large pool |     260    |     301    |   20193 K  |   20193 K  |
|       from small pool |     285    |     356    |   24768 K  |   24767 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     133    |     338    |    8458    |    8325    |
|       from large pool |     122    |     122    |    3843    |    3721    |
|       from small pool |      11    |     216    |    4615    |    4604    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     139    |     140    |   25004 K  |   25004 K  |
|       from large pool |     117    |     118    |   12759 K  |   12759 K  |
|       from small pool |      22    |      46    |   12245 K  |   12245 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:22:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:22:24] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:22:29]    INFO >> epoch 016:    426 / 1539 loss=3.14, wps=3929.9, ups=5.46, wpb=719.4, bsz=719.4, num_updates=23450, lr=4.8e-05, gnorm=1.683, clip=0, train_wall=8, gb_free=72.5, wall=3935 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:22:36] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 57.25 MiB is free. Including non-PyTorch memory, this process has 79.06 GiB memory in use. Of the allocated memory 76.40 GiB is allocated by PyTorch, and 2.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:22:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:22:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:22:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 62           |        cudaMalloc retries: 105       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78174 MiB |  78234 MiB | 720599 GiB | 720523 GiB |
|       from large pool |  77788 MiB |  77848 MiB | 716822 GiB | 716746 GiB |
|       from small pool |    385 MiB |    386 MiB |   3777 GiB |   3776 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78174 MiB |  78234 MiB | 720599 GiB | 720523 GiB |
|       from large pool |  77788 MiB |  77848 MiB | 716822 GiB | 716746 GiB |
|       from small pool |    385 MiB |    386 MiB |   3777 GiB |   3776 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78087 MiB |  78147 MiB | 719560 GiB | 719484 GiB |
|       from large pool |  77704 MiB |  77763 MiB | 715789 GiB | 715713 GiB |
|       from small pool |    383 MiB |    384 MiB |   3771 GiB |   3770 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80448 MiB |  80448 MiB |    812 GiB | 751980 MiB |
|       from large pool |  80022 MiB |  80022 MiB |    803 GiB | 742772 MiB |
|       from small pool |    426 MiB |    426 MiB |      9 GiB |   9208 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2213 MiB |   5585 MiB | 712582 GiB | 712580 GiB |
|       from large pool |   2173 MiB |   5580 MiB | 708314 GiB | 708312 GiB |
|       from small pool |     40 MiB |     41 MiB |   4267 GiB |   4267 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7212    |    7215    |   45111 K  |   45104 K  |
|       from large pool |     876    |     877    |   20254 K  |   20254 K  |
|       from small pool |    6336    |    6339    |   24856 K  |   24850 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7212    |    7215    |   45111 K  |   45104 K  |
|       from large pool |     876    |     877    |   20254 K  |   20254 K  |
|       from small pool |    6336    |    6339    |   24856 K  |   24850 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     467    |     467    |    8795    |    8328    |
|       from large pool |     254    |     254    |    3978    |    3724    |
|       from small pool |     213    |     213    |    4817    |    4604    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     493    |     493    |   25088 K  |   25088 K  |
|       from large pool |     109    |     109    |   12798 K  |   12798 K  |
|       from small pool |     384    |     384    |   12290 K  |   12290 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:22:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:22:36] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:22:38]    INFO >> epoch 016:    477 / 1539 loss=3.177, wps=4640.9, ups=5.63, wpb=824.9, bsz=824.9, num_updates=23500, lr=4.8e-05, gnorm=1.724, clip=0, train_wall=8, gb_free=69.3, wall=3944 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:22:48]    INFO >> epoch 016:    527 / 1539 loss=3.269, wps=4208.7, ups=6.38, wpb=659.3, bsz=659.3, num_updates=23550, lr=4.8e-05, gnorm=1.833, clip=0, train_wall=7, gb_free=73.6, wall=3952 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:22:56]    INFO >> epoch 016:    577 / 1539 loss=3.118, wps=4839, ups=6.16, wpb=785, bsz=785, num_updates=23600, lr=4.8e-05, gnorm=1.887, clip=0, train_wall=8, gb_free=75.9, wall=3960 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:23:03]    INFO >> epoch 016:    627 / 1539 loss=3.115, wps=4309.3, ups=6.65, wpb=647.6, bsz=647.6, num_updates=23650, lr=4.8e-05, gnorm=1.558, clip=0, train_wall=7, gb_free=73.1, wall=3967 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:23:12]    INFO >> epoch 016:    677 / 1539 loss=3.141, wps=3674, ups=5.58, wpb=657.9, bsz=657.9, num_updates=23700, lr=4.8e-05, gnorm=1.648, clip=0, train_wall=8, gb_free=71.7, wall=3976 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:23:22]    INFO >> epoch 016:    727 / 1539 loss=3.251, wps=4227.8, ups=6.16, wpb=686.5, bsz=686.5, num_updates=23750, lr=4.8e-05, gnorm=1.783, clip=0, train_wall=8, gb_free=72, wall=3985 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:23:30]    INFO >> epoch 016:    777 / 1539 loss=3.123, wps=4401.9, ups=5.78, wpb=761.6, bsz=761.6, num_updates=23800, lr=4.8e-05, gnorm=1.869, clip=0, train_wall=8, gb_free=61.2, wall=3993 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:23:39]    INFO >> epoch 016:    827 / 1539 loss=3.105, wps=4516.2, ups=6.03, wpb=749, bsz=749, num_updates=23850, lr=4.8e-05, gnorm=1.637, clip=0, train_wall=8, gb_free=71.4, wall=4001 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:23:47]    INFO >> epoch 016:    877 / 1539 loss=3.105, wps=3967.3, ups=5.85, wpb=678.3, bsz=678.3, num_updates=23900, lr=4.8e-05, gnorm=2.083, clip=0, train_wall=8, gb_free=65.9, wall=4010 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:23:56]    INFO >> epoch 016:    927 / 1539 loss=3.311, wps=4771.1, ups=6.62, wpb=720.8, bsz=720.8, num_updates=23950, lr=4.8e-05, gnorm=1.838, clip=0, train_wall=7, gb_free=74.4, wall=4018 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:24:04]    INFO >> epoch 016:    977 / 1539 loss=3.287, wps=4578.2, ups=6.11, wpb=748.9, bsz=748.9, num_updates=24000, lr=4.8e-05, gnorm=1.813, clip=0, train_wall=8, gb_free=71.5, wall=4026 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:24:12]    INFO >> epoch 016:   1027 / 1539 loss=3.216, wps=3890.2, ups=6.3, wpb=617.4, bsz=617.4, num_updates=24050, lr=4.8e-05, gnorm=1.612, clip=0, train_wall=7, gb_free=66.2, wall=4034 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:24:20]    INFO >> epoch 016:   1077 / 1539 loss=3.211, wps=4020.6, ups=6.28, wpb=639.8, bsz=639.8, num_updates=24100, lr=4.8e-05, gnorm=1.802, clip=0, train_wall=7, gb_free=70.6, wall=4042 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:24:27] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 9.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 76.99 GiB is allocated by PyTorch, and 1.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:24:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:24:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:24:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 63           |        cudaMalloc retries: 108       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78778 MiB |  78838 MiB | 738859 GiB | 738782 GiB |
|       from large pool |  78645 MiB |  78705 MiB | 734994 GiB | 734918 GiB |
|       from small pool |    132 MiB |    133 MiB |   3865 GiB |   3864 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78778 MiB |  78838 MiB | 738859 GiB | 738782 GiB |
|       from large pool |  78645 MiB |  78705 MiB | 734994 GiB | 734918 GiB |
|       from small pool |    132 MiB |    133 MiB |   3865 GiB |   3864 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78699 MiB |  78758 MiB | 737794 GiB | 737717 GiB |
|       from large pool |  78567 MiB |  78626 MiB | 733935 GiB | 733858 GiB |
|       from small pool |    132 MiB |    133 MiB |   3858 GiB |   3858 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80496 MiB |  80498 MiB |    834 GiB | 773582 MiB |
|       from large pool |  80360 MiB |  80360 MiB |    824 GiB | 763886 MiB |
|       from small pool |    136 MiB |    218 MiB |      9 GiB |   9696 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1657 MiB |   6579 MiB | 730558 GiB | 730556 GiB |
|       from large pool |   1654 MiB |   6572 MiB | 726189 GiB | 726187 GiB |
|       from small pool |      3 MiB |     21 MiB |   4369 GiB |   4369 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2713    |    2716    |   46232 K  |   46229 K  |
|       from large pool |     481    |     482    |   20795 K  |   20795 K  |
|       from small pool |    2232    |    2235    |   25436 K  |   25434 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2713    |    2716    |   46232 K  |   46229 K  |
|       from large pool |     481    |     482    |   20795 K  |   20795 K  |
|       from small pool |    2232    |    2235    |   25436 K  |   25434 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     321    |     360    |    9029    |    8708    |
|       from large pool |     253    |     253    |    4113    |    3860    |
|       from small pool |      68    |     109    |    4916    |    4848    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     178    |     179    |   25700 K  |   25700 K  |
|       from large pool |     113    |     115    |   13142 K  |   13141 K  |
|       from small pool |      65    |      66    |   12558 K  |   12558 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:24:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:24:27] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:24:30]    INFO >> epoch 016:   1128 / 1539 loss=3.162, wps=4846.8, ups=5.84, wpb=830.1, bsz=830.1, num_updates=24150, lr=4.8e-05, gnorm=1.814, clip=0, train_wall=7, gb_free=70.5, wall=4050 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:24:39]    INFO >> epoch 016:   1178 / 1539 loss=3.207, wps=5504.9, ups=5.54, wpb=994.5, bsz=994.5, num_updates=24200, lr=4.8e-05, gnorm=2.056, clip=0, train_wall=9, gb_free=71.9, wall=4059 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:24:47]    INFO >> epoch 016:   1228 / 1539 loss=3.104, wps=4459.2, ups=6.55, wpb=680.8, bsz=680.8, num_updates=24250, lr=4.8e-05, gnorm=1.832, clip=0, train_wall=7, gb_free=71.7, wall=4067 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:24:56]    INFO >> epoch 016:   1278 / 1539 loss=3.162, wps=3814.6, ups=6.46, wpb=590.4, bsz=590.4, num_updates=24300, lr=4.8e-05, gnorm=1.669, clip=0, train_wall=7, gb_free=76.4, wall=4075 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:25:04]    INFO >> epoch 016:   1328 / 1539 loss=3.104, wps=4148.2, ups=6.38, wpb=649.8, bsz=649.8, num_updates=24350, lr=4.8e-05, gnorm=1.613, clip=0, train_wall=7, gb_free=69.1, wall=4082 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:25:12]    INFO >> epoch 016:   1378 / 1539 loss=2.894, wps=4513.7, ups=6.02, wpb=749.3, bsz=749.3, num_updates=24400, lr=4.8e-05, gnorm=1.705, clip=0, train_wall=8, gb_free=71.9, wall=4091 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:25:19]    INFO >> epoch 016:   1428 / 1539 loss=3.197, wps=4860.8, ups=6.61, wpb=734.9, bsz=734.9, num_updates=24450, lr=4.8e-05, gnorm=1.809, clip=0, train_wall=7, gb_free=71.9, wall=4098 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:25:26] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.17 GiB. GPU 2 has a total capacity of 79.14 GiB of which 839.25 MiB is free. Including non-PyTorch memory, this process has 78.30 GiB memory in use. Of the allocated memory 74.62 GiB is allocated by PyTorch, and 3.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:25:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:25:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:25:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 64           |        cudaMalloc retries: 111       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  70625 MiB |  76890 MiB | 748505 GiB | 748436 GiB |
|       from large pool |  70614 MiB |  76879 MiB | 744590 GiB | 744521 GiB |
|       from small pool |     11 MiB |     25 MiB |   3915 GiB |   3915 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  70625 MiB |  76890 MiB | 748505 GiB | 748436 GiB |
|       from large pool |  70614 MiB |  76879 MiB | 744590 GiB | 744521 GiB |
|       from small pool |     11 MiB |     25 MiB |   3915 GiB |   3915 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  70612 MiB |  76876 MiB | 747425 GiB | 747356 GiB |
|       from large pool |  70600 MiB |  76865 MiB | 743515 GiB | 743447 GiB |
|       from small pool |     11 MiB |     25 MiB |   3909 GiB |   3909 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79666 MiB |  80078 MiB |    868 GiB |    791 GiB |
|       from large pool |  79642 MiB |  79840 MiB |    859 GiB |    781 GiB |
|       from small pool |     24 MiB |    238 MiB |      9 GiB |      9 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4732 MiB |   7568 MiB | 739264 GiB | 739259 GiB |
|       from large pool |   4719 MiB |   7555 MiB | 734835 GiB | 734831 GiB |
|       from small pool |     12 MiB |     21 MiB |   4428 GiB |   4428 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     541    |     559    |   46859 K  |   46859 K  |
|       from large pool |     256    |     274    |   21091 K  |   21091 K  |
|       from small pool |     285    |     356    |   25768 K  |   25768 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     541    |     559    |   46859 K  |   46859 K  |
|       from large pool |     256    |     274    |   21091 K  |   21091 K  |
|       from small pool |     285    |     356    |   25768 K  |   25768 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     128    |     423    |    9152    |    9024    |
|       from large pool |     116    |     304    |    4185    |    4069    |
|       from small pool |      12    |     119    |    4967    |    4955    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     130    |     130    |   26056 K  |   26056 K  |
|       from large pool |     104    |     104    |   13337 K  |   13337 K  |
|       from small pool |      26    |      49    |   12718 K  |   12718 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:25:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:25:26] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:25:30]    INFO >> epoch 016:   1479 / 1539 loss=3.099, wps=4006.3, ups=5.69, wpb=704.5, bsz=704.5, num_updates=24500, lr=4.8e-05, gnorm=1.623, clip=0, train_wall=7, gb_free=71.7, wall=4107 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:25:37]    INFO >> epoch 016:   1529 / 1539 loss=2.989, wps=4486.3, ups=6.34, wpb=708.1, bsz=708.1, num_updates=24550, lr=4.8e-05, gnorm=1.564, clip=0, train_wall=7, gb_free=68.5, wall=4115 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:25:39]    INFO >> epoch 016 | loss 3.166 | wps 4088.6 | ups 5.74 | wpb 712.7 | bsz 712.7 | num_updates 24560 | lr 4.8e-05 | gnorm 1.767 | clip 0 | train_wall 232 | gb_free 70.4 | wall 4117 (progress_bar.py:267, print())[0m
[33m[2025-11-21 15:25:39] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:25:55]    INFO >> epoch 016 | valid on 'valid' subset | loss 3.499 | wps 9536.4 | wpb 5412.5 | bsz 5412.5 | num_updates 24560 | best_loss 3.524 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 15:25:55]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 15:25:55]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_last.pt (epoch 16 @ 24560 updates, score 3.499) (writing took 0.014993 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 15:25:55] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:26:01]    INFO >> epoch 017:     40 / 1539 loss=3.098, wps=1498.1, ups=2.08, wpb=718.6, bsz=718.6, num_updates=24600, lr=3.5e-05, gnorm=1.72, clip=0, train_wall=7, gb_free=72, wall=4139 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:26:10]    INFO >> epoch 017:     90 / 1539 loss=3.036, wps=4293.1, ups=6.69, wpb=642.2, bsz=642.2, num_updates=24650, lr=3.5e-05, gnorm=1.687, clip=0, train_wall=7, gb_free=70.9, wall=4146 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:26:20]    INFO >> epoch 017:    140 / 1539 loss=3.244, wps=4510.7, ups=5.28, wpb=854.2, bsz=854.2, num_updates=24700, lr=3.5e-05, gnorm=1.776, clip=0, train_wall=9, gb_free=74.8, wall=4156 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:26:27]    INFO >> epoch 017:    190 / 1539 loss=3.244, wps=4281.7, ups=6.6, wpb=648.4, bsz=648.4, num_updates=24750, lr=3.5e-05, gnorm=1.854, clip=0, train_wall=7, gb_free=72.2, wall=4164 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:26:39] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 938.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 579.25 MiB is free. Including non-PyTorch memory, this process has 78.55 GiB memory in use. Of the allocated memory 77.38 GiB is allocated by PyTorch, and 683.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:26:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:26:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:26:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 65           |        cudaMalloc retries: 113       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73490 MiB |  79430 MiB | 761647 GiB | 761575 GiB |
|       from large pool |  73481 MiB |  79421 MiB | 757651 GiB | 757580 GiB |
|       from small pool |      8 MiB |     17 MiB |   3995 GiB |   3995 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73490 MiB |  79430 MiB | 761647 GiB | 761575 GiB |
|       from large pool |  73481 MiB |  79421 MiB | 757651 GiB | 757580 GiB |
|       from small pool |      8 MiB |     17 MiB |   3995 GiB |   3995 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73471 MiB |  79407 MiB | 760548 GiB | 760476 GiB |
|       from large pool |  73462 MiB |  79398 MiB | 756559 GiB | 756487 GiB |
|       from small pool |      8 MiB |     17 MiB |   3988 GiB |   3988 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79926 MiB |  80114 MiB |    941 GiB |    863 GiB |
|       from large pool |  79904 MiB |  80092 MiB |    931 GiB |    853 GiB |
|       from small pool |     22 MiB |    238 MiB |      9 GiB |      9 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2955 MiB |   3940 MiB | 750685 GiB | 750682 GiB |
|       from large pool |   2942 MiB |   3926 MiB | 746170 GiB | 746167 GiB |
|       from small pool |     13 MiB |     17 MiB |   4514 GiB |   4514 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     571    |     589    |   47684 K  |   47683 K  |
|       from large pool |     286    |     304    |   21394 K  |   21394 K  |
|       from small pool |     285    |     356    |   26289 K  |   26289 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     571    |     589    |   47684 K  |   47683 K  |
|       from large pool |     286    |     304    |   21394 K  |   21394 K  |
|       from small pool |     285    |     356    |   26289 K  |   26289 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     146    |     232    |    9359    |    9213    |
|       from large pool |     135    |     136    |    4285    |    4150    |
|       from small pool |      11    |     119    |    5074    |    5063    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     146    |     147    |   26538 K  |   26538 K  |
|       from large pool |     122    |     123    |   13530 K  |   13530 K  |
|       from small pool |      24    |      46    |   13008 K  |   13008 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:26:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:26:39] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:26:39]    INFO >> epoch 017:    241 / 1539 loss=3.184, wps=3298.9, ups=4.73, wpb=696.8, bsz=696.8, num_updates=24800, lr=3.5e-05, gnorm=1.65, clip=0, train_wall=8, gb_free=2.4, wall=4174 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:26:46]    INFO >> epoch 017:    291 / 1539 loss=3.248, wps=4345.9, ups=6.84, wpb=635.2, bsz=635.2, num_updates=24850, lr=3.5e-05, gnorm=1.675, clip=0, train_wall=7, gb_free=70.5, wall=4181 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:26:54]    INFO >> epoch 017:    341 / 1539 loss=3.172, wps=4210.4, ups=6.34, wpb=664.5, bsz=664.5, num_updates=24900, lr=3.5e-05, gnorm=1.614, clip=0, train_wall=7, gb_free=69.9, wall=4189 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:27:02]    INFO >> epoch 017:    391 / 1539 loss=3.145, wps=4362.5, ups=6.27, wpb=696, bsz=696, num_updates=24950, lr=3.5e-05, gnorm=1.604, clip=0, train_wall=8, gb_free=76.3, wall=4197 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:27:12]    INFO >> epoch 017:    441 / 1539 loss=3.305, wps=4461.4, ups=6.33, wpb=704.9, bsz=704.9, num_updates=25000, lr=3.5e-05, gnorm=1.691, clip=0, train_wall=7, gb_free=72.5, wall=4205 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:27:20]    INFO >> epoch 017:    491 / 1539 loss=3.228, wps=4462.9, ups=6.19, wpb=721.4, bsz=721.4, num_updates=25050, lr=3.5e-05, gnorm=1.826, clip=0, train_wall=8, gb_free=71.6, wall=4213 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:27:27]    INFO >> epoch 017:    541 / 1539 loss=3.236, wps=4084.8, ups=6.42, wpb=636.3, bsz=636.3, num_updates=25100, lr=3.5e-05, gnorm=1.811, clip=0, train_wall=7, gb_free=62.2, wall=4221 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:27:36]    INFO >> epoch 017:    591 / 1539 loss=3.181, wps=4248.5, ups=6.09, wpb=698.2, bsz=698.2, num_updates=25150, lr=3.5e-05, gnorm=1.607, clip=0, train_wall=8, gb_free=73.3, wall=4229 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:27:41] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 1.25 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 76.01 GiB is allocated by PyTorch, and 2.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:27:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:27:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:27:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 66           |        cudaMalloc retries: 117       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77772 MiB |  77832 MiB | 771978 GiB | 771902 GiB |
|       from large pool |  77391 MiB |  77451 MiB | 767932 GiB | 767857 GiB |
|       from small pool |    381 MiB |    383 MiB |   4045 GiB |   4045 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77772 MiB |  77832 MiB | 771978 GiB | 771902 GiB |
|       from large pool |  77391 MiB |  77451 MiB | 767932 GiB | 767857 GiB |
|       from small pool |    381 MiB |    383 MiB |   4045 GiB |   4045 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77727 MiB |  77786 MiB | 770864 GiB | 770788 GiB |
|       from large pool |  77347 MiB |  77407 MiB | 766825 GiB | 766749 GiB |
|       from small pool |    379 MiB |    380 MiB |   4039 GiB |   4039 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80504 MiB |  80504 MiB |    945 GiB |    866 GiB |
|       from large pool |  80084 MiB |  80084 MiB |    935 GiB |    857 GiB |
|       from small pool |    420 MiB |    422 MiB |     10 GiB |      9 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2671 MiB |   7559 MiB | 761111 GiB | 761108 GiB |
|       from large pool |   2632 MiB |   7555 MiB | 756538 GiB | 756535 GiB |
|       from small pool |     38 MiB |     40 MiB |   4573 GiB |   4573 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7146    |    7149    |   48332 K  |   48325 K  |
|       from large pool |     870    |     871    |   21708 K  |   21707 K  |
|       from small pool |    6276    |    6279    |   26623 K  |   26617 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7146    |    7149    |   48332 K  |   48325 K  |
|       from large pool |     870    |     871    |   21708 K  |   21707 K  |
|       from small pool |    6276    |    6279    |   26623 K  |   26617 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     401    |     401    |    9622    |    9221    |
|       from large pool |     191    |     191    |    4347    |    4156    |
|       from small pool |     210    |     211    |    5275    |    5065    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     505    |     507    |   26890 K  |   26890 K  |
|       from large pool |     126    |     126    |   13729 K  |   13729 K  |
|       from small pool |     379    |     381    |   13161 K  |   13161 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:27:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:27:41] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:27:45]    INFO >> epoch 017:    642 / 1539 loss=3.162, wps=4172.2, ups=6.15, wpb=678.2, bsz=678.2, num_updates=25200, lr=3.5e-05, gnorm=1.67, clip=0, train_wall=7, gb_free=75.1, wall=4237 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:27:53]    INFO >> epoch 017:    692 / 1539 loss=3.369, wps=4347.7, ups=6.17, wpb=704.2, bsz=704.2, num_updates=25250, lr=3.5e-05, gnorm=1.94, clip=0, train_wall=8, gb_free=72.1, wall=4245 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:28:01]    INFO >> epoch 017:    742 / 1539 loss=3.017, wps=4572.3, ups=6.15, wpb=742.9, bsz=742.9, num_updates=25300, lr=3.5e-05, gnorm=1.945, clip=0, train_wall=8, gb_free=74.3, wall=4254 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:28:09]    INFO >> epoch 017:    792 / 1539 loss=3.092, wps=4050.3, ups=6.38, wpb=634.9, bsz=634.9, num_updates=25350, lr=3.5e-05, gnorm=1.597, clip=0, train_wall=7, gb_free=65.7, wall=4261 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:28:11] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 57.25 MiB is free. Including non-PyTorch memory, this process has 79.06 GiB memory in use. Of the allocated memory 75.69 GiB is allocated by PyTorch, and 2.87 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:28:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:28:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:28:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 67           |        cudaMalloc retries: 119       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77449 MiB |  77509 MiB | 776778 GiB | 776703 GiB |
|       from large pool |  77329 MiB |  77389 MiB | 772709 GiB | 772634 GiB |
|       from small pool |    119 MiB |    121 MiB |   4068 GiB |   4068 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77449 MiB |  77509 MiB | 776778 GiB | 776703 GiB |
|       from large pool |  77329 MiB |  77389 MiB | 772709 GiB | 772634 GiB |
|       from small pool |    119 MiB |    121 MiB |   4068 GiB |   4068 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77437 MiB |  77496 MiB | 775656 GiB | 775581 GiB |
|       from large pool |  77317 MiB |  77377 MiB | 771594 GiB | 771518 GiB |
|       from small pool |    119 MiB |    120 MiB |   4062 GiB |   4062 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80448 MiB |  80504 MiB |    945 GiB |    867 GiB |
|       from large pool |  80324 MiB |  80324 MiB |    935 GiB |    857 GiB |
|       from small pool |    124 MiB |    420 MiB |     10 GiB |     10 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2938 MiB |   9201 MiB | 765245 GiB | 765242 GiB |
|       from large pool |   2934 MiB |   9190 MiB | 760645 GiB | 760642 GiB |
|       from small pool |      4 MiB |     21 MiB |   4599 GiB |   4599 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2482    |    2485    |   48631 K  |   48628 K  |
|       from large pool |     460    |     461    |   21855 K  |   21854 K  |
|       from small pool |    2022    |    2025    |   26776 K  |   26774 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2482    |    2485    |   48631 K  |   48628 K  |
|       from large pool |     460    |     461    |   21855 K  |   21854 K  |
|       from small pool |    2022    |    2025    |   26776 K  |   26774 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     257    |     401    |    9629    |    9372    |
|       from large pool |     195    |     195    |    4352    |    4157    |
|       from small pool |      62    |     210    |    5277    |    5215    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     240    |     241    |   27058 K  |   27057 K  |
|       from large pool |     181    |     181    |   13826 K  |   13825 K  |
|       from small pool |      59    |      60    |   13231 K  |   13231 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:28:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:28:11] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:28:19]    INFO >> epoch 017:    843 / 1539 loss=3.097, wps=4012.4, ups=5.72, wpb=701.9, bsz=701.9, num_updates=25400, lr=3.5e-05, gnorm=1.757, clip=0, train_wall=8, gb_free=72.4, wall=4270 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:28:28]    INFO >> epoch 017:    893 / 1539 loss=3.054, wps=4352.8, ups=5.96, wpb=729.9, bsz=729.9, num_updates=25450, lr=3.5e-05, gnorm=1.648, clip=0, train_wall=8, gb_free=73.5, wall=4279 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:28:35]    INFO >> epoch 017:    943 / 1539 loss=3.137, wps=4429.7, ups=6.46, wpb=685.7, bsz=685.7, num_updates=25500, lr=3.5e-05, gnorm=1.779, clip=0, train_wall=7, gb_free=74.5, wall=4286 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:28:43]    INFO >> epoch 017:    993 / 1539 loss=3.312, wps=4162.9, ups=6.25, wpb=665.9, bsz=665.9, num_updates=25550, lr=3.5e-05, gnorm=1.908, clip=0, train_wall=8, gb_free=73.7, wall=4294 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:28:53]    INFO >> epoch 017:   1043 / 1539 loss=3.27, wps=3969.9, ups=5.81, wpb=683.1, bsz=683.1, num_updates=25600, lr=3.5e-05, gnorm=1.747, clip=0, train_wall=8, gb_free=67.1, wall=4303 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:29:01]    INFO >> epoch 017:   1093 / 1539 loss=3.399, wps=4541, ups=6.27, wpb=724.4, bsz=724.4, num_updates=25650, lr=3.5e-05, gnorm=1.774, clip=0, train_wall=7, gb_free=73, wall=4311 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:29:09]    INFO >> epoch 017:   1143 / 1539 loss=3.033, wps=4455.5, ups=6.35, wpb=702.1, bsz=702.1, num_updates=25700, lr=3.5e-05, gnorm=1.65, clip=0, train_wall=7, gb_free=67.1, wall=4319 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:29:17]    INFO >> epoch 017:   1193 / 1539 loss=2.803, wps=4878.9, ups=6.16, wpb=792.2, bsz=792.2, num_updates=25750, lr=3.5e-05, gnorm=1.748, clip=0, train_wall=8, gb_free=73.3, wall=4327 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:29:26]    INFO >> epoch 017:   1243 / 1539 loss=3.211, wps=4053.4, ups=6.54, wpb=619.5, bsz=619.5, num_updates=25800, lr=3.5e-05, gnorm=1.676, clip=0, train_wall=7, gb_free=71.7, wall=4335 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:29:36]    INFO >> epoch 017:   1293 / 1539 loss=3.152, wps=5053.6, ups=5.12, wpb=988, bsz=988, num_updates=25850, lr=3.5e-05, gnorm=2.025, clip=0, train_wall=9, gb_free=72, wall=4344 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:29:44]    INFO >> epoch 017:   1343 / 1539 loss=3.385, wps=4127.5, ups=6.05, wpb=681.8, bsz=681.8, num_updates=25900, lr=3.5e-05, gnorm=1.791, clip=0, train_wall=8, gb_free=58.4, wall=4353 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:29:50] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.88 GiB. GPU 2 has a total capacity of 79.14 GiB of which 807.25 MiB is free. Including non-PyTorch memory, this process has 78.33 GiB memory in use. Of the allocated memory 72.51 GiB is allocated by PyTorch, and 5.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:29:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:29:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:29:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 68           |        cudaMalloc retries: 121       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  63159 MiB |  75128 MiB |    774 TiB |    774 TiB |
|       from large pool |  63148 MiB |  75117 MiB |    770 TiB |    770 TiB |
|       from small pool |     11 MiB |     20 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| Active memory         |  63159 MiB |  75128 MiB |    774 TiB |    774 TiB |
|       from large pool |  63148 MiB |  75117 MiB |    770 TiB |    770 TiB |
|       from small pool |     11 MiB |     20 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| Requested memory      |  63147 MiB |  75114 MiB |    773 TiB |    773 TiB |
|       from large pool |  63136 MiB |  75103 MiB |    769 TiB |    769 TiB |
|       from small pool |     11 MiB |     20 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79698 MiB |  80482 MiB |    967 GiB |    890 GiB |
|       from large pool |  79676 MiB |  80264 MiB |    957 GiB |    879 GiB |
|       from small pool |     22 MiB |    218 MiB |     10 GiB |     10 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   8422 MiB |   9997 MiB | 779364 GiB | 779355 GiB |
|       from large pool |   8411 MiB |   9986 MiB | 774671 GiB | 774663 GiB |
|       from small pool |     10 MiB |     19 MiB |   4692 GiB |   4692 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     515    |     557    |   49645 K  |   49644 K  |
|       from large pool |     230    |     272    |   22338 K  |   22337 K  |
|       from small pool |     285    |     356    |   27307 K  |   27306 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     515    |     557    |   49645 K  |   49644 K  |
|       from large pool |     230    |     272    |   22338 K  |   22337 K  |
|       from small pool |     285    |     356    |   27307 K  |   27306 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      96    |     303    |    9696    |    9600    |
|       from large pool |      85    |     194    |    4372    |    4287    |
|       from small pool |      11    |     109    |    5324    |    5313    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      96    |      98    |   27631 K  |   27631 K  |
|       from large pool |      74    |      76    |   14146 K  |   14146 K  |
|       from small pool |      22    |      44    |   13484 K  |   13484 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:29:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:29:50] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:29:54]    INFO >> epoch 017:   1394 / 1539 loss=3.085, wps=4039.6, ups=5.23, wpb=772.8, bsz=772.8, num_updates=25950, lr=3.5e-05, gnorm=1.861, clip=0, train_wall=8, gb_free=63.7, wall=4362 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:30:04]    INFO >> epoch 017:   1444 / 1539 loss=3.192, wps=4663.9, ups=5.83, wpb=800.2, bsz=800.2, num_updates=26000, lr=3.5e-05, gnorm=2.096, clip=0, train_wall=8, gb_free=70.8, wall=4371 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:30:12]    INFO >> epoch 017:   1494 / 1539 loss=3.035, wps=4975.8, ups=6.19, wpb=804.2, bsz=804.2, num_updates=26050, lr=3.5e-05, gnorm=1.842, clip=0, train_wall=8, gb_free=72.1, wall=4379 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:30:19]    INFO >> epoch 017 | loss 3.165 | wps 4059.5 | ups 5.7 | wpb 712.7 | bsz 712.7 | num_updates 26095 | lr 3.5e-05 | gnorm 1.761 | clip 0 | train_wall 233 | gb_free 76.4 | wall 4386 (progress_bar.py:267, print())[0m
[33m[2025-11-21 15:30:19] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:30:35]    INFO >> epoch 017 | valid on 'valid' subset | loss 3.499 | wps 10199.5 | wpb 5412.5 | bsz 5412.5 | num_updates 26095 | best_loss 3.524 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 15:30:36]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 15:30:36]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_last.pt (epoch 17 @ 26095 updates, score 3.499) (writing took 0.014002 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 15:30:36] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:30:37]    INFO >> epoch 018:      5 / 1539 loss=3.029, wps=1421.5, ups=2.15, wpb=660.6, bsz=660.6, num_updates=26100, lr=2.6e-05, gnorm=1.666, clip=0, train_wall=7, gb_free=74.2, wall=4402 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:30:45]    INFO >> epoch 018:     55 / 1539 loss=3.34, wps=3966, ups=6.13, wpb=646.5, bsz=646.5, num_updates=26150, lr=2.6e-05, gnorm=1.855, clip=0, train_wall=8, gb_free=71.8, wall=4410 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:30:53]    INFO >> epoch 018:    105 / 1539 loss=3.115, wps=4605.6, ups=6.01, wpb=765.9, bsz=765.9, num_updates=26200, lr=2.6e-05, gnorm=1.814, clip=0, train_wall=8, gb_free=73.3, wall=4418 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:31:01]    INFO >> epoch 018:    155 / 1539 loss=3.01, wps=4287.7, ups=6.4, wpb=669.5, bsz=669.5, num_updates=26250, lr=2.6e-05, gnorm=1.74, clip=0, train_wall=7, gb_free=73.6, wall=4426 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:31:03] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 43.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 77.16 GiB is allocated by PyTorch, and 1.41 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:31:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:31:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:31:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 69           |        cudaMalloc retries: 122       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78955 MiB |  79015 MiB |    787 TiB |    787 TiB |
|       from large pool |  78821 MiB |  78881 MiB |    783 TiB |    783 TiB |
|       from small pool |    134 MiB |    135 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| Active memory         |  78955 MiB |  79015 MiB |    787 TiB |    787 TiB |
|       from large pool |  78821 MiB |  78881 MiB |    783 TiB |    783 TiB |
|       from small pool |    134 MiB |    135 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78879 MiB |  78939 MiB |    786 TiB |    786 TiB |
|       from large pool |  78745 MiB |  78805 MiB |    782 TiB |    782 TiB |
|       from small pool |    133 MiB |    135 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80462 MiB |  80464 MiB |    976 GiB |    897 GiB |
|       from large pool |  80324 MiB |  80324 MiB |    966 GiB |    887 GiB |
|       from small pool |    138 MiB |    140 MiB |     10 GiB |     10 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1446 MiB |   7464 MiB |    773 TiB |    773 TiB |
|       from large pool |   1442 MiB |   7455 MiB |    769 TiB |    769 TiB |
|       from small pool |      3 MiB |     19 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| Allocations           |    2746    |    2749    |   50500 K  |   50497 K  |
|       from large pool |     484    |     485    |   22654 K  |   22653 K  |
|       from small pool |    2262    |    2265    |   27846 K  |   27843 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2746    |    2749    |   50500 K  |   50497 K  |
|       from large pool |     484    |     485    |   22654 K  |   22653 K  |
|       from small pool |    2262    |    2265    |   27846 K  |   27843 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     278    |     279    |    9887    |    9609    |
|       from large pool |     209    |     209    |    4504    |    4295    |
|       from small pool |      69    |      70    |    5383    |    5314    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     156    |     157    |   28124 K  |   28124 K  |
|       from large pool |      91    |      94    |   14345 K  |   14345 K  |
|       from small pool |      65    |      66    |   13779 K  |   13779 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:31:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:31:03] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:31:11]    INFO >> epoch 018:    206 / 1539 loss=3.338, wps=3771.7, ups=5.57, wpb=677.6, bsz=677.6, num_updates=26300, lr=2.6e-05, gnorm=1.701, clip=0, train_wall=8, gb_free=72.7, wall=4435 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:31:19]    INFO >> epoch 018:    256 / 1539 loss=3.017, wps=4538.7, ups=6.4, wpb=709, bsz=709, num_updates=26350, lr=2.6e-05, gnorm=1.684, clip=0, train_wall=7, gb_free=74.8, wall=4443 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:31:27]    INFO >> epoch 018:    306 / 1539 loss=3.034, wps=3870.4, ups=6.21, wpb=623.4, bsz=623.4, num_updates=26400, lr=2.6e-05, gnorm=1.535, clip=0, train_wall=8, gb_free=72.8, wall=4451 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:31:35]    INFO >> epoch 018:    356 / 1539 loss=3.103, wps=4695.7, ups=6.12, wpb=766.7, bsz=766.7, num_updates=26450, lr=2.6e-05, gnorm=1.651, clip=0, train_wall=8, gb_free=73.1, wall=4459 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:31:45]    INFO >> epoch 018:    406 / 1539 loss=3.096, wps=4309.3, ups=5.66, wpb=760.9, bsz=760.9, num_updates=26500, lr=2.6e-05, gnorm=1.671, clip=0, train_wall=8, gb_free=69.8, wall=4468 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:31:47] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.88 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.70 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 72.51 GiB is allocated by PyTorch, and 4.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:31:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:31:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:31:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 70           |        cudaMalloc retries: 124       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  63159 MiB |  75130 MiB |    794 TiB |    794 TiB |
|       from large pool |  63148 MiB |  75119 MiB |    790 TiB |    790 TiB |
|       from small pool |     11 MiB |     19 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| Active memory         |  63159 MiB |  75130 MiB |    794 TiB |    794 TiB |
|       from large pool |  63148 MiB |  75119 MiB |    790 TiB |    790 TiB |
|       from small pool |     11 MiB |     19 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| Requested memory      |  63147 MiB |  75114 MiB |    793 TiB |    793 TiB |
|       from large pool |  63136 MiB |  75103 MiB |    789 TiB |    789 TiB |
|       from small pool |     11 MiB |     19 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78760 MiB |  80402 MiB |    982 GiB |    905 GiB |
|       from large pool |  78738 MiB |  80264 MiB |    972 GiB |    895 GiB |
|       from small pool |     22 MiB |    138 MiB |     10 GiB |     10 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   9386 MiB |   9579 MiB |    780 TiB |    780 TiB |
|       from large pool |   9375 MiB |   9568 MiB |    775 TiB |    775 TiB |
|       from small pool |     10 MiB |     19 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| Allocations           |     515    |     557    |   50917 K  |   50916 K  |
|       from large pool |     230    |     272    |   22861 K  |   22861 K  |
|       from small pool |     285    |     356    |   28055 K  |   28054 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     515    |     557    |   50917 K  |   50916 K  |
|       from large pool |     230    |     272    |   22861 K  |   22861 K  |
|       from small pool |     285    |     356    |   28055 K  |   28054 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      97    |     277    |    9894    |    9797    |
|       from large pool |      86    |     208    |    4511    |    4425    |
|       from small pool |      11    |      69    |    5383    |    5372    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     102    |     102    |   28355 K  |   28354 K  |
|       from large pool |      80    |      80    |   14482 K  |   14482 K  |
|       from small pool |      22    |      42    |   13872 K  |   13872 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:31:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:31:47] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:31:54]    INFO >> epoch 018:    457 / 1539 loss=3.004, wps=4132.9, ups=5.5, wpb=750.9, bsz=750.9, num_updates=26550, lr=2.6e-05, gnorm=1.679, clip=0, train_wall=8, gb_free=69.5, wall=4477 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:32:02]    INFO >> epoch 018:    507 / 1539 loss=3.268, wps=4502.6, ups=6.37, wpb=706.6, bsz=706.6, num_updates=26600, lr=2.6e-05, gnorm=2.003, clip=0, train_wall=7, gb_free=69.4, wall=4485 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:32:11]    INFO >> epoch 018:    557 / 1539 loss=3.251, wps=4743.9, ups=5.97, wpb=794.3, bsz=794.3, num_updates=26650, lr=2.6e-05, gnorm=1.676, clip=0, train_wall=8, gb_free=71.6, wall=4493 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:32:20]    INFO >> epoch 018:    607 / 1539 loss=3.286, wps=4577.3, ups=6.22, wpb=736.1, bsz=736.1, num_updates=26700, lr=2.6e-05, gnorm=1.762, clip=0, train_wall=8, gb_free=64.7, wall=4501 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:32:28]    INFO >> epoch 018:    657 / 1539 loss=3.06, wps=4043.4, ups=6.41, wpb=631.1, bsz=631.1, num_updates=26750, lr=2.6e-05, gnorm=1.631, clip=0, train_wall=7, gb_free=76.3, wall=4509 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:32:36]    INFO >> epoch 018:    707 / 1539 loss=3.035, wps=4035, ups=6.36, wpb=634.9, bsz=634.9, num_updates=26800, lr=2.6e-05, gnorm=1.648, clip=0, train_wall=7, gb_free=58.9, wall=4517 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:32:44]    INFO >> epoch 018:    757 / 1539 loss=3.139, wps=4598.1, ups=6.19, wpb=743.4, bsz=743.4, num_updates=26850, lr=2.6e-05, gnorm=1.75, clip=0, train_wall=8, gb_free=63.9, wall=4525 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:32:51] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 938.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 63.25 MiB is free. Including non-PyTorch memory, this process has 79.05 GiB memory in use. Of the allocated memory 77.38 GiB is allocated by PyTorch, and 1.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:32:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:32:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:32:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 71           |        cudaMalloc retries: 126       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73483 MiB |  79421 MiB |    804 TiB |    804 TiB |
|       from large pool |  73474 MiB |  79412 MiB |    800 TiB |    800 TiB |
|       from small pool |      8 MiB |     20 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| Active memory         |  73483 MiB |  79421 MiB |    804 TiB |    804 TiB |
|       from large pool |  73474 MiB |  79412 MiB |    800 TiB |    800 TiB |
|       from small pool |      8 MiB |     20 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73471 MiB |  79407 MiB |    803 TiB |    803 TiB |
|       from large pool |  73462 MiB |  79398 MiB |    799 TiB |    799 TiB |
|       from small pool |      8 MiB |     20 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80442 MiB |  80442 MiB |    990 GiB |    912 GiB |
|       from large pool |  80420 MiB |  80420 MiB |    979 GiB |    901 GiB |
|       from small pool |     22 MiB |    236 MiB |     10 GiB |     10 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4042 MiB |   4728 MiB |    791 TiB |    791 TiB |
|       from large pool |   4029 MiB |   4715 MiB |    786 TiB |    786 TiB |
|       from small pool |     13 MiB |     19 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| Allocations           |     571    |     589    |   51566 K  |   51566 K  |
|       from large pool |     286    |     304    |   23175 K  |   23175 K  |
|       from small pool |     285    |     356    |   28391 K  |   28390 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     571    |     589    |   51566 K  |   51566 K  |
|       from large pool |     286    |     304    |   23175 K  |   23175 K  |
|       from small pool |     285    |     356    |   28391 K  |   28390 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      96    |     202    |   10007    |    9911    |
|       from large pool |      85    |      85    |    4517    |    4432    |
|       from small pool |      11    |     118    |    5490    |    5479    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     108    |     108    |   28705 K  |   28705 K  |
|       from large pool |      87    |      87    |   14675 K  |   14675 K  |
|       from small pool |      21    |      45    |   14029 K  |   14029 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:32:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:32:51] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:32:54]    INFO >> epoch 018:    808 / 1539 loss=3.138, wps=4304.2, ups=5.88, wpb=731.9, bsz=731.9, num_updates=26900, lr=2.6e-05, gnorm=1.856, clip=0, train_wall=7, gb_free=73.2, wall=4534 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:33:01]    INFO >> epoch 018:    858 / 1539 loss=3.172, wps=4529.3, ups=6.48, wpb=699.1, bsz=699.1, num_updates=26950, lr=2.6e-05, gnorm=1.767, clip=0, train_wall=7, gb_free=66.4, wall=4541 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:33:09]    INFO >> epoch 018:    908 / 1539 loss=3.081, wps=4155.8, ups=6.41, wpb=648.8, bsz=648.8, num_updates=27000, lr=2.6e-05, gnorm=1.687, clip=0, train_wall=7, gb_free=73.4, wall=4549 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:33:17]    INFO >> epoch 018:    958 / 1539 loss=3.348, wps=4039.9, ups=6.31, wpb=640.3, bsz=640.3, num_updates=27050, lr=2.6e-05, gnorm=1.686, clip=0, train_wall=7, gb_free=73.7, wall=4557 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:33:26]    INFO >> epoch 018:   1008 / 1539 loss=3.243, wps=4368.3, ups=6.37, wpb=685.5, bsz=685.5, num_updates=27100, lr=2.6e-05, gnorm=1.732, clip=0, train_wall=7, gb_free=68.6, wall=4565 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:33:34]    INFO >> epoch 018:   1058 / 1539 loss=3.269, wps=4625.5, ups=6.51, wpb=710.7, bsz=710.7, num_updates=27150, lr=2.6e-05, gnorm=1.726, clip=0, train_wall=7, gb_free=69.9, wall=4573 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:33:42]    INFO >> epoch 018:   1108 / 1539 loss=3.358, wps=4666.9, ups=6.18, wpb=755.4, bsz=755.4, num_updates=27200, lr=2.6e-05, gnorm=2.093, clip=0, train_wall=8, gb_free=67.7, wall=4581 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:33:46] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 51.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 76.76 GiB is allocated by PyTorch, and 1.81 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:33:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:33:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:33:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 72           |        cudaMalloc retries: 127       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78544 MiB |  78604 MiB |    813 TiB |    813 TiB |
|       from large pool |  78155 MiB |  78215 MiB |    809 TiB |    809 TiB |
|       from small pool |    389 MiB |    390 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| Active memory         |  78544 MiB |  78604 MiB |    813 TiB |    813 TiB |
|       from large pool |  78155 MiB |  78215 MiB |    809 TiB |    809 TiB |
|       from small pool |    389 MiB |    390 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78508 MiB |  78567 MiB |    812 TiB |    812 TiB |
|       from large pool |  78120 MiB |  78179 MiB |    808 TiB |    808 TiB |
|       from small pool |    387 MiB |    388 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80454 MiB |  80454 MiB |    993 GiB |    914 GiB |
|       from large pool |  80024 MiB |  80024 MiB |    982 GiB |    904 GiB |
|       from small pool |    430 MiB |    430 MiB |     11 GiB |     10 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1849 MiB |   4460 MiB |    801 TiB |    801 TiB |
|       from large pool |   1808 MiB |   4424 MiB |    797 TiB |    797 TiB |
|       from small pool |     40 MiB |     41 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| Allocations           |    7289    |    7292    |   52162 K  |   52155 K  |
|       from large pool |     883    |     884    |   23458 K  |   23457 K  |
|       from small pool |    6406    |    6409    |   28704 K  |   28697 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7289    |    7292    |   52162 K  |   52155 K  |
|       from large pool |     883    |     884    |   23458 K  |   23457 K  |
|       from small pool |    6406    |    6409    |   28704 K  |   28697 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     340    |     340    |   10253    |    9913    |
|       from large pool |     125    |     125    |    4559    |    4434    |
|       from small pool |     215    |     215    |    5694    |    5479    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     470    |     471    |   29024 K  |   29024 K  |
|       from large pool |      83    |      83    |   14848 K  |   14848 K  |
|       from small pool |     387    |     388    |   14176 K  |   14175 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:33:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:33:46] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:33:51]    INFO >> epoch 018:   1159 / 1539 loss=3.278, wps=4188.9, ups=5.82, wpb=719.9, bsz=719.9, num_updates=27250, lr=2.6e-05, gnorm=1.986, clip=0, train_wall=8, gb_free=74.3, wall=4589 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:34:00]    INFO >> epoch 018:   1209 / 1539 loss=3.2, wps=4580.4, ups=6.22, wpb=736.9, bsz=736.9, num_updates=27300, lr=2.6e-05, gnorm=1.807, clip=0, train_wall=8, gb_free=68.2, wall=4597 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:34:09]    INFO >> epoch 018:   1259 / 1539 loss=3.176, wps=4301.7, ups=5.41, wpb=795, bsz=795, num_updates=27350, lr=2.6e-05, gnorm=1.862, clip=0, train_wall=9, gb_free=68.2, wall=4607 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:34:17]    INFO >> epoch 018:   1309 / 1539 loss=3.047, wps=4783.4, ups=6.48, wpb=738.3, bsz=738.3, num_updates=27400, lr=2.6e-05, gnorm=1.834, clip=0, train_wall=7, gb_free=71.8, wall=4614 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:34:25]    INFO >> epoch 018:   1359 / 1539 loss=3.179, wps=4263.2, ups=6.15, wpb=693.2, bsz=693.2, num_updates=27450, lr=2.6e-05, gnorm=1.797, clip=0, train_wall=8, gb_free=71.5, wall=4623 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:34:35]    INFO >> epoch 018:   1409 / 1539 loss=3.178, wps=4649.2, ups=6.18, wpb=752.7, bsz=752.7, num_updates=27500, lr=2.6e-05, gnorm=1.752, clip=0, train_wall=8, gb_free=70.4, wall=4631 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:34:43]    INFO >> epoch 018:   1459 / 1539 loss=3.185, wps=4128.8, ups=5.87, wpb=703.6, bsz=703.6, num_updates=27550, lr=2.6e-05, gnorm=1.856, clip=0, train_wall=8, gb_free=72.4, wall=4639 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:34:51]    INFO >> epoch 018:   1509 / 1539 loss=3.258, wps=4070.3, ups=6.47, wpb=629.4, bsz=629.4, num_updates=27600, lr=2.6e-05, gnorm=1.735, clip=0, train_wall=7, gb_free=73, wall=4647 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:34:56]    INFO >> epoch 018 | loss 3.163 | wps 4106.5 | ups 5.76 | wpb 712.7 | bsz 712.7 | num_updates 27630 | lr 2.6e-05 | gnorm 1.766 | clip 0 | train_wall 233 | gb_free 70.8 | wall 4652 (progress_bar.py:267, print())[0m
[33m[2025-11-21 15:34:56] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:35:12]    INFO >> epoch 018 | valid on 'valid' subset | loss 3.498 | wps 10186.5 | wpb 5412.5 | bsz 5412.5 | num_updates 27630 | best_loss 3.524 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 15:35:13]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 15:35:13]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_last.pt (epoch 18 @ 27630 updates, score 3.498) (writing took 0.014742 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[32m[2025-11-21 15:35:13]    INFO >> Êó©ÂÅú: È™åËØÅÊÄßËÉΩÂ∑≤10ËΩÆÊú™ÊèêÂçá (train_enhanced.py:616, single_main())[0m
[32m[2025-11-21 15:35:13]    INFO >> ËÆ≠ÁªÉÂÆåÊàêÔºåÁî®Êó∂ 4602.8 Áßí (train_enhanced.py:626, single_main())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 15:35:13]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 15:35:13]    INFO >> ÊâÄÊúâÊó•ÂøóÂ∑≤‰øùÂ≠òÂà∞: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs (train_enhanced.py:631, single_main())[0m
[32m[2025-11-21 15:35:13]    INFO >> 
================================================================================ (train_enhanced.py:634, single_main())[0m
[32m[2025-11-21 15:35:13]    INFO >> ÂºÄÂßãÊµãËØï... (train_enhanced.py:635, single_main())[0m
[32m[2025-11-21 15:35:13]    INFO >> ================================================================================ (train_enhanced.py:636, single_main())[0m
[32m[2025-11-21 15:35:13]    INFO >> Âä†ËΩΩÊúÄ‰Ω≥checkpoint: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_best.pt (train_enhanced.py:50, run_test_after_training())[0m
[32m[2025-11-21 15:35:13]    INFO >> ÊµãËØïÈõÜ: test (train_enhanced.py:51, run_test_after_training())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/checkpoint_utils.py:212: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(
[32m[2025-11-21 15:36:06]    INFO >> 
================================================================================ (train_enhanced.py:168, run_test_after_training())[0m
[32m[2025-11-21 15:36:06]    INFO >> ÊµãËØïÁªìÊûú: (train_enhanced.py:169, run_test_after_training())[0m
[32m[2025-11-21 15:36:06]    INFO >> -------------------------------------------------------------------------------- (train_enhanced.py:170, run_test_after_training())[0m
[32m[2025-11-21 15:36:06]    INFO >> Âπ≥ÂùáLoss:      7.9390 (train_enhanced.py:171, run_test_after_training())[0m
[32m[2025-11-21 15:36:06]    INFO >> Acc@1:         0.78% (train_enhanced.py:172, run_test_after_training())[0m
[32m[2025-11-21 15:36:06]    INFO >> Acc@5:         2.68% (train_enhanced.py:173, run_test_after_training())[0m
[32m[2025-11-21 15:36:06]    INFO >> Acc@1 (Âê´any): 0.78% (train_enhanced.py:174, run_test_after_training())[0m
[32m[2025-11-21 15:36:06]    INFO >> Acc@5 (Âê´any): 2.68% (train_enhanced.py:175, run_test_after_training())[0m
[32m[2025-11-21 15:36:06]    INFO >> ================================================================================ (train_enhanced.py:176, run_test_after_training())[0m
[32m[2025-11-21 15:36:06]    INFO >> ÊµãËØïÁªìÊûúÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/res.txt (train_enhanced.py:187, run_test_after_training())[0m
[32m[2025-11-21 15:36:06]    INFO >> ËÆ≠ÁªÉÊó•ÂøóÂ∑≤Êõ¥Êñ∞: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs (train_enhanced.py:222, run_test_after_training())[0m
[TrainingLogger] Êó•ÂøóÁõÆÂΩï: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs
[TrainingLogger] ÂéüÂßãËæìÂá∫Â∞Ü‰øùÂ≠òÂà∞: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/training_output.log
[TrainingLogger] Epoch 1 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json
[TrainingLogger] Epoch 2 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json
[TrainingLogger] Epoch 3 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json
[TrainingLogger] Epoch 4 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json
[TrainingLogger] Epoch 5 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json
[TrainingLogger] Epoch 6 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json
[TrainingLogger] Epoch 7 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json
[TrainingLogger] Epoch 8 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json
[TrainingLogger] Epoch 9 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json
[TrainingLogger] Epoch 10 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json
[TrainingLogger] Epoch 11 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json
[TrainingLogger] Epoch 12 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json
[TrainingLogger] Epoch 13 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json
[TrainingLogger] Epoch 14 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json
[TrainingLogger] Epoch 15 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json
[TrainingLogger] Epoch 16 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json
[TrainingLogger] Epoch 17 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json
[TrainingLogger] Epoch 18 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json

‚úì dropout_0 ÊàêÂäü

Á≠âÂæÖ3Áßí...

ËøõÂ∫¶: 2/4

============================================================
ÂÆûÈ™å: dropout_015 - Dropout=0.15 (ÈÄÇÂ∫¶Â¢ûÂº∫)
Êó∂Èó¥: 2025-11-21 15:36:51
============================================================

[32m[2025-11-21 15:36:53]    INFO >> Âä†ËΩΩÈÖçÁΩÆ: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/config.yml (train_enhanced.py:666, cli_main())[0m
[32m[2025-11-21 15:36:53]    INFO >> ÂçïGPUËÆ≠ÁªÉ... (train_enhanced.py:694, cli_main())[0m
[32m[2025-11-21 15:36:53]    INFO >> ËÆ≠ÁªÉÊó•ÂøóÂ∞Ü‰øùÂ≠òÂà∞: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs (train_enhanced.py:561, single_main())[0m
[32m[2025-11-21 15:36:53]    INFO >> [nodes] dictionary: 9999 types (typilus.py:102, setup_task())[0m
[32m[2025-11-21 15:36:53]    INFO >> [edges] dictionary: 0 types (typilus.py:102, setup_task())[0m
[32m[2025-11-21 15:36:53]    INFO >> [supernodes.annotation] dictionary: 99 types (typilus.py:106, setup_task())[0m
[32m[2025-11-21 15:37:03]    INFO >> Typilus(
  (encoder): GGNNEncoder(
    (node_embedding): Embedding(9999, 64, padding_idx=0)
    (node_layer): Sequential(
      (0): Dropout(p=0.15, inplace=False)
      (1): Linear(in_features=64, out_features=64, bias=False)
      (2): Dropout(p=0.15, inplace=False)
    )
    (ggnns): ModuleList(
      (0): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=64, out_features=64, bias=False)
          (OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (NEXT): Linear(in_features=64, out_features=64, bias=False)
          (SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
          (_CHILD): Linear(in_features=64, out_features=64, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT): Linear(in_features=64, out_features=64, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (_COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (_RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
        )
        (rnn_cell): GRUCell(64, 64)
      )
      (1): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=64, out_features=64, bias=False)
          (OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (NEXT): Linear(in_features=64, out_features=64, bias=False)
          (SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
          (_CHILD): Linear(in_features=64, out_features=64, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT): Linear(in_features=64, out_features=64, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (_COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (_RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
        )
        (rnn_cell): GRUCell(128, 64)
      )
    )
  )
  (decoder): DenseDecoder(
    (cls_layers): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=False)
      (1): Dropout(p=0.15, inplace=False)
      (2): Linear(in_features=64, out_features=99, bias=True)
    )
  )
) (train_enhanced.py:568, single_main())[0m
[32m[2025-11-21 15:37:03]    INFO >> Ê®°Âûã: typilus, ÊçüÂ§±ÂáΩÊï∞: TypilusCriterion (train_enhanced.py:569, single_main())[0m
[32m[2025-11-21 15:37:03]    INFO >> Ê®°ÂûãÂèÇÊï∞: 847843 (ÂèØËÆ≠ÁªÉ: 847843) (train_enhanced.py:570, single_main())[0m
[32m[2025-11-21 15:37:03]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 15:37:03]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 80579 MB ; used memory = 1340 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 15:37:03]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 15:37:03]    INFO >> ‰ΩøÁî® 1 ‰∏™GPUËÆ≠ÁªÉ (train_enhanced.py:576, single_main())[0m
[32m[2025-11-21 15:37:03]    INFO >> no existing checkpoint found /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-21 15:37:03]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[32m[2025-11-21 15:38:12]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-21 15:38:13] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-21 15:38:20]    INFO >> epoch 001:     50 / 1539 loss=5.513, wps=5039.9, ups=6.97, wpb=720, bsz=720, num_updates=50, lr=0.0004, gnorm=7.179, clip=0, train_wall=7, gb_free=74.2, wall=75 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:38:23] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 43.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 73.64 GiB is allocated by PyTorch, and 4.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:38:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:38:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:38:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 2         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75347 MiB |  75407 MiB |   1756 GiB |   1682 GiB |
|       from large pool |  74991 MiB |  75051 MiB |   1743 GiB |   1670 GiB |
|       from small pool |    356 MiB |    357 MiB |     12 GiB |     12 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75347 MiB |  75407 MiB |   1756 GiB |   1682 GiB |
|       from large pool |  74991 MiB |  75051 MiB |   1743 GiB |   1670 GiB |
|       from small pool |    356 MiB |    357 MiB |     12 GiB |     12 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75195 MiB |  75254 MiB |   1750 GiB |   1676 GiB |
|       from large pool |  74841 MiB |  74900 MiB |   1737 GiB |   1664 GiB |
|       from small pool |    354 MiB |    355 MiB |     12 GiB |     12 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80462 MiB |  80498 MiB |  91994 MiB |  11532 MiB |
|       from large pool |  80070 MiB |  80136 MiB |  91590 MiB |  11520 MiB |
|       from small pool |    392 MiB |    394 MiB |    404 MiB |     12 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5054 MiB |   5960 MiB |    875 GiB |    870 GiB |
|       from large pool |   5018 MiB |   5948 MiB |    859 GiB |    855 GiB |
|       from small pool |     35 MiB |     37 MiB |     15 GiB |     14 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6579    |    6582    |  136626    |  130047    |
|       from large pool |     868    |     869    |   57719    |   56851    |
|       from small pool |    5711    |    5714    |   78907    |   73196    |
|---------------------------------------------------------------------------|
| Active allocs         |    6579    |    6582    |  136626    |  130047    |
|       from large pool |     868    |     869    |   57719    |   56851    |
|       from small pool |    5711    |    5714    |   78907    |   73196    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     758    |     793    |    1006    |     248    |
|       from large pool |     562    |     612    |     804    |     242    |
|       from small pool |     196    |     197    |     202    |       6    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     654    |     656    |   86039    |   85385    |
|       from large pool |     306    |     306    |   44256    |   43950    |
|       from small pool |     348    |     350    |   41783    |   41435    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:38:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:38:23] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:38:29]    INFO >> epoch 001:    101 / 1539 loss=5.849, wps=4382, ups=7.18, wpb=610.7, bsz=610.7, num_updates=100, lr=0.0004, gnorm=7.784, clip=0, train_wall=6, gb_free=75.6, wall=82 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:38:36]    INFO >> epoch 001:    151 / 1539 loss=6.018, wps=5442.4, ups=6.66, wpb=816.7, bsz=816.7, num_updates=150, lr=0.0004, gnorm=7.445, clip=0, train_wall=7, gb_free=74.2, wall=90 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:38:43]    INFO >> epoch 001:    201 / 1539 loss=5.995, wps=4841.1, ups=7.54, wpb=641.7, bsz=641.7, num_updates=200, lr=0.0004, gnorm=8.323, clip=0, train_wall=6, gb_free=74.8, wall=96 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:38:50]    INFO >> epoch 001:    251 / 1539 loss=6.021, wps=4636.4, ups=7.27, wpb=637.9, bsz=637.9, num_updates=250, lr=0.0004, gnorm=7.717, clip=0, train_wall=6, gb_free=71.5, wall=103 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:38:59]    INFO >> epoch 001:    301 / 1539 loss=6.016, wps=5059, ups=6.43, wpb=786.5, bsz=786.5, num_updates=300, lr=0.0004, gnorm=7.47, clip=0, train_wall=7, gb_free=73.8, wall=111 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:39:06]    INFO >> epoch 001:    351 / 1539 loss=6.015, wps=4853.9, ups=7.23, wpb=671.6, bsz=671.6, num_updates=350, lr=0.0004, gnorm=6.708, clip=0, train_wall=6, gb_free=72.4, wall=118 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:39:13]    INFO >> epoch 001:    401 / 1539 loss=5.662, wps=5485.8, ups=6.44, wpb=851.7, bsz=851.7, num_updates=400, lr=0.0004, gnorm=8.185, clip=0, train_wall=7, gb_free=73.2, wall=125 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:39:23] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 7.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.81 GiB is allocated by PyTorch, and 823.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:39:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:39:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:39:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 4         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79614 MiB |  79674 MiB |  12419 GiB |  12342 GiB |
|       from large pool |  79520 MiB |  79580 MiB |  12346 GiB |  12268 GiB |
|       from small pool |     94 MiB |     95 MiB |     73 GiB |     73 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79614 MiB |  79674 MiB |  12419 GiB |  12342 GiB |
|       from large pool |  79520 MiB |  79580 MiB |  12346 GiB |  12268 GiB |
|       from small pool |     94 MiB |     95 MiB |     73 GiB |     73 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79544 MiB |  79604 MiB |  12379 GiB |  12302 GiB |
|       from large pool |  79450 MiB |  79510 MiB |  12306 GiB |  12228 GiB |
|       from small pool |     93 MiB |     95 MiB |     73 GiB |     73 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80498 MiB |  80500 MiB | 166722 MiB |  86224 MiB |
|       from large pool |  80400 MiB |  80400 MiB | 166242 MiB |  85842 MiB |
|       from small pool |     98 MiB |    392 MiB |    480 MiB |    382 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    823 MiB |   3119 MiB |   6143 GiB |   6143 GiB |
|       from large pool |    819 MiB |   3112 MiB |   6057 GiB |   6057 GiB |
|       from small pool |      3 MiB |     23 MiB |     85 GiB |     85 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2036    |    2039    |     869 K  |     867 K  |
|       from large pool |     477    |     478    |     421 K  |     421 K  |
|       from small pool |    1559    |    1562    |     447 K  |     445 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2036    |    2039    |     869 K  |     867 K  |
|       from large pool |     477    |     478    |     421 K  |     421 K  |
|       from small pool |    1559    |    1562    |     447 K  |     445 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     251    |     757    |    1201    |     950    |
|       from large pool |     202    |     561    |     961    |     759    |
|       from small pool |      49    |     196    |     240    |     191    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     132    |     133    |  540020    |  539888    |
|       from large pool |      79    |      81    |  324754    |  324675    |
|       from small pool |      53    |      55    |  215266    |  215213    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:39:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:39:23] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:39:27]    INFO >> epoch 001:    452 / 1539 loss=5.956, wps=2314.8, ups=3.66, wpb=631.6, bsz=631.6, num_updates=450, lr=0.0004, gnorm=6.63, clip=0, train_wall=6, gb_free=71.8, wall=139 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:39:38]    INFO >> epoch 001:    502 / 1539 loss=5.701, wps=3723.7, ups=5.01, wpb=743.1, bsz=743.1, num_updates=500, lr=0.0004, gnorm=7.33, clip=0, train_wall=9, gb_free=72.6, wall=149 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:39:46]    INFO >> epoch 001:    552 / 1539 loss=5.817, wps=4489.5, ups=6.83, wpb=657, bsz=657, num_updates=550, lr=0.0004, gnorm=6.856, clip=0, train_wall=7, gb_free=65.5, wall=156 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:39:53]    INFO >> epoch 001:    602 / 1539 loss=5.763, wps=4490.1, ups=6.72, wpb=668.6, bsz=668.6, num_updates=600, lr=0.0004, gnorm=7.85, clip=0, train_wall=7, gb_free=73.2, wall=164 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:40:01]    INFO >> epoch 001:    652 / 1539 loss=5.706, wps=4481, ups=6.29, wpb=712.2, bsz=712.2, num_updates=650, lr=0.0004, gnorm=7.235, clip=0, train_wall=7, gb_free=73.5, wall=172 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:40:11]    INFO >> epoch 001:    702 / 1539 loss=5.759, wps=3887.3, ups=5.78, wpb=672.5, bsz=672.5, num_updates=700, lr=0.0004, gnorm=6.868, clip=0, train_wall=8, gb_free=74.1, wall=180 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:40:19]    INFO >> epoch 001:    752 / 1539 loss=5.633, wps=4723.8, ups=6.24, wpb=757.4, bsz=757.4, num_updates=750, lr=0.0004, gnorm=7.373, clip=0, train_wall=7, gb_free=73.7, wall=188 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:40:27]    INFO >> epoch 001:    802 / 1539 loss=5.631, wps=4799.7, ups=6.62, wpb=725.2, bsz=725.2, num_updates=800, lr=0.0004, gnorm=7.154, clip=0, train_wall=7, gb_free=73.4, wall=196 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:40:34]    INFO >> epoch 001:    852 / 1539 loss=5.564, wps=4118.9, ups=6.42, wpb=641.1, bsz=641.1, num_updates=850, lr=0.0004, gnorm=7.27, clip=0, train_wall=7, gb_free=71.8, wall=204 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:40:43]    INFO >> epoch 001:    902 / 1539 loss=5.568, wps=4526.2, ups=6.86, wpb=660.1, bsz=660.1, num_updates=900, lr=0.0004, gnorm=6.743, clip=0, train_wall=7, gb_free=72.1, wall=211 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:40:51]    INFO >> epoch 001:    952 / 1539 loss=5.562, wps=4641.8, ups=6.51, wpb=713.2, bsz=713.2, num_updates=950, lr=0.0004, gnorm=6.943, clip=0, train_wall=7, gb_free=71.8, wall=219 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:40:54] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 901.25 MiB is free. Including non-PyTorch memory, this process has 78.24 GiB memory in use. Of the allocated memory 74.92 GiB is allocated by PyTorch, and 2.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:40:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:40:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:40:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 3            |        cudaMalloc retries: 7         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76322 MiB |  78597 MiB |  28415 GiB |  28340 GiB |
|       from large pool |  76303 MiB |  78579 MiB |  28261 GiB |  28187 GiB |
|       from small pool |     18 MiB |     19 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76322 MiB |  78597 MiB |  28415 GiB |  28340 GiB |
|       from large pool |  76303 MiB |  78579 MiB |  28261 GiB |  28187 GiB |
|       from small pool |     18 MiB |     19 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB |  28342 GiB |  28268 GiB |
|       from large pool |  76285 MiB |  78562 MiB |  28189 GiB |  28114 GiB |
|       from small pool |     18 MiB |     19 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79604 MiB |  79604 MiB | 305536 MiB | 225932 MiB |
|       from large pool |  79580 MiB |  79580 MiB | 304982 MiB | 225402 MiB |
|       from small pool |     24 MiB |     98 MiB |    554 MiB |    530 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3281 MiB |   4789 MiB |  24278 GiB |  24274 GiB |
|       from large pool |   3276 MiB |   4783 MiB |  24100 GiB |  24097 GiB |
|       from small pool |      5 MiB |     27 MiB |    177 GiB |    177 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     638    |     644    |    1881 K  |    1880 K  |
|       from large pool |     340    |     346    |     949 K  |     948 K  |
|       from small pool |     298    |     354    |     931 K  |     931 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     638    |     644    |    1881 K  |    1880 K  |
|       from large pool |     340    |     346    |     949 K  |     948 K  |
|       from small pool |     298    |     354    |     931 K  |     931 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      96    |     123    |    1327    |    1231    |
|       from large pool |      84    |      84    |    1050    |     966    |
|       from small pool |      12    |      49    |     277    |     265    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     115    |     117    |    1087 K  |    1087 K  |
|       from large pool |      93    |      95    |     661 K  |     661 K  |
|       from small pool |      22    |      56    |     426 K  |     426 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:40:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:40:54] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:41:00]    INFO >> epoch 001:   1003 / 1539 loss=5.542, wps=3359.9, ups=5.17, wpb=649.4, bsz=649.4, num_updates=1000, lr=0.0004, gnorm=6.768, clip=0, train_wall=7, gb_free=72.1, wall=228 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:41:09]    INFO >> epoch 001:   1053 / 1539 loss=5.449, wps=5025, ups=6.11, wpb=822.6, bsz=822.6, num_updates=1050, lr=0.0004, gnorm=8.009, clip=0, train_wall=7, gb_free=68, wall=237 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:41:18] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 197.25 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 75.66 GiB is allocated by PyTorch, and 2.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:41:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:41:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:41:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 4            |        cudaMalloc retries: 9         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76955 MiB |  77477 MiB |  32758 GiB |  32683 GiB |
|       from large pool |  76942 MiB |  77464 MiB |  32579 GiB |  32504 GiB |
|       from small pool |     12 MiB |     15 MiB |    178 GiB |    178 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76955 MiB |  77477 MiB |  32758 GiB |  32683 GiB |
|       from large pool |  76942 MiB |  77464 MiB |  32579 GiB |  32504 GiB |
|       from small pool |     12 MiB |     15 MiB |    178 GiB |    178 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76936 MiB |  77458 MiB |  32677 GiB |  32602 GiB |
|       from large pool |  76923 MiB |  77445 MiB |  32498 GiB |  32423 GiB |
|       from small pool |     12 MiB |     15 MiB |    178 GiB |    178 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80308 MiB |  80308 MiB | 310972 MiB | 230664 MiB |
|       from large pool |  80284 MiB |  80284 MiB | 310232 MiB | 229948 MiB |
|       from small pool |     24 MiB |    210 MiB |    740 MiB |    716 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3352 MiB |   9143 MiB |  29235 GiB |  29232 GiB |
|       from large pool |   3341 MiB |   9131 MiB |  29029 GiB |  29026 GiB |
|       from small pool |     11 MiB |     23 MiB |    206 GiB |    206 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     604    |     611    |    2166 K  |    2165 K  |
|       from large pool |     315    |     322    |    1085 K  |    1085 K  |
|       from small pool |     289    |     354    |    1080 K  |    1079 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     604    |     611    |    2166 K  |    2165 K  |
|       from large pool |     315    |     322    |    1085 K  |    1085 K  |
|       from small pool |     289    |     354    |    1080 K  |    1079 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      88    |     189    |    1425    |    1337    |
|       from large pool |      76    |      84    |    1055    |     979    |
|       from small pool |      12    |     105    |     370    |     358    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      84    |      84    |    1245 K  |    1245 K  |
|       from large pool |      60    |      60    |     746 K  |     746 K  |
|       from small pool |      24    |      53    |     498 K  |     498 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:41:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:41:18] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:41:20]    INFO >> epoch 001:   1104 / 1539 loss=5.48, wps=4841.1, ups=5.21, wpb=929.6, bsz=929.6, num_updates=1100, lr=0.0004, gnorm=7.487, clip=2, train_wall=8, gb_free=72.8, wall=246 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:41:27]    INFO >> epoch 001:   1154 / 1539 loss=5.447, wps=4546.8, ups=6.56, wpb=692.6, bsz=692.6, num_updates=1150, lr=0.0004, gnorm=7.052, clip=0, train_wall=7, gb_free=73.1, wall=254 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:41:35]    INFO >> epoch 001:   1204 / 1539 loss=5.385, wps=4428.5, ups=6.53, wpb=678.4, bsz=678.4, num_updates=1200, lr=0.0004, gnorm=7.251, clip=0, train_wall=7, gb_free=71.2, wall=262 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:41:43]    INFO >> epoch 001:   1254 / 1539 loss=5.369, wps=4709.5, ups=6.44, wpb=730.9, bsz=730.9, num_updates=1250, lr=0.0004, gnorm=7.134, clip=0, train_wall=7, gb_free=71.3, wall=269 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:41:52]    INFO >> epoch 001:   1304 / 1539 loss=5.185, wps=4524.4, ups=6.15, wpb=735.4, bsz=735.4, num_updates=1300, lr=0.0004, gnorm=6.455, clip=0, train_wall=7, gb_free=74.3, wall=277 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:42:00]    INFO >> epoch 001:   1354 / 1539 loss=5.283, wps=4274, ups=6.5, wpb=657.2, bsz=657.2, num_updates=1350, lr=0.0004, gnorm=6.78, clip=2, train_wall=7, gb_free=73.4, wall=285 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:42:08]    INFO >> epoch 001:   1404 / 1539 loss=5.095, wps=4434.7, ups=6.22, wpb=712.5, bsz=712.5, num_updates=1400, lr=0.0004, gnorm=7.247, clip=0, train_wall=7, gb_free=73.3, wall=293 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:42:16]    INFO >> epoch 001:   1454 / 1539 loss=5.094, wps=4534, ups=6.46, wpb=702.2, bsz=702.2, num_updates=1450, lr=0.0004, gnorm=7.271, clip=2, train_wall=7, gb_free=71.6, wall=301 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:42:25]    INFO >> epoch 001:   1504 / 1539 loss=4.935, wps=4548.6, ups=6.54, wpb=695.9, bsz=695.9, num_updates=1500, lr=0.0004, gnorm=6.525, clip=0, train_wall=7, gb_free=70.8, wall=309 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:42:30]    INFO >> epoch 001 | loss 5.577 | wps 4446.1 | ups 6.24 | wpb 712.7 | bsz 712.7 | num_updates 1535 | lr 0.0004 | gnorm 7.228 | clip 0.2 | train_wall 214 | gb_free 76.6 | wall 314 (progress_bar.py:267, print())[0m
[33m[2025-11-21 15:42:30] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:42:45]    INFO >> epoch 001 | valid on 'valid' subset | loss 4.689 | wps 9884 | wpb 5412.5 | bsz 5412.5 | num_updates 1535 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
[32m[2025-11-21 15:42:46]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 15:42:46]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/checkpoints/checkpoint_best.pt (epoch 1 @ 1535 updates, score 4.689) (writing took 0.016955 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 15:42:46] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-21 15:42:48]    INFO >> epoch 002:     15 / 1539 loss=4.811, wps=1558.6, ups=2.13, wpb=731.3, bsz=731.3, num_updates=1550, lr=0.0004, gnorm=6.499, clip=0, train_wall=7, gb_free=74.4, wall=332 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:42:56]    INFO >> epoch 002:     65 / 1539 loss=4.602, wps=4687.5, ups=7.12, wpb=658.8, bsz=658.8, num_updates=1600, lr=0.0004, gnorm=7.509, clip=0, train_wall=7, gb_free=73.4, wall=339 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:43:04]    INFO >> epoch 002:    115 / 1539 loss=4.7, wps=4653.3, ups=6.51, wpb=714.6, bsz=714.6, num_updates=1650, lr=0.0004, gnorm=6.391, clip=0, train_wall=7, gb_free=65.7, wall=347 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:43:12]    INFO >> epoch 002:    165 / 1539 loss=4.367, wps=4787.8, ups=6.48, wpb=738.8, bsz=738.8, num_updates=1700, lr=0.0004, gnorm=7.008, clip=0, train_wall=7, gb_free=73.6, wall=354 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:43:19]    INFO >> epoch 002:    215 / 1539 loss=4.696, wps=4739.3, ups=6.73, wpb=704.2, bsz=704.2, num_updates=1750, lr=0.0004, gnorm=7.718, clip=0, train_wall=7, gb_free=71.2, wall=362 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:43:29]    INFO >> epoch 002:    265 / 1539 loss=4.442, wps=5193.6, ups=5.95, wpb=873.5, bsz=873.5, num_updates=1800, lr=0.0004, gnorm=7.621, clip=2, train_wall=8, gb_free=74.7, wall=370 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:43:36]    INFO >> epoch 002:    315 / 1539 loss=4.472, wps=4359, ups=6.76, wpb=645.1, bsz=645.1, num_updates=1850, lr=0.0004, gnorm=7.762, clip=0, train_wall=7, gb_free=71.9, wall=378 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:43:44]    INFO >> epoch 002:    365 / 1539 loss=4.453, wps=4502.9, ups=6.87, wpb=655.8, bsz=655.8, num_updates=1900, lr=0.0004, gnorm=7.416, clip=0, train_wall=7, gb_free=74, wall=385 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:43:51]    INFO >> epoch 002:    415 / 1539 loss=4.328, wps=4545.5, ups=6.37, wpb=713.2, bsz=713.2, num_updates=1950, lr=0.0004, gnorm=6.957, clip=0, train_wall=7, gb_free=76.2, wall=393 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:43:52] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 197.25 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 74.93 GiB is allocated by PyTorch, and 3.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:43:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:43:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:43:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 5            |        cudaMalloc retries: 10        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76327 MiB |  78599 MiB |  61960 GiB |  61886 GiB |
|       from large pool |  76308 MiB |  78581 MiB |  61611 GiB |  61537 GiB |
|       from small pool |     18 MiB |     21 MiB |    348 GiB |    348 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76327 MiB |  78599 MiB |  61960 GiB |  61886 GiB |
|       from large pool |  76308 MiB |  78581 MiB |  61611 GiB |  61537 GiB |
|       from small pool |     18 MiB |     21 MiB |    348 GiB |    348 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB |  61824 GiB |  61750 GiB |
|       from large pool |  76285 MiB |  78562 MiB |  61476 GiB |  61402 GiB |
|       from small pool |     18 MiB |     21 MiB |    348 GiB |    348 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80308 MiB |  80494 MiB | 311158 MiB | 230850 MiB |
|       from large pool |  80284 MiB |  80284 MiB | 310232 MiB | 229948 MiB |
|       from small pool |     24 MiB |    210 MiB |    926 MiB |    902 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3980 MiB |   5490 MiB |  61031 GiB |  61027 GiB |
|       from large pool |   3975 MiB |   5484 MiB |  60635 GiB |  60631 GiB |
|       from small pool |      5 MiB |     27 MiB |    395 GiB |    395 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     640    |     646    |    4090 K  |    4090 K  |
|       from large pool |     340    |     346    |    1969 K  |    1969 K  |
|       from small pool |     300    |     355    |    2121 K  |    2120 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     640    |     646    |    4090 K  |    4090 K  |
|       from large pool |     340    |     346    |    1969 K  |    1969 K  |
|       from small pool |     300    |     355    |    2121 K  |    2120 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      88    |     181    |    1518    |    1430    |
|       from large pool |      76    |      76    |    1055    |     979    |
|       from small pool |      12    |     105    |     463    |     451    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      93    |      99    |    2309 K  |    2309 K  |
|       from large pool |      70    |      76    |    1305 K  |    1305 K  |
|       from small pool |      23    |      51    |    1004 K  |    1004 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:43:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:43:52] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:44:01]    INFO >> epoch 002:    466 / 1539 loss=4.479, wps=4366.2, ups=5.99, wpb=729.4, bsz=729.4, num_updates=2000, lr=0.0004, gnorm=6.885, clip=0, train_wall=7, gb_free=71.6, wall=401 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:44:09]    INFO >> epoch 002:    516 / 1539 loss=4.476, wps=4671.7, ups=6.59, wpb=708.4, bsz=708.4, num_updates=2050, lr=0.0004, gnorm=8.204, clip=0, train_wall=7, gb_free=71.2, wall=409 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:44:16]    INFO >> epoch 002:    566 / 1539 loss=4.424, wps=4377.4, ups=7.03, wpb=622.3, bsz=622.3, num_updates=2100, lr=0.0004, gnorm=6.444, clip=0, train_wall=7, gb_free=74, wall=416 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:44:24]    INFO >> epoch 002:    616 / 1539 loss=4.251, wps=5394.7, ups=6.19, wpb=871.1, bsz=871.1, num_updates=2150, lr=0.0004, gnorm=7.916, clip=2, train_wall=8, gb_free=68.6, wall=424 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:44:33]    INFO >> epoch 002:    666 / 1539 loss=4.272, wps=4842.7, ups=6.27, wpb=772.9, bsz=772.9, num_updates=2200, lr=0.0004, gnorm=8.213, clip=2, train_wall=7, gb_free=68.4, wall=432 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:44:41]    INFO >> epoch 002:    716 / 1539 loss=4.352, wps=4763.6, ups=6.72, wpb=708.6, bsz=708.6, num_updates=2250, lr=0.0004, gnorm=6.903, clip=0, train_wall=7, gb_free=73.1, wall=439 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:44:48]    INFO >> epoch 002:    766 / 1539 loss=4.367, wps=4291.4, ups=6.45, wpb=665.2, bsz=665.2, num_updates=2300, lr=0.0004, gnorm=5.941, clip=0, train_wall=7, gb_free=75.7, wall=447 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:44:56]    INFO >> epoch 002:    816 / 1539 loss=4.41, wps=4300.1, ups=6.56, wpb=656, bsz=656, num_updates=2350, lr=0.0004, gnorm=6.871, clip=0, train_wall=7, gb_free=72.2, wall=455 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:45:04]    INFO >> epoch 002:    866 / 1539 loss=4.302, wps=4514.8, ups=6.27, wpb=719.8, bsz=719.8, num_updates=2400, lr=0.0004, gnorm=7.573, clip=0, train_wall=8, gb_free=75.7, wall=463 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:45:13]    INFO >> epoch 002:    916 / 1539 loss=4.343, wps=4342.7, ups=6.47, wpb=671.5, bsz=671.5, num_updates=2450, lr=0.0004, gnorm=5.952, clip=0, train_wall=7, gb_free=73.1, wall=470 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:45:21]    INFO >> epoch 002:    966 / 1539 loss=4.177, wps=4147.4, ups=6.5, wpb=637.9, bsz=637.9, num_updates=2500, lr=0.0004, gnorm=7.008, clip=0, train_wall=7, gb_free=69.2, wall=478 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:45:22] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 37.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 77.54 GiB is allocated by PyTorch, and 1.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:45:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:45:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:45:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 6            |        cudaMalloc retries: 12        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79337 MiB |  79397 MiB |  78153 GiB |  78075 GiB |
|       from large pool |  78941 MiB |  79001 MiB |  77714 GiB |  77637 GiB |
|       from small pool |    395 MiB |    396 MiB |    438 GiB |    437 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79337 MiB |  79397 MiB |  78153 GiB |  78075 GiB |
|       from large pool |  78941 MiB |  79001 MiB |  77714 GiB |  77637 GiB |
|       from small pool |    395 MiB |    396 MiB |    438 GiB |    437 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79098 MiB |  79157 MiB |  77983 GiB |  77905 GiB |
|       from large pool |  78705 MiB |  78764 MiB |  77545 GiB |  77468 GiB |
|       from small pool |    393 MiB |    394 MiB |    437 GiB |    437 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80468 MiB |  80470 MiB | 335872 MiB | 255404 MiB |
|       from large pool |  80032 MiB |  80032 MiB | 334532 MiB | 254500 MiB |
|       from small pool |    436 MiB |    438 MiB |   1340 MiB |    904 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1070 MiB |   4610 MiB |  79127 GiB |  79126 GiB |
|       from large pool |   1030 MiB |   4603 MiB |  78628 GiB |  78627 GiB |
|       from small pool |     40 MiB |     41 MiB |    498 GiB |    498 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7296    |    7299    |    5179 K  |    5172 K  |
|       from large pool |     933    |     934    |    2516 K  |    2515 K  |
|       from small pool |    6363    |    6366    |    2663 K  |    2656 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7296    |    7299    |    5179 K  |    5172 K  |
|       from large pool |     933    |     934    |    2516 K  |    2515 K  |
|       from small pool |    6363    |    6366    |    2663 K  |    2656 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     690    |     691    |    2130    |    1440    |
|       from large pool |     472    |     472    |    1460    |     988    |
|       from small pool |     218    |     219    |     670    |     452    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     454    |     454    |    2903 K  |    2902 K  |
|       from large pool |      63    |      63    |    1651 K  |    1651 K  |
|       from small pool |     391    |     391    |    1251 K  |    1251 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:45:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:45:22] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 15:45:27] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 1.25 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 76.95 GiB is allocated by PyTorch, and 1.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:45:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:45:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:45:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 7            |        cudaMalloc retries: 15        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78678 MiB |  78798 MiB |  79117 GiB |  79040 GiB |
|       from large pool |  78593 MiB |  78713 MiB |  78673 GiB |  78596 GiB |
|       from small pool |     85 MiB |     86 MiB |    443 GiB |    443 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78678 MiB |  78798 MiB |  79117 GiB |  79040 GiB |
|       from large pool |  78593 MiB |  78713 MiB |  78673 GiB |  78596 GiB |
|       from small pool |     85 MiB |     86 MiB |    443 GiB |    443 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78642 MiB |  78762 MiB |  78944 GiB |  78868 GiB |
|       from large pool |  78558 MiB |  78677 MiB |  78501 GiB |  78425 GiB |
|       from small pool |     84 MiB |     85 MiB |    443 GiB |    442 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80504 MiB |  80504 MiB | 356340 MiB | 275836 MiB |
|       from large pool |  80416 MiB |  80416 MiB | 354934 MiB | 274518 MiB |
|       from small pool |     88 MiB |    436 MiB |   1406 MiB |   1318 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1705 MiB |   8357 MiB |  80006 GiB |  80005 GiB |
|       from large pool |   1702 MiB |   8349 MiB |  79501 GiB |  79499 GiB |
|       from small pool |      2 MiB |     29 MiB |    505 GiB |    505 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1873    |    1876    |    5242 K  |    5240 K  |
|       from large pool |     462    |     464    |    2546 K  |    2545 K  |
|       from small pool |    1411    |    1414    |    2696 K  |    2694 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1873    |    1876    |    5242 K  |    5240 K  |
|       from large pool |     462    |     464    |    2546 K  |    2545 K  |
|       from small pool |    1411    |    1414    |    2696 K  |    2694 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     221    |     689    |    2198    |    1977    |
|       from large pool |     177    |     471    |    1495    |    1318    |
|       from small pool |      44    |     218    |     703    |     659    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     167    |     168    |    2940 K  |    2940 K  |
|       from large pool |     120    |     125    |    1671 K  |    1671 K  |
|       from small pool |      47    |      59    |    1269 K  |    1269 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:45:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:45:27] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:45:30]    INFO >> epoch 002:   1018 / 1539 loss=4.356, wps=3879.4, ups=5.64, wpb=687.9, bsz=687.9, num_updates=2550, lr=0.0004, gnorm=7.042, clip=0, train_wall=7, gb_free=72.9, wall=487 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:45:38]    INFO >> epoch 002:   1068 / 1539 loss=4.269, wps=4805.9, ups=6.29, wpb=763.7, bsz=763.7, num_updates=2600, lr=0.0004, gnorm=6.337, clip=0, train_wall=7, gb_free=73.5, wall=495 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:45:47]    INFO >> epoch 002:   1118 / 1539 loss=4.01, wps=4824.8, ups=6.09, wpb=791.9, bsz=791.9, num_updates=2650, lr=0.0004, gnorm=8.164, clip=0, train_wall=8, gb_free=69.2, wall=503 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:45:55]    INFO >> epoch 002:   1168 / 1539 loss=4.323, wps=4915.4, ups=6.38, wpb=770.1, bsz=770.1, num_updates=2700, lr=0.0004, gnorm=6.3, clip=0, train_wall=7, gb_free=72.3, wall=511 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:46:03]    INFO >> epoch 002:   1218 / 1539 loss=4.087, wps=4510.5, ups=6.35, wpb=710, bsz=710, num_updates=2750, lr=0.0004, gnorm=7.287, clip=2, train_wall=7, gb_free=70.9, wall=519 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:46:06] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 121.25 MiB is free. Including non-PyTorch memory, this process has 79.00 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 5.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:46:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:46:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:46:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 8            |        cudaMalloc retries: 18        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72821 MiB |  75425 MiB |  86354 GiB |  86283 GiB |
|       from large pool |  72808 MiB |  75412 MiB |  85870 GiB |  85799 GiB |
|       from small pool |     12 MiB |     14 MiB |    483 GiB |    483 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72821 MiB |  75425 MiB |  86354 GiB |  86283 GiB |
|       from large pool |  72808 MiB |  75412 MiB |  85870 GiB |  85799 GiB |
|       from small pool |     12 MiB |     14 MiB |    483 GiB |    483 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72801 MiB |  75403 MiB |  86167 GiB |  86096 GiB |
|       from large pool |  72788 MiB |  75391 MiB |  85684 GiB |  85613 GiB |
|       from small pool |     12 MiB |     14 MiB |    483 GiB |    483 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80384 MiB |  80384 MiB | 387082 MiB | 306698 MiB |
|       from large pool |  80360 MiB |  80360 MiB | 385538 MiB | 305178 MiB |
|       from small pool |     24 MiB |    226 MiB |   1544 MiB |   1520 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5660 MiB |  11732 MiB |  87442 GiB |  87437 GiB |
|       from large pool |   5649 MiB |  11720 MiB |  86890 GiB |  86885 GiB |
|       from small pool |     11 MiB |     25 MiB |    551 GiB |    551 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     603    |     612    |    5719 K  |    5718 K  |
|       from large pool |     312    |     320    |    2781 K  |    2781 K  |
|       from small pool |     291    |     356    |    2937 K  |    2937 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     603    |     612    |    5719 K  |    5718 K  |
|       from large pool |     312    |     320    |    2781 K  |    2781 K  |
|       from small pool |     291    |     356    |    2937 K  |    2937 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     120    |     229    |    2295    |    2175    |
|       from large pool |     108    |     116    |    1523    |    1415    |
|       from small pool |      12    |     113    |     772    |     760    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     118    |     118    |    3207 K  |    3206 K  |
|       from large pool |      91    |      91    |    1823 K  |    1823 K  |
|       from small pool |      27    |      54    |    1383 K  |    1383 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:46:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:46:06] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:46:11]    INFO >> epoch 002:   1269 / 1539 loss=4.179, wps=4513.8, ups=5.9, wpb=765.3, bsz=765.3, num_updates=2800, lr=0.0004, gnorm=7.296, clip=0, train_wall=7, gb_free=70.5, wall=527 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:46:19]    INFO >> epoch 002:   1319 / 1539 loss=4.166, wps=4491.5, ups=6.79, wpb=661.5, bsz=661.5, num_updates=2850, lr=0.0004, gnorm=6.222, clip=0, train_wall=7, gb_free=75, wall=535 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:46:26]    INFO >> epoch 002:   1369 / 1539 loss=4.02, wps=4777.8, ups=6.56, wpb=728.5, bsz=728.5, num_updates=2900, lr=0.0004, gnorm=7.953, clip=0, train_wall=7, gb_free=70.5, wall=542 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:46:34]    INFO >> epoch 002:   1419 / 1539 loss=4.163, wps=4228.6, ups=6.47, wpb=653.6, bsz=653.6, num_updates=2950, lr=0.0004, gnorm=6.939, clip=0, train_wall=7, gb_free=64.8, wall=550 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:46:44]    INFO >> epoch 002:   1469 / 1539 loss=4.124, wps=3539.4, ups=5.04, wpb=702.5, bsz=702.5, num_updates=3000, lr=0.0004, gnorm=7.452, clip=2, train_wall=9, gb_free=70.3, wall=560 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:46:55]    INFO >> epoch 002:   1519 / 1539 loss=4.157, wps=4287.7, ups=6.37, wpb=673.3, bsz=673.3, num_updates=3050, lr=0.0004, gnorm=6.92, clip=2, train_wall=7, gb_free=74.2, wall=568 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:46:58]    INFO >> epoch 002 | loss 4.323 | wps 4255.8 | ups 5.97 | wpb 712.7 | bsz 712.7 | num_updates 3070 | lr 0.0004 | gnorm 7.129 | clip 0.4 | train_wall 224 | gb_free 72.4 | wall 571 (progress_bar.py:267, print())[0m
[33m[2025-11-21 15:46:58] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:47:11]    INFO >> epoch 002 | valid on 'valid' subset | loss 3.997 | wps 11132.6 | wpb 5412.5 | bsz 5412.5 | num_updates 3070 | best_loss 4.689 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 15:47:12]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 15:47:12]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/checkpoints/checkpoint_last.pt (epoch 2 @ 3070 updates, score 3.997) (writing took 0.017663 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 15:47:12] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:47:16]    INFO >> epoch 003:     30 / 1539 loss=4.054, wps=1571.2, ups=2.31, wpb=681.1, bsz=681.1, num_updates=3100, lr=0.000392, gnorm=6.801, clip=0, train_wall=7, gb_free=70.7, wall=589 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:47:25]    INFO >> epoch 003:     80 / 1539 loss=3.993, wps=4966.6, ups=6.43, wpb=772.8, bsz=772.8, num_updates=3150, lr=0.000392, gnorm=7.652, clip=2, train_wall=7, gb_free=73.4, wall=597 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:47:29] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.08 GiB is free. Including non-PyTorch memory, this process has 78.04 GiB memory in use. Of the allocated memory 74.93 GiB is allocated by PyTorch, and 2.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:47:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:47:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:47:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 9            |        cudaMalloc retries: 21        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76326 MiB |  78600 MiB | 102451 GiB | 102377 GiB |
|       from large pool |  76307 MiB |  78583 MiB | 101871 GiB | 101796 GiB |
|       from small pool |     18 MiB |     19 MiB |    580 GiB |    580 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76326 MiB |  78600 MiB | 102451 GiB | 102377 GiB |
|       from large pool |  76307 MiB |  78583 MiB | 101871 GiB | 101796 GiB |
|       from small pool |     18 MiB |     19 MiB |    580 GiB |    580 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB | 102235 GiB | 102160 GiB |
|       from large pool |  76285 MiB |  78562 MiB | 101655 GiB | 101581 GiB |
|       from small pool |     18 MiB |     19 MiB |    579 GiB |    579 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79400 MiB |  79400 MiB | 483464 MiB | 404064 MiB |
|       from large pool |  79376 MiB |  79376 MiB | 481770 MiB | 402394 MiB |
|       from small pool |     24 MiB |     74 MiB |   1694 MiB |   1670 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3073 MiB |   4962 MiB | 103111 GiB | 103108 GiB |
|       from large pool |   3068 MiB |   4948 MiB | 102454 GiB | 102451 GiB |
|       from small pool |      5 MiB |     27 MiB |    657 GiB |    657 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     640    |     646    |    6746 K  |    6745 K  |
|       from large pool |     340    |     346    |    3209 K  |    3208 K  |
|       from small pool |     300    |     356    |    3536 K  |    3536 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     640    |     646    |    6746 K  |    6745 K  |
|       from large pool |     340    |     346    |    3209 K  |    3208 K  |
|       from small pool |     300    |     356    |    3536 K  |    3536 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     106    |     118    |    2420    |    2314    |
|       from large pool |      94    |      94    |    1573    |    1479    |
|       from small pool |      12    |      37    |     847    |     835    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     110    |     112    |    3792 K  |    3792 K  |
|       from large pool |      85    |      87    |    2095 K  |    2095 K  |
|       from small pool |      25    |      57    |    1696 K  |    1696 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:47:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:47:29] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:47:35]    INFO >> epoch 003:    131 / 1539 loss=4.133, wps=4457.5, ups=5.34, wpb=835, bsz=835, num_updates=3200, lr=0.000392, gnorm=7.202, clip=0, train_wall=7, gb_free=71.9, wall=607 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:47:42]    INFO >> epoch 003:    181 / 1539 loss=4.229, wps=4479.6, ups=6.82, wpb=656.8, bsz=656.8, num_updates=3250, lr=0.000392, gnorm=6.987, clip=0, train_wall=7, gb_free=73, wall=614 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:47:47] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 153.25 MiB is free. Including non-PyTorch memory, this process has 78.97 GiB memory in use. Of the allocated memory 75.67 GiB is allocated by PyTorch, and 2.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:47:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:47:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:47:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 10           |        cudaMalloc retries: 23        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76958 MiB |  77481 MiB | 105988 GiB | 105912 GiB |
|       from large pool |  76945 MiB |  77468 MiB | 105386 GiB | 105311 GiB |
|       from small pool |     12 MiB |     21 MiB |    601 GiB |    601 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76958 MiB |  77481 MiB | 105988 GiB | 105912 GiB |
|       from large pool |  76945 MiB |  77468 MiB | 105386 GiB | 105311 GiB |
|       from small pool |     12 MiB |     21 MiB |    601 GiB |    601 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76936 MiB |  77458 MiB | 105764 GiB | 105689 GiB |
|       from large pool |  76923 MiB |  77445 MiB | 105164 GiB | 105089 GiB |
|       from small pool |     12 MiB |     21 MiB |    600 GiB |    600 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80352 MiB |  80352 MiB | 489864 MiB | 409512 MiB |
|       from large pool |  80326 MiB |  80326 MiB | 487984 MiB | 407658 MiB |
|       from small pool |     26 MiB |    210 MiB |   1880 MiB |   1854 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3393 MiB |   8163 MiB | 107065 GiB | 107061 GiB |
|       from large pool |   3380 MiB |   8148 MiB | 106384 GiB | 106380 GiB |
|       from small pool |     13 MiB |     27 MiB |    681 GiB |    681 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |    6985 K  |    6984 K  |
|       from large pool |     315    |     322    |    3323 K  |    3323 K  |
|       from small pool |     291    |     356    |    3662 K  |    3661 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |    6985 K  |    6984 K  |
|       from large pool |     315    |     322    |    3323 K  |    3323 K  |
|       from small pool |     291    |     356    |    3662 K  |    3661 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     100    |     199    |    2520    |    2420    |
|       from large pool |      87    |      94    |    1580    |    1493    |
|       from small pool |      13    |     105    |     940    |     927    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      95    |      95    |    3925 K  |    3925 K  |
|       from large pool |      69    |      69    |    2166 K  |    2166 K  |
|       from small pool |      26    |      58    |    1758 K  |    1758 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:47:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:47:47] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:47:51]    INFO >> epoch 003:    232 / 1539 loss=4.088, wps=4389.9, ups=5.72, wpb=767.2, bsz=767.2, num_updates=3300, lr=0.000392, gnorm=7.422, clip=4, train_wall=8, gb_free=74, wall=623 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:48:00]    INFO >> epoch 003:    282 / 1539 loss=4.066, wps=4912, ups=6.53, wpb=751.9, bsz=751.9, num_updates=3350, lr=0.000392, gnorm=6.398, clip=0, train_wall=7, gb_free=70.6, wall=630 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:48:08]    INFO >> epoch 003:    332 / 1539 loss=4.078, wps=4802.6, ups=6.08, wpb=790.4, bsz=790.4, num_updates=3400, lr=0.000392, gnorm=7.224, clip=0, train_wall=8, gb_free=73.8, wall=639 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:48:16]    INFO >> epoch 003:    382 / 1539 loss=4.014, wps=4146.3, ups=6.02, wpb=688.4, bsz=688.4, num_updates=3450, lr=0.000392, gnorm=7.242, clip=0, train_wall=8, gb_free=72.5, wall=647 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:48:24]    INFO >> epoch 003:    432 / 1539 loss=4.117, wps=4311.9, ups=6.56, wpb=657.8, bsz=657.8, num_updates=3500, lr=0.000392, gnorm=6.676, clip=2, train_wall=7, gb_free=66.2, wall=654 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:48:33]    INFO >> epoch 003:    482 / 1539 loss=4.01, wps=4735.9, ups=6.28, wpb=754, bsz=754, num_updates=3550, lr=0.000392, gnorm=6.534, clip=0, train_wall=7, gb_free=73.1, wall=662 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:48:41]    INFO >> epoch 003:    532 / 1539 loss=3.978, wps=4978.9, ups=6.46, wpb=770.5, bsz=770.5, num_updates=3600, lr=0.000392, gnorm=6.887, clip=4, train_wall=7, gb_free=73.8, wall=670 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:48:49]    INFO >> epoch 003:    582 / 1539 loss=3.928, wps=4487.2, ups=6.23, wpb=720.1, bsz=720.1, num_updates=3650, lr=0.000392, gnorm=6.831, clip=0, train_wall=8, gb_free=71.6, wall=678 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:48:56]    INFO >> epoch 003:    632 / 1539 loss=3.941, wps=4693.7, ups=6.96, wpb=674.5, bsz=674.5, num_updates=3700, lr=0.000392, gnorm=6.76, clip=0, train_wall=7, gb_free=66.5, wall=685 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:49:05]    INFO >> epoch 003:    682 / 1539 loss=4.051, wps=4909.2, ups=6.49, wpb=756.5, bsz=756.5, num_updates=3750, lr=0.000392, gnorm=6.579, clip=0, train_wall=7, gb_free=75.1, wall=693 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:49:14]    INFO >> epoch 003:    732 / 1539 loss=3.834, wps=4638.4, ups=5.72, wpb=810.3, bsz=810.3, num_updates=3800, lr=0.000392, gnorm=6.808, clip=2, train_wall=8, gb_free=73.9, wall=702 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:49:22]    INFO >> epoch 003:    782 / 1539 loss=3.863, wps=4994.2, ups=6.28, wpb=795, bsz=795, num_updates=3850, lr=0.000392, gnorm=6.411, clip=2, train_wall=7, gb_free=73.7, wall=710 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:49:23] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 29.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.96 GiB is allocated by PyTorch, and 1.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:49:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:49:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:49:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 11           |        cudaMalloc retries: 26        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78744 MiB |  78804 MiB | 123508 GiB | 123431 GiB |
|       from large pool |  78658 MiB |  78718 MiB | 122809 GiB | 122733 GiB |
|       from small pool |     85 MiB |     86 MiB |    698 GiB |    697 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78744 MiB |  78804 MiB | 123508 GiB | 123431 GiB |
|       from large pool |  78658 MiB |  78718 MiB | 122809 GiB | 122733 GiB |
|       from small pool |     85 MiB |     86 MiB |    698 GiB |    697 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78703 MiB |  78762 MiB | 123247 GiB | 123170 GiB |
|       from large pool |  78617 MiB |  78677 MiB | 122550 GiB | 122473 GiB |
|       from small pool |     85 MiB |     86 MiB |    697 GiB |    696 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80476 MiB |  80476 MiB | 517426 MiB | 436950 MiB |
|       from large pool |  80386 MiB |  80386 MiB | 515324 MiB | 434938 MiB |
|       from small pool |     90 MiB |    246 MiB |   2102 MiB |   2012 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1671 MiB |   7867 MiB | 124788 GiB | 124786 GiB |
|       from large pool |   1667 MiB |   7857 MiB | 123995 GiB | 123994 GiB |
|       from small pool |      4 MiB |     27 MiB |    792 GiB |    792 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1884    |    1887    |    8145 K  |    8143 K  |
|       from large pool |     463    |     464    |    3900 K  |    3900 K  |
|       from small pool |    1421    |    1424    |    4244 K  |    4243 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1884    |    1887    |    8145 K  |    8143 K  |
|       from large pool |     463    |     464    |    3900 K  |    3900 K  |
|       from small pool |    1421    |    1424    |    4244 K  |    4243 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     225    |     300    |    2734    |    2509    |
|       from large pool |     180    |     180    |    1683    |    1503    |
|       from small pool |      45    |     123    |    1051    |    1006    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     169    |     169    |    4572 K  |    4572 K  |
|       from large pool |     121    |     124    |    2540 K  |    2540 K  |
|       from small pool |      48    |      51    |    2031 K  |    2031 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:49:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:49:23] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:49:30]    INFO >> epoch 003:    833 / 1539 loss=3.89, wps=4466.4, ups=6.07, wpb=736, bsz=736, num_updates=3900, lr=0.000392, gnorm=7.727, clip=0, train_wall=7, gb_free=73, wall=718 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:49:39]    INFO >> epoch 003:    883 / 1539 loss=4.05, wps=4535, ups=6.33, wpb=717, bsz=717, num_updates=3950, lr=0.000392, gnorm=6.294, clip=0, train_wall=7, gb_free=72.5, wall=726 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:49:47]    INFO >> epoch 003:    933 / 1539 loss=3.868, wps=4634.5, ups=6.35, wpb=729.5, bsz=729.5, num_updates=4000, lr=0.000392, gnorm=6.845, clip=0, train_wall=7, gb_free=67.3, wall=734 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:49:56]    INFO >> epoch 003:    983 / 1539 loss=4.045, wps=3804.6, ups=5.56, wpb=684.3, bsz=684.3, num_updates=4050, lr=0.000392, gnorm=7.237, clip=0, train_wall=8, gb_free=71.5, wall=743 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:50:04]    INFO >> epoch 003:   1033 / 1539 loss=4.035, wps=4155.6, ups=6.53, wpb=636.3, bsz=636.3, num_updates=4100, lr=0.000392, gnorm=6.209, clip=0, train_wall=7, gb_free=68.3, wall=750 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:50:13]    INFO >> epoch 003:   1083 / 1539 loss=3.9, wps=4630.2, ups=6.87, wpb=673.9, bsz=673.9, num_updates=4150, lr=0.000392, gnorm=6.889, clip=0, train_wall=7, gb_free=72.9, wall=758 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:50:20] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 39.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 76.52 GiB is allocated by PyTorch, and 2.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:50:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:50:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:50:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 12           |        cudaMalloc retries: 28        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78301 MiB |  78361 MiB | 133746 GiB | 133670 GiB |
|       from large pool |  77916 MiB |  77976 MiB | 132993 GiB | 132917 GiB |
|       from small pool |    385 MiB |    386 MiB |    753 GiB |    752 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78301 MiB |  78361 MiB | 133746 GiB | 133670 GiB |
|       from large pool |  77916 MiB |  77976 MiB | 132993 GiB | 132917 GiB |
|       from small pool |    385 MiB |    386 MiB |    753 GiB |    752 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78077 MiB |  78137 MiB | 133463 GiB | 133387 GiB |
|       from large pool |  77694 MiB |  77753 MiB | 132711 GiB | 132635 GiB |
|       from small pool |    383 MiB |    384 MiB |    752 GiB |    751 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80466 MiB |  80504 MiB | 539302 MiB | 458836 MiB |
|       from large pool |  80042 MiB |  80326 MiB | 536864 MiB | 456822 MiB |
|       from small pool |    424 MiB |    426 MiB |   2438 MiB |   2014 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2104 MiB |   6644 MiB | 134066 GiB | 134064 GiB |
|       from large pool |   2065 MiB |   6637 MiB | 133209 GiB | 133207 GiB |
|       from small pool |     38 MiB |     40 MiB |    856 GiB |    856 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7109    |    7112    |    8817 K  |    8810 K  |
|       from large pool |     916    |     917    |    4239 K  |    4238 K  |
|       from small pool |    6193    |    6196    |    4578 K  |    4572 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7109    |    7112    |    8817 K  |    8810 K  |
|       from large pool |     916    |     917    |    4239 K  |    4238 K  |
|       from small pool |    6193    |    6196    |    4578 K  |    4572 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     742    |     743    |    3261    |    2519    |
|       from large pool |     530    |     530    |    2042    |    1512    |
|       from small pool |     212    |     213    |    1219    |    1007    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     510    |     511    |    4950 K  |    4949 K  |
|       from large pool |     132    |     132    |    2765 K  |    2764 K  |
|       from small pool |     378    |     379    |    2185 K  |    2184 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:50:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:50:20] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:50:21]    INFO >> epoch 003:   1134 / 1539 loss=3.94, wps=4090.2, ups=5.97, wpb=684.8, bsz=684.8, num_updates=4200, lr=0.000392, gnorm=6.733, clip=0, train_wall=7, gb_free=73, wall=766 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:50:29]    INFO >> epoch 003:   1184 / 1539 loss=3.948, wps=4260.9, ups=6.33, wpb=673.6, bsz=673.6, num_updates=4250, lr=0.000392, gnorm=6.037, clip=0, train_wall=7, gb_free=73.5, wall=774 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:50:37]    INFO >> epoch 003:   1234 / 1539 loss=3.876, wps=4437.7, ups=6.18, wpb=718.1, bsz=718.1, num_updates=4300, lr=0.000392, gnorm=6.441, clip=2, train_wall=8, gb_free=70.5, wall=782 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:50:46]    INFO >> epoch 003:   1284 / 1539 loss=3.873, wps=4192.7, ups=6.77, wpb=619.7, bsz=619.7, num_updates=4350, lr=0.000392, gnorm=5.925, clip=2, train_wall=7, gb_free=73.5, wall=789 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:50:53]    INFO >> epoch 003:   1334 / 1539 loss=3.872, wps=4701.5, ups=6.64, wpb=708, bsz=708, num_updates=4400, lr=0.000392, gnorm=6.81, clip=4, train_wall=7, gb_free=72.1, wall=797 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:51:01]    INFO >> epoch 003:   1384 / 1539 loss=3.793, wps=4572.1, ups=6.6, wpb=692.7, bsz=692.7, num_updates=4450, lr=0.000392, gnorm=7.498, clip=0, train_wall=7, gb_free=74.2, wall=805 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:51:08]    INFO >> epoch 003:   1434 / 1539 loss=3.951, wps=4357.4, ups=6.69, wpb=651.7, bsz=651.7, num_updates=4500, lr=0.000392, gnorm=6.036, clip=0, train_wall=7, gb_free=72.1, wall=812 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:51:16]    INFO >> epoch 003:   1484 / 1539 loss=3.889, wps=4198.1, ups=6.37, wpb=658.9, bsz=658.9, num_updates=4550, lr=0.000392, gnorm=7.016, clip=0, train_wall=7, gb_free=72.4, wall=820 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:51:24]    INFO >> epoch 003:   1534 / 1539 loss=3.938, wps=4399.5, ups=6.64, wpb=662.9, bsz=662.9, num_updates=4600, lr=0.000392, gnorm=8.05, clip=0, train_wall=7, gb_free=72, wall=827 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:51:24]    INFO >> epoch 003 | loss 3.974 | wps 4253.1 | ups 5.97 | wpb 712.7 | bsz 712.7 | num_updates 4605 | lr 0.000392 | gnorm 6.848 | clip 0.8 | train_wall 225 | gb_free 74.4 | wall 828 (progress_bar.py:267, print())[0m
[33m[2025-11-21 15:51:24] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:51:39]    INFO >> epoch 003 | valid on 'valid' subset | loss 3.902 | wps 10378.5 | wpb 5412.5 | bsz 5412.5 | num_updates 4605 | best_loss 4.689 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 15:51:39]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 15:51:39]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/checkpoints/checkpoint_last.pt (epoch 3 @ 4605 updates, score 3.902) (writing took 0.014377 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 15:51:39] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:51:47]    INFO >> epoch 004:     45 / 1539 loss=3.882, wps=1624.9, ups=2.16, wpb=751.7, bsz=751.7, num_updates=4650, lr=0.000376, gnorm=6.998, clip=0, train_wall=8, gb_free=72.7, wall=851 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:51:57]    INFO >> epoch 004:     95 / 1539 loss=3.895, wps=4711.9, ups=6.23, wpb=756, bsz=756, num_updates=4700, lr=0.000376, gnorm=6.493, clip=0, train_wall=8, gb_free=67.8, wall=859 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:52:08]    INFO >> epoch 004:    145 / 1539 loss=3.459, wps=5169.6, ups=4.93, wpb=1048.6, bsz=1048.6, num_updates=4750, lr=0.000376, gnorm=7.774, clip=2, train_wall=10, gb_free=75.1, wall=869 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:52:15]    INFO >> epoch 004:    195 / 1539 loss=3.919, wps=4676.6, ups=6.67, wpb=701.3, bsz=701.3, num_updates=4800, lr=0.000376, gnorm=5.99, clip=0, train_wall=7, gb_free=71.9, wall=876 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:52:24]    INFO >> epoch 004:    245 / 1539 loss=3.818, wps=4797.9, ups=6.45, wpb=743.6, bsz=743.6, num_updates=4850, lr=0.000376, gnorm=8.303, clip=4, train_wall=7, gb_free=73.5, wall=884 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:52:32]    INFO >> epoch 004:    295 / 1539 loss=3.995, wps=4369.3, ups=6.75, wpb=647.4, bsz=647.4, num_updates=4900, lr=0.000376, gnorm=5.975, clip=0, train_wall=7, gb_free=74.6, wall=891 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:52:39]    INFO >> epoch 004:    345 / 1539 loss=3.903, wps=4649.2, ups=6.76, wpb=687.4, bsz=687.4, num_updates=4950, lr=0.000376, gnorm=5.471, clip=0, train_wall=7, gb_free=69.9, wall=899 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:52:47]    INFO >> epoch 004:    395 / 1539 loss=3.618, wps=4913.3, ups=6.46, wpb=761.1, bsz=761.1, num_updates=5000, lr=0.000376, gnorm=6.774, clip=0, train_wall=7, gb_free=67.8, wall=907 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:52:55]    INFO >> epoch 004:    445 / 1539 loss=3.969, wps=3729.3, ups=5.91, wpb=630.5, bsz=630.5, num_updates=5050, lr=0.000376, gnorm=6.225, clip=0, train_wall=8, gb_free=71.4, wall=915 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:53:05]    INFO >> epoch 004:    495 / 1539 loss=3.888, wps=4514.5, ups=6.27, wpb=720.1, bsz=720.1, num_updates=5100, lr=0.000376, gnorm=6.131, clip=0, train_wall=7, gb_free=74, wall=923 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:53:12]    INFO >> epoch 004:    545 / 1539 loss=3.875, wps=4464.2, ups=6.36, wpb=701.6, bsz=701.6, num_updates=5150, lr=0.000376, gnorm=6.642, clip=0, train_wall=7, gb_free=73.5, wall=931 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:53:20]    INFO >> epoch 004:    595 / 1539 loss=3.883, wps=4042.2, ups=6.42, wpb=629.8, bsz=629.8, num_updates=5200, lr=0.000376, gnorm=6.043, clip=0, train_wall=7, gb_free=71, wall=939 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:53:21] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 13.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 77.38 GiB is allocated by PyTorch, and 1.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:53:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:53:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:53:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 13           |        cudaMalloc retries: 30        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79173 MiB |  79233 MiB | 167559 GiB | 167482 GiB |
|       from large pool |  79083 MiB |  79143 MiB | 166611 GiB | 166534 GiB |
|       from small pool |     89 MiB |     91 MiB |    948 GiB |    948 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79173 MiB |  79233 MiB | 167559 GiB | 167482 GiB |
|       from large pool |  79083 MiB |  79143 MiB | 166611 GiB | 166534 GiB |
|       from small pool |     89 MiB |     91 MiB |    948 GiB |    948 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79123 MiB |  79183 MiB | 167207 GiB | 167130 GiB |
|       from large pool |  79034 MiB |  79093 MiB | 166260 GiB | 166183 GiB |
|       from small pool |     89 MiB |     90 MiB |    946 GiB |    946 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80492 MiB |  80492 MiB | 564646 MiB | 484154 MiB |
|       from large pool |  80398 MiB |  80398 MiB | 562140 MiB | 481742 MiB |
|       from small pool |     94 MiB |     94 MiB |   2506 MiB |   2412 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1258 MiB |   7162 MiB | 164240 GiB | 164239 GiB |
|       from large pool |   1254 MiB |   7152 MiB | 163165 GiB | 163164 GiB |
|       from small pool |      4 MiB |     27 MiB |   1074 GiB |   1074 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1961    |    1964    |   11044 K  |   11042 K  |
|       from large pool |     470    |     471    |    5271 K  |    5271 K  |
|       from small pool |    1491    |    1494    |    5772 K  |    5770 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1961    |    1964    |   11044 K  |   11042 K  |
|       from large pool |     470    |     471    |    5271 K  |    5271 K  |
|       from small pool |    1491    |    1494    |    5772 K  |    5770 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     223    |     223    |    3348    |    3125    |
|       from large pool |     176    |     176    |    2095    |    1919    |
|       from small pool |      47    |      47    |    1253    |    1206    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     140    |     140    |    6216 K  |    6216 K  |
|       from large pool |      92    |      96    |    3444 K  |    3444 K  |
|       from small pool |      48    |      58    |    2772 K  |    2772 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:53:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:53:21] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:53:28]    INFO >> epoch 004:    646 / 1539 loss=3.763, wps=4198.3, ups=6.26, wpb=671.1, bsz=671.1, num_updates=5250, lr=0.000376, gnorm=7.285, clip=2, train_wall=7, gb_free=68.7, wall=947 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:53:38] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 983.25 MiB is free. Including non-PyTorch memory, this process has 78.16 GiB memory in use. Of the allocated memory 74.92 GiB is allocated by PyTorch, and 2.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:53:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:53:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:53:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 14           |        cudaMalloc retries: 32        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76323 MiB |  78596 MiB | 170178 GiB | 170104 GiB |
|       from large pool |  76304 MiB |  78579 MiB | 169217 GiB | 169142 GiB |
|       from small pool |     18 MiB |     24 MiB |    961 GiB |    961 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76323 MiB |  78596 MiB | 170178 GiB | 170104 GiB |
|       from large pool |  76304 MiB |  78579 MiB | 169217 GiB | 169142 GiB |
|       from small pool |     18 MiB |     24 MiB |    961 GiB |    961 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB | 169821 GiB | 169746 GiB |
|       from large pool |  76285 MiB |  78562 MiB | 168861 GiB | 168786 GiB |
|       from small pool |     18 MiB |     24 MiB |    960 GiB |    959 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79522 MiB |  80432 MiB | 637044 MiB | 557522 MiB |
|       from large pool |  79496 MiB |  80338 MiB | 634538 MiB | 555042 MiB |
|       from small pool |     26 MiB |     94 MiB |   2506 MiB |   2480 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3198 MiB |   4143 MiB | 166736 GiB | 166733 GiB |
|       from large pool |   3191 MiB |   4135 MiB | 165646 GiB | 165643 GiB |
|       from small pool |      7 MiB |     23 MiB |   1089 GiB |   1089 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     640    |     646    |   11213 K  |   11212 K  |
|       from large pool |     340    |     346    |    5361 K  |    5361 K  |
|       from small pool |     300    |     356    |    5852 K  |    5851 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     640    |     646    |   11213 K  |   11212 K  |
|       from large pool |     340    |     346    |    5361 K  |    5361 K  |
|       from small pool |     300    |     356    |    5852 K  |    5851 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     107    |     222    |    3403    |    3296    |
|       from large pool |      94    |     175    |    2150    |    2056    |
|       from small pool |      13    |      47    |    1253    |    1240    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     126    |     128    |    6309 K  |    6309 K  |
|       from large pool |      98    |     100    |    3503 K  |    3503 K  |
|       from small pool |      28    |      51    |    2805 K  |    2805 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:53:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:53:38] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:53:39]    INFO >> epoch 004:    697 / 1539 loss=3.843, wps=3267, ups=5.13, wpb=636.6, bsz=636.6, num_updates=5300, lr=0.000376, gnorm=5.877, clip=0, train_wall=7, gb_free=67.8, wall=956 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:53:48]    INFO >> epoch 004:    747 / 1539 loss=3.932, wps=3802.9, ups=5.65, wpb=673.1, bsz=673.1, num_updates=5350, lr=0.000376, gnorm=6.006, clip=0, train_wall=8, gb_free=72.9, wall=965 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:53:56]    INFO >> epoch 004:    797 / 1539 loss=3.778, wps=4629.2, ups=6.52, wpb=710.1, bsz=710.1, num_updates=5400, lr=0.000376, gnorm=5.676, clip=0, train_wall=7, gb_free=72.3, wall=973 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:54:03]    INFO >> epoch 004:    847 / 1539 loss=3.815, wps=4866.2, ups=6.51, wpb=747.5, bsz=747.5, num_updates=5450, lr=0.000376, gnorm=6.151, clip=0, train_wall=7, gb_free=63.6, wall=981 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:54:12]    INFO >> epoch 004:    897 / 1539 loss=3.83, wps=4676.2, ups=6.01, wpb=778.3, bsz=778.3, num_updates=5500, lr=0.000376, gnorm=7.275, clip=0, train_wall=8, gb_free=70.2, wall=989 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:54:19]    INFO >> epoch 004:    947 / 1539 loss=3.847, wps=4275.9, ups=6.61, wpb=646.7, bsz=646.7, num_updates=5550, lr=0.000376, gnorm=5.443, clip=0, train_wall=7, gb_free=67.5, wall=996 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:54:28]    INFO >> epoch 004:    997 / 1539 loss=3.777, wps=4651.1, ups=6.08, wpb=764.9, bsz=764.9, num_updates=5600, lr=0.000376, gnorm=6.099, clip=0, train_wall=8, gb_free=75.5, wall=1005 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:54:35]    INFO >> epoch 004:   1047 / 1539 loss=3.92, wps=4251.7, ups=6.76, wpb=628.6, bsz=628.6, num_updates=5650, lr=0.000376, gnorm=5.957, clip=0, train_wall=7, gb_free=72.5, wall=1012 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:54:45]    INFO >> epoch 004:   1097 / 1539 loss=3.793, wps=4669.4, ups=6.4, wpb=729.6, bsz=729.6, num_updates=5700, lr=0.000376, gnorm=5.892, clip=0, train_wall=7, gb_free=71.1, wall=1020 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:54:53]    INFO >> epoch 004:   1147 / 1539 loss=3.857, wps=4392.8, ups=6.49, wpb=676.4, bsz=676.4, num_updates=5750, lr=0.000376, gnorm=5.802, clip=0, train_wall=7, gb_free=56.5, wall=1028 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:55:01]    INFO >> epoch 004:   1197 / 1539 loss=3.772, wps=4723.7, ups=6.26, wpb=754.7, bsz=754.7, num_updates=5800, lr=0.000376, gnorm=6.18, clip=0, train_wall=7, gb_free=75.6, wall=1036 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:55:08]    INFO >> epoch 004:   1247 / 1539 loss=3.774, wps=4402.9, ups=7.01, wpb=627.8, bsz=627.8, num_updates=5850, lr=0.000376, gnorm=5.229, clip=0, train_wall=7, gb_free=75.6, wall=1043 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:55:18]    INFO >> epoch 004:   1297 / 1539 loss=3.825, wps=4005.8, ups=6.21, wpb=644.6, bsz=644.6, num_updates=5900, lr=0.000376, gnorm=5.988, clip=2, train_wall=8, gb_free=74.6, wall=1051 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:55:26]    INFO >> epoch 004:   1347 / 1539 loss=3.674, wps=4984.1, ups=6.22, wpb=801.1, bsz=801.1, num_updates=5950, lr=0.000376, gnorm=6.455, clip=0, train_wall=8, gb_free=66.8, wall=1059 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:55:34] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.29 GiB memory in use. Of the allocated memory 73.80 GiB is allocated by PyTorch, and 2.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:55:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:55:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:55:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 15           |        cudaMalloc retries: 34        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75053 MiB |  75996 MiB | 190873 GiB | 190800 GiB |
|       from large pool |  75040 MiB |  75983 MiB | 189802 GiB | 189729 GiB |
|       from small pool |     12 MiB |     13 MiB |   1070 GiB |   1070 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75053 MiB |  75996 MiB | 190873 GiB | 190800 GiB |
|       from large pool |  75040 MiB |  75983 MiB | 189802 GiB | 189729 GiB |
|       from small pool |     12 MiB |     13 MiB |   1070 GiB |   1070 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75035 MiB |  75977 MiB | 190472 GiB | 190399 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 189403 GiB | 189330 GiB |
|       from small pool |     12 MiB |     13 MiB |   1069 GiB |   1069 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78634 MiB |  79742 MiB | 645400 MiB | 566766 MiB |
|       from large pool |  78608 MiB |  79496 MiB | 642674 MiB | 564066 MiB |
|       from small pool |     26 MiB |    246 MiB |   2726 MiB |   2700 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3580 MiB |   7454 MiB | 189472 GiB | 189468 GiB |
|       from large pool |   3567 MiB |   7440 MiB | 188257 GiB | 188254 GiB |
|       from small pool |     13 MiB |     21 MiB |   1214 GiB |   1214 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   12564 K  |   12563 K  |
|       from large pool |     314    |     322    |    6052 K  |    6052 K  |
|       from small pool |     291    |     336    |    6511 K  |    6510 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   12564 K  |   12563 K  |
|       from large pool |     314    |     322    |    6052 K  |    6052 K  |
|       from small pool |     291    |     336    |    6511 K  |    6510 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      91    |     217    |    3521    |    3430    |
|       from large pool |      78    |      94    |    2158    |    2080    |
|       from small pool |      13    |     123    |    1363    |    1350    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      89    |      90    |    7038 K  |    7038 K  |
|       from large pool |      60    |      61    |    3937 K  |    3937 K  |
|       from small pool |      29    |      51    |    3100 K  |    3100 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:55:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:55:34] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:55:35]    INFO >> epoch 004:   1398 / 1539 loss=3.741, wps=4440.7, ups=5.61, wpb=791.5, bsz=791.5, num_updates=6000, lr=0.000376, gnorm=6.285, clip=2, train_wall=8, gb_free=71.6, wall=1068 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:55:42]    INFO >> epoch 004:   1448 / 1539 loss=3.815, wps=4645.4, ups=6.53, wpb=711.7, bsz=711.7, num_updates=6050, lr=0.000376, gnorm=6.582, clip=0, train_wall=7, gb_free=73.5, wall=1075 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:55:51]    INFO >> epoch 004:   1498 / 1539 loss=3.766, wps=4359.6, ups=6.39, wpb=682, bsz=682, num_updates=6100, lr=0.000376, gnorm=6.744, clip=0, train_wall=7, gb_free=74.2, wall=1083 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:55:54] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 25.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.10 GiB is allocated by PyTorch, and 1.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:55:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:55:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:55:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 16           |        cudaMalloc retries: 35        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78888 MiB |  78948 MiB | 194338 GiB | 194261 GiB |
|       from large pool |  78495 MiB |  78555 MiB | 193245 GiB | 193169 GiB |
|       from small pool |    393 MiB |    394 MiB |   1092 GiB |   1092 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78888 MiB |  78948 MiB | 194338 GiB | 194261 GiB |
|       from large pool |  78495 MiB |  78555 MiB | 193245 GiB | 193169 GiB |
|       from small pool |    393 MiB |    394 MiB |   1092 GiB |   1092 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78858 MiB |  78917 MiB | 193930 GiB | 193853 GiB |
|       from large pool |  78467 MiB |  78526 MiB | 192839 GiB | 192762 GiB |
|       from small pool |    390 MiB |    392 MiB |   1090 GiB |   1090 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80480 MiB |  80482 MiB | 647248 MiB | 566768 MiB |
|       from large pool |  80048 MiB |  80048 MiB | 644114 MiB | 564066 MiB |
|       from small pool |    432 MiB |    434 MiB |   3134 MiB |   2702 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1531 MiB |   4145 MiB | 193502 GiB | 193500 GiB |
|       from large pool |   1492 MiB |   4108 MiB | 192262 GiB | 192260 GiB |
|       from small pool |     38 MiB |     41 MiB |   1239 GiB |   1239 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7252    |    7255    |   12813 K  |   12806 K  |
|       from large pool |     929    |     930    |    6169 K  |    6168 K  |
|       from small pool |    6323    |    6326    |    6643 K  |    6637 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7252    |    7255    |   12813 K  |   12806 K  |
|       from large pool |     929    |     930    |    6169 K  |    6168 K  |
|       from small pool |    6323    |    6326    |    6643 K  |    6637 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     318    |     319    |    3749    |    3431    |
|       from large pool |     102    |     102    |    2182    |    2080    |
|       from small pool |     216    |     217    |    1567    |    1351    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     462    |     464    |    7175 K  |    7174 K  |
|       from large pool |      77    |      77    |    4009 K  |    4009 K  |
|       from small pool |     385    |     387    |    3165 K  |    3165 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:55:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:55:54] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:55:58]    INFO >> epoch 004 | loss 3.814 | wps 4184 | ups 5.87 | wpb 712.7 | bsz 712.7 | num_updates 6140 | lr 0.000376 | gnorm 6.31 | clip 0.4 | train_wall 227 | gb_free 70.1 | wall 1090 (progress_bar.py:267, print())[0m
[33m[2025-11-21 15:55:58] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:56:13]    INFO >> epoch 004 | valid on 'valid' subset | loss 3.846 | wps 9969.1 | wpb 5412.5 | bsz 5412.5 | num_updates 6140 | best_loss 4.689 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 15:56:13]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 15:56:13]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/checkpoints/checkpoint_last.pt (epoch 4 @ 6140 updates, score 3.846) (writing took 0.021527 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 15:56:13] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:56:15]    INFO >> epoch 005:     10 / 1539 loss=3.853, wps=1370.7, ups=2.13, wpb=643.2, bsz=643.2, num_updates=6150, lr=0.000354, gnorm=6.845, clip=0, train_wall=7, gb_free=72.1, wall=1107 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:56:25]    INFO >> epoch 005:     60 / 1539 loss=3.929, wps=4207.8, ups=5.97, wpb=704.5, bsz=704.5, num_updates=6200, lr=0.000354, gnorm=5.878, clip=0, train_wall=8, gb_free=73.4, wall=1115 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:56:32]    INFO >> epoch 005:    110 / 1539 loss=3.838, wps=4661, ups=6.61, wpb=705.4, bsz=705.4, num_updates=6250, lr=0.000354, gnorm=6.679, clip=0, train_wall=7, gb_free=71.5, wall=1123 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:56:40]    INFO >> epoch 005:    160 / 1539 loss=3.857, wps=4344.3, ups=6.79, wpb=639.6, bsz=639.6, num_updates=6300, lr=0.000354, gnorm=5.968, clip=0, train_wall=7, gb_free=74, wall=1130 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:56:47]    INFO >> epoch 005:    210 / 1539 loss=3.775, wps=4364.8, ups=6.5, wpb=671.6, bsz=671.6, num_updates=6350, lr=0.000354, gnorm=5.302, clip=0, train_wall=7, gb_free=71, wall=1138 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:56:57]    INFO >> epoch 005:    260 / 1539 loss=3.75, wps=5167.9, ups=5.77, wpb=895.2, bsz=895.2, num_updates=6400, lr=0.000354, gnorm=6.123, clip=0, train_wall=8, gb_free=67.8, wall=1146 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:57:05]    INFO >> epoch 005:    310 / 1539 loss=3.706, wps=4533, ups=6.43, wpb=704.8, bsz=704.8, num_updates=6450, lr=0.000354, gnorm=6.131, clip=0, train_wall=7, gb_free=71.8, wall=1154 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:57:13]    INFO >> epoch 005:    360 / 1539 loss=3.565, wps=4879.6, ups=6.31, wpb=772.8, bsz=772.8, num_updates=6500, lr=0.000354, gnorm=6.614, clip=2, train_wall=7, gb_free=69.5, wall=1162 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:57:20] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 27.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.85 GiB is allocated by PyTorch, and 1.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:57:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:57:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:57:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 17           |        cudaMalloc retries: 36        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78634 MiB |  78694 MiB | 211161 GiB | 211084 GiB |
|       from large pool |  78243 MiB |  78303 MiB | 209958 GiB | 209881 GiB |
|       from small pool |    390 MiB |    391 MiB |   1202 GiB |   1202 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78634 MiB |  78694 MiB | 211161 GiB | 211084 GiB |
|       from large pool |  78243 MiB |  78303 MiB | 209958 GiB | 209881 GiB |
|       from small pool |    390 MiB |    391 MiB |   1202 GiB |   1202 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78618 MiB |  78677 MiB | 210720 GiB | 210643 GiB |
|       from large pool |  78229 MiB |  78288 MiB | 209518 GiB | 209442 GiB |
|       from small pool |    388 MiB |    389 MiB |   1201 GiB |   1200 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80478 MiB |  80480 MiB | 647308 MiB | 566830 MiB |
|       from large pool |  80048 MiB |  80048 MiB | 644174 MiB | 564126 MiB |
|       from small pool |    430 MiB |    432 MiB |   3134 MiB |   2704 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1783 MiB |   5192 MiB | 209174 GiB | 209172 GiB |
|       from large pool |   1744 MiB |   5187 MiB | 207812 GiB | 207810 GiB |
|       from small pool |     39 MiB |     41 MiB |   1361 GiB |   1361 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7208    |    7211    |   13967 K  |   13960 K  |
|       from large pool |     925    |     926    |    6636 K  |    6635 K  |
|       from small pool |    6283    |    6286    |    7330 K  |    7324 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7208    |    7211    |   13967 K  |   13960 K  |
|       from large pool |     925    |     926    |    6636 K  |    6635 K  |
|       from small pool |    6283    |    6286    |    7330 K  |    7324 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     317    |     318    |    3750    |    3433    |
|       from large pool |     102    |     102    |    2183    |    2081    |
|       from small pool |     215    |     216    |    1567    |    1352    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     476    |     477    |    7847 K  |    7846 K  |
|       from large pool |      90    |      90    |    4312 K  |    4312 K  |
|       from small pool |     386    |     387    |    3534 K  |    3534 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:57:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:57:20] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:57:21]    INFO >> epoch 005:    411 / 1539 loss=3.829, wps=4264.8, ups=6.26, wpb=681.6, bsz=681.6, num_updates=6550, lr=0.000354, gnorm=6.156, clip=0, train_wall=7, gb_free=73.9, wall=1170 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:57:28]    INFO >> epoch 005:    461 / 1539 loss=3.818, wps=4084.7, ups=6.72, wpb=608, bsz=608, num_updates=6600, lr=0.000354, gnorm=5.59, clip=0, train_wall=7, gb_free=74.7, wall=1177 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:57:41]    INFO >> epoch 005:    511 / 1539 loss=3.602, wps=2931.6, ups=4.01, wpb=730.5, bsz=730.5, num_updates=6650, lr=0.000354, gnorm=5.923, clip=0, train_wall=12, gb_free=69.8, wall=1190 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:57:48]    INFO >> epoch 005:    561 / 1539 loss=3.915, wps=4104.8, ups=6.61, wpb=620.7, bsz=620.7, num_updates=6700, lr=0.000354, gnorm=5.655, clip=0, train_wall=7, gb_free=72.5, wall=1197 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:57:53] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 3.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.48 GiB is allocated by PyTorch, and 1.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:57:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:57:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:57:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 18           |        cudaMalloc retries: 38        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79276 MiB |  79336 MiB | 216573 GiB | 216496 GiB |
|       from large pool |  79185 MiB |  79245 MiB | 215343 GiB | 215265 GiB |
|       from small pool |     91 MiB |     92 MiB |   1230 GiB |   1230 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79276 MiB |  79336 MiB | 216573 GiB | 216496 GiB |
|       from large pool |  79185 MiB |  79245 MiB | 215343 GiB | 215265 GiB |
|       from small pool |     91 MiB |     92 MiB |   1230 GiB |   1230 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79244 MiB |  79303 MiB | 216121 GiB | 216043 GiB |
|       from large pool |  79153 MiB |  79212 MiB | 214892 GiB | 214815 GiB |
|       from small pool |     90 MiB |     92 MiB |   1228 GiB |   1228 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80502 MiB |  80504 MiB | 647732 MiB | 567230 MiB |
|       from large pool |  80408 MiB |  80408 MiB | 644594 MiB | 564186 MiB |
|       from small pool |     94 MiB |    430 MiB |   3138 MiB |   3044 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1165 MiB |   6539 MiB | 214884 GiB | 214883 GiB |
|       from large pool |   1162 MiB |   6529 MiB | 213490 GiB | 213489 GiB |
|       from small pool |      2 MiB |     25 MiB |   1393 GiB |   1393 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1983    |    1986    |   14316 K  |   14314 K  |
|       from large pool |     472    |     473    |    6818 K  |    6818 K  |
|       from small pool |    1511    |    1514    |    7497 K  |    7496 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1983    |    1986    |   14316 K  |   14314 K  |
|       from large pool |     472    |     473    |    6818 K  |    6818 K  |
|       from small pool |    1511    |    1514    |    7497 K  |    7496 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     155    |     317    |    3759    |    3604    |
|       from large pool |     108    |     108    |    2190    |    2082    |
|       from small pool |      47    |     215    |    1569    |    1522    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     138    |     140    |    8036 K  |    8036 K  |
|       from large pool |      88    |      91    |    4428 K  |    4428 K  |
|       from small pool |      50    |      58    |    3608 K  |    3608 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:57:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:57:53] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:57:57]    INFO >> epoch 005:    612 / 1539 loss=3.81, wps=4402.1, ups=5.82, wpb=756, bsz=756, num_updates=6750, lr=0.000354, gnorm=6.454, clip=0, train_wall=7, gb_free=70.1, wall=1206 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:58:08]    INFO >> epoch 005:    662 / 1539 loss=3.778, wps=4652.1, ups=6.31, wpb=736.8, bsz=736.8, num_updates=6800, lr=0.000354, gnorm=5.874, clip=0, train_wall=7, gb_free=74.2, wall=1214 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:58:16]    INFO >> epoch 005:    712 / 1539 loss=3.854, wps=4587.8, ups=6.29, wpb=729, bsz=729, num_updates=6850, lr=0.000354, gnorm=6.724, clip=0, train_wall=7, gb_free=71.4, wall=1222 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:58:23]    INFO >> epoch 005:    762 / 1539 loss=3.733, wps=4743.2, ups=6.61, wpb=717.1, bsz=717.1, num_updates=6900, lr=0.000354, gnorm=5.735, clip=0, train_wall=7, gb_free=72.9, wall=1229 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:58:32]    INFO >> epoch 005:    812 / 1539 loss=3.702, wps=3897.2, ups=5.79, wpb=673.7, bsz=673.7, num_updates=6950, lr=0.000354, gnorm=5.913, clip=0, train_wall=8, gb_free=69.7, wall=1238 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:58:41]    INFO >> epoch 005:    862 / 1539 loss=3.648, wps=4238.7, ups=6.32, wpb=670.9, bsz=670.9, num_updates=7000, lr=0.000354, gnorm=5.964, clip=0, train_wall=7, gb_free=66.5, wall=1246 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:58:49]    INFO >> epoch 005:    912 / 1539 loss=3.743, wps=4351.1, ups=6.52, wpb=667, bsz=667, num_updates=7050, lr=0.000354, gnorm=5.898, clip=0, train_wall=7, gb_free=71.7, wall=1254 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:58:55] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 131.25 MiB is free. Including non-PyTorch memory, this process has 78.99 GiB memory in use. Of the allocated memory 74.93 GiB is allocated by PyTorch, and 3.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:58:55] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:58:55] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:58:55] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 19           |        cudaMalloc retries: 39        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76325 MiB |  78599 MiB | 227233 GiB | 227158 GiB |
|       from large pool |  76306 MiB |  78582 MiB | 225947 GiB | 225873 GiB |
|       from small pool |     18 MiB |     24 MiB |   1285 GiB |   1285 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76325 MiB |  78599 MiB | 227233 GiB | 227158 GiB |
|       from large pool |  76306 MiB |  78582 MiB | 225947 GiB | 225873 GiB |
|       from small pool |     18 MiB |     24 MiB |   1285 GiB |   1285 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB | 226757 GiB | 226683 GiB |
|       from large pool |  76285 MiB |  78562 MiB | 225474 GiB | 225399 GiB |
|       from small pool |     18 MiB |     24 MiB |   1283 GiB |   1283 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80374 MiB |  80442 MiB | 647732 MiB | 567358 MiB |
|       from large pool |  80348 MiB |  80348 MiB | 644594 MiB | 564246 MiB |
|       from small pool |     26 MiB |     94 MiB |   3138 MiB |   3112 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4048 MiB |   5924 MiB | 225933 GiB | 225929 GiB |
|       from large pool |   4041 MiB |   5910 MiB | 224477 GiB | 224474 GiB |
|       from small pool |      7 MiB |     23 MiB |   1455 GiB |   1455 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     640    |     646    |   14999 K  |   14999 K  |
|       from large pool |     340    |     346    |    7173 K  |    7173 K  |
|       from small pool |     300    |     356    |    7826 K  |    7825 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     640    |     646    |   14999 K  |   14999 K  |
|       from large pool |     340    |     346    |    7173 K  |    7173 K  |
|       from small pool |     300    |     356    |    7826 K  |    7825 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     120    |     154    |    3759    |    3639    |
|       from large pool |     107    |     107    |    2190    |    2083    |
|       from small pool |      13    |      47    |    1569    |    1556    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     111    |     113    |    8408 K  |    8408 K  |
|       from large pool |      84    |      86    |    4655 K  |    4655 K  |
|       from small pool |      27    |      48    |    3752 K  |    3752 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:58:55] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:58:55] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:58:57]    INFO >> epoch 005:    963 / 1539 loss=3.743, wps=4006.7, ups=5.92, wpb=676.6, bsz=676.6, num_updates=7100, lr=0.000354, gnorm=5.954, clip=0, train_wall=7, gb_free=70.4, wall=1262 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:59:06]    INFO >> epoch 005:   1013 / 1539 loss=3.406, wps=5245.9, ups=5.5, wpb=954.2, bsz=954.2, num_updates=7150, lr=0.000354, gnorm=6.741, clip=2, train_wall=9, gb_free=70.3, wall=1271 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:59:16]    INFO >> epoch 005:   1063 / 1539 loss=3.583, wps=4641.2, ups=5.87, wpb=791.1, bsz=791.1, num_updates=7200, lr=0.000354, gnorm=5.801, clip=0, train_wall=8, gb_free=64.7, wall=1280 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:59:24]    INFO >> epoch 005:   1113 / 1539 loss=3.662, wps=4071.1, ups=6.5, wpb=626.1, bsz=626.1, num_updates=7250, lr=0.000354, gnorm=6.185, clip=0, train_wall=7, gb_free=74.2, wall=1287 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:59:26] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 199.25 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 4.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:59:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:59:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:59:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 20           |        cudaMalloc retries: 43        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72823 MiB |  75427 MiB | 232756 GiB | 232685 GiB |
|       from large pool |  72810 MiB |  75414 MiB | 231439 GiB | 231368 GiB |
|       from small pool |     12 MiB |     21 MiB |   1316 GiB |   1316 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72823 MiB |  75427 MiB | 232756 GiB | 232685 GiB |
|       from large pool |  72810 MiB |  75414 MiB | 231439 GiB | 231368 GiB |
|       from small pool |     12 MiB |     21 MiB |   1316 GiB |   1316 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72801 MiB |  75403 MiB | 232269 GiB | 232198 GiB |
|       from large pool |  72788 MiB |  75391 MiB | 230954 GiB | 230883 GiB |
|       from small pool |     12 MiB |     21 MiB |   1314 GiB |   1314 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80306 MiB |  80306 MiB | 688436 MiB | 608130 MiB |
|       from large pool |  80280 MiB |  80280 MiB | 685098 MiB | 604818 MiB |
|       from small pool |     26 MiB |    226 MiB |   3338 MiB |   3312 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5580 MiB |   9572 MiB | 230915 GiB | 230909 GiB |
|       from large pool |   5567 MiB |   9557 MiB | 229423 GiB | 229418 GiB |
|       from small pool |     13 MiB |     31 MiB |   1491 GiB |   1491 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     603    |     612    |   15362 K  |   15361 K  |
|       from large pool |     312    |     320    |    7349 K  |    7348 K  |
|       from small pool |     291    |     356    |    8012 K  |    8012 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     603    |     612    |   15362 K  |   15361 K  |
|       from large pool |     312    |     320    |    7349 K  |    7348 K  |
|       from small pool |     291    |     356    |    8012 K  |    8012 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     170    |     326    |    3990    |    3820    |
|       from large pool |     157    |     213    |    2321    |    2164    |
|       from small pool |      13    |     113    |    1669    |    1656    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     159    |     161    |    8616 K  |    8616 K  |
|       from large pool |     133    |     135    |    4773 K  |    4773 K  |
|       from small pool |      26    |      58    |    3842 K  |    3842 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:59:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:59:26] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:59:33]    INFO >> epoch 005:   1164 / 1539 loss=3.7, wps=4356.8, ups=5.58, wpb=780.4, bsz=780.4, num_updates=7300, lr=0.000354, gnorm=6.197, clip=2, train_wall=8, gb_free=69.6, wall=1296 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:59:42]    INFO >> epoch 005:   1214 / 1539 loss=3.809, wps=4061.1, ups=6.17, wpb=657.7, bsz=657.7, num_updates=7350, lr=0.000354, gnorm=5.535, clip=0, train_wall=8, gb_free=68.7, wall=1305 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:59:49]    INFO >> epoch 005:   1264 / 1539 loss=3.69, wps=4428.6, ups=6.97, wpb=635.1, bsz=635.1, num_updates=7400, lr=0.000354, gnorm=5.463, clip=0, train_wall=7, gb_free=73.5, wall=1312 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:59:57]    INFO >> epoch 005:   1314 / 1539 loss=3.695, wps=4112.6, ups=6.31, wpb=651.5, bsz=651.5, num_updates=7450, lr=0.000354, gnorm=5.382, clip=0, train_wall=7, gb_free=70, wall=1320 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:00:05]    INFO >> epoch 005:   1364 / 1539 loss=3.697, wps=4542.1, ups=6.26, wpb=725.6, bsz=725.6, num_updates=7500, lr=0.000354, gnorm=6.003, clip=0, train_wall=7, gb_free=74.7, wall=1328 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:00:13]    INFO >> epoch 005:   1414 / 1539 loss=3.86, wps=4168, ups=6.62, wpb=630.1, bsz=630.1, num_updates=7550, lr=0.000354, gnorm=5.507, clip=0, train_wall=7, gb_free=74.2, wall=1335 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:00:22]    INFO >> epoch 005:   1464 / 1539 loss=3.652, wps=4701.5, ups=6.16, wpb=763.7, bsz=763.7, num_updates=7600, lr=0.000354, gnorm=6.112, clip=0, train_wall=8, gb_free=48.5, wall=1343 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:00:31]    INFO >> epoch 005:   1514 / 1539 loss=3.7, wps=4664.4, ups=5.81, wpb=802.7, bsz=802.7, num_updates=7650, lr=0.000354, gnorm=6.281, clip=2, train_wall=8, gb_free=54.7, wall=1352 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:00:35]    INFO >> epoch 005 | loss 3.731 | wps 4107 | ups 5.76 | wpb 712.7 | bsz 712.7 | num_updates 7675 | lr 0.000354 | gnorm 6.008 | clip 0.3 | train_wall 233 | gb_free 63.5 | wall 1356 (progress_bar.py:267, print())[0m
[33m[2025-11-21 16:00:35] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:00:52]    INFO >> epoch 005 | valid on 'valid' subset | loss 3.779 | wps 9821 | wpb 5412.5 | bsz 5412.5 | num_updates 7675 | best_loss 4.689 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 16:00:52]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 16:00:52]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/checkpoints/checkpoint_last.pt (epoch 5 @ 7675 updates, score 3.779) (writing took 0.018395 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 16:00:52] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:00:56]    INFO >> epoch 006:     25 / 1539 loss=3.76, wps=1503.4, ups=2.12, wpb=708.8, bsz=708.8, num_updates=7700, lr=0.000327, gnorm=5.68, clip=0, train_wall=7, gb_free=74.9, wall=1375 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:01:04]    INFO >> epoch 006:     75 / 1539 loss=3.754, wps=5092, ups=5.9, wpb=863.7, bsz=863.7, num_updates=7750, lr=0.000327, gnorm=5.87, clip=0, train_wall=8, gb_free=75.2, wall=1384 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:01:12]    INFO >> epoch 006:    125 / 1539 loss=3.671, wps=4425.5, ups=6.55, wpb=675.3, bsz=675.3, num_updates=7800, lr=0.000327, gnorm=5.908, clip=0, train_wall=7, gb_free=70.3, wall=1392 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:01:20]    INFO >> epoch 006:    175 / 1539 loss=3.688, wps=4375, ups=6.35, wpb=689, bsz=689, num_updates=7850, lr=0.000327, gnorm=6.062, clip=0, train_wall=7, gb_free=74.3, wall=1399 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:01:23] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 535.25 MiB is free. Including non-PyTorch memory, this process has 78.59 GiB memory in use. Of the allocated memory 73.81 GiB is allocated by PyTorch, and 4.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:01:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:01:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:01:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 21           |        cudaMalloc retries: 44        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75055 MiB |  75998 MiB | 254753 GiB | 254679 GiB |
|       from large pool |  75043 MiB |  75985 MiB | 253307 GiB | 253233 GiB |
|       from small pool |     12 MiB |     15 MiB |   1446 GiB |   1446 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75055 MiB |  75998 MiB | 254753 GiB | 254679 GiB |
|       from large pool |  75043 MiB |  75985 MiB | 253307 GiB | 253233 GiB |
|       from small pool |     12 MiB |     15 MiB |   1446 GiB |   1446 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75035 MiB |  75977 MiB | 254221 GiB | 254148 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 252777 GiB | 252704 GiB |
|       from small pool |     12 MiB |     15 MiB |   1443 GiB |   1443 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79970 MiB |  80406 MiB | 690438 MiB | 610468 MiB |
|       from large pool |  79944 MiB |  80306 MiB | 687026 MiB | 607082 MiB |
|       from small pool |     26 MiB |    100 MiB |   3412 MiB |   3386 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4914 MiB |   8989 MiB | 249563 GiB | 249559 GiB |
|       from large pool |   4900 MiB |   8976 MiB | 247929 GiB | 247925 GiB |
|       from small pool |     13 MiB |     25 MiB |   1633 GiB |   1633 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   16790 K  |   16790 K  |
|       from large pool |     314    |     322    |    7980 K  |    7979 K  |
|       from small pool |     291    |     356    |    8810 K  |    8810 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   16790 K  |   16790 K  |
|       from large pool |     314    |     322    |    7980 K  |    7979 K  |
|       from small pool |     291    |     356    |    8810 K  |    8810 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     172    |     210    |    4031    |    3859    |
|       from large pool |     159    |     160    |    2325    |    2166    |
|       from small pool |      13    |      50    |    1706    |    1693    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     162    |     162    |    9442 K  |    9442 K  |
|       from large pool |     134    |     134    |    5197 K  |    5197 K  |
|       from small pool |      28    |      57    |    4245 K  |    4245 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:01:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:01:23] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 16:01:30] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 904.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 535.25 MiB is free. Including non-PyTorch memory, this process has 78.59 GiB memory in use. Of the allocated memory 72.94 GiB is allocated by PyTorch, and 5.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:01:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:01:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:01:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 22           |        cudaMalloc retries: 45        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69875 MiB |  74872 MiB | 255914 GiB | 255845 GiB |
|       from large pool |  69858 MiB |  74855 MiB | 254463 GiB | 254394 GiB |
|       from small pool |     17 MiB |     17 MiB |   1451 GiB |   1451 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69875 MiB |  74872 MiB | 255914 GiB | 255845 GiB |
|       from large pool |  69858 MiB |  74855 MiB | 254463 GiB | 254394 GiB |
|       from small pool |     17 MiB |     17 MiB |   1451 GiB |   1451 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  69846 MiB |  74842 MiB | 255380 GiB | 255311 GiB |
|       from large pool |  69829 MiB |  74825 MiB | 253930 GiB | 253862 GiB |
|       from small pool |     17 MiB |     17 MiB |   1449 GiB |   1449 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79970 MiB |  80006 MiB | 690474 MiB | 610504 MiB |
|       from large pool |  79944 MiB |  79944 MiB | 687026 MiB | 607082 MiB |
|       from small pool |     26 MiB |     62 MiB |   3448 MiB |   3422 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  10094 MiB |  11472 MiB | 250619 GiB | 250609 GiB |
|       from large pool |  10085 MiB |  11463 MiB | 248979 GiB | 248969 GiB |
|       from small pool |      8 MiB |     27 MiB |   1639 GiB |   1639 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     643    |     661    |   16854 K  |   16853 K  |
|       from large pool |     344    |     362    |    8013 K  |    8013 K  |
|       from small pool |     299    |     356    |    8840 K  |    8840 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     643    |     661    |   16854 K  |   16853 K  |
|       from large pool |     344    |     362    |    8013 K  |    8013 K  |
|       from small pool |     299    |     356    |    8840 K  |    8840 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     172    |     190    |    4049    |    3877    |
|       from large pool |     159    |     159    |    2325    |    2166    |
|       from small pool |      13    |      31    |    1724    |    1711    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     151    |     153    |    9478 K  |    9477 K  |
|       from large pool |     121    |     123    |    5219 K  |    5219 K  |
|       from small pool |      30    |      62    |    4258 K  |    4258 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:01:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:01:30] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:01:31]    INFO >> epoch 006:    227 / 1539 loss=3.747, wps=3784.7, ups=5.26, wpb=719.2, bsz=719.2, num_updates=7900, lr=0.000327, gnorm=5.515, clip=0, train_wall=8, gb_free=71.4, wall=1409 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:01:38]    INFO >> epoch 006:    277 / 1539 loss=3.731, wps=4142.8, ups=6.62, wpb=625.7, bsz=625.7, num_updates=7950, lr=0.000327, gnorm=4.791, clip=0, train_wall=7, gb_free=74.5, wall=1416 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:01:46]    INFO >> epoch 006:    327 / 1539 loss=3.49, wps=5055.2, ups=6.09, wpb=830.5, bsz=830.5, num_updates=8000, lr=0.000327, gnorm=6.444, clip=0, train_wall=8, gb_free=70.3, wall=1425 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:01:56]    INFO >> epoch 006:    377 / 1539 loss=3.795, wps=4044.3, ups=6.15, wpb=657.6, bsz=657.6, num_updates=8050, lr=0.000327, gnorm=5.116, clip=0, train_wall=8, gb_free=71.1, wall=1433 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:02:04]    INFO >> epoch 006:    427 / 1539 loss=3.595, wps=4886.3, ups=6.33, wpb=772.1, bsz=772.1, num_updates=8100, lr=0.000327, gnorm=5.414, clip=0, train_wall=7, gb_free=75, wall=1441 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:02:06] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 2.39 GiB is free. Including non-PyTorch memory, this process has 76.72 GiB memory in use. Of the allocated memory 70.76 GiB is allocated by PyTorch, and 5.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:02:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:02:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:02:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 23           |        cudaMalloc retries: 47        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72059 MiB |  72460 MiB | 262338 GiB | 262268 GiB |
|       from large pool |  72042 MiB |  72442 MiB | 260852 GiB | 260782 GiB |
|       from small pool |     17 MiB |     21 MiB |   1486 GiB |   1486 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72059 MiB |  72460 MiB | 262338 GiB | 262268 GiB |
|       from large pool |  72042 MiB |  72442 MiB | 260852 GiB | 260782 GiB |
|       from small pool |     17 MiB |     21 MiB |   1486 GiB |   1486 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  72444 MiB | 261790 GiB | 261719 GiB |
|       from large pool |  72027 MiB |  72426 MiB | 260305 GiB | 260235 GiB |
|       from small pool |     17 MiB |     21 MiB |   1484 GiB |   1484 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78056 MiB |  80154 MiB | 696490 MiB | 618434 MiB |
|       from large pool |  78026 MiB |  79944 MiB | 692858 MiB | 614832 MiB |
|       from small pool |     30 MiB |    210 MiB |   3632 MiB |   3602 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5996 MiB |   8377 MiB | 256685 GiB | 256680 GiB |
|       from large pool |   5983 MiB |   8364 MiB | 255005 GiB | 255000 GiB |
|       from small pool |     12 MiB |     31 MiB |   1680 GiB |   1680 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   17287 K  |   17286 K  |
|       from large pool |     308    |     315    |    8230 K  |    8230 K  |
|       from small pool |     298    |     356    |    9056 K  |    9056 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   17287 K  |   17286 K  |
|       from large pool |     308    |     315    |    8230 K  |    8230 K  |
|       from small pool |     298    |     356    |    9056 K  |    9056 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     172    |     264    |    4144    |    3972    |
|       from large pool |     157    |     159    |    2328    |    2171    |
|       from small pool |      15    |     105    |    1816    |    1801    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     169    |     169    |    9721 K  |    9721 K  |
|       from large pool |     137    |     137    |    5362 K  |    5362 K  |
|       from small pool |      32    |      55    |    4358 K  |    4358 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:02:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:02:06] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:02:12]    INFO >> epoch 006:    478 / 1539 loss=3.864, wps=4170.6, ups=5.91, wpb=705.8, bsz=705.8, num_updates=8150, lr=0.000327, gnorm=5.285, clip=0, train_wall=7, gb_free=65, wall=1449 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:02:21]    INFO >> epoch 006:    528 / 1539 loss=3.689, wps=4157.1, ups=5.87, wpb=707.6, bsz=707.6, num_updates=8200, lr=0.000327, gnorm=5.52, clip=0, train_wall=8, gb_free=72, wall=1458 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:02:30]    INFO >> epoch 006:    578 / 1539 loss=3.745, wps=4935.2, ups=6.19, wpb=797.6, bsz=797.6, num_updates=8250, lr=0.000327, gnorm=5.679, clip=0, train_wall=8, gb_free=74.1, wall=1466 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:02:37]    INFO >> epoch 006:    628 / 1539 loss=3.698, wps=4425.2, ups=6.86, wpb=644.9, bsz=644.9, num_updates=8300, lr=0.000327, gnorm=5.699, clip=0, train_wall=7, gb_free=70.5, wall=1473 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:02:46]    INFO >> epoch 006:    678 / 1539 loss=3.67, wps=4445.1, ups=6.09, wpb=730.2, bsz=730.2, num_updates=8350, lr=0.000327, gnorm=5.896, clip=0, train_wall=8, gb_free=73.6, wall=1481 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:02:53]    INFO >> epoch 006:    728 / 1539 loss=3.762, wps=4088.9, ups=6.64, wpb=616.1, bsz=616.1, num_updates=8400, lr=0.000327, gnorm=4.652, clip=0, train_wall=7, gb_free=62, wall=1489 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:02:54] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 17.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 76.16 GiB is allocated by PyTorch, and 2.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:02:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:02:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:02:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 24           |        cudaMalloc retries: 48        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77932 MiB |  77992 MiB | 270810 GiB | 270734 GiB |
|       from large pool |  77549 MiB |  77609 MiB | 269273 GiB | 269198 GiB |
|       from small pool |    383 MiB |    384 MiB |   1536 GiB |   1536 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77932 MiB |  77992 MiB | 270810 GiB | 270734 GiB |
|       from large pool |  77549 MiB |  77609 MiB | 269273 GiB | 269198 GiB |
|       from small pool |    383 MiB |    384 MiB |   1536 GiB |   1536 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77897 MiB |  77956 MiB | 270242 GiB | 270166 GiB |
|       from large pool |  77516 MiB |  77575 MiB | 268708 GiB | 268632 GiB |
|       from small pool |    381 MiB |    382 MiB |   1534 GiB |   1533 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80488 MiB |  80490 MiB | 698924 MiB | 618436 MiB |
|       from large pool |  80066 MiB |  80066 MiB | 694898 MiB | 614832 MiB |
|       from small pool |    422 MiB |    424 MiB |   4026 MiB |   3604 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2495 MiB |   6667 MiB | 264813 GiB | 264811 GiB |
|       from large pool |   2456 MiB |   6661 MiB | 263075 GiB | 263073 GiB |
|       from small pool |     38 MiB |     40 MiB |   1737 GiB |   1737 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7076    |    7079    |   17882 K  |   17874 K  |
|       from large pool |     913    |     914    |    8520 K  |    8519 K  |
|       from small pool |    6163    |    6166    |    9361 K  |    9355 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7076    |    7079    |   17882 K  |   17874 K  |
|       from large pool |     913    |     914    |    8520 K  |    8519 K  |
|       from small pool |    6163    |    6166    |    9361 K  |    9355 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     402    |     403    |    4375    |    3973    |
|       from large pool |     191    |     191    |    2362    |    2171    |
|       from small pool |     211    |     212    |    2013    |    1802    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     527    |     528    |   10056 K  |   10055 K  |
|       from large pool |     150    |     150    |    5552 K  |    5552 K  |
|       from small pool |     377    |     378    |    4503 K  |    4503 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:02:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:02:54] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:03:01]    INFO >> epoch 006:    779 / 1539 loss=3.611, wps=4040.7, ups=6.29, wpb=642.5, bsz=642.5, num_updates=8450, lr=0.000327, gnorm=5.29, clip=0, train_wall=7, gb_free=74.1, wall=1497 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:03:10]    INFO >> epoch 006:    829 / 1539 loss=3.695, wps=4292.7, ups=6.54, wpb=656.8, bsz=656.8, num_updates=8500, lr=0.000327, gnorm=5.355, clip=0, train_wall=7, gb_free=66.6, wall=1504 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:03:18]    INFO >> epoch 006:    879 / 1539 loss=3.575, wps=4671.4, ups=6.4, wpb=729.6, bsz=729.6, num_updates=8550, lr=0.000327, gnorm=5.82, clip=0, train_wall=7, gb_free=70.2, wall=1512 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:03:27]    INFO >> epoch 006:    929 / 1539 loss=3.655, wps=4754.3, ups=5.75, wpb=826.1, bsz=826.1, num_updates=8600, lr=0.000327, gnorm=5.78, clip=0, train_wall=8, gb_free=73.1, wall=1521 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:03:35]    INFO >> epoch 006:    979 / 1539 loss=3.597, wps=4317, ups=6.16, wpb=701.1, bsz=701.1, num_updates=8650, lr=0.000327, gnorm=5.317, clip=0, train_wall=8, gb_free=75.1, wall=1529 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:03:45] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.57 GiB. GPU 2 has a total capacity of 79.14 GiB of which 2.27 GiB is free. Including non-PyTorch memory, this process has 76.84 GiB memory in use. Of the allocated memory 66.03 GiB is allocated by PyTorch, and 10.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:03:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:03:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:03:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 25           |        cudaMalloc retries: 49        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  63499 MiB |  70130 MiB | 279630 GiB | 279567 GiB |
|       from large pool |  63479 MiB |  70110 MiB | 278045 GiB | 277983 GiB |
|       from small pool |     20 MiB |     60 MiB |   1584 GiB |   1584 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  63499 MiB |  70130 MiB | 279630 GiB | 279567 GiB |
|       from large pool |  63479 MiB |  70110 MiB | 278045 GiB | 277983 GiB |
|       from small pool |     20 MiB |     60 MiB |   1584 GiB |   1584 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  63481 MiB |  70111 MiB | 279042 GiB | 278980 GiB |
|       from large pool |  63461 MiB |  70091 MiB | 277460 GiB | 277398 GiB |
|       from small pool |     20 MiB |     60 MiB |   1582 GiB |   1582 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78178 MiB |  80428 MiB | 698924 MiB | 620746 MiB |
|       from large pool |  78146 MiB |  80006 MiB | 694898 MiB | 616752 MiB |
|       from small pool |     32 MiB |    422 MiB |   4026 MiB |   3994 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   7300 MiB |  10790 MiB | 272795 GiB | 272788 GiB |
|       from large pool |   7288 MiB |  10778 MiB | 271002 GiB | 270994 GiB |
|       from small pool |     11 MiB |     27 MiB |   1793 GiB |   1793 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     646    |    1326    |   18465 K  |   18465 K  |
|       from large pool |     330    |     403    |    8814 K  |    8814 K  |
|       from small pool |     316    |     924    |    9651 K  |    9650 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     646    |    1326    |   18465 K  |   18465 K  |
|       from large pool |     330    |     403    |    8814 K  |    8814 K  |
|       from small pool |     316    |     924    |    9651 K  |    9650 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     176    |     401    |    4375    |    4199    |
|       from large pool |     160    |     190    |    2362    |    2202    |
|       from small pool |      16    |     211    |    2013    |    1997    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     164    |     213    |   10388 K  |   10388 K  |
|       from large pool |     129    |     160    |    5749 K  |    5749 K  |
|       from small pool |      35    |      54    |    4638 K  |    4638 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:03:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:03:45] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:03:45]    INFO >> epoch 006:   1030 / 1539 loss=3.723, wps=4064.6, ups=5.57, wpb=730.1, bsz=730.1, num_updates=8700, lr=0.000327, gnorm=4.684, clip=0, train_wall=8, gb_free=11.5, wall=1538 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:03:53]    INFO >> epoch 006:   1080 / 1539 loss=3.654, wps=4457.4, ups=6.16, wpb=723.5, bsz=723.5, num_updates=8750, lr=0.000327, gnorm=5.81, clip=2, train_wall=8, gb_free=72.4, wall=1546 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:04:02]    INFO >> epoch 006:   1130 / 1539 loss=3.625, wps=4458.7, ups=5.89, wpb=757.6, bsz=757.6, num_updates=8800, lr=0.000327, gnorm=4.935, clip=0, train_wall=8, gb_free=72.1, wall=1555 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:04:11]    INFO >> epoch 006:   1180 / 1539 loss=3.701, wps=4723.9, ups=6.38, wpb=740.5, bsz=740.5, num_updates=8850, lr=0.000327, gnorm=5.818, clip=0, train_wall=7, gb_free=69, wall=1562 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:04:19]    INFO >> epoch 006:   1230 / 1539 loss=3.711, wps=4130.6, ups=6.26, wpb=659.3, bsz=659.3, num_updates=8900, lr=0.000327, gnorm=5.5, clip=0, train_wall=7, gb_free=73.8, wall=1570 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:04:26]    INFO >> epoch 006:   1280 / 1539 loss=3.756, wps=4321.9, ups=6.64, wpb=650.6, bsz=650.6, num_updates=8950, lr=0.000327, gnorm=5.694, clip=0, train_wall=7, gb_free=75.3, wall=1578 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:04:34]    INFO >> epoch 006:   1330 / 1539 loss=3.647, wps=4730.7, ups=6.38, wpb=741, bsz=741, num_updates=9000, lr=0.000327, gnorm=5.273, clip=0, train_wall=7, gb_free=71.3, wall=1586 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:04:42]    INFO >> epoch 006:   1380 / 1539 loss=3.711, wps=4189.3, ups=6.88, wpb=608.9, bsz=608.9, num_updates=9050, lr=0.000327, gnorm=5.434, clip=0, train_wall=7, gb_free=68.9, wall=1593 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:04:49]    INFO >> epoch 006:   1430 / 1539 loss=3.642, wps=4769.1, ups=6.34, wpb=751.8, bsz=751.8, num_updates=9100, lr=0.000327, gnorm=5.67, clip=0, train_wall=7, gb_free=70.5, wall=1601 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:04:58]    INFO >> epoch 006:   1480 / 1539 loss=3.5, wps=4419.1, ups=6.16, wpb=717, bsz=717, num_updates=9150, lr=0.000327, gnorm=5.691, clip=2, train_wall=8, gb_free=69.4, wall=1609 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:05:05]    INFO >> epoch 006:   1530 / 1539 loss=3.68, wps=4585.3, ups=6.8, wpb=674.1, bsz=674.1, num_updates=9200, lr=0.000327, gnorm=4.577, clip=0, train_wall=7, gb_free=74.6, wall=1616 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:05:06]    INFO >> epoch 006 | loss 3.678 | wps 4165.3 | ups 5.86 | wpb 711.1 | bsz 711.1 | num_updates 9209 | lr 0.000327 | gnorm 5.49 | clip 0.1 | train_wall 228 | gb_free 72.3 | wall 1618 (progress_bar.py:267, print())[0m
[33m[2025-11-21 16:05:06] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:05:24]    INFO >> epoch 006 | valid on 'valid' subset | loss 3.804 | wps 10147 | wpb 5412.5 | bsz 5412.5 | num_updates 9209 | best_loss 4.689 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 16:05:24]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 16:05:24]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/checkpoints/checkpoint_last.pt (epoch 6 @ 9209 updates, score 3.804) (writing took 0.013676 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 16:05:24] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:05:32]    INFO >> epoch 007:     41 / 1539 loss=3.728, wps=1468.8, ups=2.09, wpb=704.3, bsz=704.3, num_updates=9250, lr=0.000295, gnorm=6.028, clip=0, train_wall=8, gb_free=71.6, wall=1640 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:05:33] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.82 GiB is free. Including non-PyTorch memory, this process has 77.29 GiB memory in use. Of the allocated memory 70.77 GiB is allocated by PyTorch, and 6.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:05:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:05:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:05:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 26           |        cudaMalloc retries: 51        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72064 MiB |  72463 MiB | 300570 GiB | 300499 GiB |
|       from large pool |  72047 MiB |  72446 MiB | 298863 GiB | 298793 GiB |
|       from small pool |     17 MiB |     25 MiB |   1706 GiB |   1706 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72064 MiB |  72463 MiB | 300570 GiB | 300499 GiB |
|       from large pool |  72047 MiB |  72446 MiB | 298863 GiB | 298793 GiB |
|       from small pool |     17 MiB |     25 MiB |   1706 GiB |   1706 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  72444 MiB | 299940 GiB | 299870 GiB |
|       from large pool |  72027 MiB |  72426 MiB | 298236 GiB | 298165 GiB |
|       from small pool |     17 MiB |     25 MiB |   1704 GiB |   1704 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78638 MiB |  78794 MiB | 712374 MiB | 633736 MiB |
|       from large pool |  78610 MiB |  78670 MiB | 708256 MiB | 629646 MiB |
|       from small pool |     28 MiB |    124 MiB |   4118 MiB |   4090 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6573 MiB |   8375 MiB | 290276 GiB | 290269 GiB |
|       from large pool |   6562 MiB |   8363 MiB | 288349 GiB | 288342 GiB |
|       from small pool |     10 MiB |     35 MiB |   1927 GiB |   1927 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   19804 K  |   19803 K  |
|       from large pool |     308    |     315    |    9401 K  |    9401 K  |
|       from small pool |     298    |     356    |   10402 K  |   10402 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   19804 K  |   19803 K  |
|       from large pool |     308    |     315    |    9401 K  |    9401 K  |
|       from small pool |     298    |     356    |   10402 K  |   10402 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     170    |     222    |    4426    |    4256    |
|       from large pool |     156    |     160    |    2367    |    2211    |
|       from small pool |      14    |      62    |    2059    |    2045    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     165    |     166    |   11170 K  |   11169 K  |
|       from large pool |     137    |     138    |    6143 K  |    6143 K  |
|       from small pool |      28    |      60    |    5026 K  |    5026 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:05:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:05:33] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:05:40]    INFO >> epoch 007:     92 / 1539 loss=3.603, wps=4209.4, ups=6.24, wpb=674.7, bsz=674.7, num_updates=9300, lr=0.000295, gnorm=6.011, clip=0, train_wall=7, gb_free=74.6, wall=1648 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:05:47]    INFO >> epoch 007:    142 / 1539 loss=3.713, wps=4306.9, ups=6.49, wpb=663.1, bsz=663.1, num_updates=9350, lr=0.000295, gnorm=5.742, clip=0, train_wall=7, gb_free=73.4, wall=1656 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:05:55]    INFO >> epoch 007:    192 / 1539 loss=3.429, wps=5449.9, ups=6.16, wpb=885.1, bsz=885.1, num_updates=9400, lr=0.000295, gnorm=5.25, clip=2, train_wall=8, gb_free=67.7, wall=1664 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:06:04]    INFO >> epoch 007:    242 / 1539 loss=3.704, wps=4745.7, ups=5.97, wpb=794.3, bsz=794.3, num_updates=9450, lr=0.000295, gnorm=5.355, clip=0, train_wall=8, gb_free=70.7, wall=1673 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:06:12]    INFO >> epoch 007:    292 / 1539 loss=3.58, wps=4475.2, ups=6.4, wpb=699.6, bsz=699.6, num_updates=9500, lr=0.000295, gnorm=5.959, clip=0, train_wall=7, gb_free=71.4, wall=1680 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:06:17] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.29 GiB memory in use. Of the allocated memory 70.08 GiB is allocated by PyTorch, and 6.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:06:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:06:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:06:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 27           |        cudaMalloc retries: 52        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69006 MiB |  71763 MiB | 308769 GiB | 308702 GiB |
|       from large pool |  68989 MiB |  71746 MiB | 307018 GiB | 306951 GiB |
|       from small pool |     16 MiB |     18 MiB |   1751 GiB |   1751 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69006 MiB |  71763 MiB | 308769 GiB | 308702 GiB |
|       from large pool |  68989 MiB |  71746 MiB | 307018 GiB | 306951 GiB |
|       from small pool |     16 MiB |     18 MiB |   1751 GiB |   1751 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  68980 MiB |  71736 MiB | 308120 GiB | 308053 GiB |
|       from large pool |  68963 MiB |  71719 MiB | 306372 GiB | 306304 GiB |
|       from small pool |     16 MiB |     17 MiB |   1748 GiB |   1748 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78634 MiB |  78836 MiB | 712572 MiB | 633938 MiB |
|       from large pool |  78610 MiB |  78610 MiB | 708256 MiB | 629646 MiB |
|       from small pool |     24 MiB |    226 MiB |   4316 MiB |   4292 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6899 MiB |  10616 MiB | 298160 GiB | 298153 GiB |
|       from large pool |   6892 MiB |  10608 MiB | 296182 GiB | 296175 GiB |
|       from small pool |      7 MiB |     23 MiB |   1978 GiB |   1978 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     649    |   20344 K  |   20344 K  |
|       from large pool |     343    |     351    |    9674 K  |    9674 K  |
|       from small pool |     298    |     342    |   10670 K  |   10669 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     649    |   20344 K  |   20344 K  |
|       from large pool |     343    |     351    |    9674 K  |    9674 K  |
|       from small pool |     298    |     342    |   10670 K  |   10669 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     168    |     269    |    4525    |    4357    |
|       from large pool |     156    |     156    |    2367    |    2211    |
|       from small pool |      12    |     113    |    2158    |    2146    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     150    |     154    |   11473 K  |   11473 K  |
|       from large pool |     124    |     128    |    6324 K  |    6323 K  |
|       from small pool |      26    |      52    |    5149 K  |    5149 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:06:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:06:17] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:06:21]    INFO >> epoch 007:    343 / 1539 loss=3.67, wps=3681, ups=5.36, wpb=687.1, bsz=687.1, num_updates=9550, lr=0.000295, gnorm=5.189, clip=0, train_wall=8, gb_free=69.5, wall=1690 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:06:31]    INFO >> epoch 007:    393 / 1539 loss=3.76, wps=4089, ups=6.84, wpb=598.1, bsz=598.1, num_updates=9600, lr=0.000295, gnorm=5.011, clip=0, train_wall=7, gb_free=62, wall=1697 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:06:39]    INFO >> epoch 007:    443 / 1539 loss=3.616, wps=4329.8, ups=6.39, wpb=677.8, bsz=677.8, num_updates=9650, lr=0.000295, gnorm=4.998, clip=0, train_wall=7, gb_free=74.5, wall=1705 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:06:44] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 123.25 MiB is free. Including non-PyTorch memory, this process has 79.00 GiB memory in use. Of the allocated memory 75.66 GiB is allocated by PyTorch, and 2.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:06:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:06:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:06:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 28           |        cudaMalloc retries: 54        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76958 MiB |  77480 MiB | 313195 GiB | 313120 GiB |
|       from large pool |  76945 MiB |  77467 MiB | 311423 GiB | 311348 GiB |
|       from small pool |     12 MiB |     18 MiB |   1772 GiB |   1772 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76958 MiB |  77480 MiB | 313195 GiB | 313120 GiB |
|       from large pool |  76945 MiB |  77467 MiB | 311423 GiB | 311348 GiB |
|       from small pool |     12 MiB |     18 MiB |   1772 GiB |   1772 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76936 MiB |  77458 MiB | 312537 GiB | 312462 GiB |
|       from large pool |  76923 MiB |  77445 MiB | 310767 GiB | 310692 GiB |
|       from small pool |     12 MiB |     18 MiB |   1769 GiB |   1769 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80382 MiB |  80382 MiB | 719316 MiB | 638934 MiB |
|       from large pool |  80358 MiB |  80358 MiB | 714952 MiB | 634594 MiB |
|       from small pool |     24 MiB |     72 MiB |   4364 MiB |   4340 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3423 MiB |   7648 MiB | 302334 GiB | 302331 GiB |
|       from large pool |   3412 MiB |   7637 MiB | 300332 GiB | 300329 GiB |
|       from small pool |     11 MiB |     23 MiB |   2002 GiB |   2002 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   20614 K  |   20613 K  |
|       from large pool |     315    |     322    |    9816 K  |    9816 K  |
|       from small pool |     291    |     356    |   10797 K  |   10797 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   20614 K  |   20613 K  |
|       from large pool |     315    |     322    |    9816 K  |    9816 K  |
|       from small pool |     291    |     356    |   10797 K  |   10797 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     138    |     197    |    4557    |    4419    |
|       from large pool |     126    |     161    |    2375    |    2249    |
|       from small pool |      12    |      36    |    2182    |    2170    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     132    |     132    |   11621 K  |   11621 K  |
|       from large pool |     106    |     106    |    6417 K  |    6417 K  |
|       from small pool |      26    |      51    |    5203 K  |    5203 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:06:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:06:44] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:06:47]    INFO >> epoch 007:    494 / 1539 loss=3.623, wps=4285, ups=5.77, wpb=742.8, bsz=742.8, num_updates=9700, lr=0.000295, gnorm=5.774, clip=0, train_wall=8, gb_free=72.2, wall=1714 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:06:55]    INFO >> epoch 007:    544 / 1539 loss=3.798, wps=4357.8, ups=6.9, wpb=631.2, bsz=631.2, num_updates=9750, lr=0.000295, gnorm=4.31, clip=0, train_wall=7, gb_free=74.3, wall=1721 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:07:02]    INFO >> epoch 007:    594 / 1539 loss=3.782, wps=4549.2, ups=6.65, wpb=684.3, bsz=684.3, num_updates=9800, lr=0.000295, gnorm=4.912, clip=0, train_wall=7, gb_free=75.1, wall=1728 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:07:10]    INFO >> epoch 007:    644 / 1539 loss=3.458, wps=4454.7, ups=6.38, wpb=697.8, bsz=697.8, num_updates=9850, lr=0.000295, gnorm=7.247, clip=2, train_wall=7, gb_free=70.6, wall=1736 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:07:18]    INFO >> epoch 007:    694 / 1539 loss=3.473, wps=5167.8, ups=6.18, wpb=835.9, bsz=835.9, num_updates=9900, lr=0.000295, gnorm=6.491, clip=0, train_wall=8, gb_free=73.9, wall=1744 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:07:26]    INFO >> epoch 007:    744 / 1539 loss=3.714, wps=4596.9, ups=6.46, wpb=712, bsz=712, num_updates=9950, lr=0.000295, gnorm=5.479, clip=0, train_wall=7, gb_free=72.8, wall=1752 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:07:36]    INFO >> epoch 007:    794 / 1539 loss=3.666, wps=4315.2, ups=6.59, wpb=654.6, bsz=654.6, num_updates=10000, lr=0.000295, gnorm=4.94, clip=0, train_wall=7, gb_free=72.1, wall=1760 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:07:44]    INFO >> epoch 007:    844 / 1539 loss=3.667, wps=4620.4, ups=6.39, wpb=722.7, bsz=722.7, num_updates=10050, lr=0.000295, gnorm=4.986, clip=0, train_wall=7, gb_free=70.6, wall=1767 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:07:52]    INFO >> epoch 007:    894 / 1539 loss=3.661, wps=4436.2, ups=6.45, wpb=688.1, bsz=688.1, num_updates=10100, lr=0.000295, gnorm=5.349, clip=0, train_wall=7, gb_free=73.8, wall=1775 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:07:59]    INFO >> epoch 007:    944 / 1539 loss=3.676, wps=4578.9, ups=6.55, wpb=699, bsz=699, num_updates=10150, lr=0.000295, gnorm=5.245, clip=0, train_wall=7, gb_free=71.9, wall=1783 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:08:09]    INFO >> epoch 007:    994 / 1539 loss=3.668, wps=4695.5, ups=6.38, wpb=736.1, bsz=736.1, num_updates=10200, lr=0.000295, gnorm=5.531, clip=0, train_wall=7, gb_free=70.3, wall=1791 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:08:16]    INFO >> epoch 007:   1044 / 1539 loss=3.666, wps=4673.3, ups=6.62, wpb=705.7, bsz=705.7, num_updates=10250, lr=0.000295, gnorm=4.949, clip=0, train_wall=7, gb_free=73, wall=1798 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:08:24]    INFO >> epoch 007:   1094 / 1539 loss=3.626, wps=4416.3, ups=6.37, wpb=693.4, bsz=693.4, num_updates=10300, lr=0.000295, gnorm=5.006, clip=0, train_wall=7, gb_free=71.6, wall=1806 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:08:32]    INFO >> epoch 007:   1144 / 1539 loss=3.584, wps=4642.3, ups=5.91, wpb=784.9, bsz=784.9, num_updates=10350, lr=0.000295, gnorm=5.196, clip=0, train_wall=8, gb_free=74, wall=1815 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:08:42]    INFO >> epoch 007:   1194 / 1539 loss=3.541, wps=4937.5, ups=6.08, wpb=811.5, bsz=811.5, num_updates=10400, lr=0.000295, gnorm=6.255, clip=2, train_wall=8, gb_free=72.8, wall=1823 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:08:49]    INFO >> epoch 007:   1244 / 1539 loss=3.656, wps=4623.3, ups=6.74, wpb=686.2, bsz=686.2, num_updates=10450, lr=0.000295, gnorm=5.285, clip=0, train_wall=7, gb_free=68.3, wall=1830 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:08:57]    INFO >> epoch 007:   1294 / 1539 loss=3.623, wps=4258.7, ups=6.65, wpb=640.4, bsz=640.4, num_updates=10500, lr=0.000295, gnorm=5.491, clip=0, train_wall=7, gb_free=70.7, wall=1838 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:09:04]    INFO >> epoch 007:   1344 / 1539 loss=3.72, wps=4695.9, ups=6.73, wpb=697.4, bsz=697.4, num_updates=10550, lr=0.000295, gnorm=5.714, clip=0, train_wall=7, gb_free=70.4, wall=1845 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:09:13]    INFO >> epoch 007:   1394 / 1539 loss=3.529, wps=4472.7, ups=6.4, wpb=699.1, bsz=699.1, num_updates=10600, lr=0.000295, gnorm=5.621, clip=0, train_wall=7, gb_free=67.1, wall=1853 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:09:19] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 51.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 76.76 GiB is allocated by PyTorch, and 1.81 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:09:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:09:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:09:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 29           |        cudaMalloc retries: 56        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78538 MiB |  78598 MiB | 341065 GiB | 340988 GiB |
|       from large pool |  78151 MiB |  78211 MiB | 339139 GiB | 339063 GiB |
|       from small pool |    386 MiB |    388 MiB |   1925 GiB |   1925 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78538 MiB |  78598 MiB | 341065 GiB | 340988 GiB |
|       from large pool |  78151 MiB |  78211 MiB | 339139 GiB | 339063 GiB |
|       from small pool |    386 MiB |    388 MiB |   1925 GiB |   1925 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78257 MiB |  78317 MiB | 340344 GiB | 340268 GiB |
|       from large pool |  77872 MiB |  77932 MiB | 338421 GiB | 338345 GiB |
|       from small pool |    384 MiB |    386 MiB |   1922 GiB |   1922 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80454 MiB |  80456 MiB | 750380 MiB | 669926 MiB |
|       from large pool |  80028 MiB |  80028 MiB | 745612 MiB | 665584 MiB |
|       from small pool |    426 MiB |    428 MiB |   4768 MiB |   4342 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1855 MiB |   6794 MiB | 329930 GiB | 329928 GiB |
|       from large pool |   1816 MiB |   6788 MiB | 327751 GiB | 327749 GiB |
|       from small pool |     39 MiB |     40 MiB |   2178 GiB |   2178 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7142    |    7145    |   22485 K  |   22478 K  |
|       from large pool |     919    |     920    |   10761 K  |   10760 K  |
|       from small pool |    6223    |    6226    |   11723 K  |   11717 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7142    |    7145    |   22485 K  |   22478 K  |
|       from large pool |     919    |     920    |   10761 K  |   10760 K  |
|       from small pool |    6223    |    6226    |   11723 K  |   11717 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     838    |     839    |    5270    |    4432    |
|       from large pool |     625    |     625    |    2886    |    2261    |
|       from small pool |     213    |     214    |    2384    |    2171    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     517    |     518    |   12658 K  |   12658 K  |
|       from large pool |     138    |     138    |    7033 K  |    7032 K  |
|       from small pool |     379    |     380    |    5625 K  |    5625 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:09:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:09:19] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:09:22]    INFO >> epoch 007:   1445 / 1539 loss=3.662, wps=4100.6, ups=5.73, wpb=716, bsz=716, num_updates=10650, lr=0.000295, gnorm=5.785, clip=0, train_wall=8, gb_free=68.1, wall=1862 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:09:30]    INFO >> epoch 007:   1495 / 1539 loss=3.601, wps=4837.2, ups=6.78, wpb=713.5, bsz=713.5, num_updates=10700, lr=0.000295, gnorm=5.736, clip=0, train_wall=7, gb_free=70.4, wall=1869 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:09:37]    INFO >> epoch 007 | loss 3.63 | wps 4236.9 | ups 5.95 | wpb 712.7 | bsz 712.7 | num_updates 10744 | lr 0.000295 | gnorm 5.464 | clip 0.2 | train_wall 226 | gb_free 70.2 | wall 1876 (progress_bar.py:267, print())[0m
[33m[2025-11-21 16:09:37] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:09:52]    INFO >> epoch 007 | valid on 'valid' subset | loss 3.809 | wps 10346.7 | wpb 5412.5 | bsz 5412.5 | num_updates 10744 | best_loss 4.689 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 16:09:53]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 16:09:53]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/checkpoints/checkpoint_last.pt (epoch 7 @ 10744 updates, score 3.809) (writing took 0.013661 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 16:09:53] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:09:54]    INFO >> epoch 008:      6 / 1539 loss=3.511, wps=1592.2, ups=2.18, wpb=730.3, bsz=730.3, num_updates=10750, lr=0.000262, gnorm=4.619, clip=0, train_wall=8, gb_free=68.7, wall=1892 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:10:02]    INFO >> epoch 008:     56 / 1539 loss=3.462, wps=4646.6, ups=6.32, wpb=735.8, bsz=735.8, num_updates=10800, lr=0.000262, gnorm=5.271, clip=0, train_wall=7, gb_free=73.9, wall=1900 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:10:10]    INFO >> epoch 008:    106 / 1539 loss=3.562, wps=4605.4, ups=6.01, wpb=766.8, bsz=766.8, num_updates=10850, lr=0.000262, gnorm=5.841, clip=2, train_wall=8, gb_free=73.2, wall=1908 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:10:18]    INFO >> epoch 008:    156 / 1539 loss=3.692, wps=4263.8, ups=6.54, wpb=652.3, bsz=652.3, num_updates=10900, lr=0.000262, gnorm=4.39, clip=0, train_wall=7, gb_free=75.6, wall=1916 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:10:27]    INFO >> epoch 008:    206 / 1539 loss=3.615, wps=5067.8, ups=6.07, wpb=834.7, bsz=834.7, num_updates=10950, lr=0.000262, gnorm=5.049, clip=0, train_wall=8, gb_free=73.5, wall=1924 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:10:35]    INFO >> epoch 008:    256 / 1539 loss=3.551, wps=4723.2, ups=6.61, wpb=714.1, bsz=714.1, num_updates=11000, lr=0.000262, gnorm=5.845, clip=0, train_wall=7, gb_free=71.5, wall=1932 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:10:43]    INFO >> epoch 008:    306 / 1539 loss=3.79, wps=4481.4, ups=6.22, wpb=720.8, bsz=720.8, num_updates=11050, lr=0.000262, gnorm=5.839, clip=0, train_wall=8, gb_free=71.8, wall=1940 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:10:51]    INFO >> epoch 008:    356 / 1539 loss=3.615, wps=4001.8, ups=6.01, wpb=665.6, bsz=665.6, num_updates=11100, lr=0.000262, gnorm=5.369, clip=0, train_wall=8, gb_free=74.7, wall=1948 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:11:00]    INFO >> epoch 008:    406 / 1539 loss=3.697, wps=4585.5, ups=6.49, wpb=707, bsz=707, num_updates=11150, lr=0.000262, gnorm=5.306, clip=0, train_wall=7, gb_free=72.1, wall=1956 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:11:08]    INFO >> epoch 008:    456 / 1539 loss=3.679, wps=5369.8, ups=6.13, wpb=875.6, bsz=875.6, num_updates=11200, lr=0.000262, gnorm=5.713, clip=0, train_wall=8, gb_free=72.6, wall=1964 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:11:16]    INFO >> epoch 008:    506 / 1539 loss=3.592, wps=4794.9, ups=6.4, wpb=749.6, bsz=749.6, num_updates=11250, lr=0.000262, gnorm=6.37, clip=0, train_wall=7, gb_free=74.3, wall=1972 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:11:24]    INFO >> epoch 008:    556 / 1539 loss=3.663, wps=4205.4, ups=6.46, wpb=651.2, bsz=651.2, num_updates=11300, lr=0.000262, gnorm=4.901, clip=0, train_wall=7, gb_free=71.8, wall=1979 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:11:33]    INFO >> epoch 008:    606 / 1539 loss=3.743, wps=4327.6, ups=6.34, wpb=682.8, bsz=682.8, num_updates=11350, lr=0.000262, gnorm=5.191, clip=0, train_wall=7, gb_free=75.2, wall=1987 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:11:41]    INFO >> epoch 008:    656 / 1539 loss=3.68, wps=4364, ups=6.86, wpb=636.2, bsz=636.2, num_updates=11400, lr=0.000262, gnorm=4.692, clip=0, train_wall=7, gb_free=72.5, wall=1995 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:11:48]    INFO >> epoch 008:    706 / 1539 loss=3.656, wps=4584.3, ups=6.63, wpb=691.9, bsz=691.9, num_updates=11450, lr=0.000262, gnorm=5.253, clip=0, train_wall=7, gb_free=70, wall=2002 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:11:56]    INFO >> epoch 008:    756 / 1539 loss=3.588, wps=4629, ups=6.37, wpb=727, bsz=727, num_updates=11500, lr=0.000262, gnorm=5.354, clip=0, train_wall=7, gb_free=70.9, wall=2010 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:12:05]    INFO >> epoch 008:    806 / 1539 loss=3.519, wps=4476.8, ups=6.45, wpb=694.2, bsz=694.2, num_updates=11550, lr=0.000262, gnorm=5.808, clip=0, train_wall=7, gb_free=68.9, wall=2018 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:12:12]    INFO >> epoch 008:    856 / 1539 loss=3.708, wps=4093, ups=6.87, wpb=595.7, bsz=595.7, num_updates=11600, lr=0.000262, gnorm=4.608, clip=0, train_wall=7, gb_free=71.5, wall=2025 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:12:21]    INFO >> epoch 008:    906 / 1539 loss=3.313, wps=4953.7, ups=6.09, wpb=813.3, bsz=813.3, num_updates=11650, lr=0.000262, gnorm=5.475, clip=0, train_wall=8, gb_free=74, wall=2033 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:12:28]    INFO >> epoch 008:    956 / 1539 loss=3.669, wps=4551.5, ups=6.75, wpb=674.1, bsz=674.1, num_updates=11700, lr=0.000262, gnorm=5.18, clip=0, train_wall=7, gb_free=72.9, wall=2041 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:12:35] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 3.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 2.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:12:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:12:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:12:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 30           |        cudaMalloc retries: 59        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78273 MiB |  78333 MiB | 377371 GiB | 377294 GiB |
|       from large pool |  78192 MiB |  78252 MiB | 375231 GiB | 375154 GiB |
|       from small pool |     80 MiB |     82 MiB |   2139 GiB |   2139 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78273 MiB |  78333 MiB | 377371 GiB | 377294 GiB |
|       from large pool |  78192 MiB |  78252 MiB | 375231 GiB | 375154 GiB |
|       from small pool |     80 MiB |     82 MiB |   2139 GiB |   2139 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78222 MiB |  78281 MiB | 376569 GiB | 376492 GiB |
|       from large pool |  78141 MiB |  78201 MiB | 374432 GiB | 374356 GiB |
|       from small pool |     80 MiB |     81 MiB |   2136 GiB |   2136 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80502 MiB |  80504 MiB |    830 GiB | 770022 MiB |
|       from large pool |  80418 MiB |  80418 MiB |    825 GiB | 765058 MiB |
|       from small pool |     84 MiB |    246 MiB |      4 GiB |   4964 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2168 MiB |   7147 MiB | 358201 GiB | 358199 GiB |
|       from large pool |   2165 MiB |   7138 MiB | 355780 GiB | 355778 GiB |
|       from small pool |      3 MiB |     29 MiB |   2420 GiB |   2420 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1796    |    1799    |   24927 K  |   24926 K  |
|       from large pool |     455    |     456    |   11890 K  |   11890 K  |
|       from small pool |    1341    |    1344    |   13037 K  |   13035 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1796    |    1799    |   24927 K  |   24926 K  |
|       from large pool |     455    |     456    |   11890 K  |   11890 K  |
|       from small pool |    1341    |    1344    |   13037 K  |   13035 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     242    |     559    |    5862    |    5620    |
|       from large pool |     200    |     436    |    3338    |    3138    |
|       from small pool |      42    |     123    |    2524    |    2482    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     161    |     163    |   14093 K  |   14093 K  |
|       from large pool |     116    |     116    |    7817 K  |    7816 K  |
|       from small pool |      45    |      54    |    6276 K  |    6276 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:12:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:12:35] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:12:38]    INFO >> epoch 008:   1007 / 1539 loss=3.645, wps=3558.5, ups=5.9, wpb=602.8, bsz=602.8, num_updates=11750, lr=0.000262, gnorm=4.354, clip=0, train_wall=7, gb_free=70.6, wall=2049 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:12:49]    INFO >> epoch 008:   1057 / 1539 loss=3.511, wps=3838.3, ups=4.63, wpb=828.4, bsz=828.4, num_updates=11800, lr=0.000262, gnorm=5.529, clip=2, train_wall=10, gb_free=71.7, wall=2060 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:12:56]    INFO >> epoch 008:   1107 / 1539 loss=3.61, wps=4466.5, ups=6.76, wpb=661.2, bsz=661.2, num_updates=11850, lr=0.000262, gnorm=5.045, clip=0, train_wall=7, gb_free=72.8, wall=2067 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:13:03]    INFO >> epoch 008:   1157 / 1539 loss=3.814, wps=3810.6, ups=6.84, wpb=557.5, bsz=557.5, num_updates=11900, lr=0.000262, gnorm=4.489, clip=0, train_wall=7, gb_free=73.2, wall=2075 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:13:06] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 493.25 MiB is free. Including non-PyTorch memory, this process has 78.63 GiB memory in use. Of the allocated memory 75.66 GiB is allocated by PyTorch, and 2.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:13:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:13:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:13:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 31           |        cudaMalloc retries: 61        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76956 MiB |  77478 MiB | 382821 GiB | 382746 GiB |
|       from large pool |  76943 MiB |  77465 MiB | 380654 GiB | 380579 GiB |
|       from small pool |     12 MiB |     19 MiB |   2167 GiB |   2167 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76956 MiB |  77478 MiB | 382821 GiB | 382746 GiB |
|       from large pool |  76943 MiB |  77465 MiB | 380654 GiB | 380579 GiB |
|       from small pool |     12 MiB |     19 MiB |   2167 GiB |   2167 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76936 MiB |  77458 MiB | 382008 GiB | 381933 GiB |
|       from large pool |  76923 MiB |  77445 MiB | 379844 GiB | 379769 GiB |
|       from small pool |     12 MiB |     19 MiB |   2164 GiB |   2164 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80012 MiB |  80054 MiB |    903 GiB |    825 GiB |
|       from large pool |  79988 MiB |  79988 MiB |    898 GiB |    820 GiB |
|       from small pool |     24 MiB |     66 MiB |      4 GiB |      4 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3055 MiB |   8643 MiB | 364118 GiB | 364115 GiB |
|       from large pool |   3044 MiB |   8631 MiB | 361666 GiB | 361663 GiB |
|       from small pool |     11 MiB |     19 MiB |   2452 GiB |   2452 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   25272 K  |   25272 K  |
|       from large pool |     315    |     322    |   12070 K  |   12070 K  |
|       from small pool |     291    |     348    |   13202 K  |   13201 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   25272 K  |   25272 K  |
|       from large pool |     315    |     322    |   12070 K  |   12070 K  |
|       from small pool |     291    |     348    |   13202 K  |   13201 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      91    |     112    |    5924    |    5833    |
|       from large pool |      79    |      79    |    3381    |    3302    |
|       from small pool |      12    |      33    |    2543    |    2531    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      98    |      98    |   14279 K  |   14279 K  |
|       from large pool |      69    |      69    |    7931 K  |    7930 K  |
|       from small pool |      29    |      50    |    6348 K  |    6348 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:13:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:13:06] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:13:13]    INFO >> epoch 008:   1208 / 1539 loss=3.631, wps=4079, ups=5.71, wpb=715, bsz=715, num_updates=11950, lr=0.000262, gnorm=4.814, clip=0, train_wall=8, gb_free=76, wall=2083 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:13:19] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 17.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 77.61 GiB is allocated by PyTorch, and 1012.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:13:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:13:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:13:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 32           |        cudaMalloc retries: 62        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79415 MiB |  79475 MiB | 385104 GiB | 385026 GiB |
|       from large pool |  79016 MiB |  79076 MiB | 382922 GiB | 382845 GiB |
|       from small pool |    398 MiB |    399 MiB |   2181 GiB |   2181 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79415 MiB |  79475 MiB | 385104 GiB | 385026 GiB |
|       from large pool |  79016 MiB |  79076 MiB | 382922 GiB | 382845 GiB |
|       from small pool |    398 MiB |    399 MiB |   2181 GiB |   2181 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79398 MiB |  79458 MiB | 384287 GiB | 384209 GiB |
|       from large pool |  79002 MiB |  79061 MiB | 382108 GiB | 382031 GiB |
|       from small pool |    396 MiB |    397 MiB |   2178 GiB |   2178 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80488 MiB |  80488 MiB |    903 GiB |    825 GiB |
|       from large pool |  80048 MiB |  80048 MiB |    898 GiB |    820 GiB |
|       from small pool |    440 MiB |    440 MiB |      5 GiB |      4 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1012 MiB |   5703 MiB | 366750 GiB | 366749 GiB |
|       from large pool |    971 MiB |   5696 MiB | 364281 GiB | 364280 GiB |
|       from small pool |     41 MiB |     42 MiB |   2468 GiB |   2468 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7351    |    7354    |   25429 K  |   25421 K  |
|       from large pool |     938    |     939    |   12139 K  |   12138 K  |
|       from small pool |    6413    |    6416    |   13290 K  |   13283 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7351    |    7354    |   25429 K  |   25421 K  |
|       from large pool |     938    |     939    |   12139 K  |   12138 K  |
|       from small pool |    6413    |    6416    |   13290 K  |   13283 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     300    |     300    |    6133    |    5833    |
|       from large pool |      80    |      80    |    3382    |    3302    |
|       from small pool |     220    |     220    |    2751    |    2531    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     468    |     468    |   14366 K  |   14366 K  |
|       from large pool |      76    |      76    |    7973 K  |    7973 K  |
|       from small pool |     392    |     392    |    6393 K  |    6392 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:13:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:13:19] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:13:22]    INFO >> epoch 008:   1259 / 1539 loss=3.572, wps=4780.5, ups=5.62, wpb=850.7, bsz=850.7, num_updates=12000, lr=0.000262, gnorm=6.316, clip=2, train_wall=8, gb_free=73.2, wall=2092 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:13:30]    INFO >> epoch 008:   1309 / 1539 loss=3.655, wps=4415.1, ups=6.27, wpb=704.3, bsz=704.3, num_updates=12050, lr=0.000262, gnorm=5.446, clip=0, train_wall=7, gb_free=72.7, wall=2100 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:13:38]    INFO >> epoch 008:   1359 / 1539 loss=3.657, wps=4327.8, ups=6.51, wpb=664.7, bsz=664.7, num_updates=12100, lr=0.000262, gnorm=4.526, clip=0, train_wall=7, gb_free=69.7, wall=2108 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:13:40] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 803.25 MiB is free. Including non-PyTorch memory, this process has 78.33 GiB memory in use. Of the allocated memory 74.93 GiB is allocated by PyTorch, and 2.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:13:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:13:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:13:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 33           |        cudaMalloc retries: 64        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76325 MiB |  78600 MiB | 388762 GiB | 388687 GiB |
|       from large pool |  76306 MiB |  78582 MiB | 386562 GiB | 386487 GiB |
|       from small pool |     18 MiB |     19 MiB |   2200 GiB |   2200 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76325 MiB |  78600 MiB | 388762 GiB | 388687 GiB |
|       from large pool |  76306 MiB |  78582 MiB | 386562 GiB | 386487 GiB |
|       from small pool |     18 MiB |     19 MiB |   2200 GiB |   2200 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB | 387938 GiB | 387863 GiB |
|       from large pool |  76285 MiB |  78562 MiB | 385741 GiB | 385666 GiB |
|       from small pool |     18 MiB |     19 MiB |   2197 GiB |   2197 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79702 MiB |  80428 MiB |    974 GiB |    897 GiB |
|       from large pool |  79678 MiB |  79988 MiB |    969 GiB |    891 GiB |
|       from small pool |     24 MiB |    440 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3376 MiB |   4508 MiB | 370853 GiB | 370849 GiB |
|       from large pool |   3371 MiB |   4502 MiB | 368363 GiB | 368359 GiB |
|       from small pool |      5 MiB |     27 MiB |   2490 GiB |   2490 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     637    |     643    |   25655 K  |   25655 K  |
|       from large pool |     340    |     346    |   12256 K  |   12256 K  |
|       from small pool |     297    |     348    |   13399 K  |   13398 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     637    |     643    |   25655 K  |   25655 K  |
|       from large pool |     340    |     346    |   12256 K  |   12256 K  |
|       from small pool |     297    |     348    |   13399 K  |   13398 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     108    |     299    |    6192    |    6084    |
|       from large pool |      96    |      96    |    3441    |    3345    |
|       from small pool |      12    |     220    |    2751    |    2739    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     116    |     116    |   14490 K  |   14489 K  |
|       from large pool |      91    |      91    |    8047 K  |    8047 K  |
|       from small pool |      25    |      54    |    6442 K  |    6442 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:13:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:13:40] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:13:49]    INFO >> epoch 008:   1410 / 1539 loss=3.643, wps=4028.7, ups=4.89, wpb=824.4, bsz=824.4, num_updates=12150, lr=0.000262, gnorm=5.385, clip=0, train_wall=8, gb_free=73, wall=2118 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:13:58]    INFO >> epoch 008:   1460 / 1539 loss=3.668, wps=4139, ups=5.79, wpb=715.2, bsz=715.2, num_updates=12200, lr=0.000262, gnorm=5.135, clip=0, train_wall=8, gb_free=75, wall=2127 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:14:06]    INFO >> epoch 008:   1510 / 1539 loss=3.586, wps=4349, ups=6.53, wpb=666.4, bsz=666.4, num_updates=12250, lr=0.000262, gnorm=5.497, clip=0, train_wall=7, gb_free=67.3, wall=2134 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:14:10]    INFO >> epoch 008 | loss 3.62 | wps 4160.6 | ups 5.84 | wpb 712.7 | bsz 712.7 | num_updates 12279 | lr 0.000262 | gnorm 5.261 | clip 0.2 | train_wall 229 | gb_free 74.8 | wall 2139 (progress_bar.py:267, print())[0m
[33m[2025-11-21 16:14:10] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:14:26]    INFO >> epoch 008 | valid on 'valid' subset | loss 3.819 | wps 10712.4 | wpb 5412.5 | bsz 5412.5 | num_updates 12279 | best_loss 4.689 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 16:14:26]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 16:14:26]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/checkpoints/checkpoint_last.pt (epoch 8 @ 12279 updates, score 3.819) (writing took 0.013759 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 16:14:26] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:14:29]    INFO >> epoch 009:     21 / 1539 loss=3.624, wps=1514.3, ups=2.26, wpb=668.9, bsz=668.9, num_updates=12300, lr=0.000227, gnorm=5.159, clip=0, train_wall=7, gb_free=73.1, wall=2157 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:14:37]    INFO >> epoch 009:     71 / 1539 loss=3.671, wps=4209.4, ups=6.34, wpb=663.6, bsz=663.6, num_updates=12350, lr=0.000227, gnorm=4.562, clip=0, train_wall=7, gb_free=73.5, wall=2164 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:14:45]    INFO >> epoch 009:    121 / 1539 loss=3.533, wps=4405.1, ups=6.4, wpb=688.4, bsz=688.4, num_updates=12400, lr=0.000227, gnorm=5.582, clip=0, train_wall=7, gb_free=73.7, wall=2172 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:14:55]    INFO >> epoch 009:    171 / 1539 loss=3.44, wps=5054.2, ups=6, wpb=842.4, bsz=842.4, num_updates=12450, lr=0.000227, gnorm=5.481, clip=0, train_wall=8, gb_free=71.8, wall=2181 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:15:02]    INFO >> epoch 009:    221 / 1539 loss=3.64, wps=4604.1, ups=6.66, wpb=691.5, bsz=691.5, num_updates=12500, lr=0.000227, gnorm=5.024, clip=0, train_wall=7, gb_free=71.5, wall=2188 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:15:09]    INFO >> epoch 009:    271 / 1539 loss=3.566, wps=4828.2, ups=6.86, wpb=703.6, bsz=703.6, num_updates=12550, lr=0.000227, gnorm=5.857, clip=0, train_wall=7, gb_free=70, wall=2195 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:15:17]    INFO >> epoch 009:    321 / 1539 loss=3.65, wps=4847.2, ups=6.18, wpb=784.5, bsz=784.5, num_updates=12600, lr=0.000227, gnorm=4.829, clip=0, train_wall=8, gb_free=66.3, wall=2203 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:15:28]    INFO >> epoch 009:    371 / 1539 loss=3.538, wps=3996.3, ups=5.7, wpb=701, bsz=701, num_updates=12650, lr=0.000227, gnorm=5.128, clip=0, train_wall=8, gb_free=65.8, wall=2212 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:15:28] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 165.25 MiB is free. Including non-PyTorch memory, this process has 78.96 GiB memory in use. Of the allocated memory 73.81 GiB is allocated by PyTorch, and 4.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:15:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:15:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:15:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 34           |        cudaMalloc retries: 66        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75055 MiB |  75998 MiB | 409586 GiB | 409512 GiB |
|       from large pool |  75042 MiB |  75985 MiB | 407263 GiB | 407189 GiB |
|       from small pool |     12 MiB |     16 MiB |   2323 GiB |   2323 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75055 MiB |  75998 MiB | 409586 GiB | 409512 GiB |
|       from large pool |  75042 MiB |  75985 MiB | 407263 GiB | 407189 GiB |
|       from small pool |     12 MiB |     16 MiB |   2323 GiB |   2323 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75035 MiB |  75977 MiB | 408722 GiB | 408649 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 406402 GiB | 406329 GiB |
|       from small pool |     12 MiB |     16 MiB |   2319 GiB |   2319 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80340 MiB |  80340 MiB |    979 GiB |    901 GiB |
|       from large pool |  80316 MiB |  80316 MiB |    974 GiB |    895 GiB |
|       from small pool |     24 MiB |    124 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5284 MiB |   9096 MiB | 391527 GiB | 391522 GiB |
|       from large pool |   5273 MiB |   9085 MiB | 388901 GiB | 388896 GiB |
|       from small pool |     11 MiB |     21 MiB |   2625 GiB |   2625 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   26995 K  |   26995 K  |
|       from large pool |     314    |     322    |   12839 K  |   12839 K  |
|       from small pool |     291    |     341    |   14155 K  |   14155 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   26995 K  |   26995 K  |
|       from large pool |     314    |     322    |   12839 K  |   12839 K  |
|       from small pool |     291    |     341    |   14155 K  |   14155 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     103    |     158    |    6249    |    6146    |
|       from large pool |      91    |      96    |    3448    |    3357    |
|       from small pool |      12    |      62    |    2801    |    2789    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     100    |     100    |   15254 K  |   15254 K  |
|       from large pool |      73    |      73    |    8420 K  |    8420 K  |
|       from small pool |      27    |      48    |    6833 K  |    6833 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:15:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:15:28] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:15:36]    INFO >> epoch 009:    422 / 1539 loss=3.638, wps=4214.5, ups=5.88, wpb=716.4, bsz=716.4, num_updates=12700, lr=0.000227, gnorm=5.003, clip=0, train_wall=7, gb_free=73.1, wall=2221 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:15:44]    INFO >> epoch 009:    472 / 1539 loss=3.518, wps=4632.2, ups=6.7, wpb=691.3, bsz=691.3, num_updates=12750, lr=0.000227, gnorm=5.038, clip=2, train_wall=7, gb_free=74, wall=2228 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:15:51]    INFO >> epoch 009:    522 / 1539 loss=3.602, wps=4948.4, ups=6.5, wpb=760.8, bsz=760.8, num_updates=12800, lr=0.000227, gnorm=4.56, clip=0, train_wall=7, gb_free=72.7, wall=2236 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:16:01]    INFO >> epoch 009:    572 / 1539 loss=3.617, wps=4582.2, ups=6.1, wpb=750.9, bsz=750.9, num_updates=12850, lr=0.000227, gnorm=4.966, clip=0, train_wall=8, gb_free=74.2, wall=2244 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:16:02] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 35.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 77.30 GiB is allocated by PyTorch, and 1.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:16:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:16:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:16:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 35           |        cudaMalloc retries: 67        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79090 MiB |  79150 MiB | 415679 GiB | 415602 GiB |
|       from large pool |  79001 MiB |  79061 MiB | 413324 GiB | 413247 GiB |
|       from small pool |     89 MiB |     90 MiB |   2355 GiB |   2354 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79090 MiB |  79150 MiB | 415679 GiB | 415602 GiB |
|       from large pool |  79001 MiB |  79061 MiB | 413324 GiB | 413247 GiB |
|       from small pool |     89 MiB |     90 MiB |   2355 GiB |   2354 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79063 MiB |  79123 MiB | 414803 GiB | 414726 GiB |
|       from large pool |  78974 MiB |  79034 MiB | 412452 GiB | 412374 GiB |
|       from small pool |     89 MiB |     90 MiB |   2351 GiB |   2351 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80470 MiB |  80470 MiB |    979 GiB |    901 GiB |
|       from large pool |  80376 MiB |  80376 MiB |    974 GiB |    895 GiB |
|       from small pool |     94 MiB |     94 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1319 MiB |   7421 MiB | 398239 GiB | 398238 GiB |
|       from large pool |   1314 MiB |   7410 MiB | 395577 GiB | 395576 GiB |
|       from small pool |      4 MiB |     25 MiB |   2662 GiB |   2662 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1950    |    1953    |   27385 K  |   27383 K  |
|       from large pool |     469    |     470    |   13039 K  |   13038 K  |
|       from small pool |    1481    |    1484    |   14346 K  |   14345 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1950    |    1953    |   27385 K  |   27383 K  |
|       from large pool |     469    |     470    |   13039 K  |   13038 K  |
|       from small pool |    1481    |    1484    |   14346 K  |   14345 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     139    |     139    |    6285    |    6146    |
|       from large pool |      92    |      92    |    3449    |    3357    |
|       from small pool |      47    |      47    |    2836    |    2789    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     128    |     128    |   15464 K  |   15464 K  |
|       from large pool |      76    |      78    |    8546 K  |    8546 K  |
|       from small pool |      52    |      61    |    6918 K  |    6918 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:16:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:16:02] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:16:09]    INFO >> epoch 009:    623 / 1539 loss=3.571, wps=4148.7, ups=5.85, wpb=709.4, bsz=709.4, num_updates=12900, lr=0.000227, gnorm=5.266, clip=0, train_wall=7, gb_free=65.9, wall=2253 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:16:17]    INFO >> epoch 009:    673 / 1539 loss=3.621, wps=4335.9, ups=6.58, wpb=658.7, bsz=658.7, num_updates=12950, lr=0.000227, gnorm=4.747, clip=0, train_wall=7, gb_free=71.5, wall=2260 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:16:24]    INFO >> epoch 009:    723 / 1539 loss=3.596, wps=4533.1, ups=6.75, wpb=671.1, bsz=671.1, num_updates=13000, lr=0.000227, gnorm=5.315, clip=0, train_wall=7, gb_free=71.2, wall=2268 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:16:34]    INFO >> epoch 009:    773 / 1539 loss=3.525, wps=4325.6, ups=6.23, wpb=693.9, bsz=693.9, num_updates=13050, lr=0.000227, gnorm=5.69, clip=0, train_wall=8, gb_free=57.9, wall=2276 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:16:42] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 165.25 MiB is free. Including non-PyTorch memory, this process has 78.96 GiB memory in use. Of the allocated memory 77.59 GiB is allocated by PyTorch, and 888.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:16:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:16:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:16:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 36           |        cudaMalloc retries: 68        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79053 MiB |  79451 MiB | 422841 GiB | 422764 GiB |
|       from large pool |  79034 MiB |  79432 MiB | 420450 GiB | 420372 GiB |
|       from small pool |     18 MiB |     19 MiB |   2391 GiB |   2391 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79053 MiB |  79451 MiB | 422841 GiB | 422764 GiB |
|       from large pool |  79034 MiB |  79432 MiB | 420450 GiB | 420372 GiB |
|       from small pool |     18 MiB |     19 MiB |   2391 GiB |   2391 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79031 MiB |  79429 MiB | 421950 GiB | 421873 GiB |
|       from large pool |  79012 MiB |  79410 MiB | 419562 GiB | 419485 GiB |
|       from small pool |     18 MiB |     19 MiB |   2388 GiB |   2388 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80340 MiB |  80410 MiB |    979 GiB |    901 GiB |
|       from large pool |  80316 MiB |  80316 MiB |    974 GiB |    895 GiB |
|       from small pool |     24 MiB |     94 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1286 MiB |   5521 MiB | 406124 GiB | 406123 GiB |
|       from large pool |   1281 MiB |   5515 MiB | 403419 GiB | 403418 GiB |
|       from small pool |      5 MiB |     21 MiB |   2704 GiB |   2704 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     647    |   27845 K  |   27844 K  |
|       from large pool |     341    |     347    |   13278 K  |   13278 K  |
|       from small pool |     300    |     348    |   14567 K  |   14566 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     647    |   27845 K  |   27844 K  |
|       from large pool |     341    |     347    |   13278 K  |   13278 K  |
|       from small pool |     300    |     348    |   14567 K  |   14566 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     103    |     138    |    6285    |    6182    |
|       from large pool |      91    |      91    |    3449    |    3358    |
|       from small pool |      12    |      47    |    2836    |    2824    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      99    |     102    |   15713 K  |   15713 K  |
|       from large pool |      71    |      74    |    8697 K  |    8697 K  |
|       from small pool |      28    |      51    |    7015 K  |    7015 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:16:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:16:42] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:16:42]    INFO >> epoch 009:    824 / 1539 loss=3.645, wps=4266.4, ups=5.93, wpb=719.6, bsz=719.6, num_updates=13100, lr=0.000227, gnorm=4.828, clip=0, train_wall=7, gb_free=70.5, wall=2284 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:16:49]    INFO >> epoch 009:    874 / 1539 loss=3.617, wps=4492.7, ups=6.97, wpb=644.4, bsz=644.4, num_updates=13150, lr=0.000227, gnorm=4.344, clip=0, train_wall=7, gb_free=74.8, wall=2291 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:16:57]    INFO >> epoch 009:    924 / 1539 loss=3.655, wps=4206.6, ups=6.8, wpb=618.7, bsz=618.7, num_updates=13200, lr=0.000227, gnorm=5.098, clip=0, train_wall=7, gb_free=73.5, wall=2299 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:17:08]    INFO >> epoch 009:    974 / 1539 loss=3.531, wps=4465.4, ups=5.19, wpb=859.7, bsz=859.7, num_updates=13250, lr=0.000227, gnorm=5.618, clip=0, train_wall=9, gb_free=70.8, wall=2308 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:17:16]    INFO >> epoch 009:   1024 / 1539 loss=3.639, wps=4500, ups=6.26, wpb=718.9, bsz=718.9, num_updates=13300, lr=0.000227, gnorm=4.574, clip=0, train_wall=7, gb_free=69.9, wall=2316 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:17:24]    INFO >> epoch 009:   1074 / 1539 loss=3.461, wps=4580.2, ups=6.31, wpb=725.8, bsz=725.8, num_updates=13350, lr=0.000227, gnorm=5.539, clip=0, train_wall=7, gb_free=65.3, wall=2324 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:17:32]    INFO >> epoch 009:   1124 / 1539 loss=3.586, wps=4686.6, ups=6.16, wpb=760.3, bsz=760.3, num_updates=13400, lr=0.000227, gnorm=5.214, clip=0, train_wall=8, gb_free=73.5, wall=2332 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:17:41]    INFO >> epoch 009:   1174 / 1539 loss=3.637, wps=4453.7, ups=6.65, wpb=669.6, bsz=669.6, num_updates=13450, lr=0.000227, gnorm=4.988, clip=0, train_wall=7, gb_free=72.7, wall=2340 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:17:49]    INFO >> epoch 009:   1224 / 1539 loss=3.326, wps=4999.6, ups=5.8, wpb=862, bsz=862, num_updates=13500, lr=0.000227, gnorm=5.557, clip=0, train_wall=8, gb_free=69, wall=2348 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:17:57]    INFO >> epoch 009:   1274 / 1539 loss=3.605, wps=4903.9, ups=6.32, wpb=776.3, bsz=776.3, num_updates=13550, lr=0.000227, gnorm=4.848, clip=0, train_wall=7, gb_free=72.7, wall=2356 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:18:05]    INFO >> epoch 009:   1324 / 1539 loss=3.609, wps=4291.2, ups=6.02, wpb=712.6, bsz=712.6, num_updates=13600, lr=0.000227, gnorm=5.717, clip=0, train_wall=8, gb_free=75.1, wall=2365 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:18:15]    INFO >> epoch 009:   1374 / 1539 loss=3.289, wps=4598.6, ups=6.13, wpb=749.6, bsz=749.6, num_updates=13650, lr=0.000227, gnorm=4.498, clip=0, train_wall=8, gb_free=69.9, wall=2373 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:18:21] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 47.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 76.36 GiB is allocated by PyTorch, and 2.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:18:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:18:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:18:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 37           |        cudaMalloc retries: 70        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78127 MiB |  78187 MiB | 440587 GiB | 440511 GiB |
|       from large pool |  77744 MiB |  77804 MiB | 438093 GiB | 438017 GiB |
|       from small pool |    383 MiB |    384 MiB |   2494 GiB |   2493 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78127 MiB |  78187 MiB | 440587 GiB | 440511 GiB |
|       from large pool |  77744 MiB |  77804 MiB | 438093 GiB | 438017 GiB |
|       from small pool |    383 MiB |    384 MiB |   2494 GiB |   2493 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77897 MiB |  77956 MiB | 439658 GiB | 439582 GiB |
|       from large pool |  77516 MiB |  77575 MiB | 437167 GiB | 437092 GiB |
|       from small pool |    381 MiB |    382 MiB |   2490 GiB |   2490 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80458 MiB |  80460 MiB |   1006 GiB |    928 GiB |
|       from large pool |  80036 MiB |  80036 MiB |   1000 GiB |    922 GiB |
|       from small pool |    422 MiB |    424 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2270 MiB |   7290 MiB | 423445 GiB | 423443 GiB |
|       from large pool |   2231 MiB |   7283 MiB | 420622 GiB | 420620 GiB |
|       from small pool |     38 MiB |     40 MiB |   2822 GiB |   2822 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7076    |    7079    |   29063 K  |   29056 K  |
|       from large pool |     913    |     914    |   13873 K  |   13872 K  |
|       from small pool |    6163    |    6166    |   15190 K  |   15184 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7076    |    7079    |   29063 K  |   29056 K  |
|       from large pool |     913    |     914    |   13873 K  |   13872 K  |
|       from small pool |    6163    |    6166    |   15190 K  |   15184 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     742    |     743    |    6935    |    6193    |
|       from large pool |     531    |     531    |    3899    |    3368    |
|       from small pool |     211    |     212    |    3036    |    2825    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     505    |     505    |   16399 K  |   16398 K  |
|       from large pool |     129    |     129    |    9086 K  |    9086 K  |
|       from small pool |     376    |     376    |    7312 K  |    7312 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:18:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:18:21] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:18:23]    INFO >> epoch 009:   1425 / 1539 loss=3.483, wps=4151.2, ups=6.13, wpb=676.7, bsz=676.7, num_updates=13700, lr=0.000227, gnorm=5.631, clip=0, train_wall=7, gb_free=72, wall=2381 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:18:31]    INFO >> epoch 009:   1475 / 1539 loss=3.691, wps=4126.8, ups=6.51, wpb=634.1, bsz=634.1, num_updates=13750, lr=0.000227, gnorm=4.489, clip=0, train_wall=7, gb_free=72.9, wall=2389 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:18:39]    INFO >> epoch 009:   1525 / 1539 loss=3.612, wps=3984.8, ups=6.39, wpb=623.3, bsz=623.3, num_updates=13800, lr=0.000227, gnorm=5.114, clip=0, train_wall=7, gb_free=67.9, wall=2396 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:18:41]    INFO >> epoch 009 | loss 3.57 | wps 4211.9 | ups 5.91 | wpb 712.7 | bsz 712.7 | num_updates 13814 | lr 0.000227 | gnorm 5.103 | clip 0.1 | train_wall 228 | gb_free 74.2 | wall 2399 (progress_bar.py:267, print())[0m
[33m[2025-11-21 16:18:41] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:18:56]    INFO >> epoch 009 | valid on 'valid' subset | loss 3.776 | wps 10815.4 | wpb 5412.5 | bsz 5412.5 | num_updates 13814 | best_loss 4.689 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 16:18:57]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 16:18:57]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/checkpoints/checkpoint_last.pt (epoch 9 @ 13814 updates, score 3.776) (writing took 0.015754 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 16:18:57] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:19:04]    INFO >> epoch 010:     36 / 1539 loss=3.637, wps=1472.9, ups=2.1, wpb=701.4, bsz=701.4, num_updates=13850, lr=0.000193, gnorm=4.727, clip=0, train_wall=9, gb_free=72.8, wall=2420 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:19:11]    INFO >> epoch 010:     86 / 1539 loss=3.512, wps=4578.6, ups=6.39, wpb=716.9, bsz=716.9, num_updates=13900, lr=0.000193, gnorm=6.195, clip=0, train_wall=7, gb_free=69.6, wall=2428 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:19:20]    INFO >> epoch 010:    136 / 1539 loss=3.547, wps=4116.1, ups=6.57, wpb=626.5, bsz=626.5, num_updates=13950, lr=0.000193, gnorm=4.642, clip=0, train_wall=7, gb_free=74.1, wall=2436 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:19:29]    INFO >> epoch 010:    186 / 1539 loss=3.516, wps=4140.3, ups=5.56, wpb=744.2, bsz=744.2, num_updates=14000, lr=0.000193, gnorm=5.624, clip=2, train_wall=8, gb_free=74.2, wall=2445 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:19:38]    INFO >> epoch 010:    236 / 1539 loss=3.324, wps=4954.5, ups=5.74, wpb=863.9, bsz=863.9, num_updates=14050, lr=0.000193, gnorm=5.489, clip=0, train_wall=8, gb_free=74.4, wall=2453 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:19:46]    INFO >> epoch 010:    286 / 1539 loss=3.578, wps=4256.9, ups=6.3, wpb=676, bsz=676, num_updates=14100, lr=0.000193, gnorm=5.373, clip=0, train_wall=7, gb_free=73.9, wall=2461 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:19:54] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 901.25 MiB is free. Including non-PyTorch memory, this process has 78.24 GiB memory in use. Of the allocated memory 74.10 GiB is allocated by PyTorch, and 3.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:19:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:19:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:19:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 38           |        cudaMalloc retries: 72        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72064 MiB |  75874 MiB | 458286 GiB | 458216 GiB |
|       from large pool |  72046 MiB |  75856 MiB | 455686 GiB | 455616 GiB |
|       from small pool |     17 MiB |     18 MiB |   2600 GiB |   2600 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72064 MiB |  75874 MiB | 458286 GiB | 458216 GiB |
|       from large pool |  72046 MiB |  75856 MiB | 455686 GiB | 455616 GiB |
|       from small pool |     17 MiB |     18 MiB |   2600 GiB |   2600 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  75852 MiB | 457323 GiB | 457252 GiB |
|       from large pool |  72027 MiB |  75835 MiB | 454726 GiB | 454656 GiB |
|       from small pool |     17 MiB |     18 MiB |   2596 GiB |   2596 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79604 MiB |  79804 MiB |   1030 GiB |    952 GiB |
|       from large pool |  79578 MiB |  79578 MiB |   1023 GiB |    946 GiB |
|       from small pool |     26 MiB |    226 MiB |      6 GiB |      6 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4811 MiB |   6891 MiB | 438610 GiB | 438606 GiB |
|       from large pool |   4803 MiB |   6882 MiB | 435671 GiB | 435666 GiB |
|       from small pool |      8 MiB |     23 MiB |   2939 GiB |   2939 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     615    |   30202 K  |   30201 K  |
|       from large pool |     308    |     317    |   14352 K  |   14352 K  |
|       from small pool |     298    |     342    |   15849 K  |   15849 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     615    |   30202 K  |   30201 K  |
|       from large pool |     308    |     317    |   14352 K  |   14352 K  |
|       from small pool |     298    |     342    |   15849 K  |   15849 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     151    |     251    |    7044    |    6893    |
|       from large pool |     138    |     138    |    3908    |    3770    |
|       from small pool |      13    |     113    |    3136    |    3123    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     141    |     143    |   17068 K  |   17068 K  |
|       from large pool |     114    |     116    |    9404 K  |    9404 K  |
|       from small pool |      27    |      47    |    7664 K  |    7664 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:19:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:19:54] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:19:56]    INFO >> epoch 010:    337 / 1539 loss=3.543, wps=4270.5, ups=6.07, wpb=703.9, bsz=703.9, num_updates=14150, lr=0.000193, gnorm=5.638, clip=0, train_wall=7, gb_free=67.2, wall=2470 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:20:04]    INFO >> epoch 010:    387 / 1539 loss=3.685, wps=4435.6, ups=6.13, wpb=723.5, bsz=723.5, num_updates=14200, lr=0.000193, gnorm=4.676, clip=0, train_wall=8, gb_free=73.4, wall=2478 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:20:12]    INFO >> epoch 010:    437 / 1539 loss=3.598, wps=4024.2, ups=6.37, wpb=631.4, bsz=631.4, num_updates=14250, lr=0.000193, gnorm=4.072, clip=0, train_wall=7, gb_free=72.6, wall=2486 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:20:20]    INFO >> epoch 010:    487 / 1539 loss=3.601, wps=4036.3, ups=6.04, wpb=668.5, bsz=668.5, num_updates=14300, lr=0.000193, gnorm=4.362, clip=0, train_wall=8, gb_free=72.5, wall=2494 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:20:29]    INFO >> epoch 010:    537 / 1539 loss=3.386, wps=4412.1, ups=6.83, wpb=645.7, bsz=645.7, num_updates=14350, lr=0.000193, gnorm=5.241, clip=2, train_wall=7, gb_free=75, wall=2501 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:20:37]    INFO >> epoch 010:    587 / 1539 loss=3.625, wps=4077.9, ups=6.13, wpb=665.2, bsz=665.2, num_updates=14400, lr=0.000193, gnorm=4.775, clip=0, train_wall=8, gb_free=71.4, wall=2509 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:20:45]    INFO >> epoch 010:    637 / 1539 loss=3.541, wps=4648.2, ups=6.46, wpb=719.7, bsz=719.7, num_updates=14450, lr=0.000193, gnorm=5.632, clip=0, train_wall=7, gb_free=69.1, wall=2517 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:20:52]    INFO >> epoch 010:    687 / 1539 loss=3.555, wps=4741, ups=6.53, wpb=726.6, bsz=726.6, num_updates=14500, lr=0.000193, gnorm=5.206, clip=0, train_wall=7, gb_free=73.3, wall=2525 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:20:56] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 53.25 MiB is free. Including non-PyTorch memory, this process has 79.06 GiB memory in use. Of the allocated memory 76.23 GiB is allocated by PyTorch, and 2.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:20:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:20:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:20:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 39           |        cudaMalloc retries: 73        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78000 MiB |  78060 MiB | 469354 GiB | 469278 GiB |
|       from large pool |  77616 MiB |  77676 MiB | 466693 GiB | 466617 GiB |
|       from small pool |    383 MiB |    385 MiB |   2661 GiB |   2660 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78000 MiB |  78060 MiB | 469354 GiB | 469278 GiB |
|       from large pool |  77616 MiB |  77676 MiB | 466693 GiB | 466617 GiB |
|       from small pool |    383 MiB |    385 MiB |   2661 GiB |   2660 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77957 MiB |  78016 MiB | 468366 GiB | 468290 GiB |
|       from large pool |  77575 MiB |  77635 MiB | 465709 GiB | 465633 GiB |
|       from small pool |    381 MiB |    383 MiB |   2657 GiB |   2656 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80452 MiB |  80454 MiB |   1033 GiB |    955 GiB |
|       from large pool |  80030 MiB |  80030 MiB |   1027 GiB |    948 GiB |
|       from small pool |    422 MiB |    424 MiB |      6 GiB |      6 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2391 MiB |   6658 MiB | 449489 GiB | 449487 GiB |
|       from large pool |   2353 MiB |   6651 MiB | 446479 GiB | 446477 GiB |
|       from small pool |     38 MiB |     40 MiB |   3009 GiB |   3009 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7087    |    7090    |   30947 K  |   30940 K  |
|       from large pool |     914    |     915    |   14731 K  |   14730 K  |
|       from small pool |    6173    |    6176    |   16216 K  |   16209 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7087    |    7090    |   30947 K  |   30940 K  |
|       from large pool |     914    |     915    |   14731 K  |   14730 K  |
|       from small pool |    6173    |    6176    |   16216 K  |   16209 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     401    |     402    |    7296    |    6895    |
|       from large pool |     190    |     190    |    3961    |    3771    |
|       from small pool |     211    |     212    |    3335    |    3124    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     503    |     506    |   17482 K  |   17482 K  |
|       from large pool |     127    |     127    |    9653 K  |    9653 K  |
|       from small pool |     376    |     379    |    7829 K  |    7828 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:20:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:20:56] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:21:02]    INFO >> epoch 010:    738 / 1539 loss=3.678, wps=3896.4, ups=6.06, wpb=642.7, bsz=642.7, num_updates=14550, lr=0.000193, gnorm=4.891, clip=0, train_wall=7, gb_free=68.7, wall=2533 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:21:10]    INFO >> epoch 010:    788 / 1539 loss=3.571, wps=4377.6, ups=6.05, wpb=723.3, bsz=723.3, num_updates=14600, lr=0.000193, gnorm=4.57, clip=0, train_wall=8, gb_free=67.8, wall=2541 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:21:18]    INFO >> epoch 010:    838 / 1539 loss=3.562, wps=4205.5, ups=6.24, wpb=673.9, bsz=673.9, num_updates=14650, lr=0.000193, gnorm=4.959, clip=0, train_wall=7, gb_free=66.8, wall=2549 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:21:27]    INFO >> epoch 010:    888 / 1539 loss=3.315, wps=4832.5, ups=5.69, wpb=848.7, bsz=848.7, num_updates=14700, lr=0.000193, gnorm=5.234, clip=0, train_wall=8, gb_free=76.2, wall=2558 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:21:36]    INFO >> epoch 010:    938 / 1539 loss=3.617, wps=4541.1, ups=6.36, wpb=713.5, bsz=713.5, num_updates=14750, lr=0.000193, gnorm=4.962, clip=0, train_wall=7, gb_free=74.7, wall=2566 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:21:44]    INFO >> epoch 010:    988 / 1539 loss=3.692, wps=4564.5, ups=6.61, wpb=690.7, bsz=690.7, num_updates=14800, lr=0.000193, gnorm=4.979, clip=0, train_wall=7, gb_free=74.4, wall=2573 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:21:51]    INFO >> epoch 010:   1038 / 1539 loss=3.627, wps=4370.1, ups=6.51, wpb=671.7, bsz=671.7, num_updates=14850, lr=0.000193, gnorm=4.669, clip=0, train_wall=7, gb_free=70, wall=2581 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:21:59]    INFO >> epoch 010:   1088 / 1539 loss=3.602, wps=4971.4, ups=6.18, wpb=803.8, bsz=803.8, num_updates=14900, lr=0.000193, gnorm=5.276, clip=0, train_wall=8, gb_free=72.8, wall=2589 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:22:09]    INFO >> epoch 010:   1138 / 1539 loss=3.595, wps=3931.3, ups=6.42, wpb=612.7, bsz=612.7, num_updates=14950, lr=0.000193, gnorm=4.44, clip=0, train_wall=7, gb_free=71.8, wall=2597 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:22:16] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 29.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.30 GiB is allocated by PyTorch, and 2.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:22:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:22:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:22:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 40           |        cudaMalloc retries: 75        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78067 MiB |  78127 MiB | 483183 GiB | 483107 GiB |
|       from large pool |  77988 MiB |  78048 MiB | 480446 GiB | 480370 GiB |
|       from small pool |     79 MiB |     80 MiB |   2736 GiB |   2736 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78067 MiB |  78127 MiB | 483183 GiB | 483107 GiB |
|       from large pool |  77988 MiB |  78048 MiB | 480446 GiB | 480370 GiB |
|       from small pool |     79 MiB |     80 MiB |   2736 GiB |   2736 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78041 MiB |  78101 MiB | 482164 GiB | 482088 GiB |
|       from large pool |  77963 MiB |  78022 MiB | 479431 GiB | 479355 GiB |
|       from small pool |     78 MiB |     79 MiB |   2732 GiB |   2732 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80476 MiB |  80478 MiB |   1034 GiB |    955 GiB |
|       from large pool |  80394 MiB |  80394 MiB |   1028 GiB |    949 GiB |
|       from small pool |     82 MiB |    422 MiB |      6 GiB |      6 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2348 MiB |  10074 MiB | 462318 GiB | 462316 GiB |
|       from large pool |   2345 MiB |  10062 MiB | 459222 GiB | 459219 GiB |
|       from small pool |      2 MiB |     23 MiB |   3096 GiB |   3096 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1763    |    1766    |   31868 K  |   31866 K  |
|       from large pool |     452    |     453    |   15195 K  |   15195 K  |
|       from small pool |    1311    |    1314    |   16672 K  |   16671 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1763    |    1766    |   31868 K  |   31866 K  |
|       from large pool |     452    |     453    |   15195 K  |   15195 K  |
|       from small pool |    1311    |    1314    |   16672 K  |   16671 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     223    |     400    |    7326    |    7103    |
|       from large pool |     182    |     189    |    3963    |    3781    |
|       from small pool |      41    |     211    |    3363    |    3322    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     191    |     192    |   18002 K  |   18002 K  |
|       from large pool |     147    |     152    |    9963 K  |    9963 K  |
|       from small pool |      44    |      52    |    8038 K  |    8038 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:22:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:22:16] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:22:18]    INFO >> epoch 010:   1189 / 1539 loss=3.587, wps=4289.2, ups=5.44, wpb=787.9, bsz=787.9, num_updates=15000, lr=0.000193, gnorm=4.819, clip=0, train_wall=8, gb_free=70.1, wall=2606 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:22:26]    INFO >> epoch 010:   1239 / 1539 loss=3.537, wps=4715.7, ups=6, wpb=786.3, bsz=786.3, num_updates=15050, lr=0.000193, gnorm=5.278, clip=0, train_wall=8, gb_free=73.2, wall=2615 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:22:35]    INFO >> epoch 010:   1289 / 1539 loss=3.558, wps=4468.8, ups=5.83, wpb=766.5, bsz=766.5, num_updates=15100, lr=0.000193, gnorm=4.715, clip=0, train_wall=8, gb_free=72.7, wall=2623 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:22:44]    INFO >> epoch 010:   1339 / 1539 loss=3.607, wps=4317.2, ups=6.39, wpb=675.5, bsz=675.5, num_updates=15150, lr=0.000193, gnorm=5.208, clip=0, train_wall=7, gb_free=74.4, wall=2631 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:22:52]    INFO >> epoch 010:   1389 / 1539 loss=3.423, wps=4723.6, ups=6.33, wpb=746.1, bsz=746.1, num_updates=15200, lr=0.000193, gnorm=5.568, clip=0, train_wall=7, gb_free=73.6, wall=2639 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:22:59]    INFO >> epoch 010:   1439 / 1539 loss=3.54, wps=4370.6, ups=6.43, wpb=679.6, bsz=679.6, num_updates=15250, lr=0.000193, gnorm=5.304, clip=0, train_wall=7, gb_free=70.1, wall=2647 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:23:05] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 911.25 MiB is free. Including non-PyTorch memory, this process has 78.23 GiB memory in use. Of the allocated memory 73.81 GiB is allocated by PyTorch, and 3.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:23:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:23:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:23:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 41           |        cudaMalloc retries: 78        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75058 MiB |  76000 MiB | 491923 GiB | 491849 GiB |
|       from large pool |  75045 MiB |  75988 MiB | 489139 GiB | 489065 GiB |
|       from small pool |     12 MiB |     24 MiB |   2784 GiB |   2783 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75058 MiB |  76000 MiB | 491923 GiB | 491849 GiB |
|       from large pool |  75045 MiB |  75988 MiB | 489139 GiB | 489065 GiB |
|       from small pool |     12 MiB |     24 MiB |   2784 GiB |   2783 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75035 MiB |  75977 MiB | 490883 GiB | 490810 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 488103 GiB | 488030 GiB |
|       from small pool |     12 MiB |     24 MiB |   2780 GiB |   2780 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79594 MiB |  79594 MiB |   1070 GiB |    993 GiB |
|       from large pool |  79568 MiB |  79568 MiB |   1064 GiB |    986 GiB |
|       from small pool |     26 MiB |    210 MiB |      6 GiB |      6 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4535 MiB |   9492 MiB | 470276 GiB | 470272 GiB |
|       from large pool |   4522 MiB |   9478 MiB | 467125 GiB | 467120 GiB |
|       from small pool |     13 MiB |     31 MiB |   3151 GiB |   3151 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   32444 K  |   32443 K  |
|       from large pool |     314    |     322    |   15487 K  |   15486 K  |
|       from small pool |     291    |     356    |   16957 K  |   16957 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   32444 K  |   32443 K  |
|       from large pool |     314    |     322    |   15487 K  |   15486 K  |
|       from small pool |     291    |     356    |   16957 K  |   16957 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     164    |     281    |    7455    |    7291    |
|       from large pool |     151    |     176    |    4028    |    3877    |
|       from small pool |      13    |     105    |    3427    |    3414    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     149    |     151    |   18327 K  |   18327 K  |
|       from large pool |     123    |     125    |   10158 K  |   10158 K  |
|       from small pool |      26    |      62    |    8168 K  |    8168 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:23:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:23:05] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:23:08]    INFO >> epoch 010:   1490 / 1539 loss=3.616, wps=4055.5, ups=5.84, wpb=694.7, bsz=694.7, num_updates=15300, lr=0.000193, gnorm=4.95, clip=0, train_wall=7, gb_free=64.7, wall=2655 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:23:16]    INFO >> epoch 010 | loss 3.556 | wps 4137.1 | ups 5.81 | wpb 712.7 | bsz 712.7 | num_updates 15349 | lr 0.000193 | gnorm 5.032 | clip 0.1 | train_wall 232 | gb_free 70.8 | wall 2663 (progress_bar.py:267, print())[0m
[33m[2025-11-21 16:23:16] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:23:30]    INFO >> epoch 010 | valid on 'valid' subset | loss 3.761 | wps 10720.2 | wpb 5412.5 | bsz 5412.5 | num_updates 15349 | best_loss 4.689 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 16:23:31]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 16:23:31]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/checkpoints/checkpoint_last.pt (epoch 10 @ 15349 updates, score 3.761) (writing took 0.019015 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 16:23:31] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:23:31]    INFO >> epoch 011:      1 / 1539 loss=3.616, wps=1606.9, ups=2.21, wpb=728.6, bsz=728.6, num_updates=15350, lr=0.000161, gnorm=4.475, clip=0, train_wall=8, gb_free=68.7, wall=2678 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:23:38]    INFO >> epoch 011:     51 / 1539 loss=3.646, wps=4197.4, ups=6.48, wpb=647.3, bsz=647.3, num_updates=15400, lr=0.000161, gnorm=4.653, clip=0, train_wall=7, gb_free=75, wall=2686 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:23:49]    INFO >> epoch 011:    101 / 1539 loss=3.556, wps=4546.8, ups=6.23, wpb=729.7, bsz=729.7, num_updates=15450, lr=0.000161, gnorm=5.43, clip=0, train_wall=8, gb_free=74.4, wall=2694 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:23:57]    INFO >> epoch 011:    151 / 1539 loss=3.669, wps=4444.8, ups=6.67, wpb=666, bsz=666, num_updates=15500, lr=0.000161, gnorm=4.871, clip=0, train_wall=7, gb_free=71.4, wall=2701 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:24:05]    INFO >> epoch 011:    201 / 1539 loss=3.522, wps=4752.2, ups=6.18, wpb=769.4, bsz=769.4, num_updates=15550, lr=0.000161, gnorm=4.692, clip=0, train_wall=8, gb_free=71.9, wall=2709 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:24:12]    INFO >> epoch 011:    251 / 1539 loss=3.525, wps=4840.7, ups=6.64, wpb=729.4, bsz=729.4, num_updates=15600, lr=0.000161, gnorm=5.427, clip=0, train_wall=7, gb_free=73.3, wall=2717 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:24:21]    INFO >> epoch 011:    301 / 1539 loss=3.531, wps=3949.2, ups=6.34, wpb=623.3, bsz=623.3, num_updates=15650, lr=0.000161, gnorm=4.844, clip=0, train_wall=7, gb_free=74.6, wall=2725 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:24:30]    INFO >> epoch 011:    351 / 1539 loss=3.529, wps=4562.1, ups=6.07, wpb=751.3, bsz=751.3, num_updates=15700, lr=0.000161, gnorm=5.757, clip=2, train_wall=8, gb_free=72, wall=2733 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:24:37]    INFO >> epoch 011:    401 / 1539 loss=3.573, wps=4536, ups=6.7, wpb=677.5, bsz=677.5, num_updates=15750, lr=0.000161, gnorm=5.03, clip=0, train_wall=7, gb_free=72.3, wall=2740 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:24:45]    INFO >> epoch 011:    451 / 1539 loss=3.606, wps=4177.2, ups=6.69, wpb=624.1, bsz=624.1, num_updates=15800, lr=0.000161, gnorm=5.342, clip=0, train_wall=7, gb_free=72.8, wall=2748 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:24:55]    INFO >> epoch 011:    501 / 1539 loss=3.71, wps=3524.9, ups=5.81, wpb=606.7, bsz=606.7, num_updates=15850, lr=0.000161, gnorm=4.549, clip=0, train_wall=8, gb_free=75, wall=2756 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:25:02]    INFO >> epoch 011:    551 / 1539 loss=3.633, wps=4118.2, ups=6.38, wpb=645.8, bsz=645.8, num_updates=15900, lr=0.000161, gnorm=4.746, clip=2, train_wall=7, gb_free=73.9, wall=2764 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:25:03] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 33.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 76.32 GiB is allocated by PyTorch, and 2.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:25:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:25:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:25:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 42           |        cudaMalloc retries: 79        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78095 MiB |  78155 MiB | 514020 GiB | 513944 GiB |
|       from large pool |  77710 MiB |  77770 MiB | 511104 GiB | 511028 GiB |
|       from small pool |    385 MiB |    386 MiB |   2916 GiB |   2916 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78095 MiB |  78155 MiB | 514020 GiB | 513944 GiB |
|       from large pool |  77710 MiB |  77770 MiB | 511104 GiB | 511028 GiB |
|       from small pool |    385 MiB |    386 MiB |   2916 GiB |   2916 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78077 MiB |  78137 MiB | 512935 GiB | 512859 GiB |
|       from large pool |  77694 MiB |  77753 MiB | 510023 GiB | 509947 GiB |
|       from small pool |    383 MiB |    384 MiB |   2912 GiB |   2912 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80472 MiB |  80474 MiB |   1071 GiB |    993 GiB |
|       from large pool |  80048 MiB |  80048 MiB |   1064 GiB |    986 GiB |
|       from small pool |    424 MiB |    426 MiB |      7 GiB |      6 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2316 MiB |   7005 MiB | 489661 GiB | 489659 GiB |
|       from large pool |   2277 MiB |   7000 MiB | 486363 GiB | 486361 GiB |
|       from small pool |     38 MiB |     40 MiB |   3298 GiB |   3298 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7109    |    7112    |   33914 K  |   33907 K  |
|       from large pool |     916    |     917    |   16134 K  |   16134 K  |
|       from small pool |    6193    |    6196    |   17779 K  |   17773 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7109    |    7112    |   33914 K  |   33907 K  |
|       from large pool |     916    |     917    |   16134 K  |   16134 K  |
|       from small pool |    6193    |    6196    |   17779 K  |   17773 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     371    |     372    |    7663    |    7292    |
|       from large pool |     159    |     159    |    4036    |    3877    |
|       from small pool |     212    |     213    |    3627    |    3415    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     522    |     524    |   19167 K  |   19167 K  |
|       from large pool |     145    |     145    |   10591 K  |   10591 K  |
|       from small pool |     377    |     379    |    8575 K  |    8575 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:25:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:25:03] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:25:11]    INFO >> epoch 011:    602 / 1539 loss=3.558, wps=3966.9, ups=5.72, wpb=694.1, bsz=694.1, num_updates=15950, lr=0.000161, gnorm=4.617, clip=0, train_wall=8, gb_free=71.6, wall=2773 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:25:20]    INFO >> epoch 011:    652 / 1539 loss=3.534, wps=4822.7, ups=5.92, wpb=815.3, bsz=815.3, num_updates=16000, lr=0.000161, gnorm=4.541, clip=0, train_wall=8, gb_free=71.5, wall=2781 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:25:29]    INFO >> epoch 011:    702 / 1539 loss=3.563, wps=4247.3, ups=6.42, wpb=661.1, bsz=661.1, num_updates=16050, lr=0.000161, gnorm=4.656, clip=0, train_wall=7, gb_free=71, wall=2789 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:25:38]    INFO >> epoch 011:    752 / 1539 loss=3.447, wps=4118.6, ups=5.63, wpb=731.8, bsz=731.8, num_updates=16100, lr=0.000161, gnorm=5.354, clip=0, train_wall=8, gb_free=72.6, wall=2798 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:25:43] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.08 GiB is free. Including non-PyTorch memory, this process has 78.03 GiB memory in use. Of the allocated memory 73.81 GiB is allocated by PyTorch, and 3.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:25:43] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:25:43] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:25:43] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 43           |        cudaMalloc retries: 80        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75056 MiB |  75999 MiB | 521029 GiB | 520955 GiB |
|       from large pool |  75044 MiB |  75986 MiB | 518075 GiB | 518001 GiB |
|       from small pool |     12 MiB |     21 MiB |   2953 GiB |   2953 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75056 MiB |  75999 MiB | 521029 GiB | 520955 GiB |
|       from large pool |  75044 MiB |  75986 MiB | 518075 GiB | 518001 GiB |
|       from small pool |     12 MiB |     21 MiB |   2953 GiB |   2953 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75035 MiB |  75977 MiB | 519927 GiB | 519854 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 516977 GiB | 516904 GiB |
|       from small pool |     12 MiB |     21 MiB |   2949 GiB |   2949 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79396 MiB |  80412 MiB |   1071 GiB |    994 GiB |
|       from large pool |  79372 MiB |  79988 MiB |   1064 GiB |    987 GiB |
|       from small pool |     24 MiB |    424 MiB |      7 GiB |      7 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4279 MiB |   8182 MiB | 496383 GiB | 496378 GiB |
|       from large pool |   4267 MiB |   8171 MiB | 493041 GiB | 493037 GiB |
|       from small pool |     11 MiB |     27 MiB |   3341 GiB |   3341 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   34373 K  |   34372 K  |
|       from large pool |     314    |     322    |   16366 K  |   16366 K  |
|       from small pool |     291    |     356    |   18006 K  |   18006 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   34373 K  |   34372 K  |
|       from large pool |     314    |     322    |   16366 K  |   16366 K  |
|       from small pool |     291    |     356    |   18006 K  |   18006 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     165    |     370    |    7663    |    7498    |
|       from large pool |     153    |     158    |    4036    |    3883    |
|       from small pool |      12    |     212    |    3627    |    3615    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     154    |     157    |   19428 K  |   19427 K  |
|       from large pool |     127    |     130    |   10745 K  |   10745 K  |
|       from small pool |      27    |      60    |    8682 K  |    8682 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:25:43] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:25:43] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:25:46]    INFO >> epoch 011:    803 / 1539 loss=3.626, wps=3683.6, ups=6.28, wpb=586.3, bsz=586.3, num_updates=16150, lr=0.000161, gnorm=4.684, clip=0, train_wall=7, gb_free=62.8, wall=2806 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:25:53]    INFO >> epoch 011:    853 / 1539 loss=3.49, wps=4268.9, ups=6.5, wpb=657, bsz=657, num_updates=16200, lr=0.000161, gnorm=5.373, clip=0, train_wall=7, gb_free=74.8, wall=2814 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:26:02]    INFO >> epoch 011:    903 / 1539 loss=3.623, wps=4991.6, ups=6.4, wpb=779.8, bsz=779.8, num_updates=16250, lr=0.000161, gnorm=4.782, clip=0, train_wall=7, gb_free=72.7, wall=2822 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:26:10]    INFO >> epoch 011:    953 / 1539 loss=3.553, wps=4693.2, ups=6.29, wpb=746, bsz=746, num_updates=16300, lr=0.000161, gnorm=4.827, clip=0, train_wall=7, gb_free=72, wall=2830 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:26:19]    INFO >> epoch 011:   1003 / 1539 loss=3.487, wps=4554.9, ups=6.17, wpb=737.8, bsz=737.8, num_updates=16350, lr=0.000161, gnorm=5.584, clip=2, train_wall=8, gb_free=71.3, wall=2838 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:26:26]    INFO >> epoch 011:   1053 / 1539 loss=3.499, wps=4351.8, ups=6.28, wpb=692.9, bsz=692.9, num_updates=16400, lr=0.000161, gnorm=4.162, clip=0, train_wall=7, gb_free=73.7, wall=2846 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:26:32] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 19.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 76.94 GiB is allocated by PyTorch, and 1.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:26:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:26:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:26:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 44           |        cudaMalloc retries: 82        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78725 MiB |  78785 MiB | 529610 GiB | 529533 GiB |
|       from large pool |  78639 MiB |  78699 MiB | 526609 GiB | 526533 GiB |
|       from small pool |     85 MiB |     86 MiB |   3000 GiB |   3000 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78725 MiB |  78785 MiB | 529610 GiB | 529533 GiB |
|       from large pool |  78639 MiB |  78699 MiB | 526609 GiB | 526533 GiB |
|       from small pool |     85 MiB |     86 MiB |   3000 GiB |   3000 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78703 MiB |  78762 MiB | 528490 GiB | 528413 GiB |
|       from large pool |  78617 MiB |  78677 MiB | 525494 GiB | 525417 GiB |
|       from small pool |     85 MiB |     86 MiB |   2996 GiB |   2996 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80486 MiB |  80486 MiB |   1072 GiB |    994 GiB |
|       from large pool |  80396 MiB |  80396 MiB |   1065 GiB |    987 GiB |
|       from small pool |     90 MiB |    124 MiB |      7 GiB |      7 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1700 MiB |   9160 MiB | 504805 GiB | 504803 GiB |
|       from large pool |   1696 MiB |   9152 MiB | 501410 GiB | 501408 GiB |
|       from small pool |      4 MiB |     23 MiB |   3395 GiB |   3395 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1884    |    1887    |   34936 K  |   34934 K  |
|       from large pool |     463    |     464    |   16651 K  |   16651 K  |
|       from small pool |    1421    |    1424    |   18284 K  |   18282 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1884    |    1887    |   34936 K  |   34934 K  |
|       from large pool |     463    |     464    |   16651 K  |   16651 K  |
|       from small pool |    1421    |    1424    |   18284 K  |   18282 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     201    |     217    |    7718    |    7517    |
|       from large pool |     156    |     156    |    4040    |    3884    |
|       from small pool |      45    |      62    |    3678    |    3633    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     170    |     170    |   19742 K  |   19741 K  |
|       from large pool |     123    |     125    |   10933 K  |   10933 K  |
|       from small pool |      47    |      52    |    8808 K  |    8808 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:26:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:26:32] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:26:37]    INFO >> epoch 011:   1104 / 1539 loss=3.534, wps=4267.7, ups=5.59, wpb=764, bsz=764, num_updates=16450, lr=0.000161, gnorm=5.364, clip=0, train_wall=8, gb_free=75, wall=2855 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:26:46]    INFO >> epoch 011:   1154 / 1539 loss=3.534, wps=5137.3, ups=5.65, wpb=909, bsz=909, num_updates=16500, lr=0.000161, gnorm=5.478, clip=0, train_wall=8, gb_free=69.3, wall=2863 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:26:54]    INFO >> epoch 011:   1204 / 1539 loss=3.597, wps=3780.8, ups=6.22, wpb=608.2, bsz=608.2, num_updates=16550, lr=0.000161, gnorm=4.344, clip=0, train_wall=8, gb_free=68.8, wall=2871 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:27:02]    INFO >> epoch 011:   1254 / 1539 loss=3.516, wps=4713.8, ups=5.69, wpb=828.9, bsz=828.9, num_updates=16600, lr=0.000161, gnorm=5.084, clip=0, train_wall=8, gb_free=73.1, wall=2880 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:27:12]    INFO >> epoch 011:   1304 / 1539 loss=3.535, wps=4557.6, ups=6.17, wpb=739.1, bsz=739.1, num_updates=16650, lr=0.000161, gnorm=4.959, clip=0, train_wall=8, gb_free=71.9, wall=2888 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:27:15] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.06 GiB is free. Including non-PyTorch memory, this process has 78.05 GiB memory in use. Of the allocated memory 74.92 GiB is allocated by PyTorch, and 2.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:27:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:27:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:27:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 45           |        cudaMalloc retries: 86        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76321 MiB |  78595 MiB | 536742 GiB | 536667 GiB |
|       from large pool |  76302 MiB |  78578 MiB | 533702 GiB | 533628 GiB |
|       from small pool |     18 MiB |     23 MiB |   3039 GiB |   3039 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76321 MiB |  78595 MiB | 536742 GiB | 536667 GiB |
|       from large pool |  76302 MiB |  78578 MiB | 533702 GiB | 533628 GiB |
|       from small pool |     18 MiB |     23 MiB |   3039 GiB |   3039 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB | 535606 GiB | 535532 GiB |
|       from large pool |  76285 MiB |  78562 MiB | 532571 GiB | 532497 GiB |
|       from small pool |     18 MiB |     23 MiB |   3035 GiB |   3035 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79418 MiB |  79536 MiB |   1178 GiB |   1101 GiB |
|       from large pool |  79392 MiB |  79460 MiB |   1171 GiB |   1094 GiB |
|       from small pool |     26 MiB |     76 MiB |      7 GiB |      7 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3096 MiB |   4417 MiB | 511218 GiB | 511215 GiB |
|       from large pool |   3089 MiB |   4409 MiB | 507778 GiB | 507775 GiB |
|       from small pool |      7 MiB |     31 MiB |   3439 GiB |   3439 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     640    |     646    |   35400 K  |   35399 K  |
|       from large pool |     340    |     346    |   16881 K  |   16881 K  |
|       from small pool |     300    |     356    |   18518 K  |   18518 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     640    |     646    |   35400 K  |   35399 K  |
|       from large pool |     340    |     346    |   16881 K  |   16881 K  |
|       from small pool |     300    |     356    |   18518 K  |   18518 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     109    |     212    |    7929    |    7820    |
|       from large pool |      96    |     174    |    4167    |    4071    |
|       from small pool |      13    |      38    |    3762    |    3749    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     128    |     130    |   20008 K  |   20008 K  |
|       from large pool |     101    |     103    |   11090 K  |   11089 K  |
|       from small pool |      27    |      55    |    8918 K  |    8918 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:27:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:27:15] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:27:22]    INFO >> epoch 011:   1355 / 1539 loss=3.53, wps=3346.4, ups=4.86, wpb=689.1, bsz=689.1, num_updates=16700, lr=0.000161, gnorm=5.149, clip=0, train_wall=8, gb_free=56.8, wall=2899 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:27:31]    INFO >> epoch 011:   1405 / 1539 loss=3.658, wps=4722.3, ups=5.97, wpb=791.3, bsz=791.3, num_updates=16750, lr=0.000161, gnorm=5.055, clip=0, train_wall=8, gb_free=73.5, wall=2907 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:27:39]    INFO >> epoch 011:   1455 / 1539 loss=3.624, wps=4136.9, ups=6.22, wpb=665.3, bsz=665.3, num_updates=16800, lr=0.000161, gnorm=4.771, clip=0, train_wall=8, gb_free=65.3, wall=2915 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:27:49]    INFO >> epoch 011:   1505 / 1539 loss=3.189, wps=4728.4, ups=5.78, wpb=818.6, bsz=818.6, num_updates=16850, lr=0.000161, gnorm=5.457, clip=0, train_wall=8, gb_free=72.5, wall=2924 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:27:54]    INFO >> epoch 011 | loss 3.549 | wps 4110.7 | ups 5.77 | wpb 712.7 | bsz 712.7 | num_updates 16884 | lr 0.000161 | gnorm 4.994 | clip 0.2 | train_wall 232 | gb_free 73.9 | wall 2929 (progress_bar.py:267, print())[0m
[33m[2025-11-21 16:27:54] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:28:08]    INFO >> epoch 011 | valid on 'valid' subset | loss 3.766 | wps 10621.6 | wpb 5412.5 | bsz 5412.5 | num_updates 16884 | best_loss 4.689 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 16:28:09]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 16:28:09]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/checkpoints/checkpoint_last.pt (epoch 11 @ 16884 updates, score 3.766) (writing took 0.013845 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[32m[2025-11-21 16:28:09]    INFO >> Êó©ÂÅú: È™åËØÅÊÄßËÉΩÂ∑≤10ËΩÆÊú™ÊèêÂçá (train_enhanced.py:616, single_main())[0m
[32m[2025-11-21 16:28:09]    INFO >> ËÆ≠ÁªÉÂÆåÊàêÔºåÁî®Êó∂ 2876.5 Áßí (train_enhanced.py:626, single_main())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 16:28:09]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 16:28:09]    INFO >> ÊâÄÊúâÊó•ÂøóÂ∑≤‰øùÂ≠òÂà∞: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs (train_enhanced.py:631, single_main())[0m
[32m[2025-11-21 16:28:09]    INFO >> 
================================================================================ (train_enhanced.py:634, single_main())[0m
[32m[2025-11-21 16:28:09]    INFO >> ÂºÄÂßãÊµãËØï... (train_enhanced.py:635, single_main())[0m
[32m[2025-11-21 16:28:09]    INFO >> ================================================================================ (train_enhanced.py:636, single_main())[0m
[32m[2025-11-21 16:28:09]    INFO >> Âä†ËΩΩÊúÄ‰Ω≥checkpoint: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/checkpoints/checkpoint_best.pt (train_enhanced.py:50, run_test_after_training())[0m
[32m[2025-11-21 16:28:09]    INFO >> ÊµãËØïÈõÜ: test (train_enhanced.py:51, run_test_after_training())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/checkpoint_utils.py:212: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(
[32m[2025-11-21 16:29:02]    INFO >> 
================================================================================ (train_enhanced.py:168, run_test_after_training())[0m
[32m[2025-11-21 16:29:02]    INFO >> ÊµãËØïÁªìÊûú: (train_enhanced.py:169, run_test_after_training())[0m
[32m[2025-11-21 16:29:02]    INFO >> -------------------------------------------------------------------------------- (train_enhanced.py:170, run_test_after_training())[0m
[32m[2025-11-21 16:29:02]    INFO >> Âπ≥ÂùáLoss:      3.8948 (train_enhanced.py:171, run_test_after_training())[0m
[32m[2025-11-21 16:29:02]    INFO >> Acc@1:         22.18% (train_enhanced.py:172, run_test_after_training())[0m
[32m[2025-11-21 16:29:02]    INFO >> Acc@5:         60.94% (train_enhanced.py:173, run_test_after_training())[0m
[32m[2025-11-21 16:29:02]    INFO >> Acc@1 (Âê´any): 22.18% (train_enhanced.py:174, run_test_after_training())[0m
[32m[2025-11-21 16:29:02]    INFO >> Acc@5 (Âê´any): 60.94% (train_enhanced.py:175, run_test_after_training())[0m
[32m[2025-11-21 16:29:02]    INFO >> ================================================================================ (train_enhanced.py:176, run_test_after_training())[0m
[32m[2025-11-21 16:29:02]    INFO >> ÊµãËØïÁªìÊûúÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/checkpoints/res.txt (train_enhanced.py:187, run_test_after_training())[0m
[32m[2025-11-21 16:29:02]    INFO >> ËÆ≠ÁªÉÊó•ÂøóÂ∑≤Êõ¥Êñ∞: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs (train_enhanced.py:222, run_test_after_training())[0m
[TrainingLogger] Êó•ÂøóÁõÆÂΩï: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs
[TrainingLogger] ÂéüÂßãËæìÂá∫Â∞Ü‰øùÂ≠òÂà∞: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/training_output.log
[TrainingLogger] Epoch 1 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/metrics.json
[TrainingLogger] Epoch 2 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/metrics.json
[TrainingLogger] Epoch 3 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/metrics.json
[TrainingLogger] Epoch 4 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/metrics.json
[TrainingLogger] Epoch 5 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/metrics.json
[TrainingLogger] Epoch 6 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/metrics.json
[TrainingLogger] Epoch 7 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/metrics.json
[TrainingLogger] Epoch 8 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/metrics.json
[TrainingLogger] Epoch 9 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/metrics.json
[TrainingLogger] Epoch 10 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/metrics.json
[TrainingLogger] Epoch 11 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/metrics.json

‚úì dropout_015 ÊàêÂäü

Á≠âÂæÖ3Áßí...

ËøõÂ∫¶: 3/4

============================================================
ÂÆûÈ™å: dropout_02 - Dropout=0.2 (Âº∫Ê≠£ÂàôÂåñ)
Êó∂Èó¥: 2025-11-21 16:29:45
============================================================

[32m[2025-11-21 16:29:47]    INFO >> Âä†ËΩΩÈÖçÁΩÆ: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/config.yml (train_enhanced.py:666, cli_main())[0m
[32m[2025-11-21 16:29:47]    INFO >> ÂçïGPUËÆ≠ÁªÉ... (train_enhanced.py:694, cli_main())[0m
[32m[2025-11-21 16:29:48]    INFO >> ËÆ≠ÁªÉÊó•ÂøóÂ∞Ü‰øùÂ≠òÂà∞: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/logs (train_enhanced.py:561, single_main())[0m
[32m[2025-11-21 16:29:48]    INFO >> [nodes] dictionary: 9999 types (typilus.py:102, setup_task())[0m
[32m[2025-11-21 16:29:48]    INFO >> [edges] dictionary: 0 types (typilus.py:102, setup_task())[0m
[32m[2025-11-21 16:29:48]    INFO >> [supernodes.annotation] dictionary: 99 types (typilus.py:106, setup_task())[0m
[32m[2025-11-21 16:30:00]    INFO >> Typilus(
  (encoder): GGNNEncoder(
    (node_embedding): Embedding(9999, 64, padding_idx=0)
    (node_layer): Sequential(
      (0): Dropout(p=0.2, inplace=False)
      (1): Linear(in_features=64, out_features=64, bias=False)
      (2): Dropout(p=0.2, inplace=False)
    )
    (ggnns): ModuleList(
      (0): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=64, out_features=64, bias=False)
          (OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (NEXT): Linear(in_features=64, out_features=64, bias=False)
          (SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
          (_CHILD): Linear(in_features=64, out_features=64, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT): Linear(in_features=64, out_features=64, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (_COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (_RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
        )
        (rnn_cell): GRUCell(64, 64)
      )
      (1): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=64, out_features=64, bias=False)
          (OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (NEXT): Linear(in_features=64, out_features=64, bias=False)
          (SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
          (_CHILD): Linear(in_features=64, out_features=64, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT): Linear(in_features=64, out_features=64, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (_COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (_RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
        )
        (rnn_cell): GRUCell(128, 64)
      )
    )
  )
  (decoder): DenseDecoder(
    (cls_layers): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=False)
      (1): Dropout(p=0.2, inplace=False)
      (2): Linear(in_features=64, out_features=99, bias=True)
    )
  )
) (train_enhanced.py:568, single_main())[0m
[32m[2025-11-21 16:30:00]    INFO >> Ê®°Âûã: typilus, ÊçüÂ§±ÂáΩÊï∞: TypilusCriterion (train_enhanced.py:569, single_main())[0m
[32m[2025-11-21 16:30:00]    INFO >> Ê®°ÂûãÂèÇÊï∞: 847843 (ÂèØËÆ≠ÁªÉ: 847843) (train_enhanced.py:570, single_main())[0m
[32m[2025-11-21 16:30:00]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 16:30:00]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 80579 MB ; used memory = 1340 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 16:30:00]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 16:30:00]    INFO >> ‰ΩøÁî® 1 ‰∏™GPUËÆ≠ÁªÉ (train_enhanced.py:576, single_main())[0m
[32m[2025-11-21 16:30:00]    INFO >> no existing checkpoint found /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-21 16:30:00]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[32m[2025-11-21 16:31:11]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-21 16:31:11] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-21 16:31:19]    INFO >> epoch 001:     50 / 1539 loss=5.515, wps=5008.7, ups=6.93, wpb=720, bsz=720, num_updates=50, lr=0.0004, gnorm=7.714, clip=0, train_wall=7, gb_free=74.2, wall=76 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:31:20] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 43.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 73.64 GiB is allocated by PyTorch, and 4.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:31:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:31:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:31:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 2         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75347 MiB |  75407 MiB |   1756 GiB |   1682 GiB |
|       from large pool |  74991 MiB |  75051 MiB |   1743 GiB |   1670 GiB |
|       from small pool |    356 MiB |    357 MiB |     12 GiB |     12 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75347 MiB |  75407 MiB |   1756 GiB |   1682 GiB |
|       from large pool |  74991 MiB |  75051 MiB |   1743 GiB |   1670 GiB |
|       from small pool |    356 MiB |    357 MiB |     12 GiB |     12 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75195 MiB |  75254 MiB |   1750 GiB |   1676 GiB |
|       from large pool |  74841 MiB |  74900 MiB |   1737 GiB |   1664 GiB |
|       from small pool |    354 MiB |    355 MiB |     12 GiB |     12 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80462 MiB |  80498 MiB |  91994 MiB |  11532 MiB |
|       from large pool |  80070 MiB |  80136 MiB |  91590 MiB |  11520 MiB |
|       from small pool |    392 MiB |    394 MiB |    404 MiB |     12 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5054 MiB |   5960 MiB |    875 GiB |    870 GiB |
|       from large pool |   5018 MiB |   5948 MiB |    859 GiB |    855 GiB |
|       from small pool |     35 MiB |     37 MiB |     15 GiB |     14 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6579    |    6582    |  136626    |  130047    |
|       from large pool |     868    |     869    |   57719    |   56851    |
|       from small pool |    5711    |    5714    |   78907    |   73196    |
|---------------------------------------------------------------------------|
| Active allocs         |    6579    |    6582    |  136626    |  130047    |
|       from large pool |     868    |     869    |   57719    |   56851    |
|       from small pool |    5711    |    5714    |   78907    |   73196    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     758    |     793    |    1006    |     248    |
|       from large pool |     562    |     612    |     804    |     242    |
|       from small pool |     196    |     197    |     202    |       6    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     654    |     656    |   86039    |   85385    |
|       from large pool |     306    |     306    |   44256    |   43950    |
|       from small pool |     348    |     350    |   41783    |   41435    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:31:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:31:20] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:31:26]    INFO >> epoch 001:    101 / 1539 loss=5.849, wps=4393.4, ups=7.19, wpb=610.7, bsz=610.7, num_updates=100, lr=0.0004, gnorm=8.46, clip=0, train_wall=6, gb_free=75.6, wall=83 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:31:33]    INFO >> epoch 001:    151 / 1539 loss=6.109, wps=5466.6, ups=6.69, wpb=816.7, bsz=816.7, num_updates=150, lr=0.0004, gnorm=8.618, clip=0, train_wall=7, gb_free=74.2, wall=91 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:31:41]    INFO >> epoch 001:    201 / 1539 loss=6.187, wps=4831.6, ups=7.53, wpb=641.7, bsz=641.7, num_updates=200, lr=0.0004, gnorm=8.557, clip=0, train_wall=6, gb_free=74.8, wall=97 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:31:48]    INFO >> epoch 001:    251 / 1539 loss=6.09, wps=4644.2, ups=7.28, wpb=637.9, bsz=637.9, num_updates=250, lr=0.0004, gnorm=8.407, clip=0, train_wall=6, gb_free=71.5, wall=104 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:31:56]    INFO >> epoch 001:    301 / 1539 loss=6.005, wps=5066.3, ups=6.44, wpb=786.5, bsz=786.5, num_updates=300, lr=0.0004, gnorm=8.337, clip=0, train_wall=7, gb_free=73.8, wall=112 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:32:03]    INFO >> epoch 001:    351 / 1539 loss=6.136, wps=4805, ups=7.15, wpb=671.6, bsz=671.6, num_updates=350, lr=0.0004, gnorm=8.44, clip=0, train_wall=6, gb_free=72.4, wall=119 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:32:12]    INFO >> epoch 001:    401 / 1539 loss=5.9, wps=5284.2, ups=6.2, wpb=851.7, bsz=851.7, num_updates=400, lr=0.0004, gnorm=8.079, clip=0, train_wall=7, gb_free=73.2, wall=127 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:32:22] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 7.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.81 GiB is allocated by PyTorch, and 823.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:32:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:32:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:32:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 4         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79614 MiB |  79674 MiB |  12419 GiB |  12342 GiB |
|       from large pool |  79520 MiB |  79580 MiB |  12346 GiB |  12268 GiB |
|       from small pool |     94 MiB |     95 MiB |     73 GiB |     73 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79614 MiB |  79674 MiB |  12419 GiB |  12342 GiB |
|       from large pool |  79520 MiB |  79580 MiB |  12346 GiB |  12268 GiB |
|       from small pool |     94 MiB |     95 MiB |     73 GiB |     73 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79544 MiB |  79604 MiB |  12379 GiB |  12302 GiB |
|       from large pool |  79450 MiB |  79510 MiB |  12306 GiB |  12228 GiB |
|       from small pool |     93 MiB |     95 MiB |     73 GiB |     73 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80498 MiB |  80500 MiB | 166722 MiB |  86224 MiB |
|       from large pool |  80400 MiB |  80400 MiB | 166242 MiB |  85842 MiB |
|       from small pool |     98 MiB |    392 MiB |    480 MiB |    382 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    823 MiB |   3119 MiB |   6143 GiB |   6143 GiB |
|       from large pool |    819 MiB |   3112 MiB |   6057 GiB |   6057 GiB |
|       from small pool |      3 MiB |     23 MiB |     85 GiB |     85 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2036    |    2039    |     869 K  |     867 K  |
|       from large pool |     477    |     478    |     421 K  |     421 K  |
|       from small pool |    1559    |    1562    |     447 K  |     445 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2036    |    2039    |     869 K  |     867 K  |
|       from large pool |     477    |     478    |     421 K  |     421 K  |
|       from small pool |    1559    |    1562    |     447 K  |     445 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     251    |     757    |    1201    |     950    |
|       from large pool |     202    |     561    |     961    |     759    |
|       from small pool |      49    |     196    |     240    |     191    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     132    |     133    |  540020    |  539888    |
|       from large pool |      79    |      81    |  324754    |  324675    |
|       from small pool |      53    |      55    |  215266    |  215213    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:32:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:32:22] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:32:26]    INFO >> epoch 001:    452 / 1539 loss=5.947, wps=2246.4, ups=3.56, wpb=631.6, bsz=631.6, num_updates=450, lr=0.0004, gnorm=7.921, clip=0, train_wall=7, gb_free=71.8, wall=141 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:32:36]    INFO >> epoch 001:    502 / 1539 loss=5.772, wps=3662.2, ups=4.93, wpb=743.1, bsz=743.1, num_updates=500, lr=0.0004, gnorm=8.356, clip=0, train_wall=9, gb_free=72.6, wall=151 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:32:44]    INFO >> epoch 001:    552 / 1539 loss=5.915, wps=4299.3, ups=6.54, wpb=657, bsz=657, num_updates=550, lr=0.0004, gnorm=8.566, clip=0, train_wall=7, gb_free=65.5, wall=159 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:32:53]    INFO >> epoch 001:    602 / 1539 loss=5.881, wps=4405.3, ups=6.59, wpb=668.6, bsz=668.6, num_updates=600, lr=0.0004, gnorm=7.818, clip=0, train_wall=7, gb_free=73.2, wall=166 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:33:01]    INFO >> epoch 001:    652 / 1539 loss=5.718, wps=4398.6, ups=6.18, wpb=712.2, bsz=712.2, num_updates=650, lr=0.0004, gnorm=8.263, clip=0, train_wall=7, gb_free=73.5, wall=174 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:33:10]    INFO >> epoch 001:    702 / 1539 loss=5.725, wps=3886.1, ups=5.78, wpb=672.5, bsz=672.5, num_updates=700, lr=0.0004, gnorm=8.099, clip=0, train_wall=8, gb_free=74.1, wall=183 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:33:18]    INFO >> epoch 001:    752 / 1539 loss=5.659, wps=4718.3, ups=6.23, wpb=757.4, bsz=757.4, num_updates=750, lr=0.0004, gnorm=8.58, clip=4, train_wall=7, gb_free=73.7, wall=191 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:33:27]    INFO >> epoch 001:    802 / 1539 loss=5.707, wps=4810.2, ups=6.63, wpb=725.2, bsz=725.2, num_updates=800, lr=0.0004, gnorm=8.296, clip=0, train_wall=7, gb_free=73.4, wall=199 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:33:34]    INFO >> epoch 001:    852 / 1539 loss=5.704, wps=4106.2, ups=6.4, wpb=641.1, bsz=641.1, num_updates=850, lr=0.0004, gnorm=8, clip=0, train_wall=7, gb_free=71.8, wall=206 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:33:42]    INFO >> epoch 001:    902 / 1539 loss=5.691, wps=4515.6, ups=6.84, wpb=660.1, bsz=660.1, num_updates=900, lr=0.0004, gnorm=7.702, clip=0, train_wall=7, gb_free=72.1, wall=214 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:33:49]    INFO >> epoch 001:    952 / 1539 loss=5.58, wps=4637.8, ups=6.5, wpb=713.2, bsz=713.2, num_updates=950, lr=0.0004, gnorm=8.003, clip=0, train_wall=7, gb_free=71.8, wall=221 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:33:54] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 901.25 MiB is free. Including non-PyTorch memory, this process has 78.24 GiB memory in use. Of the allocated memory 74.92 GiB is allocated by PyTorch, and 2.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:33:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:33:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:33:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 3            |        cudaMalloc retries: 7         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76322 MiB |  78597 MiB |  28415 GiB |  28340 GiB |
|       from large pool |  76303 MiB |  78579 MiB |  28261 GiB |  28187 GiB |
|       from small pool |     18 MiB |     19 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76322 MiB |  78597 MiB |  28415 GiB |  28340 GiB |
|       from large pool |  76303 MiB |  78579 MiB |  28261 GiB |  28187 GiB |
|       from small pool |     18 MiB |     19 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB |  28342 GiB |  28268 GiB |
|       from large pool |  76285 MiB |  78562 MiB |  28189 GiB |  28114 GiB |
|       from small pool |     18 MiB |     19 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79604 MiB |  79604 MiB | 305536 MiB | 225932 MiB |
|       from large pool |  79580 MiB |  79580 MiB | 304982 MiB | 225402 MiB |
|       from small pool |     24 MiB |     98 MiB |    554 MiB |    530 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3281 MiB |   4789 MiB |  24277 GiB |  24274 GiB |
|       from large pool |   3276 MiB |   4783 MiB |  24099 GiB |  24096 GiB |
|       from small pool |      5 MiB |     27 MiB |    177 GiB |    177 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     638    |     644    |    1881 K  |    1880 K  |
|       from large pool |     340    |     346    |     949 K  |     948 K  |
|       from small pool |     298    |     354    |     931 K  |     931 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     638    |     644    |    1881 K  |    1880 K  |
|       from large pool |     340    |     346    |     949 K  |     948 K  |
|       from small pool |     298    |     354    |     931 K  |     931 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      96    |     123    |    1327    |    1231    |
|       from large pool |      84    |      84    |    1050    |     966    |
|       from small pool |      12    |      49    |     277    |     265    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     115    |     117    |    1087 K  |    1087 K  |
|       from large pool |      93    |      95    |     661 K  |     661 K  |
|       from small pool |      22    |      56    |     426 K  |     426 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:33:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:33:54] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:34:01]    INFO >> epoch 001:   1003 / 1539 loss=5.613, wps=3297.2, ups=5.08, wpb=649.4, bsz=649.4, num_updates=1000, lr=0.0004, gnorm=7.662, clip=0, train_wall=7, gb_free=72.1, wall=231 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:34:09]    INFO >> epoch 001:   1053 / 1539 loss=5.579, wps=5009.7, ups=6.09, wpb=822.6, bsz=822.6, num_updates=1050, lr=0.0004, gnorm=8.316, clip=0, train_wall=7, gb_free=68, wall=240 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:34:17] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 197.25 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 75.66 GiB is allocated by PyTorch, and 2.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:34:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:34:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:34:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 4            |        cudaMalloc retries: 9         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76955 MiB |  77477 MiB |  32758 GiB |  32683 GiB |
|       from large pool |  76942 MiB |  77464 MiB |  32579 GiB |  32504 GiB |
|       from small pool |     12 MiB |     15 MiB |    178 GiB |    178 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76955 MiB |  77477 MiB |  32758 GiB |  32683 GiB |
|       from large pool |  76942 MiB |  77464 MiB |  32579 GiB |  32504 GiB |
|       from small pool |     12 MiB |     15 MiB |    178 GiB |    178 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76936 MiB |  77458 MiB |  32677 GiB |  32602 GiB |
|       from large pool |  76923 MiB |  77445 MiB |  32498 GiB |  32423 GiB |
|       from small pool |     12 MiB |     15 MiB |    178 GiB |    178 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80308 MiB |  80308 MiB | 310972 MiB | 230664 MiB |
|       from large pool |  80284 MiB |  80284 MiB | 310232 MiB | 229948 MiB |
|       from small pool |     24 MiB |    210 MiB |    740 MiB |    716 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3352 MiB |   9143 MiB |  29234 GiB |  29231 GiB |
|       from large pool |   3341 MiB |   9131 MiB |  29028 GiB |  29024 GiB |
|       from small pool |     11 MiB |     23 MiB |    206 GiB |    206 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     604    |     611    |    2166 K  |    2165 K  |
|       from large pool |     315    |     322    |    1085 K  |    1085 K  |
|       from small pool |     289    |     354    |    1080 K  |    1079 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     604    |     611    |    2166 K  |    2165 K  |
|       from large pool |     315    |     322    |    1085 K  |    1085 K  |
|       from small pool |     289    |     354    |    1080 K  |    1079 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      88    |     189    |    1425    |    1337    |
|       from large pool |      76    |      84    |    1055    |     979    |
|       from small pool |      12    |     105    |     370    |     358    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      84    |      84    |    1245 K  |    1245 K  |
|       from large pool |      60    |      60    |     746 K  |     746 K  |
|       from small pool |      24    |      53    |     498 K  |     498 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:34:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:34:17] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:34:18]    INFO >> epoch 001:   1104 / 1539 loss=5.307, wps=4888.2, ups=5.26, wpb=929.6, bsz=929.6, num_updates=1100, lr=0.0004, gnorm=8.569, clip=2, train_wall=8, gb_free=72.8, wall=249 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:34:27]    INFO >> epoch 001:   1154 / 1539 loss=5.573, wps=4548.5, ups=6.57, wpb=692.6, bsz=692.6, num_updates=1150, lr=0.0004, gnorm=8.365, clip=0, train_wall=7, gb_free=73.1, wall=257 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:34:35]    INFO >> epoch 001:   1204 / 1539 loss=5.496, wps=4502, ups=6.64, wpb=678.4, bsz=678.4, num_updates=1200, lr=0.0004, gnorm=7.774, clip=0, train_wall=7, gb_free=71.2, wall=264 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:34:43]    INFO >> epoch 001:   1254 / 1539 loss=5.403, wps=4643.2, ups=6.35, wpb=730.9, bsz=730.9, num_updates=1250, lr=0.0004, gnorm=8.279, clip=0, train_wall=7, gb_free=71.3, wall=272 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:34:51]    INFO >> epoch 001:   1304 / 1539 loss=5.322, wps=4478.3, ups=6.09, wpb=735.4, bsz=735.4, num_updates=1300, lr=0.0004, gnorm=7.823, clip=0, train_wall=7, gb_free=74.3, wall=280 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:34:59]    INFO >> epoch 001:   1354 / 1539 loss=5.304, wps=4258.9, ups=6.48, wpb=657.2, bsz=657.2, num_updates=1350, lr=0.0004, gnorm=8.023, clip=2, train_wall=7, gb_free=73.4, wall=288 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:35:08]    INFO >> epoch 001:   1404 / 1539 loss=5.345, wps=4485.7, ups=6.3, wpb=712.5, bsz=712.5, num_updates=1400, lr=0.0004, gnorm=7.423, clip=0, train_wall=7, gb_free=73.3, wall=296 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:35:16]    INFO >> epoch 001:   1454 / 1539 loss=5.192, wps=4541.7, ups=6.47, wpb=702.2, bsz=702.2, num_updates=1450, lr=0.0004, gnorm=8.06, clip=4, train_wall=7, gb_free=71.6, wall=304 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:35:23]    INFO >> epoch 001:   1504 / 1539 loss=5.195, wps=4442.3, ups=6.38, wpb=695.9, bsz=695.9, num_updates=1500, lr=0.0004, gnorm=7.066, clip=0, train_wall=7, gb_free=70.8, wall=312 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:35:29]    INFO >> epoch 001 | loss 5.659 | wps 4410.2 | ups 6.19 | wpb 712.7 | bsz 712.7 | num_updates 1535 | lr 0.0004 | gnorm 8.126 | clip 0.4 | train_wall 216 | gb_free 76.6 | wall 317 (progress_bar.py:267, print())[0m
[33m[2025-11-21 16:35:29] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:35:46]    INFO >> epoch 001 | valid on 'valid' subset | loss 5.057 | wps 9348.2 | wpb 5412.5 | bsz 5412.5 | num_updates 1535 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
[32m[2025-11-21 16:35:47]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 16:35:47]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/checkpoints/checkpoint_best.pt (epoch 1 @ 1535 updates, score 5.057) (writing took 0.020210 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 16:35:47] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-21 16:35:49]    INFO >> epoch 002:     15 / 1539 loss=5.063, wps=1496.3, ups=2.05, wpb=731.3, bsz=731.3, num_updates=1550, lr=0.0004, gnorm=7.945, clip=0, train_wall=7, gb_free=74.4, wall=336 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:35:56]    INFO >> epoch 002:     65 / 1539 loss=5.119, wps=4610.5, ups=7, wpb=658.8, bsz=658.8, num_updates=1600, lr=0.0004, gnorm=7.069, clip=0, train_wall=7, gb_free=73.4, wall=343 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:36:04]    INFO >> epoch 002:    115 / 1539 loss=5.036, wps=4561.9, ups=6.38, wpb=714.6, bsz=714.6, num_updates=1650, lr=0.0004, gnorm=6.287, clip=0, train_wall=7, gb_free=65.7, wall=351 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:36:13]    INFO >> epoch 002:    165 / 1539 loss=4.604, wps=4773.8, ups=6.46, wpb=738.8, bsz=738.8, num_updates=1700, lr=0.0004, gnorm=7.767, clip=2, train_wall=7, gb_free=73.6, wall=359 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:36:21]    INFO >> epoch 002:    215 / 1539 loss=4.768, wps=4602.5, ups=6.54, wpb=704.2, bsz=704.2, num_updates=1750, lr=0.0004, gnorm=8.423, clip=0, train_wall=7, gb_free=71.2, wall=366 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:36:29]    INFO >> epoch 002:    265 / 1539 loss=4.54, wps=5166.4, ups=5.91, wpb=873.5, bsz=873.5, num_updates=1800, lr=0.0004, gnorm=7.771, clip=2, train_wall=8, gb_free=74.7, wall=375 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:36:37]    INFO >> epoch 002:    315 / 1539 loss=4.737, wps=4400.1, ups=6.82, wpb=645.1, bsz=645.1, num_updates=1850, lr=0.0004, gnorm=7.587, clip=0, train_wall=7, gb_free=71.9, wall=382 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:36:45]    INFO >> epoch 002:    365 / 1539 loss=4.539, wps=4430.6, ups=6.76, wpb=655.8, bsz=655.8, num_updates=1900, lr=0.0004, gnorm=8.484, clip=0, train_wall=7, gb_free=74, wall=390 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:36:53]    INFO >> epoch 002:    415 / 1539 loss=4.483, wps=4469.2, ups=6.27, wpb=713.2, bsz=713.2, num_updates=1950, lr=0.0004, gnorm=7.512, clip=0, train_wall=8, gb_free=76.2, wall=397 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:36:54] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 197.25 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 74.93 GiB is allocated by PyTorch, and 3.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:36:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:36:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:36:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 5            |        cudaMalloc retries: 10        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76327 MiB |  78599 MiB |  61960 GiB |  61886 GiB |
|       from large pool |  76308 MiB |  78581 MiB |  61611 GiB |  61537 GiB |
|       from small pool |     18 MiB |     21 MiB |    348 GiB |    348 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76327 MiB |  78599 MiB |  61960 GiB |  61886 GiB |
|       from large pool |  76308 MiB |  78581 MiB |  61611 GiB |  61537 GiB |
|       from small pool |     18 MiB |     21 MiB |    348 GiB |    348 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB |  61824 GiB |  61750 GiB |
|       from large pool |  76285 MiB |  78562 MiB |  61476 GiB |  61402 GiB |
|       from small pool |     18 MiB |     21 MiB |    348 GiB |    348 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80308 MiB |  80494 MiB | 311158 MiB | 230850 MiB |
|       from large pool |  80284 MiB |  80284 MiB | 310232 MiB | 229948 MiB |
|       from small pool |     24 MiB |    210 MiB |    926 MiB |    902 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3980 MiB |   5490 MiB |  61028 GiB |  61024 GiB |
|       from large pool |   3975 MiB |   5484 MiB |  60632 GiB |  60629 GiB |
|       from small pool |      5 MiB |     27 MiB |    395 GiB |    395 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     640    |     646    |    4090 K  |    4090 K  |
|       from large pool |     340    |     346    |    1969 K  |    1969 K  |
|       from small pool |     300    |     355    |    2121 K  |    2120 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     640    |     646    |    4090 K  |    4090 K  |
|       from large pool |     340    |     346    |    1969 K  |    1969 K  |
|       from small pool |     300    |     355    |    2121 K  |    2120 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      88    |     181    |    1518    |    1430    |
|       from large pool |      76    |      76    |    1055    |     979    |
|       from small pool |      12    |     105    |     463    |     451    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      93    |      99    |    2309 K  |    2309 K  |
|       from large pool |      70    |      76    |    1305 K  |    1305 K  |
|       from small pool |      23    |      51    |    1004 K  |    1004 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:36:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:36:54] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:37:02]    INFO >> epoch 002:    466 / 1539 loss=4.504, wps=4319.5, ups=5.92, wpb=729.4, bsz=729.4, num_updates=2000, lr=0.0004, gnorm=8.487, clip=0, train_wall=7, gb_free=71.6, wall=406 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:37:10]    INFO >> epoch 002:    516 / 1539 loss=4.659, wps=4613.1, ups=6.51, wpb=708.4, bsz=708.4, num_updates=2050, lr=0.0004, gnorm=7.996, clip=0, train_wall=7, gb_free=71.2, wall=414 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:37:18]    INFO >> epoch 002:    566 / 1539 loss=4.53, wps=4133.5, ups=6.64, wpb=622.3, bsz=622.3, num_updates=2100, lr=0.0004, gnorm=8.182, clip=0, train_wall=7, gb_free=74, wall=421 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:37:27]    INFO >> epoch 002:    616 / 1539 loss=4.513, wps=5274.5, ups=6.05, wpb=871.1, bsz=871.1, num_updates=2150, lr=0.0004, gnorm=8.047, clip=0, train_wall=8, gb_free=68.6, wall=429 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:37:35]    INFO >> epoch 002:    666 / 1539 loss=4.264, wps=4794.6, ups=6.2, wpb=772.9, bsz=772.9, num_updates=2200, lr=0.0004, gnorm=9.237, clip=0, train_wall=8, gb_free=68.4, wall=437 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:37:42]    INFO >> epoch 002:    716 / 1539 loss=4.331, wps=4725, ups=6.67, wpb=708.6, bsz=708.6, num_updates=2250, lr=0.0004, gnorm=8.854, clip=2, train_wall=7, gb_free=73.1, wall=445 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:37:51]    INFO >> epoch 002:    766 / 1539 loss=4.579, wps=4309.2, ups=6.48, wpb=665.2, bsz=665.2, num_updates=2300, lr=0.0004, gnorm=6.878, clip=0, train_wall=7, gb_free=75.7, wall=453 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:37:59]    INFO >> epoch 002:    816 / 1539 loss=4.485, wps=4256.7, ups=6.49, wpb=656, bsz=656, num_updates=2350, lr=0.0004, gnorm=8.516, clip=0, train_wall=7, gb_free=72.2, wall=460 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:38:07]    INFO >> epoch 002:    866 / 1539 loss=4.371, wps=4413.1, ups=6.13, wpb=719.8, bsz=719.8, num_updates=2400, lr=0.0004, gnorm=8.333, clip=0, train_wall=8, gb_free=75.7, wall=469 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:38:15]    INFO >> epoch 002:    916 / 1539 loss=4.423, wps=4358.6, ups=6.49, wpb=671.5, bsz=671.5, num_updates=2450, lr=0.0004, gnorm=7.278, clip=0, train_wall=7, gb_free=73.1, wall=476 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:38:24]    INFO >> epoch 002:    966 / 1539 loss=4.308, wps=4215.4, ups=6.61, wpb=637.9, bsz=637.9, num_updates=2500, lr=0.0004, gnorm=8.35, clip=0, train_wall=7, gb_free=69.2, wall=484 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:38:25] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 37.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 77.54 GiB is allocated by PyTorch, and 1.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:38:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:38:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:38:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 6            |        cudaMalloc retries: 12        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79337 MiB |  79397 MiB |  78153 GiB |  78075 GiB |
|       from large pool |  78941 MiB |  79001 MiB |  77714 GiB |  77637 GiB |
|       from small pool |    395 MiB |    396 MiB |    438 GiB |    437 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79337 MiB |  79397 MiB |  78153 GiB |  78075 GiB |
|       from large pool |  78941 MiB |  79001 MiB |  77714 GiB |  77637 GiB |
|       from small pool |    395 MiB |    396 MiB |    438 GiB |    437 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79098 MiB |  79157 MiB |  77983 GiB |  77905 GiB |
|       from large pool |  78705 MiB |  78764 MiB |  77545 GiB |  77468 GiB |
|       from small pool |    393 MiB |    394 MiB |    437 GiB |    437 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80468 MiB |  80470 MiB | 335872 MiB | 255404 MiB |
|       from large pool |  80032 MiB |  80032 MiB | 334532 MiB | 254500 MiB |
|       from small pool |    436 MiB |    438 MiB |   1340 MiB |    904 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1070 MiB |   4610 MiB |  79124 GiB |  79123 GiB |
|       from large pool |   1030 MiB |   4603 MiB |  78625 GiB |  78624 GiB |
|       from small pool |     40 MiB |     41 MiB |    498 GiB |    498 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7296    |    7299    |    5179 K  |    5172 K  |
|       from large pool |     933    |     934    |    2516 K  |    2515 K  |
|       from small pool |    6363    |    6366    |    2663 K  |    2656 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7296    |    7299    |    5179 K  |    5172 K  |
|       from large pool |     933    |     934    |    2516 K  |    2515 K  |
|       from small pool |    6363    |    6366    |    2663 K  |    2656 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     690    |     691    |    2130    |    1440    |
|       from large pool |     472    |     472    |    1460    |     988    |
|       from small pool |     218    |     219    |     670    |     452    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     454    |     454    |    2903 K  |    2902 K  |
|       from large pool |      63    |      63    |    1651 K  |    1651 K  |
|       from small pool |     391    |     391    |    1251 K  |    1251 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:38:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:38:25] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 16:38:30] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 1.25 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 76.95 GiB is allocated by PyTorch, and 1.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:38:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:38:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:38:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 7            |        cudaMalloc retries: 15        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78678 MiB |  78798 MiB |  79117 GiB |  79040 GiB |
|       from large pool |  78593 MiB |  78713 MiB |  78673 GiB |  78596 GiB |
|       from small pool |     85 MiB |     86 MiB |    443 GiB |    443 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78678 MiB |  78798 MiB |  79117 GiB |  79040 GiB |
|       from large pool |  78593 MiB |  78713 MiB |  78673 GiB |  78596 GiB |
|       from small pool |     85 MiB |     86 MiB |    443 GiB |    443 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78642 MiB |  78762 MiB |  78944 GiB |  78868 GiB |
|       from large pool |  78558 MiB |  78677 MiB |  78501 GiB |  78425 GiB |
|       from small pool |     84 MiB |     85 MiB |    443 GiB |    442 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80504 MiB |  80504 MiB | 356340 MiB | 275836 MiB |
|       from large pool |  80416 MiB |  80416 MiB | 354934 MiB | 274518 MiB |
|       from small pool |     88 MiB |    436 MiB |   1406 MiB |   1318 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1705 MiB |   8357 MiB |  80004 GiB |  80002 GiB |
|       from large pool |   1702 MiB |   8349 MiB |  79498 GiB |  79496 GiB |
|       from small pool |      2 MiB |     29 MiB |    505 GiB |    505 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1873    |    1876    |    5242 K  |    5240 K  |
|       from large pool |     462    |     464    |    2546 K  |    2545 K  |
|       from small pool |    1411    |    1414    |    2696 K  |    2694 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1873    |    1876    |    5242 K  |    5240 K  |
|       from large pool |     462    |     464    |    2546 K  |    2545 K  |
|       from small pool |    1411    |    1414    |    2696 K  |    2694 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     221    |     689    |    2198    |    1977    |
|       from large pool |     177    |     471    |    1495    |    1318    |
|       from small pool |      44    |     218    |     703    |     659    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     167    |     168    |    2940 K  |    2940 K  |
|       from large pool |     120    |     125    |    1671 K  |    1671 K  |
|       from small pool |      47    |      59    |    1269 K  |    1269 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:38:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:38:30] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:38:33]    INFO >> epoch 002:   1018 / 1539 loss=4.328, wps=3792.4, ups=5.51, wpb=687.9, bsz=687.9, num_updates=2550, lr=0.0004, gnorm=8.22, clip=0, train_wall=7, gb_free=72.9, wall=493 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:38:41]    INFO >> epoch 002:   1068 / 1539 loss=4.285, wps=4721.7, ups=6.18, wpb=763.7, bsz=763.7, num_updates=2600, lr=0.0004, gnorm=7.389, clip=0, train_wall=8, gb_free=73.5, wall=501 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:38:49]    INFO >> epoch 002:   1118 / 1539 loss=4.352, wps=4732.6, ups=5.98, wpb=791.9, bsz=791.9, num_updates=2650, lr=0.0004, gnorm=7.934, clip=0, train_wall=8, gb_free=69.2, wall=509 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:38:59]    INFO >> epoch 002:   1168 / 1539 loss=4.384, wps=4875.8, ups=6.33, wpb=770.1, bsz=770.1, num_updates=2700, lr=0.0004, gnorm=7.882, clip=0, train_wall=7, gb_free=72.3, wall=517 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:39:06]    INFO >> epoch 002:   1218 / 1539 loss=4.283, wps=4496.4, ups=6.33, wpb=710, bsz=710, num_updates=2750, lr=0.0004, gnorm=7.993, clip=0, train_wall=7, gb_free=70.9, wall=525 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:39:10] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 121.25 MiB is free. Including non-PyTorch memory, this process has 79.00 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 5.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:39:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:39:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:39:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 8            |        cudaMalloc retries: 18        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72821 MiB |  75425 MiB |  86354 GiB |  86283 GiB |
|       from large pool |  72808 MiB |  75412 MiB |  85870 GiB |  85799 GiB |
|       from small pool |     12 MiB |     14 MiB |    483 GiB |    483 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72821 MiB |  75425 MiB |  86354 GiB |  86283 GiB |
|       from large pool |  72808 MiB |  75412 MiB |  85870 GiB |  85799 GiB |
|       from small pool |     12 MiB |     14 MiB |    483 GiB |    483 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72801 MiB |  75403 MiB |  86167 GiB |  86096 GiB |
|       from large pool |  72788 MiB |  75391 MiB |  85684 GiB |  85613 GiB |
|       from small pool |     12 MiB |     14 MiB |    483 GiB |    483 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80384 MiB |  80384 MiB | 387082 MiB | 306698 MiB |
|       from large pool |  80360 MiB |  80360 MiB | 385538 MiB | 305178 MiB |
|       from small pool |     24 MiB |    226 MiB |   1544 MiB |   1520 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5660 MiB |  11732 MiB |  87441 GiB |  87435 GiB |
|       from large pool |   5649 MiB |  11720 MiB |  86889 GiB |  86883 GiB |
|       from small pool |     11 MiB |     25 MiB |    551 GiB |    551 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     603    |     612    |    5719 K  |    5718 K  |
|       from large pool |     312    |     320    |    2781 K  |    2781 K  |
|       from small pool |     291    |     356    |    2937 K  |    2937 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     603    |     612    |    5719 K  |    5718 K  |
|       from large pool |     312    |     320    |    2781 K  |    2781 K  |
|       from small pool |     291    |     356    |    2937 K  |    2937 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     120    |     229    |    2295    |    2175    |
|       from large pool |     108    |     116    |    1523    |    1415    |
|       from small pool |      12    |     113    |     772    |     760    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     118    |     118    |    3207 K  |    3207 K  |
|       from large pool |      91    |      91    |    1823 K  |    1823 K  |
|       from small pool |      27    |      54    |    1383 K  |    1383 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:39:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:39:10] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:39:15]    INFO >> epoch 002:   1269 / 1539 loss=4.219, wps=4501.8, ups=5.88, wpb=765.3, bsz=765.3, num_updates=2800, lr=0.0004, gnorm=8.14, clip=0, train_wall=7, gb_free=70.5, wall=534 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:39:22]    INFO >> epoch 002:   1319 / 1539 loss=4.275, wps=4415.8, ups=6.67, wpb=661.5, bsz=661.5, num_updates=2850, lr=0.0004, gnorm=7.341, clip=2, train_wall=7, gb_free=75, wall=541 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:39:32]    INFO >> epoch 002:   1369 / 1539 loss=4.061, wps=4691.1, ups=6.44, wpb=728.5, bsz=728.5, num_updates=2900, lr=0.0004, gnorm=8.303, clip=0, train_wall=7, gb_free=70.5, wall=549 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:39:39]    INFO >> epoch 002:   1419 / 1539 loss=4.323, wps=4243.9, ups=6.49, wpb=653.6, bsz=653.6, num_updates=2950, lr=0.0004, gnorm=7.464, clip=0, train_wall=7, gb_free=64.8, wall=557 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:39:49]    INFO >> epoch 002:   1469 / 1539 loss=4.18, wps=3543.2, ups=5.04, wpb=702.5, bsz=702.5, num_updates=3000, lr=0.0004, gnorm=8.05, clip=0, train_wall=9, gb_free=70.3, wall=566 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:39:57]    INFO >> epoch 002:   1519 / 1539 loss=4.312, wps=4189.2, ups=6.22, wpb=673.3, bsz=673.3, num_updates=3050, lr=0.0004, gnorm=7.71, clip=0, train_wall=7, gb_free=74.2, wall=575 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:40:01]    INFO >> epoch 002 | loss 4.459 | wps 4193.8 | ups 5.88 | wpb 712.7 | bsz 712.7 | num_updates 3070 | lr 0.0004 | gnorm 7.921 | clip 0.3 | train_wall 227 | gb_free 72.4 | wall 578 (progress_bar.py:267, print())[0m
[33m[2025-11-21 16:40:01] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:40:17]    INFO >> epoch 002 | valid on 'valid' subset | loss 4.036 | wps 10188.5 | wpb 5412.5 | bsz 5412.5 | num_updates 3070 | best_loss 5.057 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 16:40:17]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 16:40:17]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/checkpoints/checkpoint_last.pt (epoch 2 @ 3070 updates, score 4.036) (writing took 0.013673 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 16:40:17] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:40:22]    INFO >> epoch 003:     30 / 1539 loss=4.212, wps=1471.7, ups=2.16, wpb=681.1, bsz=681.1, num_updates=3100, lr=0.000392, gnorm=7.674, clip=0, train_wall=7, gb_free=70.7, wall=598 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:40:30]    INFO >> epoch 003:     80 / 1539 loss=4.109, wps=4755.1, ups=6.15, wpb=772.8, bsz=772.8, num_updates=3150, lr=0.000392, gnorm=7.837, clip=0, train_wall=8, gb_free=73.4, wall=606 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:40:33] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.08 GiB is free. Including non-PyTorch memory, this process has 78.04 GiB memory in use. Of the allocated memory 74.93 GiB is allocated by PyTorch, and 2.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:40:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:40:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:40:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 9            |        cudaMalloc retries: 21        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76326 MiB |  78600 MiB | 102451 GiB | 102377 GiB |
|       from large pool |  76307 MiB |  78583 MiB | 101871 GiB | 101796 GiB |
|       from small pool |     18 MiB |     19 MiB |    580 GiB |    580 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76326 MiB |  78600 MiB | 102451 GiB | 102377 GiB |
|       from large pool |  76307 MiB |  78583 MiB | 101871 GiB | 101796 GiB |
|       from small pool |     18 MiB |     19 MiB |    580 GiB |    580 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB | 102235 GiB | 102160 GiB |
|       from large pool |  76285 MiB |  78562 MiB | 101655 GiB | 101581 GiB |
|       from small pool |     18 MiB |     19 MiB |    579 GiB |    579 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79400 MiB |  79400 MiB | 483464 MiB | 404064 MiB |
|       from large pool |  79376 MiB |  79376 MiB | 481770 MiB | 402394 MiB |
|       from small pool |     24 MiB |     74 MiB |   1694 MiB |   1670 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3073 MiB |   4962 MiB | 103110 GiB | 103107 GiB |
|       from large pool |   3068 MiB |   4948 MiB | 102453 GiB | 102450 GiB |
|       from small pool |      5 MiB |     27 MiB |    657 GiB |    657 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     640    |     646    |    6746 K  |    6745 K  |
|       from large pool |     340    |     346    |    3209 K  |    3208 K  |
|       from small pool |     300    |     356    |    3536 K  |    3536 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     640    |     646    |    6746 K  |    6745 K  |
|       from large pool |     340    |     346    |    3209 K  |    3208 K  |
|       from small pool |     300    |     356    |    3536 K  |    3536 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     106    |     118    |    2420    |    2314    |
|       from large pool |      94    |      94    |    1573    |    1479    |
|       from small pool |      12    |      37    |     847    |     835    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     110    |     112    |    3792 K  |    3792 K  |
|       from large pool |      85    |      87    |    2095 K  |    2095 K  |
|       from small pool |      25    |      57    |    1696 K  |    1696 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:40:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:40:33] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:40:41]    INFO >> epoch 003:    131 / 1539 loss=4.031, wps=4387.5, ups=5.25, wpb=835, bsz=835, num_updates=3200, lr=0.000392, gnorm=8.188, clip=2, train_wall=7, gb_free=71.9, wall=615 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:40:48]    INFO >> epoch 003:    181 / 1539 loss=4.326, wps=4381.7, ups=6.67, wpb=656.8, bsz=656.8, num_updates=3250, lr=0.000392, gnorm=6.821, clip=0, train_wall=7, gb_free=73, wall=623 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:40:53] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 153.25 MiB is free. Including non-PyTorch memory, this process has 78.97 GiB memory in use. Of the allocated memory 75.67 GiB is allocated by PyTorch, and 2.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:40:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:40:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:40:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 10           |        cudaMalloc retries: 23        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76958 MiB |  77481 MiB | 105988 GiB | 105912 GiB |
|       from large pool |  76945 MiB |  77468 MiB | 105386 GiB | 105311 GiB |
|       from small pool |     12 MiB |     21 MiB |    601 GiB |    601 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76958 MiB |  77481 MiB | 105988 GiB | 105912 GiB |
|       from large pool |  76945 MiB |  77468 MiB | 105386 GiB | 105311 GiB |
|       from small pool |     12 MiB |     21 MiB |    601 GiB |    601 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76936 MiB |  77458 MiB | 105764 GiB | 105689 GiB |
|       from large pool |  76923 MiB |  77445 MiB | 105164 GiB | 105089 GiB |
|       from small pool |     12 MiB |     21 MiB |    600 GiB |    600 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80352 MiB |  80352 MiB | 489864 MiB | 409512 MiB |
|       from large pool |  80326 MiB |  80326 MiB | 487984 MiB | 407658 MiB |
|       from small pool |     26 MiB |    210 MiB |   1880 MiB |   1854 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3393 MiB |   8163 MiB | 107064 GiB | 107061 GiB |
|       from large pool |   3380 MiB |   8148 MiB | 106383 GiB | 106380 GiB |
|       from small pool |     13 MiB |     27 MiB |    681 GiB |    681 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |    6985 K  |    6984 K  |
|       from large pool |     315    |     322    |    3323 K  |    3323 K  |
|       from small pool |     291    |     356    |    3662 K  |    3661 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |    6985 K  |    6984 K  |
|       from large pool |     315    |     322    |    3323 K  |    3323 K  |
|       from small pool |     291    |     356    |    3662 K  |    3661 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     100    |     199    |    2520    |    2420    |
|       from large pool |      87    |      94    |    1580    |    1493    |
|       from small pool |      13    |     105    |     940    |     927    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      95    |      95    |    3925 K  |    3925 K  |
|       from large pool |      69    |      69    |    2166 K  |    2166 K  |
|       from small pool |      26    |      58    |    1758 K  |    1758 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:40:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:40:53] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:40:57]    INFO >> epoch 003:    232 / 1539 loss=3.756, wps=4338.4, ups=5.65, wpb=767.2, bsz=767.2, num_updates=3300, lr=0.000392, gnorm=7.003, clip=2, train_wall=8, gb_free=74, wall=632 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:41:05]    INFO >> epoch 003:    282 / 1539 loss=4.139, wps=4827.6, ups=6.42, wpb=751.9, bsz=751.9, num_updates=3350, lr=0.000392, gnorm=7.049, clip=0, train_wall=7, gb_free=70.6, wall=639 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:41:15]    INFO >> epoch 003:    332 / 1539 loss=4.219, wps=4642.5, ups=5.87, wpb=790.4, bsz=790.4, num_updates=3400, lr=0.000392, gnorm=7.435, clip=0, train_wall=8, gb_free=73.8, wall=648 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:41:23]    INFO >> epoch 003:    382 / 1539 loss=4.173, wps=4160.8, ups=6.04, wpb=688.4, bsz=688.4, num_updates=3450, lr=0.000392, gnorm=7.405, clip=0, train_wall=8, gb_free=72.5, wall=656 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:41:31]    INFO >> epoch 003:    432 / 1539 loss=4.213, wps=4199.8, ups=6.38, wpb=657.8, bsz=657.8, num_updates=3500, lr=0.000392, gnorm=7.121, clip=0, train_wall=7, gb_free=66.2, wall=664 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:41:39]    INFO >> epoch 003:    482 / 1539 loss=4.058, wps=4665, ups=6.19, wpb=754, bsz=754, num_updates=3550, lr=0.000392, gnorm=7.894, clip=0, train_wall=8, gb_free=73.1, wall=672 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:41:48]    INFO >> epoch 003:    532 / 1539 loss=3.999, wps=5004.9, ups=6.5, wpb=770.5, bsz=770.5, num_updates=3600, lr=0.000392, gnorm=8.388, clip=4, train_wall=7, gb_free=73.8, wall=680 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:41:56]    INFO >> epoch 003:    582 / 1539 loss=3.924, wps=4496.4, ups=6.24, wpb=720.1, bsz=720.1, num_updates=3650, lr=0.000392, gnorm=8.492, clip=0, train_wall=8, gb_free=71.6, wall=688 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:42:03]    INFO >> epoch 003:    632 / 1539 loss=4.121, wps=4620.3, ups=6.85, wpb=674.5, bsz=674.5, num_updates=3700, lr=0.000392, gnorm=7.737, clip=0, train_wall=7, gb_free=66.5, wall=695 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:42:11]    INFO >> epoch 003:    682 / 1539 loss=4.2, wps=4850.7, ups=6.41, wpb=756.5, bsz=756.5, num_updates=3750, lr=0.000392, gnorm=6.895, clip=0, train_wall=7, gb_free=75.1, wall=703 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:42:21]    INFO >> epoch 003:    732 / 1539 loss=4.108, wps=4570.9, ups=5.64, wpb=810.3, bsz=810.3, num_updates=3800, lr=0.000392, gnorm=6.94, clip=0, train_wall=8, gb_free=73.9, wall=712 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:42:29]    INFO >> epoch 003:    782 / 1539 loss=3.982, wps=4947.9, ups=6.22, wpb=795, bsz=795, num_updates=3850, lr=0.000392, gnorm=7.069, clip=0, train_wall=8, gb_free=73.7, wall=720 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:42:30] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 29.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.96 GiB is allocated by PyTorch, and 1.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:42:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:42:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:42:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 11           |        cudaMalloc retries: 26        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78744 MiB |  78804 MiB | 123508 GiB | 123431 GiB |
|       from large pool |  78658 MiB |  78718 MiB | 122809 GiB | 122733 GiB |
|       from small pool |     85 MiB |     86 MiB |    698 GiB |    697 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78744 MiB |  78804 MiB | 123508 GiB | 123431 GiB |
|       from large pool |  78658 MiB |  78718 MiB | 122809 GiB | 122733 GiB |
|       from small pool |     85 MiB |     86 MiB |    698 GiB |    697 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78703 MiB |  78762 MiB | 123247 GiB | 123170 GiB |
|       from large pool |  78617 MiB |  78677 MiB | 122550 GiB | 122473 GiB |
|       from small pool |     85 MiB |     86 MiB |    697 GiB |    696 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80476 MiB |  80476 MiB | 517426 MiB | 436950 MiB |
|       from large pool |  80386 MiB |  80386 MiB | 515324 MiB | 434938 MiB |
|       from small pool |     90 MiB |    246 MiB |   2102 MiB |   2012 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1671 MiB |   7867 MiB | 124786 GiB | 124784 GiB |
|       from large pool |   1667 MiB |   7857 MiB | 123993 GiB | 123991 GiB |
|       from small pool |      4 MiB |     27 MiB |    792 GiB |    792 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1884    |    1887    |    8145 K  |    8143 K  |
|       from large pool |     463    |     464    |    3900 K  |    3900 K  |
|       from small pool |    1421    |    1424    |    4244 K  |    4243 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1884    |    1887    |    8145 K  |    8143 K  |
|       from large pool |     463    |     464    |    3900 K  |    3900 K  |
|       from small pool |    1421    |    1424    |    4244 K  |    4243 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     225    |     300    |    2734    |    2509    |
|       from large pool |     180    |     180    |    1683    |    1503    |
|       from small pool |      45    |     123    |    1051    |    1006    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     169    |     169    |    4572 K  |    4572 K  |
|       from large pool |     121    |     124    |    2540 K  |    2540 K  |
|       from small pool |      48    |      51    |    2031 K  |    2031 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:42:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:42:30] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:42:38]    INFO >> epoch 003:    833 / 1539 loss=3.956, wps=4367.9, ups=5.93, wpb=736, bsz=736, num_updates=3900, lr=0.000392, gnorm=8.913, clip=2, train_wall=7, gb_free=73, wall=728 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:42:46]    INFO >> epoch 003:    883 / 1539 loss=4.193, wps=4527.2, ups=6.31, wpb=717, bsz=717, num_updates=3950, lr=0.000392, gnorm=7.44, clip=0, train_wall=7, gb_free=72.5, wall=736 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:42:55]    INFO >> epoch 003:    933 / 1539 loss=4.024, wps=4472.8, ups=6.13, wpb=729.5, bsz=729.5, num_updates=4000, lr=0.000392, gnorm=8.356, clip=2, train_wall=8, gb_free=67.3, wall=744 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:43:04]    INFO >> epoch 003:    983 / 1539 loss=4.17, wps=3726.8, ups=5.45, wpb=684.3, bsz=684.3, num_updates=4050, lr=0.000392, gnorm=7.766, clip=0, train_wall=8, gb_free=71.5, wall=754 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:43:12]    INFO >> epoch 003:   1033 / 1539 loss=4.166, wps=4129.9, ups=6.49, wpb=636.3, bsz=636.3, num_updates=4100, lr=0.000392, gnorm=6.516, clip=0, train_wall=7, gb_free=68.3, wall=761 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:43:19]    INFO >> epoch 003:   1083 / 1539 loss=4.035, wps=4547.1, ups=6.75, wpb=673.9, bsz=673.9, num_updates=4150, lr=0.000392, gnorm=7.616, clip=0, train_wall=7, gb_free=72.9, wall=769 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:43:28] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 39.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 76.52 GiB is allocated by PyTorch, and 2.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:43:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:43:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:43:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 12           |        cudaMalloc retries: 28        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78301 MiB |  78361 MiB | 133746 GiB | 133670 GiB |
|       from large pool |  77916 MiB |  77976 MiB | 132993 GiB | 132917 GiB |
|       from small pool |    385 MiB |    386 MiB |    753 GiB |    752 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78301 MiB |  78361 MiB | 133746 GiB | 133670 GiB |
|       from large pool |  77916 MiB |  77976 MiB | 132993 GiB | 132917 GiB |
|       from small pool |    385 MiB |    386 MiB |    753 GiB |    752 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78077 MiB |  78137 MiB | 133463 GiB | 133387 GiB |
|       from large pool |  77694 MiB |  77753 MiB | 132711 GiB | 132635 GiB |
|       from small pool |    383 MiB |    384 MiB |    752 GiB |    751 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80466 MiB |  80504 MiB | 539302 MiB | 458836 MiB |
|       from large pool |  80042 MiB |  80326 MiB | 536864 MiB | 456822 MiB |
|       from small pool |    424 MiB |    426 MiB |   2438 MiB |   2014 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2104 MiB |   6644 MiB | 134065 GiB | 134063 GiB |
|       from large pool |   2065 MiB |   6637 MiB | 133209 GiB | 133207 GiB |
|       from small pool |     38 MiB |     40 MiB |    856 GiB |    856 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7109    |    7112    |    8817 K  |    8810 K  |
|       from large pool |     916    |     917    |    4239 K  |    4238 K  |
|       from small pool |    6193    |    6196    |    4578 K  |    4572 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7109    |    7112    |    8817 K  |    8810 K  |
|       from large pool |     916    |     917    |    4239 K  |    4238 K  |
|       from small pool |    6193    |    6196    |    4578 K  |    4572 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     742    |     743    |    3261    |    2519    |
|       from large pool |     530    |     530    |    2042    |    1512    |
|       from small pool |     212    |     213    |    1219    |    1007    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     510    |     511    |    4950 K  |    4949 K  |
|       from large pool |     132    |     132    |    2765 K  |    2765 K  |
|       from small pool |     378    |     379    |    2185 K  |    2184 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:43:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:43:28] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:43:29]    INFO >> epoch 003:   1134 / 1539 loss=3.963, wps=4096.3, ups=5.98, wpb=684.8, bsz=684.8, num_updates=4200, lr=0.000392, gnorm=7.033, clip=0, train_wall=7, gb_free=73, wall=777 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:43:37]    INFO >> epoch 003:   1184 / 1539 loss=3.92, wps=4271.7, ups=6.34, wpb=673.6, bsz=673.6, num_updates=4250, lr=0.000392, gnorm=7.178, clip=0, train_wall=7, gb_free=73.5, wall=785 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:43:45]    INFO >> epoch 003:   1234 / 1539 loss=4.122, wps=4450, ups=6.2, wpb=718.1, bsz=718.1, num_updates=4300, lr=0.000392, gnorm=7.49, clip=0, train_wall=8, gb_free=70.5, wall=793 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:43:52]    INFO >> epoch 003:   1284 / 1539 loss=3.815, wps=4194.3, ups=6.77, wpb=619.7, bsz=619.7, num_updates=4350, lr=0.000392, gnorm=7.898, clip=0, train_wall=7, gb_free=73.5, wall=800 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:44:01]    INFO >> epoch 003:   1334 / 1539 loss=3.961, wps=4736.7, ups=6.69, wpb=708, bsz=708, num_updates=4400, lr=0.000392, gnorm=7.688, clip=2, train_wall=7, gb_free=72.1, wall=808 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:44:09]    INFO >> epoch 003:   1384 / 1539 loss=3.893, wps=4580.4, ups=6.61, wpb=692.7, bsz=692.7, num_updates=4450, lr=0.000392, gnorm=8.171, clip=0, train_wall=7, gb_free=74.2, wall=815 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:44:16]    INFO >> epoch 003:   1434 / 1539 loss=4.137, wps=4272.8, ups=6.56, wpb=651.7, bsz=651.7, num_updates=4500, lr=0.000392, gnorm=7.008, clip=0, train_wall=7, gb_free=72.1, wall=823 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:44:24]    INFO >> epoch 003:   1484 / 1539 loss=4.087, wps=4237.8, ups=6.43, wpb=658.9, bsz=658.9, num_updates=4550, lr=0.000392, gnorm=6.768, clip=0, train_wall=7, gb_free=72.4, wall=831 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:44:33]    INFO >> epoch 003:   1534 / 1539 loss=4.107, wps=4358.5, ups=6.58, wpb=662.9, bsz=662.9, num_updates=4600, lr=0.000392, gnorm=7.377, clip=0, train_wall=7, gb_free=72, wall=838 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:44:34]    INFO >> epoch 003 | loss 4.065 | wps 4188.6 | ups 5.88 | wpb 712.7 | bsz 712.7 | num_updates 4605 | lr 0.000392 | gnorm 7.506 | clip 0.5 | train_wall 227 | gb_free 74.4 | wall 839 (progress_bar.py:267, print())[0m
[33m[2025-11-21 16:44:34] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:44:48]    INFO >> epoch 003 | valid on 'valid' subset | loss 3.894 | wps 10313.5 | wpb 5412.5 | bsz 5412.5 | num_updates 4605 | best_loss 5.057 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 16:44:49]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 16:44:49]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/checkpoints/checkpoint_last.pt (epoch 3 @ 4605 updates, score 3.894) (writing took 0.017205 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 16:44:49] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:44:56]    INFO >> epoch 004:     45 / 1539 loss=3.916, wps=1627.7, ups=2.17, wpb=751.7, bsz=751.7, num_updates=4650, lr=0.000376, gnorm=7.457, clip=2, train_wall=8, gb_free=72.7, wall=861 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:45:04]    INFO >> epoch 004:     95 / 1539 loss=3.937, wps=4650.3, ups=6.15, wpb=756, bsz=756, num_updates=4700, lr=0.000376, gnorm=7.655, clip=0, train_wall=8, gb_free=67.8, wall=870 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:45:16]    INFO >> epoch 004:    145 / 1539 loss=3.66, wps=5130.8, ups=4.89, wpb=1048.6, bsz=1048.6, num_updates=4750, lr=0.000376, gnorm=7.882, clip=0, train_wall=10, gb_free=75.1, wall=880 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:45:23]    INFO >> epoch 004:    195 / 1539 loss=3.967, wps=4667.6, ups=6.66, wpb=701.3, bsz=701.3, num_updates=4800, lr=0.000376, gnorm=7.653, clip=0, train_wall=7, gb_free=71.9, wall=887 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:45:31]    INFO >> epoch 004:    245 / 1539 loss=4.015, wps=4705.1, ups=6.33, wpb=743.6, bsz=743.6, num_updates=4850, lr=0.000376, gnorm=7.586, clip=2, train_wall=7, gb_free=73.5, wall=895 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:45:40]    INFO >> epoch 004:    295 / 1539 loss=4, wps=4331.2, ups=6.69, wpb=647.4, bsz=647.4, num_updates=4900, lr=0.000376, gnorm=6.595, clip=0, train_wall=7, gb_free=74.6, wall=903 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:45:48]    INFO >> epoch 004:    345 / 1539 loss=3.975, wps=4592.4, ups=6.68, wpb=687.4, bsz=687.4, num_updates=4950, lr=0.000376, gnorm=7.273, clip=0, train_wall=7, gb_free=69.9, wall=910 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:45:55]    INFO >> epoch 004:    395 / 1539 loss=3.702, wps=4865.1, ups=6.39, wpb=761.1, bsz=761.1, num_updates=5000, lr=0.000376, gnorm=7.834, clip=0, train_wall=7, gb_free=67.8, wall=918 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:46:04]    INFO >> epoch 004:    445 / 1539 loss=4.014, wps=3682.8, ups=5.84, wpb=630.5, bsz=630.5, num_updates=5050, lr=0.000376, gnorm=7.415, clip=0, train_wall=8, gb_free=71.4, wall=927 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:46:12]    INFO >> epoch 004:    495 / 1539 loss=4.002, wps=4571.4, ups=6.35, wpb=720.1, bsz=720.1, num_updates=5100, lr=0.000376, gnorm=6.87, clip=0, train_wall=7, gb_free=74, wall=934 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:46:21]    INFO >> epoch 004:    545 / 1539 loss=4.043, wps=4443.1, ups=6.33, wpb=701.6, bsz=701.6, num_updates=5150, lr=0.000376, gnorm=8.093, clip=0, train_wall=7, gb_free=73.5, wall=942 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:46:29]    INFO >> epoch 004:    595 / 1539 loss=3.89, wps=4131.4, ups=6.56, wpb=629.8, bsz=629.8, num_updates=5200, lr=0.000376, gnorm=6.991, clip=0, train_wall=7, gb_free=71, wall=950 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:46:30] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 13.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 77.38 GiB is allocated by PyTorch, and 1.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:46:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:46:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:46:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 13           |        cudaMalloc retries: 30        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79173 MiB |  79233 MiB | 167559 GiB | 167482 GiB |
|       from large pool |  79083 MiB |  79143 MiB | 166611 GiB | 166534 GiB |
|       from small pool |     89 MiB |     91 MiB |    948 GiB |    948 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79173 MiB |  79233 MiB | 167559 GiB | 167482 GiB |
|       from large pool |  79083 MiB |  79143 MiB | 166611 GiB | 166534 GiB |
|       from small pool |     89 MiB |     91 MiB |    948 GiB |    948 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79123 MiB |  79183 MiB | 167207 GiB | 167130 GiB |
|       from large pool |  79034 MiB |  79093 MiB | 166260 GiB | 166183 GiB |
|       from small pool |     89 MiB |     90 MiB |    946 GiB |    946 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80492 MiB |  80492 MiB | 564646 MiB | 484154 MiB |
|       from large pool |  80398 MiB |  80398 MiB | 562140 MiB | 481742 MiB |
|       from small pool |     94 MiB |     94 MiB |   2506 MiB |   2412 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1258 MiB |   7162 MiB | 164239 GiB | 164238 GiB |
|       from large pool |   1254 MiB |   7152 MiB | 163165 GiB | 163163 GiB |
|       from small pool |      4 MiB |     27 MiB |   1074 GiB |   1074 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1961    |    1964    |   11044 K  |   11042 K  |
|       from large pool |     470    |     471    |    5271 K  |    5271 K  |
|       from small pool |    1491    |    1494    |    5772 K  |    5770 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1961    |    1964    |   11044 K  |   11042 K  |
|       from large pool |     470    |     471    |    5271 K  |    5271 K  |
|       from small pool |    1491    |    1494    |    5772 K  |    5770 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     223    |     223    |    3348    |    3125    |
|       from large pool |     176    |     176    |    2095    |    1919    |
|       from small pool |      47    |      47    |    1253    |    1206    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     140    |     140    |    6216 K  |    6216 K  |
|       from large pool |      92    |      96    |    3444 K  |    3444 K  |
|       from small pool |      48    |      58    |    2772 K  |    2772 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:46:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:46:30] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:46:37]    INFO >> epoch 004:    646 / 1539 loss=3.857, wps=4123.7, ups=6.14, wpb=671.1, bsz=671.1, num_updates=5250, lr=0.000376, gnorm=7.647, clip=0, train_wall=7, gb_free=68.7, wall=958 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:46:46] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 983.25 MiB is free. Including non-PyTorch memory, this process has 78.16 GiB memory in use. Of the allocated memory 74.92 GiB is allocated by PyTorch, and 2.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:46:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:46:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:46:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 14           |        cudaMalloc retries: 32        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76323 MiB |  78596 MiB | 170178 GiB | 170104 GiB |
|       from large pool |  76304 MiB |  78579 MiB | 169217 GiB | 169142 GiB |
|       from small pool |     18 MiB |     24 MiB |    961 GiB |    961 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76323 MiB |  78596 MiB | 170178 GiB | 170104 GiB |
|       from large pool |  76304 MiB |  78579 MiB | 169217 GiB | 169142 GiB |
|       from small pool |     18 MiB |     24 MiB |    961 GiB |    961 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB | 169821 GiB | 169746 GiB |
|       from large pool |  76285 MiB |  78562 MiB | 168861 GiB | 168786 GiB |
|       from small pool |     18 MiB |     24 MiB |    960 GiB |    959 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79522 MiB |  80432 MiB | 637044 MiB | 557522 MiB |
|       from large pool |  79496 MiB |  80338 MiB | 634538 MiB | 555042 MiB |
|       from small pool |     26 MiB |     94 MiB |   2506 MiB |   2480 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3198 MiB |   4143 MiB | 166735 GiB | 166732 GiB |
|       from large pool |   3191 MiB |   4135 MiB | 165646 GiB | 165643 GiB |
|       from small pool |      7 MiB |     23 MiB |   1089 GiB |   1089 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     640    |     646    |   11213 K  |   11212 K  |
|       from large pool |     340    |     346    |    5361 K  |    5361 K  |
|       from small pool |     300    |     356    |    5852 K  |    5851 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     640    |     646    |   11213 K  |   11212 K  |
|       from large pool |     340    |     346    |    5361 K  |    5361 K  |
|       from small pool |     300    |     356    |    5852 K  |    5851 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     107    |     222    |    3403    |    3296    |
|       from large pool |      94    |     175    |    2150    |    2056    |
|       from small pool |      13    |      47    |    1253    |    1240    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     126    |     128    |    6309 K  |    6309 K  |
|       from large pool |      98    |     100    |    3503 K  |    3503 K  |
|       from small pool |      28    |      51    |    2805 K  |    2805 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:46:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:46:46] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:46:48]    INFO >> epoch 004:    697 / 1539 loss=3.98, wps=3196.1, ups=5.02, wpb=636.6, bsz=636.6, num_updates=5300, lr=0.000376, gnorm=6.305, clip=0, train_wall=7, gb_free=67.8, wall=968 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:46:57]    INFO >> epoch 004:    747 / 1539 loss=3.954, wps=3839.5, ups=5.7, wpb=673.1, bsz=673.1, num_updates=5350, lr=0.000376, gnorm=6.309, clip=0, train_wall=8, gb_free=72.9, wall=977 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:47:05]    INFO >> epoch 004:    797 / 1539 loss=3.871, wps=4547.7, ups=6.4, wpb=710.1, bsz=710.1, num_updates=5400, lr=0.000376, gnorm=6.595, clip=0, train_wall=7, gb_free=72.3, wall=985 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:47:13]    INFO >> epoch 004:    847 / 1539 loss=3.833, wps=4767.3, ups=6.38, wpb=747.5, bsz=747.5, num_updates=5450, lr=0.000376, gnorm=7.457, clip=0, train_wall=7, gb_free=63.6, wall=992 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:47:22]    INFO >> epoch 004:    897 / 1539 loss=3.957, wps=4715.5, ups=6.06, wpb=778.3, bsz=778.3, num_updates=5500, lr=0.000376, gnorm=7.029, clip=0, train_wall=8, gb_free=70.2, wall=1001 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:47:30]    INFO >> epoch 004:    947 / 1539 loss=3.888, wps=4281.7, ups=6.62, wpb=646.7, bsz=646.7, num_updates=5550, lr=0.000376, gnorm=6.039, clip=0, train_wall=7, gb_free=67.5, wall=1008 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:47:38]    INFO >> epoch 004:    997 / 1539 loss=3.835, wps=4649.3, ups=6.08, wpb=764.9, bsz=764.9, num_updates=5600, lr=0.000376, gnorm=7.358, clip=0, train_wall=8, gb_free=75.5, wall=1017 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:47:46]    INFO >> epoch 004:   1047 / 1539 loss=3.979, wps=4145.4, ups=6.6, wpb=628.6, bsz=628.6, num_updates=5650, lr=0.000376, gnorm=6.574, clip=0, train_wall=7, gb_free=72.5, wall=1024 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:47:55]    INFO >> epoch 004:   1097 / 1539 loss=3.947, wps=4754, ups=6.52, wpb=729.6, bsz=729.6, num_updates=5700, lr=0.000376, gnorm=6.274, clip=0, train_wall=7, gb_free=71.1, wall=1032 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:48:02]    INFO >> epoch 004:   1147 / 1539 loss=4.057, wps=4327.8, ups=6.4, wpb=676.4, bsz=676.4, num_updates=5750, lr=0.000376, gnorm=6.472, clip=0, train_wall=7, gb_free=56.5, wall=1040 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:48:10]    INFO >> epoch 004:   1197 / 1539 loss=3.845, wps=4733.6, ups=6.27, wpb=754.7, bsz=754.7, num_updates=5800, lr=0.000376, gnorm=7.415, clip=0, train_wall=7, gb_free=75.6, wall=1048 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:48:18]    INFO >> epoch 004:   1247 / 1539 loss=3.823, wps=4305.4, ups=6.86, wpb=627.8, bsz=627.8, num_updates=5850, lr=0.000376, gnorm=6.411, clip=0, train_wall=7, gb_free=75.6, wall=1055 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:48:26]    INFO >> epoch 004:   1297 / 1539 loss=3.885, wps=3948.4, ups=6.13, wpb=644.6, bsz=644.6, num_updates=5900, lr=0.000376, gnorm=6.982, clip=2, train_wall=8, gb_free=74.6, wall=1063 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:48:35]    INFO >> epoch 004:   1347 / 1539 loss=3.725, wps=5013.9, ups=6.26, wpb=801.1, bsz=801.1, num_updates=5950, lr=0.000376, gnorm=7.719, clip=0, train_wall=7, gb_free=66.8, wall=1071 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:48:43] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.29 GiB memory in use. Of the allocated memory 73.80 GiB is allocated by PyTorch, and 2.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:48:43] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:48:43] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:48:43] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 15           |        cudaMalloc retries: 34        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75053 MiB |  75996 MiB | 190873 GiB | 190800 GiB |
|       from large pool |  75040 MiB |  75983 MiB | 189802 GiB | 189729 GiB |
|       from small pool |     12 MiB |     13 MiB |   1070 GiB |   1070 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75053 MiB |  75996 MiB | 190873 GiB | 190800 GiB |
|       from large pool |  75040 MiB |  75983 MiB | 189802 GiB | 189729 GiB |
|       from small pool |     12 MiB |     13 MiB |   1070 GiB |   1070 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75035 MiB |  75977 MiB | 190472 GiB | 190399 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 189403 GiB | 189330 GiB |
|       from small pool |     12 MiB |     13 MiB |   1069 GiB |   1069 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78636 MiB |  79742 MiB | 645400 MiB | 566764 MiB |
|       from large pool |  78608 MiB |  79496 MiB | 642674 MiB | 564066 MiB |
|       from small pool |     28 MiB |    246 MiB |   2726 MiB |   2698 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3582 MiB |   7456 MiB | 189473 GiB | 189469 GiB |
|       from large pool |   3567 MiB |   7440 MiB | 188259 GiB | 188255 GiB |
|       from small pool |     15 MiB |     23 MiB |   1214 GiB |   1214 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   12564 K  |   12563 K  |
|       from large pool |     314    |     322    |    6052 K  |    6052 K  |
|       from small pool |     291    |     336    |    6511 K  |    6510 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   12564 K  |   12563 K  |
|       from large pool |     314    |     322    |    6052 K  |    6052 K  |
|       from small pool |     291    |     336    |    6511 K  |    6510 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      92    |     217    |    3521    |    3429    |
|       from large pool |      78    |      94    |    2158    |    2080    |
|       from small pool |      14    |     123    |    1363    |    1349    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      90    |      91    |    7038 K  |    7038 K  |
|       from large pool |      60    |      61    |    3937 K  |    3937 K  |
|       from small pool |      30    |      47    |    3100 K  |    3100 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:48:43] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:48:43] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:48:44]    INFO >> epoch 004:   1398 / 1539 loss=3.922, wps=4392.6, ups=5.55, wpb=791.5, bsz=791.5, num_updates=6000, lr=0.000376, gnorm=7.648, clip=0, train_wall=8, gb_free=71.6, wall=1080 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:48:52]    INFO >> epoch 004:   1448 / 1539 loss=3.921, wps=4634.1, ups=6.51, wpb=711.7, bsz=711.7, num_updates=6050, lr=0.000376, gnorm=7.34, clip=0, train_wall=7, gb_free=73.5, wall=1088 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:49:00]    INFO >> epoch 004:   1498 / 1539 loss=3.94, wps=4355.6, ups=6.39, wpb=682, bsz=682, num_updates=6100, lr=0.000376, gnorm=7.17, clip=0, train_wall=7, gb_free=74.2, wall=1096 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:49:03] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 23.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.10 GiB is allocated by PyTorch, and 1.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:49:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:49:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:49:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 16           |        cudaMalloc retries: 35        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78888 MiB |  78948 MiB | 194338 GiB | 194261 GiB |
|       from large pool |  78495 MiB |  78555 MiB | 193245 GiB | 193169 GiB |
|       from small pool |    393 MiB |    394 MiB |   1092 GiB |   1092 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78888 MiB |  78948 MiB | 194338 GiB | 194261 GiB |
|       from large pool |  78495 MiB |  78555 MiB | 193245 GiB | 193169 GiB |
|       from small pool |    393 MiB |    394 MiB |   1092 GiB |   1092 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78858 MiB |  78917 MiB | 193930 GiB | 193853 GiB |
|       from large pool |  78467 MiB |  78526 MiB | 192839 GiB | 192762 GiB |
|       from small pool |    390 MiB |    392 MiB |   1090 GiB |   1090 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80482 MiB |  80482 MiB | 647246 MiB | 566764 MiB |
|       from large pool |  80048 MiB |  80048 MiB | 644114 MiB | 564066 MiB |
|       from small pool |    434 MiB |    434 MiB |   3132 MiB |   2698 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1533 MiB |   4145 MiB | 193496 GiB | 193495 GiB |
|       from large pool |   1492 MiB |   4108 MiB | 192257 GiB | 192256 GiB |
|       from small pool |     40 MiB |     41 MiB |   1239 GiB |   1239 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7252    |    7255    |   12813 K  |   12806 K  |
|       from large pool |     929    |     930    |    6169 K  |    6168 K  |
|       from small pool |    6323    |    6326    |    6643 K  |    6637 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7252    |    7255    |   12813 K  |   12806 K  |
|       from large pool |     929    |     930    |    6169 K  |    6168 K  |
|       from small pool |    6323    |    6326    |    6643 K  |    6637 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     319    |     319    |    3748    |    3429    |
|       from large pool |     102    |     102    |    2182    |    2080    |
|       from small pool |     217    |     217    |    1566    |    1349    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     465    |     465    |    7173 K  |    7173 K  |
|       from large pool |      77    |      77    |    4009 K  |    4009 K  |
|       from small pool |     388    |     388    |    3164 K  |    3163 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:49:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:49:03] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:49:07]    INFO >> epoch 004 | loss 3.906 | wps 4162.4 | ups 5.84 | wpb 712.7 | bsz 712.7 | num_updates 6140 | lr 0.000376 | gnorm 7.119 | clip 0.2 | train_wall 228 | gb_free 70.1 | wall 1102 (progress_bar.py:267, print())[0m
[33m[2025-11-21 16:49:07] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:49:22]    INFO >> epoch 004 | valid on 'valid' subset | loss 3.858 | wps 10560.4 | wpb 5412.5 | bsz 5412.5 | num_updates 6140 | best_loss 5.057 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 16:49:22]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 16:49:22]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/checkpoints/checkpoint_last.pt (epoch 4 @ 6140 updates, score 3.858) (writing took 0.013481 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 16:49:22] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:49:24]    INFO >> epoch 005:     10 / 1539 loss=3.917, wps=1409.3, ups=2.19, wpb=643.2, bsz=643.2, num_updates=6150, lr=0.000354, gnorm=6.996, clip=0, train_wall=7, gb_free=72.1, wall=1118 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:49:33]    INFO >> epoch 005:     60 / 1539 loss=3.928, wps=4048.1, ups=5.75, wpb=704.5, bsz=704.5, num_updates=6200, lr=0.000354, gnorm=6.705, clip=0, train_wall=8, gb_free=73.4, wall=1127 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:49:42]    INFO >> epoch 005:    110 / 1539 loss=3.908, wps=4570.6, ups=6.48, wpb=705.4, bsz=705.4, num_updates=6250, lr=0.000354, gnorm=7.252, clip=0, train_wall=7, gb_free=71.5, wall=1135 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:49:49]    INFO >> epoch 005:    160 / 1539 loss=3.896, wps=4231.4, ups=6.62, wpb=639.6, bsz=639.6, num_updates=6300, lr=0.000354, gnorm=6.923, clip=2, train_wall=7, gb_free=74, wall=1142 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:49:57]    INFO >> epoch 005:    210 / 1539 loss=3.816, wps=4352.3, ups=6.48, wpb=671.6, bsz=671.6, num_updates=6350, lr=0.000354, gnorm=7.533, clip=2, train_wall=7, gb_free=71, wall=1150 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:50:05]    INFO >> epoch 005:    260 / 1539 loss=3.831, wps=5205.1, ups=5.81, wpb=895.2, bsz=895.2, num_updates=6400, lr=0.000354, gnorm=7.109, clip=0, train_wall=8, gb_free=67.8, wall=1159 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:50:15]    INFO >> epoch 005:    310 / 1539 loss=3.857, wps=4514.3, ups=6.4, wpb=704.8, bsz=704.8, num_updates=6450, lr=0.000354, gnorm=6.527, clip=0, train_wall=7, gb_free=71.8, wall=1166 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:50:23]    INFO >> epoch 005:    360 / 1539 loss=3.467, wps=4805.6, ups=6.22, wpb=772.8, bsz=772.8, num_updates=6500, lr=0.000354, gnorm=6.825, clip=0, train_wall=8, gb_free=69.5, wall=1174 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:50:29] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 27.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.85 GiB is allocated by PyTorch, and 1.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:50:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:50:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:50:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 17           |        cudaMalloc retries: 36        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78634 MiB |  78694 MiB | 211161 GiB | 211084 GiB |
|       from large pool |  78243 MiB |  78303 MiB | 209958 GiB | 209881 GiB |
|       from small pool |    390 MiB |    391 MiB |   1202 GiB |   1202 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78634 MiB |  78694 MiB | 211161 GiB | 211084 GiB |
|       from large pool |  78243 MiB |  78303 MiB | 209958 GiB | 209881 GiB |
|       from small pool |    390 MiB |    391 MiB |   1202 GiB |   1202 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78618 MiB |  78677 MiB | 210720 GiB | 210643 GiB |
|       from large pool |  78229 MiB |  78288 MiB | 209518 GiB | 209442 GiB |
|       from small pool |    388 MiB |    389 MiB |   1201 GiB |   1200 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80478 MiB |  80482 MiB | 647306 MiB | 566828 MiB |
|       from large pool |  80048 MiB |  80048 MiB | 644174 MiB | 564126 MiB |
|       from small pool |    430 MiB |    434 MiB |   3132 MiB |   2702 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1783 MiB |   5194 MiB | 209165 GiB | 209164 GiB |
|       from large pool |   1744 MiB |   5187 MiB | 207804 GiB | 207803 GiB |
|       from small pool |     39 MiB |     41 MiB |   1361 GiB |   1361 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7208    |    7211    |   13967 K  |   13960 K  |
|       from large pool |     925    |     926    |    6636 K  |    6635 K  |
|       from small pool |    6283    |    6286    |    7330 K  |    7324 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7208    |    7211    |   13967 K  |   13960 K  |
|       from large pool |     925    |     926    |    6636 K  |    6635 K  |
|       from small pool |    6283    |    6286    |    7330 K  |    7324 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     317    |     319    |    3749    |    3432    |
|       from large pool |     102    |     102    |    2183    |    2081    |
|       from small pool |     215    |     217    |    1566    |    1351    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     475    |     476    |    7847 K  |    7847 K  |
|       from large pool |      90    |      90    |    4312 K  |    4312 K  |
|       from small pool |     385    |     386    |    3535 K  |    3534 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:50:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:50:29] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:50:31]    INFO >> epoch 005:    411 / 1539 loss=3.862, wps=4163.6, ups=6.11, wpb=681.6, bsz=681.6, num_updates=6550, lr=0.000354, gnorm=6.98, clip=2, train_wall=7, gb_free=73.9, wall=1183 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:50:38]    INFO >> epoch 005:    461 / 1539 loss=3.892, wps=4092.2, ups=6.73, wpb=608, bsz=608, num_updates=6600, lr=0.000354, gnorm=6.036, clip=0, train_wall=7, gb_free=74.7, wall=1190 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:50:52]    INFO >> epoch 005:    511 / 1539 loss=3.664, wps=2885.6, ups=3.95, wpb=730.5, bsz=730.5, num_updates=6650, lr=0.000354, gnorm=6.858, clip=0, train_wall=12, gb_free=69.8, wall=1203 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:51:00]    INFO >> epoch 005:    561 / 1539 loss=3.966, wps=4108.6, ups=6.62, wpb=620.7, bsz=620.7, num_updates=6700, lr=0.000354, gnorm=6.597, clip=0, train_wall=7, gb_free=72.5, wall=1210 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:51:05] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 3.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.48 GiB is allocated by PyTorch, and 1.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:51:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:51:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:51:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 18           |        cudaMalloc retries: 38        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79276 MiB |  79336 MiB | 216573 GiB | 216496 GiB |
|       from large pool |  79185 MiB |  79245 MiB | 215342 GiB | 215265 GiB |
|       from small pool |     91 MiB |     92 MiB |   1230 GiB |   1230 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79276 MiB |  79336 MiB | 216573 GiB | 216496 GiB |
|       from large pool |  79185 MiB |  79245 MiB | 215342 GiB | 215265 GiB |
|       from small pool |     91 MiB |     92 MiB |   1230 GiB |   1230 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79244 MiB |  79303 MiB | 216121 GiB | 216043 GiB |
|       from large pool |  79153 MiB |  79212 MiB | 214892 GiB | 214815 GiB |
|       from small pool |     90 MiB |     92 MiB |   1228 GiB |   1228 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80502 MiB |  80504 MiB | 647730 MiB | 567228 MiB |
|       from large pool |  80408 MiB |  80408 MiB | 644594 MiB | 564186 MiB |
|       from small pool |     94 MiB |    430 MiB |   3136 MiB |   3042 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1165 MiB |   6537 MiB | 214875 GiB | 214873 GiB |
|       from large pool |   1162 MiB |   6529 MiB | 213481 GiB | 213480 GiB |
|       from small pool |      2 MiB |     21 MiB |   1393 GiB |   1393 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1983    |    1986    |   14316 K  |   14314 K  |
|       from large pool |     472    |     473    |    6818 K  |    6818 K  |
|       from small pool |    1511    |    1514    |    7497 K  |    7496 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1983    |    1986    |   14316 K  |   14314 K  |
|       from large pool |     472    |     473    |    6818 K  |    6818 K  |
|       from small pool |    1511    |    1514    |    7497 K  |    7496 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     155    |     317    |    3758    |    3603    |
|       from large pool |     108    |     108    |    2190    |    2082    |
|       from small pool |      47    |     215    |    1568    |    1521    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     139    |     142    |    8037 K  |    8037 K  |
|       from large pool |      88    |      91    |    4428 K  |    4428 K  |
|       from small pool |      51    |      54    |    3609 K  |    3609 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:51:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:51:05] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:51:09]    INFO >> epoch 005:    612 / 1539 loss=3.851, wps=4347.4, ups=5.75, wpb=756, bsz=756, num_updates=6750, lr=0.000354, gnorm=6.776, clip=0, train_wall=7, gb_free=70.1, wall=1219 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:51:18]    INFO >> epoch 005:    662 / 1539 loss=3.853, wps=4662, ups=6.33, wpb=736.8, bsz=736.8, num_updates=6800, lr=0.000354, gnorm=6.857, clip=0, train_wall=7, gb_free=74.2, wall=1227 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:51:26]    INFO >> epoch 005:    712 / 1539 loss=3.887, wps=4579.7, ups=6.28, wpb=729, bsz=729, num_updates=6850, lr=0.000354, gnorm=7.201, clip=0, train_wall=7, gb_free=71.4, wall=1235 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:51:33]    INFO >> epoch 005:    762 / 1539 loss=3.97, wps=4696.9, ups=6.55, wpb=717.1, bsz=717.1, num_updates=6900, lr=0.000354, gnorm=6.38, clip=0, train_wall=7, gb_free=72.9, wall=1243 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:51:42]    INFO >> epoch 005:    812 / 1539 loss=3.766, wps=3902.3, ups=5.79, wpb=673.7, bsz=673.7, num_updates=6950, lr=0.000354, gnorm=6.51, clip=2, train_wall=8, gb_free=69.7, wall=1251 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:51:51]    INFO >> epoch 005:    862 / 1539 loss=3.726, wps=4411.1, ups=6.58, wpb=670.9, bsz=670.9, num_updates=7000, lr=0.000354, gnorm=7.091, clip=0, train_wall=7, gb_free=66.5, wall=1259 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:51:59]    INFO >> epoch 005:    912 / 1539 loss=3.884, wps=4300, ups=6.45, wpb=667, bsz=667, num_updates=7050, lr=0.000354, gnorm=6.266, clip=0, train_wall=7, gb_free=71.7, wall=1267 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:52:05] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 133.25 MiB is free. Including non-PyTorch memory, this process has 78.99 GiB memory in use. Of the allocated memory 74.93 GiB is allocated by PyTorch, and 3.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:52:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:52:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:52:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 19           |        cudaMalloc retries: 39        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76325 MiB |  78599 MiB | 227233 GiB | 227158 GiB |
|       from large pool |  76306 MiB |  78582 MiB | 225947 GiB | 225873 GiB |
|       from small pool |     18 MiB |     24 MiB |   1285 GiB |   1285 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76325 MiB |  78599 MiB | 227233 GiB | 227158 GiB |
|       from large pool |  76306 MiB |  78582 MiB | 225947 GiB | 225873 GiB |
|       from small pool |     18 MiB |     24 MiB |   1285 GiB |   1285 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB | 226757 GiB | 226683 GiB |
|       from large pool |  76285 MiB |  78562 MiB | 225474 GiB | 225399 GiB |
|       from small pool |     18 MiB |     24 MiB |   1283 GiB |   1283 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80372 MiB |  80442 MiB | 647730 MiB | 567358 MiB |
|       from large pool |  80348 MiB |  80348 MiB | 644594 MiB | 564246 MiB |
|       from small pool |     24 MiB |     94 MiB |   3136 MiB |   3112 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4046 MiB |   5922 MiB | 225924 GiB | 225920 GiB |
|       from large pool |   4041 MiB |   5910 MiB | 224468 GiB | 224464 GiB |
|       from small pool |      5 MiB |     21 MiB |   1455 GiB |   1455 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     640    |     646    |   14999 K  |   14999 K  |
|       from large pool |     340    |     346    |    7173 K  |    7173 K  |
|       from small pool |     300    |     356    |    7826 K  |    7825 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     640    |     646    |   14999 K  |   14999 K  |
|       from large pool |     340    |     346    |    7173 K  |    7173 K  |
|       from small pool |     300    |     356    |    7826 K  |    7825 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     119    |     154    |    3758    |    3639    |
|       from large pool |     107    |     107    |    2190    |    2083    |
|       from small pool |      12    |      47    |    1568    |    1556    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     109    |     111    |    8409 K  |    8409 K  |
|       from large pool |      84    |      86    |    4656 K  |    4656 K  |
|       from small pool |      25    |      52    |    3753 K  |    3753 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:52:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:52:05] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:52:07]    INFO >> epoch 005:    963 / 1539 loss=3.853, wps=4086.8, ups=6.04, wpb=676.6, bsz=676.6, num_updates=7100, lr=0.000354, gnorm=6.485, clip=0, train_wall=7, gb_free=70.4, wall=1275 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:52:16]    INFO >> epoch 005:   1013 / 1539 loss=3.677, wps=5254.6, ups=5.51, wpb=954.2, bsz=954.2, num_updates=7150, lr=0.000354, gnorm=7.355, clip=0, train_wall=9, gb_free=70.3, wall=1284 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:52:26]    INFO >> epoch 005:   1063 / 1539 loss=3.582, wps=4619.3, ups=5.84, wpb=791.1, bsz=791.1, num_updates=7200, lr=0.000354, gnorm=7.183, clip=0, train_wall=8, gb_free=64.7, wall=1292 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:52:34]    INFO >> epoch 005:   1113 / 1539 loss=3.806, wps=4146.4, ups=6.62, wpb=626.1, bsz=626.1, num_updates=7250, lr=0.000354, gnorm=7.038, clip=0, train_wall=7, gb_free=74.2, wall=1300 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:52:36] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 199.25 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 4.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:52:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:52:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:52:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 20           |        cudaMalloc retries: 43        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72823 MiB |  75427 MiB | 232756 GiB | 232684 GiB |
|       from large pool |  72810 MiB |  75414 MiB | 231439 GiB | 231368 GiB |
|       from small pool |     12 MiB |     21 MiB |   1316 GiB |   1316 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72823 MiB |  75427 MiB | 232756 GiB | 232684 GiB |
|       from large pool |  72810 MiB |  75414 MiB | 231439 GiB | 231368 GiB |
|       from small pool |     12 MiB |     21 MiB |   1316 GiB |   1316 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72801 MiB |  75403 MiB | 232269 GiB | 232198 GiB |
|       from large pool |  72788 MiB |  75391 MiB | 230954 GiB | 230883 GiB |
|       from small pool |     12 MiB |     21 MiB |   1314 GiB |   1314 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80306 MiB |  80306 MiB | 688436 MiB | 608130 MiB |
|       from large pool |  80280 MiB |  80280 MiB | 685098 MiB | 604818 MiB |
|       from small pool |     26 MiB |    226 MiB |   3338 MiB |   3312 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5580 MiB |   9572 MiB | 230907 GiB | 230902 GiB |
|       from large pool |   5567 MiB |   9557 MiB | 229416 GiB | 229410 GiB |
|       from small pool |     13 MiB |     31 MiB |   1491 GiB |   1491 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     603    |     612    |   15362 K  |   15361 K  |
|       from large pool |     312    |     320    |    7349 K  |    7348 K  |
|       from small pool |     291    |     356    |    8012 K  |    8012 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     603    |     612    |   15362 K  |   15361 K  |
|       from large pool |     312    |     320    |    7349 K  |    7348 K  |
|       from small pool |     291    |     356    |    8012 K  |    8012 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     170    |     326    |    3990    |    3820    |
|       from large pool |     157    |     213    |    2321    |    2164    |
|       from small pool |      13    |     113    |    1669    |    1656    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     159    |     161    |    8616 K  |    8616 K  |
|       from large pool |     133    |     135    |    4773 K  |    4773 K  |
|       from small pool |      26    |      62    |    3842 K  |    3842 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:52:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:52:36] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:52:42]    INFO >> epoch 005:   1164 / 1539 loss=3.76, wps=4366.6, ups=5.6, wpb=780.4, bsz=780.4, num_updates=7300, lr=0.000354, gnorm=6.668, clip=2, train_wall=8, gb_free=69.6, wall=1309 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:52:50]    INFO >> epoch 005:   1214 / 1539 loss=3.816, wps=4178.5, ups=6.35, wpb=657.7, bsz=657.7, num_updates=7350, lr=0.000354, gnorm=6.423, clip=2, train_wall=7, gb_free=68.7, wall=1317 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:52:59]    INFO >> epoch 005:   1264 / 1539 loss=3.814, wps=4484, ups=7.06, wpb=635.1, bsz=635.1, num_updates=7400, lr=0.000354, gnorm=6.299, clip=0, train_wall=7, gb_free=73.5, wall=1324 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:53:07]    INFO >> epoch 005:   1314 / 1539 loss=3.816, wps=4150.7, ups=6.37, wpb=651.5, bsz=651.5, num_updates=7450, lr=0.000354, gnorm=6.372, clip=0, train_wall=7, gb_free=70, wall=1332 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:53:14]    INFO >> epoch 005:   1364 / 1539 loss=3.769, wps=4614.5, ups=6.36, wpb=725.6, bsz=725.6, num_updates=7500, lr=0.000354, gnorm=6.428, clip=0, train_wall=7, gb_free=74.7, wall=1340 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:53:22]    INFO >> epoch 005:   1414 / 1539 loss=3.93, wps=4143.1, ups=6.58, wpb=630.1, bsz=630.1, num_updates=7550, lr=0.000354, gnorm=6.059, clip=0, train_wall=7, gb_free=74.2, wall=1347 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:53:31]    INFO >> epoch 005:   1464 / 1539 loss=3.674, wps=4843, ups=6.34, wpb=763.7, bsz=763.7, num_updates=7600, lr=0.000354, gnorm=7.54, clip=2, train_wall=7, gb_free=48.5, wall=1355 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:53:40]    INFO >> epoch 005:   1514 / 1539 loss=3.824, wps=4838.6, ups=6.03, wpb=802.7, bsz=802.7, num_updates=7650, lr=0.000354, gnorm=6.844, clip=0, train_wall=8, gb_free=54.7, wall=1363 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:53:44]    INFO >> epoch 005 | loss 3.808 | wps 4119.1 | ups 5.78 | wpb 712.7 | bsz 712.7 | num_updates 7675 | lr 0.000354 | gnorm 6.776 | clip 0.5 | train_wall 233 | gb_free 63.5 | wall 1367 (progress_bar.py:267, print())[0m
[33m[2025-11-21 16:53:44] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:53:58]    INFO >> epoch 005 | valid on 'valid' subset | loss 3.813 | wps 10893.4 | wpb 5412.5 | bsz 5412.5 | num_updates 7675 | best_loss 5.057 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 16:53:58]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 16:53:58]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/checkpoints/checkpoint_last.pt (epoch 5 @ 7675 updates, score 3.813) (writing took 0.015271 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 16:53:58] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:54:02]    INFO >> epoch 006:     25 / 1539 loss=3.857, wps=1600.2, ups=2.26, wpb=708.8, bsz=708.8, num_updates=7700, lr=0.000327, gnorm=6.374, clip=0, train_wall=7, gb_free=74.9, wall=1386 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:54:11]    INFO >> epoch 006:     75 / 1539 loss=3.784, wps=5159.5, ups=5.97, wpb=863.7, bsz=863.7, num_updates=7750, lr=0.000327, gnorm=6.711, clip=2, train_wall=8, gb_free=75.2, wall=1394 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:54:19]    INFO >> epoch 006:    125 / 1539 loss=3.686, wps=4447.7, ups=6.59, wpb=675.3, bsz=675.3, num_updates=7800, lr=0.000327, gnorm=6.69, clip=0, train_wall=7, gb_free=70.3, wall=1401 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:54:27]    INFO >> epoch 006:    175 / 1539 loss=3.713, wps=4363.9, ups=6.33, wpb=689, bsz=689, num_updates=7850, lr=0.000327, gnorm=7.151, clip=2, train_wall=7, gb_free=74.3, wall=1409 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:54:29] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 535.25 MiB is free. Including non-PyTorch memory, this process has 78.59 GiB memory in use. Of the allocated memory 73.81 GiB is allocated by PyTorch, and 4.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:54:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:54:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:54:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 21           |        cudaMalloc retries: 44        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75055 MiB |  75998 MiB | 254753 GiB | 254679 GiB |
|       from large pool |  75042 MiB |  75985 MiB | 253307 GiB | 253233 GiB |
|       from small pool |     12 MiB |     15 MiB |   1446 GiB |   1446 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75055 MiB |  75998 MiB | 254753 GiB | 254679 GiB |
|       from large pool |  75042 MiB |  75985 MiB | 253307 GiB | 253233 GiB |
|       from small pool |     12 MiB |     15 MiB |   1446 GiB |   1446 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75035 MiB |  75977 MiB | 254221 GiB | 254148 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 252777 GiB | 252704 GiB |
|       from small pool |     12 MiB |     15 MiB |   1443 GiB |   1443 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79970 MiB |  80406 MiB | 690438 MiB | 610468 MiB |
|       from large pool |  79944 MiB |  80306 MiB | 687026 MiB | 607082 MiB |
|       from small pool |     26 MiB |    100 MiB |   3412 MiB |   3386 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4914 MiB |   8989 MiB | 249559 GiB | 249554 GiB |
|       from large pool |   4901 MiB |   8976 MiB | 247925 GiB | 247920 GiB |
|       from small pool |     13 MiB |     25 MiB |   1634 GiB |   1634 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   16790 K  |   16790 K  |
|       from large pool |     314    |     322    |    7980 K  |    7979 K  |
|       from small pool |     291    |     356    |    8810 K  |    8810 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   16790 K  |   16790 K  |
|       from large pool |     314    |     322    |    7980 K  |    7979 K  |
|       from small pool |     291    |     356    |    8810 K  |    8810 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     172    |     210    |    4031    |    3859    |
|       from large pool |     159    |     160    |    2325    |    2166    |
|       from small pool |      13    |      50    |    1706    |    1693    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     162    |     162    |    9440 K  |    9440 K  |
|       from large pool |     136    |     136    |    5197 K  |    5197 K  |
|       from small pool |      26    |      57    |    4243 K  |    4243 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:54:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:54:29] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 16:54:35] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 904.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 537.25 MiB is free. Including non-PyTorch memory, this process has 78.59 GiB memory in use. Of the allocated memory 72.94 GiB is allocated by PyTorch, and 5.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:54:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:54:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:54:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 22           |        cudaMalloc retries: 45        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69875 MiB |  74872 MiB | 255914 GiB | 255845 GiB |
|       from large pool |  69858 MiB |  74855 MiB | 254462 GiB | 254394 GiB |
|       from small pool |     17 MiB |     17 MiB |   1451 GiB |   1451 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69875 MiB |  74872 MiB | 255914 GiB | 255845 GiB |
|       from large pool |  69858 MiB |  74855 MiB | 254462 GiB | 254394 GiB |
|       from small pool |     17 MiB |     17 MiB |   1451 GiB |   1451 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  69846 MiB |  74842 MiB | 255380 GiB | 255311 GiB |
|       from large pool |  69829 MiB |  74825 MiB | 253930 GiB | 253862 GiB |
|       from small pool |     17 MiB |     17 MiB |   1449 GiB |   1449 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79968 MiB |  80006 MiB | 690474 MiB | 610506 MiB |
|       from large pool |  79944 MiB |  79944 MiB | 687026 MiB | 607082 MiB |
|       from small pool |     24 MiB |     62 MiB |   3448 MiB |   3424 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  10092 MiB |  11470 MiB | 250616 GiB | 250606 GiB |
|       from large pool |  10085 MiB |  11463 MiB | 248976 GiB | 248966 GiB |
|       from small pool |      6 MiB |     25 MiB |   1640 GiB |   1640 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     643    |     661    |   16854 K  |   16853 K  |
|       from large pool |     344    |     362    |    8013 K  |    8013 K  |
|       from small pool |     299    |     356    |    8840 K  |    8840 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     643    |     661    |   16854 K  |   16853 K  |
|       from large pool |     344    |     362    |    8013 K  |    8013 K  |
|       from small pool |     299    |     356    |    8840 K  |    8840 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     171    |     190    |    4049    |    3878    |
|       from large pool |     159    |     159    |    2325    |    2166    |
|       from small pool |      12    |      31    |    1724    |    1712    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     145    |     147    |    9476 K  |    9476 K  |
|       from large pool |     121    |     123    |    5219 K  |    5219 K  |
|       from small pool |      24    |      58    |    4257 K  |    4257 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:54:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:54:35] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:54:38]    INFO >> epoch 006:    227 / 1539 loss=3.911, wps=3785.7, ups=5.26, wpb=719.2, bsz=719.2, num_updates=7900, lr=0.000327, gnorm=6.092, clip=0, train_wall=8, gb_free=71.4, wall=1419 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:54:45]    INFO >> epoch 006:    277 / 1539 loss=3.755, wps=4215.4, ups=6.74, wpb=625.7, bsz=625.7, num_updates=7950, lr=0.000327, gnorm=5.974, clip=0, train_wall=7, gb_free=74.5, wall=1426 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:54:53]    INFO >> epoch 006:    327 / 1539 loss=3.416, wps=5113.8, ups=6.16, wpb=830.5, bsz=830.5, num_updates=8000, lr=0.000327, gnorm=8.112, clip=2, train_wall=8, gb_free=70.3, wall=1434 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:55:01]    INFO >> epoch 006:    377 / 1539 loss=3.895, wps=4159.1, ups=6.32, wpb=657.6, bsz=657.6, num_updates=8050, lr=0.000327, gnorm=5.943, clip=0, train_wall=7, gb_free=71.1, wall=1442 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:55:09]    INFO >> epoch 006:    427 / 1539 loss=3.639, wps=5015.1, ups=6.5, wpb=772.1, bsz=772.1, num_updates=8100, lr=0.000327, gnorm=7.455, clip=2, train_wall=7, gb_free=75, wall=1450 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:55:13] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 2.40 GiB is free. Including non-PyTorch memory, this process has 76.72 GiB memory in use. Of the allocated memory 70.76 GiB is allocated by PyTorch, and 5.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:55:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:55:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:55:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 23           |        cudaMalloc retries: 47        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72059 MiB |  72460 MiB | 262338 GiB | 262268 GiB |
|       from large pool |  72042 MiB |  72442 MiB | 260852 GiB | 260781 GiB |
|       from small pool |     17 MiB |     21 MiB |   1486 GiB |   1486 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72059 MiB |  72460 MiB | 262338 GiB | 262268 GiB |
|       from large pool |  72042 MiB |  72442 MiB | 260852 GiB | 260781 GiB |
|       from small pool |     17 MiB |     21 MiB |   1486 GiB |   1486 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  72444 MiB | 261790 GiB | 261719 GiB |
|       from large pool |  72027 MiB |  72426 MiB | 260305 GiB | 260235 GiB |
|       from small pool |     17 MiB |     21 MiB |   1484 GiB |   1484 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78052 MiB |  80154 MiB | 696492 MiB | 618440 MiB |
|       from large pool |  78026 MiB |  79944 MiB | 692858 MiB | 614832 MiB |
|       from small pool |     26 MiB |    210 MiB |   3634 MiB |   3608 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5992 MiB |   8373 MiB | 256682 GiB | 256676 GiB |
|       from large pool |   5983 MiB |   8364 MiB | 255002 GiB | 254996 GiB |
|       from small pool |      8 MiB |     29 MiB |   1680 GiB |   1680 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   17287 K  |   17286 K  |
|       from large pool |     308    |     315    |    8230 K  |    8230 K  |
|       from small pool |     298    |     356    |    9056 K  |    9056 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   17287 K  |   17286 K  |
|       from large pool |     308    |     315    |    8230 K  |    8230 K  |
|       from small pool |     298    |     356    |    9056 K  |    9056 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     170    |     264    |    4145    |    3975    |
|       from large pool |     157    |     159    |    2328    |    2171    |
|       from small pool |      13    |     105    |    1817    |    1804    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     163    |     163    |    9720 K  |    9720 K  |
|       from large pool |     137    |     137    |    5363 K  |    5362 K  |
|       from small pool |      26    |      54    |    4357 K  |    4357 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:55:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:55:13] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:55:19]    INFO >> epoch 006:    478 / 1539 loss=3.652, wps=4235.7, ups=6, wpb=705.8, bsz=705.8, num_updates=8150, lr=0.000327, gnorm=7.015, clip=2, train_wall=7, gb_free=65, wall=1458 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:55:27]    INFO >> epoch 006:    528 / 1539 loss=3.815, wps=4211.6, ups=5.95, wpb=707.6, bsz=707.6, num_updates=8200, lr=0.000327, gnorm=6.852, clip=0, train_wall=8, gb_free=72, wall=1467 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:55:35]    INFO >> epoch 006:    578 / 1539 loss=3.73, wps=5073.7, ups=6.36, wpb=797.6, bsz=797.6, num_updates=8250, lr=0.000327, gnorm=7.249, clip=0, train_wall=7, gb_free=74.1, wall=1475 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:55:42]    INFO >> epoch 006:    628 / 1539 loss=3.849, wps=4370.6, ups=6.78, wpb=644.9, bsz=644.9, num_updates=8300, lr=0.000327, gnorm=5.84, clip=0, train_wall=7, gb_free=70.5, wall=1482 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:55:52]    INFO >> epoch 006:    678 / 1539 loss=3.709, wps=4508.7, ups=6.17, wpb=730.2, bsz=730.2, num_updates=8350, lr=0.000327, gnorm=6.561, clip=0, train_wall=8, gb_free=73.6, wall=1490 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:55:59]    INFO >> epoch 006:    728 / 1539 loss=3.831, wps=4110.7, ups=6.67, wpb=616.1, bsz=616.1, num_updates=8400, lr=0.000327, gnorm=5.422, clip=0, train_wall=7, gb_free=62, wall=1498 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:56:00] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 17.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 76.16 GiB is allocated by PyTorch, and 2.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:56:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:56:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:56:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 24           |        cudaMalloc retries: 48        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77932 MiB |  77992 MiB | 270810 GiB | 270734 GiB |
|       from large pool |  77549 MiB |  77609 MiB | 269273 GiB | 269197 GiB |
|       from small pool |    383 MiB |    384 MiB |   1536 GiB |   1536 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77932 MiB |  77992 MiB | 270810 GiB | 270734 GiB |
|       from large pool |  77549 MiB |  77609 MiB | 269273 GiB | 269197 GiB |
|       from small pool |    383 MiB |    384 MiB |   1536 GiB |   1536 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77897 MiB |  77956 MiB | 270242 GiB | 270166 GiB |
|       from large pool |  77516 MiB |  77575 MiB | 268708 GiB | 268632 GiB |
|       from small pool |    381 MiB |    382 MiB |   1534 GiB |   1533 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80488 MiB |  80490 MiB | 698930 MiB | 618442 MiB |
|       from large pool |  80066 MiB |  80066 MiB | 694898 MiB | 614832 MiB |
|       from small pool |    422 MiB |    424 MiB |   4032 MiB |   3610 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2495 MiB |   6667 MiB | 264812 GiB | 264809 GiB |
|       from large pool |   2456 MiB |   6661 MiB | 263073 GiB | 263070 GiB |
|       from small pool |     38 MiB |     40 MiB |   1739 GiB |   1739 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7076    |    7079    |   17882 K  |   17874 K  |
|       from large pool |     913    |     914    |    8520 K  |    8519 K  |
|       from small pool |    6163    |    6166    |    9361 K  |    9355 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7076    |    7079    |   17882 K  |   17874 K  |
|       from large pool |     913    |     914    |    8520 K  |    8519 K  |
|       from small pool |    6163    |    6166    |    9361 K  |    9355 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     402    |     403    |    4378    |    3976    |
|       from large pool |     191    |     191    |    2362    |    2171    |
|       from small pool |     211    |     212    |    2016    |    1805    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     526    |     527    |   10054 K  |   10053 K  |
|       from large pool |     150    |     150    |    5552 K  |    5552 K  |
|       from small pool |     376    |     377    |    4501 K  |    4501 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:56:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:56:00] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:56:07]    INFO >> epoch 006:    779 / 1539 loss=3.758, wps=4095.4, ups=6.37, wpb=642.5, bsz=642.5, num_updates=8450, lr=0.000327, gnorm=5.863, clip=0, train_wall=7, gb_free=74.1, wall=1505 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:56:15]    INFO >> epoch 006:    829 / 1539 loss=3.78, wps=4224.3, ups=6.43, wpb=656.8, bsz=656.8, num_updates=8500, lr=0.000327, gnorm=5.849, clip=0, train_wall=7, gb_free=66.6, wall=1513 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:56:24]    INFO >> epoch 006:    879 / 1539 loss=3.607, wps=4671.6, ups=6.4, wpb=729.6, bsz=729.6, num_updates=8550, lr=0.000327, gnorm=6.345, clip=0, train_wall=7, gb_free=70.2, wall=1521 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:56:33]    INFO >> epoch 006:    929 / 1539 loss=3.809, wps=4760.8, ups=5.76, wpb=826.1, bsz=826.1, num_updates=8600, lr=0.000327, gnorm=6.99, clip=0, train_wall=8, gb_free=73.1, wall=1530 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:56:41]    INFO >> epoch 006:    979 / 1539 loss=3.707, wps=4420.9, ups=6.31, wpb=701.1, bsz=701.1, num_updates=8650, lr=0.000327, gnorm=6.335, clip=0, train_wall=7, gb_free=75.1, wall=1538 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:56:49] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.57 GiB. GPU 2 has a total capacity of 79.14 GiB of which 2.28 GiB is free. Including non-PyTorch memory, this process has 76.84 GiB memory in use. Of the allocated memory 66.03 GiB is allocated by PyTorch, and 10.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:56:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:56:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:56:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 25           |        cudaMalloc retries: 49        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  63499 MiB |  70130 MiB | 279629 GiB | 279567 GiB |
|       from large pool |  63479 MiB |  70110 MiB | 278045 GiB | 277983 GiB |
|       from small pool |     20 MiB |     60 MiB |   1584 GiB |   1584 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  63499 MiB |  70130 MiB | 279629 GiB | 279567 GiB |
|       from large pool |  63479 MiB |  70110 MiB | 278045 GiB | 277983 GiB |
|       from small pool |     20 MiB |     60 MiB |   1584 GiB |   1584 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  63481 MiB |  70111 MiB | 279042 GiB | 278980 GiB |
|       from large pool |  63461 MiB |  70091 MiB | 277460 GiB | 277398 GiB |
|       from small pool |     20 MiB |     60 MiB |   1582 GiB |   1582 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78174 MiB |  80428 MiB | 698930 MiB | 620756 MiB |
|       from large pool |  78146 MiB |  80006 MiB | 694898 MiB | 616752 MiB |
|       from small pool |     28 MiB |    422 MiB |   4032 MiB |   4004 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   7296 MiB |  10786 MiB | 272794 GiB | 272787 GiB |
|       from large pool |   7288 MiB |  10778 MiB | 270999 GiB | 270992 GiB |
|       from small pool |      7 MiB |     29 MiB |   1794 GiB |   1794 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     646    |    1326    |   18465 K  |   18465 K  |
|       from large pool |     330    |     403    |    8814 K  |    8814 K  |
|       from small pool |     316    |     924    |    9651 K  |    9650 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     646    |    1326    |   18465 K  |   18465 K  |
|       from large pool |     330    |     403    |    8814 K  |    8814 K  |
|       from small pool |     316    |     924    |    9651 K  |    9650 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     174    |     401    |    4378    |    4204    |
|       from large pool |     160    |     190    |    2362    |    2202    |
|       from small pool |      14    |     211    |    2016    |    2002    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     160    |     209    |   10385 K  |   10385 K  |
|       from large pool |     129    |     160    |    5749 K  |    5749 K  |
|       from small pool |      31    |      59    |    4635 K  |    4635 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:56:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:56:49] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:56:49]    INFO >> epoch 006:   1030 / 1539 loss=3.756, wps=4115.6, ups=5.64, wpb=730.1, bsz=730.1, num_updates=8700, lr=0.000327, gnorm=6.579, clip=0, train_wall=8, gb_free=11.5, wall=1546 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:56:59]    INFO >> epoch 006:   1080 / 1539 loss=3.699, wps=4564.9, ups=6.31, wpb=723.5, bsz=723.5, num_updates=8750, lr=0.000327, gnorm=7.307, clip=2, train_wall=7, gb_free=72.4, wall=1554 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:57:07]    INFO >> epoch 006:   1130 / 1539 loss=3.639, wps=4412.9, ups=5.82, wpb=757.6, bsz=757.6, num_updates=8800, lr=0.000327, gnorm=6.693, clip=2, train_wall=8, gb_free=72.1, wall=1563 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:57:15]    INFO >> epoch 006:   1180 / 1539 loss=3.859, wps=4660.5, ups=6.29, wpb=740.5, bsz=740.5, num_updates=8850, lr=0.000327, gnorm=6.828, clip=0, train_wall=7, gb_free=69, wall=1571 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:57:24]    INFO >> epoch 006:   1230 / 1539 loss=3.807, wps=3947.2, ups=5.99, wpb=659.3, bsz=659.3, num_updates=8900, lr=0.000327, gnorm=6.28, clip=0, train_wall=8, gb_free=73.8, wall=1579 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:57:33]    INFO >> epoch 006:   1280 / 1539 loss=3.81, wps=4197.4, ups=6.45, wpb=650.6, bsz=650.6, num_updates=8950, lr=0.000327, gnorm=5.98, clip=0, train_wall=7, gb_free=75.3, wall=1587 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:57:41]    INFO >> epoch 006:   1330 / 1539 loss=3.608, wps=4654.8, ups=6.28, wpb=741, bsz=741, num_updates=9000, lr=0.000327, gnorm=7.009, clip=0, train_wall=7, gb_free=71.3, wall=1595 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:57:48]    INFO >> epoch 006:   1380 / 1539 loss=3.854, wps=4103.9, ups=6.74, wpb=608.9, bsz=608.9, num_updates=9050, lr=0.000327, gnorm=6, clip=0, train_wall=7, gb_free=68.9, wall=1602 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:57:56]    INFO >> epoch 006:   1430 / 1539 loss=3.657, wps=4640.7, ups=6.17, wpb=751.8, bsz=751.8, num_updates=9100, lr=0.000327, gnorm=6.977, clip=0, train_wall=8, gb_free=70.5, wall=1611 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:58:06]    INFO >> epoch 006:   1480 / 1539 loss=3.658, wps=4336.8, ups=6.05, wpb=717, bsz=717, num_updates=9150, lr=0.000327, gnorm=5.987, clip=2, train_wall=8, gb_free=69.4, wall=1619 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:58:13]    INFO >> epoch 006:   1530 / 1539 loss=3.667, wps=4447.7, ups=6.6, wpb=674.1, bsz=674.1, num_updates=9200, lr=0.000327, gnorm=5.626, clip=0, train_wall=7, gb_free=74.6, wall=1626 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:58:15]    INFO >> epoch 006 | loss 3.734 | wps 4189.3 | ups 5.89 | wpb 711.1 | bsz 711.1 | num_updates 9209 | lr 0.000327 | gnorm 6.508 | clip 0.5 | train_wall 228 | gb_free 72.3 | wall 1628 (progress_bar.py:267, print())[0m
[33m[2025-11-21 16:58:15] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:58:30]    INFO >> epoch 006 | valid on 'valid' subset | loss 3.825 | wps 10165 | wpb 5412.5 | bsz 5412.5 | num_updates 9209 | best_loss 5.057 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 16:58:30]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 16:58:30]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/checkpoints/checkpoint_last.pt (epoch 6 @ 9209 updates, score 3.825) (writing took 0.013506 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 16:58:30] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:58:39]    INFO >> epoch 007:     41 / 1539 loss=3.821, wps=1462.3, ups=2.08, wpb=704.3, bsz=704.3, num_updates=9250, lr=0.000295, gnorm=6.188, clip=0, train_wall=8, gb_free=71.6, wall=1650 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:58:40] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.29 GiB memory in use. Of the allocated memory 70.77 GiB is allocated by PyTorch, and 6.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:58:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:58:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:58:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 26           |        cudaMalloc retries: 51        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72064 MiB |  72463 MiB | 300570 GiB | 300499 GiB |
|       from large pool |  72047 MiB |  72446 MiB | 298863 GiB | 298793 GiB |
|       from small pool |     17 MiB |     25 MiB |   1706 GiB |   1706 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72064 MiB |  72463 MiB | 300570 GiB | 300499 GiB |
|       from large pool |  72047 MiB |  72446 MiB | 298863 GiB | 298793 GiB |
|       from small pool |     17 MiB |     25 MiB |   1706 GiB |   1706 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  72444 MiB | 299940 GiB | 299870 GiB |
|       from large pool |  72027 MiB |  72426 MiB | 298236 GiB | 298165 GiB |
|       from small pool |     17 MiB |     25 MiB |   1704 GiB |   1704 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78636 MiB |  78794 MiB | 712384 MiB | 633748 MiB |
|       from large pool |  78610 MiB |  78670 MiB | 708256 MiB | 629646 MiB |
|       from small pool |     26 MiB |    124 MiB |   4128 MiB |   4102 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6571 MiB |   8373 MiB | 290271 GiB | 290265 GiB |
|       from large pool |   6562 MiB |   8363 MiB | 288342 GiB | 288336 GiB |
|       from small pool |      8 MiB |     35 MiB |   1928 GiB |   1928 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   19804 K  |   19803 K  |
|       from large pool |     308    |     315    |    9401 K  |    9401 K  |
|       from small pool |     298    |     356    |   10402 K  |   10402 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   19804 K  |   19803 K  |
|       from large pool |     308    |     315    |    9401 K  |    9401 K  |
|       from small pool |     298    |     356    |   10402 K  |   10402 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     169    |     222    |    4431    |    4262    |
|       from large pool |     156    |     160    |    2367    |    2211    |
|       from small pool |      13    |      62    |    2064    |    2051    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     163    |     164    |   11169 K  |   11169 K  |
|       from large pool |     137    |     138    |    6143 K  |    6142 K  |
|       from small pool |      26    |      60    |    5026 K  |    5026 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:58:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:58:40] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:58:47]    INFO >> epoch 007:     92 / 1539 loss=3.672, wps=4130.9, ups=6.12, wpb=674.7, bsz=674.7, num_updates=9300, lr=0.000295, gnorm=6.627, clip=0, train_wall=7, gb_free=74.6, wall=1659 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:58:55]    INFO >> epoch 007:    142 / 1539 loss=3.73, wps=4257.8, ups=6.42, wpb=663.1, bsz=663.1, num_updates=9350, lr=0.000295, gnorm=7.072, clip=0, train_wall=7, gb_free=73.4, wall=1666 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:59:03]    INFO >> epoch 007:    192 / 1539 loss=3.47, wps=5354.2, ups=6.05, wpb=885.1, bsz=885.1, num_updates=9400, lr=0.000295, gnorm=6.678, clip=2, train_wall=8, gb_free=67.7, wall=1675 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:59:13]    INFO >> epoch 007:    242 / 1539 loss=3.816, wps=4742.7, ups=5.97, wpb=794.3, bsz=794.3, num_updates=9450, lr=0.000295, gnorm=6.976, clip=0, train_wall=8, gb_free=70.7, wall=1683 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:59:21]    INFO >> epoch 007:    292 / 1539 loss=3.633, wps=4501.3, ups=6.43, wpb=699.6, bsz=699.6, num_updates=9500, lr=0.000295, gnorm=7.188, clip=2, train_wall=7, gb_free=71.4, wall=1691 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:59:26] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.29 GiB memory in use. Of the allocated memory 70.08 GiB is allocated by PyTorch, and 6.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:59:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:59:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:59:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 27           |        cudaMalloc retries: 52        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69006 MiB |  71763 MiB | 308769 GiB | 308702 GiB |
|       from large pool |  68989 MiB |  71746 MiB | 307018 GiB | 306950 GiB |
|       from small pool |     16 MiB |     18 MiB |   1751 GiB |   1751 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69006 MiB |  71763 MiB | 308769 GiB | 308702 GiB |
|       from large pool |  68989 MiB |  71746 MiB | 307018 GiB | 306950 GiB |
|       from small pool |     16 MiB |     18 MiB |   1751 GiB |   1751 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  68980 MiB |  71736 MiB | 308120 GiB | 308053 GiB |
|       from large pool |  68963 MiB |  71719 MiB | 306372 GiB | 306304 GiB |
|       from small pool |     16 MiB |     17 MiB |   1748 GiB |   1748 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78636 MiB |  78836 MiB | 712584 MiB | 633948 MiB |
|       from large pool |  78610 MiB |  78610 MiB | 708256 MiB | 629646 MiB |
|       from small pool |     26 MiB |    226 MiB |   4328 MiB |   4302 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6901 MiB |  10618 MiB | 298153 GiB | 298147 GiB |
|       from large pool |   6892 MiB |  10608 MiB | 296174 GiB | 296167 GiB |
|       from small pool |      9 MiB |     23 MiB |   1979 GiB |   1979 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     649    |   20344 K  |   20344 K  |
|       from large pool |     343    |     351    |    9674 K  |    9674 K  |
|       from small pool |     298    |     342    |   10670 K  |   10669 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     649    |   20344 K  |   20344 K  |
|       from large pool |     343    |     351    |    9674 K  |    9674 K  |
|       from small pool |     298    |     342    |   10670 K  |   10669 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     169    |     269    |    4531    |    4362    |
|       from large pool |     156    |     156    |    2367    |    2211    |
|       from small pool |      13    |     113    |    2164    |    2151    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     151    |     155    |   11472 K  |   11472 K  |
|       from large pool |     124    |     128    |    6323 K  |    6323 K  |
|       from small pool |      27    |      51    |    5148 K  |    5148 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:59:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:59:26] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:59:30]    INFO >> epoch 007:    343 / 1539 loss=3.838, wps=3756.7, ups=5.47, wpb=687.1, bsz=687.1, num_updates=9550, lr=0.000295, gnorm=5.758, clip=0, train_wall=8, gb_free=69.5, wall=1700 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:59:37]    INFO >> epoch 007:    393 / 1539 loss=3.804, wps=4052.5, ups=6.78, wpb=598.1, bsz=598.1, num_updates=9600, lr=0.000295, gnorm=6.196, clip=0, train_wall=7, gb_free=62, wall=1707 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:59:46]    INFO >> epoch 007:    443 / 1539 loss=3.55, wps=4272.8, ups=6.3, wpb=677.8, bsz=677.8, num_updates=9650, lr=0.000295, gnorm=5.987, clip=2, train_wall=7, gb_free=74.5, wall=1715 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:59:51] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 121.25 MiB is free. Including non-PyTorch memory, this process has 79.00 GiB memory in use. Of the allocated memory 75.66 GiB is allocated by PyTorch, and 2.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:59:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:59:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:59:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 28           |        cudaMalloc retries: 54        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76958 MiB |  77480 MiB | 313195 GiB | 313120 GiB |
|       from large pool |  76945 MiB |  77467 MiB | 311423 GiB | 311348 GiB |
|       from small pool |     12 MiB |     18 MiB |   1772 GiB |   1772 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76958 MiB |  77480 MiB | 313195 GiB | 313120 GiB |
|       from large pool |  76945 MiB |  77467 MiB | 311423 GiB | 311348 GiB |
|       from small pool |     12 MiB |     18 MiB |   1772 GiB |   1772 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76936 MiB |  77458 MiB | 312537 GiB | 312462 GiB |
|       from large pool |  76923 MiB |  77445 MiB | 310767 GiB | 310692 GiB |
|       from small pool |     12 MiB |     18 MiB |   1769 GiB |   1769 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80384 MiB |  80384 MiB | 719326 MiB | 638942 MiB |
|       from large pool |  80358 MiB |  80358 MiB | 714952 MiB | 634594 MiB |
|       from small pool |     26 MiB |     72 MiB |   4374 MiB |   4348 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3425 MiB |   7650 MiB | 302326 GiB | 302323 GiB |
|       from large pool |   3412 MiB |   7637 MiB | 300323 GiB | 300320 GiB |
|       from small pool |     13 MiB |     23 MiB |   2003 GiB |   2003 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   20614 K  |   20613 K  |
|       from large pool |     315    |     322    |    9816 K  |    9816 K  |
|       from small pool |     291    |     356    |   10797 K  |   10797 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   20614 K  |   20613 K  |
|       from large pool |     315    |     322    |    9816 K  |    9816 K  |
|       from small pool |     291    |     356    |   10797 K  |   10797 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     139    |     197    |    4562    |    4423    |
|       from large pool |     126    |     161    |    2375    |    2249    |
|       from small pool |      13    |      36    |    2187    |    2174    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     132    |     132    |   11620 K  |   11620 K  |
|       from large pool |     106    |     106    |    6417 K  |    6417 K  |
|       from small pool |      26    |      48    |    5202 K  |    5202 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:59:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:59:51] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:59:55]    INFO >> epoch 007:    494 / 1539 loss=3.803, wps=4321.5, ups=5.82, wpb=742.8, bsz=742.8, num_updates=9700, lr=0.000295, gnorm=5.984, clip=0, train_wall=8, gb_free=72.2, wall=1724 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:00:02]    INFO >> epoch 007:    544 / 1539 loss=3.737, wps=4216.6, ups=6.68, wpb=631.2, bsz=631.2, num_updates=9750, lr=0.000295, gnorm=6.059, clip=0, train_wall=7, gb_free=74.3, wall=1731 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:00:10]    INFO >> epoch 007:    594 / 1539 loss=3.817, wps=4425, ups=6.47, wpb=684.3, bsz=684.3, num_updates=9800, lr=0.000295, gnorm=5.561, clip=0, train_wall=7, gb_free=75.1, wall=1739 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:00:18]    INFO >> epoch 007:    644 / 1539 loss=3.629, wps=4396.8, ups=6.3, wpb=697.8, bsz=697.8, num_updates=9850, lr=0.000295, gnorm=7.023, clip=0, train_wall=7, gb_free=70.6, wall=1747 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:00:26]    INFO >> epoch 007:    694 / 1539 loss=3.647, wps=5021.4, ups=6.01, wpb=835.9, bsz=835.9, num_updates=9900, lr=0.000295, gnorm=5.766, clip=0, train_wall=8, gb_free=73.9, wall=1755 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:00:34]    INFO >> epoch 007:    744 / 1539 loss=3.724, wps=4498.6, ups=6.32, wpb=712, bsz=712, num_updates=9950, lr=0.000295, gnorm=5.407, clip=0, train_wall=7, gb_free=72.8, wall=1763 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:00:42]    INFO >> epoch 007:    794 / 1539 loss=3.705, wps=4224.3, ups=6.45, wpb=654.6, bsz=654.6, num_updates=10000, lr=0.000295, gnorm=6.025, clip=0, train_wall=7, gb_free=72.1, wall=1771 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:00:53]    INFO >> epoch 007:    844 / 1539 loss=3.626, wps=4558.2, ups=6.31, wpb=722.7, bsz=722.7, num_updates=10050, lr=0.000295, gnorm=6.183, clip=0, train_wall=7, gb_free=70.6, wall=1779 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:01:01]    INFO >> epoch 007:    894 / 1539 loss=3.714, wps=4295, ups=6.24, wpb=688.1, bsz=688.1, num_updates=10100, lr=0.000295, gnorm=6.84, clip=2, train_wall=8, gb_free=73.8, wall=1787 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:01:08]    INFO >> epoch 007:    944 / 1539 loss=3.758, wps=4491.9, ups=6.43, wpb=699, bsz=699, num_updates=10150, lr=0.000295, gnorm=6.415, clip=2, train_wall=7, gb_free=71.9, wall=1795 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:01:17]    INFO >> epoch 007:    994 / 1539 loss=3.753, wps=4495.5, ups=6.11, wpb=736.1, bsz=736.1, num_updates=10200, lr=0.000295, gnorm=6.081, clip=0, train_wall=8, gb_free=70.3, wall=1803 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:01:26]    INFO >> epoch 007:   1044 / 1539 loss=3.64, wps=4631, ups=6.56, wpb=705.7, bsz=705.7, num_updates=10250, lr=0.000295, gnorm=6.423, clip=0, train_wall=7, gb_free=73, wall=1811 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:01:34]    INFO >> epoch 007:   1094 / 1539 loss=3.708, wps=4333.3, ups=6.25, wpb=693.4, bsz=693.4, num_updates=10300, lr=0.000295, gnorm=5.711, clip=0, train_wall=8, gb_free=71.6, wall=1819 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:01:42]    INFO >> epoch 007:   1144 / 1539 loss=3.551, wps=4507.6, ups=5.74, wpb=784.9, bsz=784.9, num_updates=10350, lr=0.000295, gnorm=6.189, clip=0, train_wall=8, gb_free=74, wall=1827 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:01:51]    INFO >> epoch 007:   1194 / 1539 loss=3.661, wps=4895.8, ups=6.03, wpb=811.5, bsz=811.5, num_updates=10400, lr=0.000295, gnorm=6.099, clip=0, train_wall=8, gb_free=72.8, wall=1836 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:02:00]    INFO >> epoch 007:   1244 / 1539 loss=3.772, wps=4394.1, ups=6.4, wpb=686.2, bsz=686.2, num_updates=10450, lr=0.000295, gnorm=5.604, clip=0, train_wall=7, gb_free=68.3, wall=1843 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:02:07]    INFO >> epoch 007:   1294 / 1539 loss=3.725, wps=4169.6, ups=6.51, wpb=640.4, bsz=640.4, num_updates=10500, lr=0.000295, gnorm=5.876, clip=0, train_wall=7, gb_free=70.7, wall=1851 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:02:15]    INFO >> epoch 007:   1344 / 1539 loss=3.752, wps=4650, ups=6.67, wpb=697.4, bsz=697.4, num_updates=10550, lr=0.000295, gnorm=5.852, clip=0, train_wall=7, gb_free=70.4, wall=1859 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:02:23]    INFO >> epoch 007:   1394 / 1539 loss=3.586, wps=4447.6, ups=6.36, wpb=699.1, bsz=699.1, num_updates=10600, lr=0.000295, gnorm=6.433, clip=0, train_wall=7, gb_free=67.1, wall=1866 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:02:30] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 51.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 76.76 GiB is allocated by PyTorch, and 1.81 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:02:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:02:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:02:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 29           |        cudaMalloc retries: 56        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78538 MiB |  78598 MiB | 341065 GiB | 340988 GiB |
|       from large pool |  78151 MiB |  78211 MiB | 339139 GiB | 339063 GiB |
|       from small pool |    386 MiB |    388 MiB |   1925 GiB |   1925 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78538 MiB |  78598 MiB | 341065 GiB | 340988 GiB |
|       from large pool |  78151 MiB |  78211 MiB | 339139 GiB | 339063 GiB |
|       from small pool |    386 MiB |    388 MiB |   1925 GiB |   1925 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78257 MiB |  78317 MiB | 340344 GiB | 340268 GiB |
|       from large pool |  77872 MiB |  77932 MiB | 338421 GiB | 338345 GiB |
|       from small pool |    384 MiB |    386 MiB |   1922 GiB |   1922 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80454 MiB |  80456 MiB | 750388 MiB | 669934 MiB |
|       from large pool |  80028 MiB |  80028 MiB | 745612 MiB | 665584 MiB |
|       from small pool |    426 MiB |    428 MiB |   4776 MiB |   4350 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1855 MiB |   6794 MiB | 329919 GiB | 329917 GiB |
|       from large pool |   1816 MiB |   6788 MiB | 327740 GiB | 327738 GiB |
|       from small pool |     39 MiB |     41 MiB |   2178 GiB |   2178 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7142    |    7145    |   22485 K  |   22478 K  |
|       from large pool |     919    |     920    |   10761 K  |   10760 K  |
|       from small pool |    6223    |    6226    |   11723 K  |   11717 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7142    |    7145    |   22485 K  |   22478 K  |
|       from large pool |     919    |     920    |   10761 K  |   10760 K  |
|       from small pool |    6223    |    6226    |   11723 K  |   11717 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     838    |     839    |    5274    |    4436    |
|       from large pool |     625    |     625    |    2886    |    2261    |
|       from small pool |     213    |     214    |    2388    |    2175    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     519    |     520    |   12655 K  |   12655 K  |
|       from large pool |     138    |     138    |    7032 K  |    7032 K  |
|       from small pool |     381    |     382    |    5623 K  |    5622 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:02:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:02:30] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:02:33]    INFO >> epoch 007:   1445 / 1539 loss=3.725, wps=4207.6, ups=5.88, wpb=716, bsz=716, num_updates=10650, lr=0.000295, gnorm=6.249, clip=0, train_wall=8, gb_free=68.1, wall=1875 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:02:40]    INFO >> epoch 007:   1495 / 1539 loss=3.732, wps=4762.5, ups=6.67, wpb=713.5, bsz=713.5, num_updates=10700, lr=0.000295, gnorm=6.293, clip=0, train_wall=7, gb_free=70.4, wall=1882 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:02:47]    INFO >> epoch 007 | loss 3.698 | wps 4178.6 | ups 5.86 | wpb 712.7 | bsz 712.7 | num_updates 10744 | lr 0.000295 | gnorm 6.224 | clip 0.3 | train_wall 229 | gb_free 70.2 | wall 1890 (progress_bar.py:267, print())[0m
[33m[2025-11-21 17:02:47] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 17:03:04]    INFO >> epoch 007 | valid on 'valid' subset | loss 3.81 | wps 10118.4 | wpb 5412.5 | bsz 5412.5 | num_updates 10744 | best_loss 5.057 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 17:03:04]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 17:03:04]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/checkpoints/checkpoint_last.pt (epoch 7 @ 10744 updates, score 3.81) (writing took 0.017292 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 17:03:04] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 17:03:05]    INFO >> epoch 008:      6 / 1539 loss=3.677, wps=1555.9, ups=2.13, wpb=730.3, bsz=730.3, num_updates=10750, lr=0.000262, gnorm=5.839, clip=0, train_wall=8, gb_free=68.7, wall=1906 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:03:13]    INFO >> epoch 008:     56 / 1539 loss=3.63, wps=4687.6, ups=6.37, wpb=735.8, bsz=735.8, num_updates=10800, lr=0.000262, gnorm=5.814, clip=0, train_wall=7, gb_free=73.9, wall=1914 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:03:21]    INFO >> epoch 008:    106 / 1539 loss=3.645, wps=4637.2, ups=6.05, wpb=766.8, bsz=766.8, num_updates=10850, lr=0.000262, gnorm=6.085, clip=0, train_wall=8, gb_free=73.2, wall=1922 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:03:29]    INFO >> epoch 008:    156 / 1539 loss=3.703, wps=4232.6, ups=6.49, wpb=652.3, bsz=652.3, num_updates=10900, lr=0.000262, gnorm=5.732, clip=0, train_wall=7, gb_free=75.6, wall=1930 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:03:38]    INFO >> epoch 008:    206 / 1539 loss=3.607, wps=5020, ups=6.01, wpb=834.7, bsz=834.7, num_updates=10950, lr=0.000262, gnorm=6.169, clip=0, train_wall=8, gb_free=73.5, wall=1938 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:03:46]    INFO >> epoch 008:    256 / 1539 loss=3.554, wps=4640.9, ups=6.5, wpb=714.1, bsz=714.1, num_updates=11000, lr=0.000262, gnorm=6.837, clip=0, train_wall=7, gb_free=71.5, wall=1946 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:03:54]    INFO >> epoch 008:    306 / 1539 loss=3.78, wps=4603, ups=6.39, wpb=720.8, bsz=720.8, num_updates=11050, lr=0.000262, gnorm=6.594, clip=0, train_wall=7, gb_free=71.8, wall=1954 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:04:02]    INFO >> epoch 008:    356 / 1539 loss=3.777, wps=3917.7, ups=5.89, wpb=665.6, bsz=665.6, num_updates=11100, lr=0.000262, gnorm=5.575, clip=0, train_wall=8, gb_free=74.7, wall=1962 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:04:12]    INFO >> epoch 008:    406 / 1539 loss=3.668, wps=4479.6, ups=6.34, wpb=707, bsz=707, num_updates=11150, lr=0.000262, gnorm=6.464, clip=0, train_wall=7, gb_free=72.1, wall=1970 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:04:20]    INFO >> epoch 008:    456 / 1539 loss=3.692, wps=5438, ups=6.21, wpb=875.6, bsz=875.6, num_updates=11200, lr=0.000262, gnorm=6.266, clip=0, train_wall=8, gb_free=72.6, wall=1978 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:04:27]    INFO >> epoch 008:    506 / 1539 loss=3.698, wps=4877, ups=6.51, wpb=749.6, bsz=749.6, num_updates=11250, lr=0.000262, gnorm=6.691, clip=0, train_wall=7, gb_free=74.3, wall=1986 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:04:35]    INFO >> epoch 008:    556 / 1539 loss=3.691, wps=4292, ups=6.59, wpb=651.2, bsz=651.2, num_updates=11300, lr=0.000262, gnorm=5.813, clip=0, train_wall=7, gb_free=71.8, wall=1993 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:04:44]    INFO >> epoch 008:    606 / 1539 loss=3.761, wps=4416.5, ups=6.47, wpb=682.8, bsz=682.8, num_updates=11350, lr=0.000262, gnorm=6.054, clip=0, train_wall=7, gb_free=75.2, wall=2001 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:04:51]    INFO >> epoch 008:    656 / 1539 loss=3.81, wps=4321.7, ups=6.79, wpb=636.2, bsz=636.2, num_updates=11400, lr=0.000262, gnorm=5.136, clip=0, train_wall=7, gb_free=72.5, wall=2008 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:04:59]    INFO >> epoch 008:    706 / 1539 loss=3.53, wps=4520.9, ups=6.53, wpb=691.9, bsz=691.9, num_updates=11450, lr=0.000262, gnorm=6.492, clip=0, train_wall=7, gb_free=70, wall=2016 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:05:07]    INFO >> epoch 008:    756 / 1539 loss=3.633, wps=4572, ups=6.29, wpb=727, bsz=727, num_updates=11500, lr=0.000262, gnorm=6.472, clip=0, train_wall=7, gb_free=70.9, wall=2024 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:05:15]    INFO >> epoch 008:    806 / 1539 loss=3.605, wps=4528.7, ups=6.52, wpb=694.2, bsz=694.2, num_updates=11550, lr=0.000262, gnorm=6.121, clip=0, train_wall=7, gb_free=68.9, wall=2032 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:05:23]    INFO >> epoch 008:    856 / 1539 loss=3.741, wps=4238.1, ups=7.11, wpb=595.7, bsz=595.7, num_updates=11600, lr=0.000262, gnorm=5.638, clip=0, train_wall=7, gb_free=71.5, wall=2039 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:05:32]    INFO >> epoch 008:    906 / 1539 loss=3.34, wps=4844.6, ups=5.96, wpb=813.3, bsz=813.3, num_updates=11650, lr=0.000262, gnorm=6.759, clip=2, train_wall=8, gb_free=74, wall=2047 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:05:39]    INFO >> epoch 008:    956 / 1539 loss=3.737, wps=4548.6, ups=6.75, wpb=674.1, bsz=674.1, num_updates=11700, lr=0.000262, gnorm=5.817, clip=0, train_wall=7, gb_free=72.9, wall=2054 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:05:45] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 33.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 2.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:05:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:05:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:05:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 30           |        cudaMalloc retries: 59        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78274 MiB |  78334 MiB | 377371 GiB | 377294 GiB |
|       from large pool |  78193 MiB |  78253 MiB | 375231 GiB | 375155 GiB |
|       from small pool |     80 MiB |     82 MiB |   2139 GiB |   2139 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78274 MiB |  78334 MiB | 377371 GiB | 377294 GiB |
|       from large pool |  78193 MiB |  78253 MiB | 375231 GiB | 375155 GiB |
|       from small pool |     80 MiB |     82 MiB |   2139 GiB |   2139 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78222 MiB |  78281 MiB | 376569 GiB | 376492 GiB |
|       from large pool |  78141 MiB |  78201 MiB | 374432 GiB | 374356 GiB |
|       from small pool |     80 MiB |     81 MiB |   2136 GiB |   2136 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80472 MiB |  80498 MiB |    830 GiB | 770062 MiB |
|       from large pool |  80388 MiB |  80388 MiB |    825 GiB | 765090 MiB |
|       from small pool |     84 MiB |    246 MiB |      4 GiB |   4972 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2137 MiB |   7144 MiB | 357992 GiB | 357990 GiB |
|       from large pool |   2134 MiB |   7133 MiB | 355571 GiB | 355569 GiB |
|       from small pool |      3 MiB |     31 MiB |   2420 GiB |   2420 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1796    |    1799    |   24927 K  |   24926 K  |
|       from large pool |     455    |     456    |   11890 K  |   11890 K  |
|       from small pool |    1341    |    1344    |   13037 K  |   13035 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1796    |    1799    |   24927 K  |   24926 K  |
|       from large pool |     455    |     456    |   11890 K  |   11890 K  |
|       from small pool |    1341    |    1344    |   13037 K  |   13035 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     241    |     562    |    5869    |    5628    |
|       from large pool |     199    |     439    |    3341    |    3142    |
|       from small pool |      42    |     123    |    2528    |    2486    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     159    |     160    |   14104 K  |   14104 K  |
|       from large pool |     113    |     113    |    7829 K  |    7829 K  |
|       from small pool |      46    |      56    |    6275 K  |    6275 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:05:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:05:45] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:05:47]    INFO >> epoch 008:   1007 / 1539 loss=3.777, wps=3579.7, ups=5.94, wpb=602.8, bsz=602.8, num_updates=11750, lr=0.000262, gnorm=5.504, clip=0, train_wall=7, gb_free=70.6, wall=2063 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:06:00]    INFO >> epoch 008:   1057 / 1539 loss=3.543, wps=3829.2, ups=4.62, wpb=828.4, bsz=828.4, num_updates=11800, lr=0.000262, gnorm=6.3, clip=2, train_wall=10, gb_free=71.7, wall=2074 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:06:07]    INFO >> epoch 008:   1107 / 1539 loss=3.642, wps=4462.9, ups=6.75, wpb=661.2, bsz=661.2, num_updates=11850, lr=0.000262, gnorm=6.242, clip=0, train_wall=7, gb_free=72.8, wall=2081 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:06:14]    INFO >> epoch 008:   1157 / 1539 loss=3.903, wps=3808.7, ups=6.83, wpb=557.5, bsz=557.5, num_updates=11900, lr=0.000262, gnorm=5.283, clip=0, train_wall=7, gb_free=73.2, wall=2088 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:06:17] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 523.25 MiB is free. Including non-PyTorch memory, this process has 78.61 GiB memory in use. Of the allocated memory 75.66 GiB is allocated by PyTorch, and 2.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:06:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:06:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:06:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 31           |        cudaMalloc retries: 61        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76954 MiB |  77477 MiB | 382821 GiB | 382746 GiB |
|       from large pool |  76941 MiB |  77464 MiB | 380654 GiB | 380579 GiB |
|       from small pool |     12 MiB |     19 MiB |   2167 GiB |   2167 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76954 MiB |  77477 MiB | 382821 GiB | 382746 GiB |
|       from large pool |  76941 MiB |  77464 MiB | 380654 GiB | 380579 GiB |
|       from small pool |     12 MiB |     19 MiB |   2167 GiB |   2167 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76936 MiB |  77458 MiB | 382008 GiB | 381933 GiB |
|       from large pool |  76923 MiB |  77445 MiB | 379844 GiB | 379769 GiB |
|       from small pool |     12 MiB |     19 MiB |   2164 GiB |   2164 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79982 MiB |  80024 MiB |    903 GiB |    825 GiB |
|       from large pool |  79958 MiB |  79958 MiB |    898 GiB |    820 GiB |
|       from small pool |     24 MiB |     66 MiB |      4 GiB |      4 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3027 MiB |   8674 MiB | 363921 GiB | 363918 GiB |
|       from large pool |   3016 MiB |   8662 MiB | 361469 GiB | 361466 GiB |
|       from small pool |     11 MiB |     21 MiB |   2451 GiB |   2451 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   25272 K  |   25272 K  |
|       from large pool |     315    |     322    |   12070 K  |   12070 K  |
|       from small pool |     291    |     348    |   13202 K  |   13201 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   25272 K  |   25272 K  |
|       from large pool |     315    |     322    |   12070 K  |   12070 K  |
|       from small pool |     291    |     348    |   13202 K  |   13201 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      90    |     111    |    5932    |    5842    |
|       from large pool |      78    |      78    |    3384    |    3306    |
|       from small pool |      12    |      33    |    2548    |    2536    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      97    |      97    |   14291 K  |   14290 K  |
|       from large pool |      67    |      67    |    7943 K  |    7943 K  |
|       from small pool |      30    |      52    |    6347 K  |    6347 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:06:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:06:17] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:06:23]    INFO >> epoch 008:   1208 / 1539 loss=3.664, wps=4114, ups=5.75, wpb=715, bsz=715, num_updates=11950, lr=0.000262, gnorm=6.235, clip=0, train_wall=8, gb_free=76, wall=2097 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:06:30] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 47.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 77.61 GiB is allocated by PyTorch, and 983.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:06:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:06:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:06:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 32           |        cudaMalloc retries: 62        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79414 MiB |  79474 MiB | 385104 GiB | 385026 GiB |
|       from large pool |  79015 MiB |  79075 MiB | 382922 GiB | 382845 GiB |
|       from small pool |    398 MiB |    399 MiB |   2181 GiB |   2181 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79414 MiB |  79474 MiB | 385104 GiB | 385026 GiB |
|       from large pool |  79015 MiB |  79075 MiB | 382922 GiB | 382845 GiB |
|       from small pool |    398 MiB |    399 MiB |   2181 GiB |   2181 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79398 MiB |  79458 MiB | 384287 GiB | 384209 GiB |
|       from large pool |  79002 MiB |  79061 MiB | 382108 GiB | 382031 GiB |
|       from small pool |    396 MiB |    397 MiB |   2178 GiB |   2178 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80458 MiB |  80458 MiB |    903 GiB |    825 GiB |
|       from large pool |  80018 MiB |  80018 MiB |    898 GiB |    820 GiB |
|       from small pool |    440 MiB |    440 MiB |      5 GiB |      4 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    983 MiB |   5675 MiB | 366559 GiB | 366558 GiB |
|       from large pool |    942 MiB |   5668 MiB | 364090 GiB | 364090 GiB |
|       from small pool |     41 MiB |     42 MiB |   2468 GiB |   2468 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7351    |    7354    |   25429 K  |   25421 K  |
|       from large pool |     938    |     939    |   12139 K  |   12138 K  |
|       from small pool |    6413    |    6416    |   13290 K  |   13283 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7351    |    7354    |   25429 K  |   25421 K  |
|       from large pool |     938    |     939    |   12139 K  |   12138 K  |
|       from small pool |    6413    |    6416    |   13290 K  |   13283 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     299    |     299    |    6141    |    5842    |
|       from large pool |      79    |      79    |    3385    |    3306    |
|       from small pool |     220    |     220    |    2756    |    2536    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     474    |     474    |   14378 K  |   14377 K  |
|       from large pool |      80    |      80    |    7985 K  |    7985 K  |
|       from small pool |     394    |     394    |    6392 K  |    6392 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:06:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:06:30] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:06:33]    INFO >> epoch 008:   1259 / 1539 loss=3.734, wps=4790.8, ups=5.63, wpb=850.7, bsz=850.7, num_updates=12000, lr=0.000262, gnorm=6.548, clip=0, train_wall=8, gb_free=73.2, wall=2106 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:06:41]    INFO >> epoch 008:   1309 / 1539 loss=3.723, wps=4512.6, ups=6.41, wpb=704.3, bsz=704.3, num_updates=12050, lr=0.000262, gnorm=5.819, clip=0, train_wall=7, gb_free=72.7, wall=2114 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:06:49]    INFO >> epoch 008:   1359 / 1539 loss=3.7, wps=4231.9, ups=6.37, wpb=664.7, bsz=664.7, num_updates=12100, lr=0.000262, gnorm=5.364, clip=0, train_wall=7, gb_free=69.7, wall=2122 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:06:51] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.20 GiB is free. Including non-PyTorch memory, this process has 77.92 GiB memory in use. Of the allocated memory 74.93 GiB is allocated by PyTorch, and 2.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:06:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:06:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:06:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 33           |        cudaMalloc retries: 64        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76325 MiB |  78600 MiB | 388762 GiB | 388687 GiB |
|       from large pool |  76306 MiB |  78582 MiB | 386562 GiB | 386487 GiB |
|       from small pool |     18 MiB |     19 MiB |   2200 GiB |   2200 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76325 MiB |  78600 MiB | 388762 GiB | 388687 GiB |
|       from large pool |  76306 MiB |  78582 MiB | 386562 GiB | 386487 GiB |
|       from small pool |     18 MiB |     19 MiB |   2200 GiB |   2200 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB | 387938 GiB | 387863 GiB |
|       from large pool |  76285 MiB |  78562 MiB | 385741 GiB | 385666 GiB |
|       from small pool |     18 MiB |     19 MiB |   2197 GiB |   2197 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79276 MiB |  80398 MiB |    974 GiB |    897 GiB |
|       from large pool |  79252 MiB |  79958 MiB |    969 GiB |    891 GiB |
|       from small pool |     24 MiB |    440 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2950 MiB |   4271 MiB | 370678 GiB | 370675 GiB |
|       from large pool |   2945 MiB |   4265 MiB | 368188 GiB | 368185 GiB |
|       from small pool |      5 MiB |     27 MiB |   2489 GiB |   2489 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     637    |     643    |   25655 K  |   25655 K  |
|       from large pool |     340    |     346    |   12256 K  |   12256 K  |
|       from small pool |     297    |     348    |   13399 K  |   13398 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     637    |     643    |   25655 K  |   25655 K  |
|       from large pool |     340    |     346    |   12256 K  |   12256 K  |
|       from small pool |     297    |     348    |   13399 K  |   13398 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     106    |     298    |    6199    |    6093    |
|       from large pool |      94    |      95    |    3443    |    3349    |
|       from small pool |      12    |     220    |    2756    |    2744    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     116    |     118    |   14501 K  |   14500 K  |
|       from large pool |      89    |      91    |    8059 K  |    8058 K  |
|       from small pool |      27    |      52    |    6442 K  |    6442 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:06:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:06:51] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:07:01]    INFO >> epoch 008:   1410 / 1539 loss=3.636, wps=3976.5, ups=4.82, wpb=824.4, bsz=824.4, num_updates=12150, lr=0.000262, gnorm=6.102, clip=2, train_wall=8, gb_free=73, wall=2132 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:07:09]    INFO >> epoch 008:   1460 / 1539 loss=3.672, wps=4087.5, ups=5.72, wpb=715.2, bsz=715.2, num_updates=12200, lr=0.000262, gnorm=6.197, clip=0, train_wall=8, gb_free=75, wall=2141 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:07:17]    INFO >> epoch 008:   1510 / 1539 loss=3.8, wps=4344.7, ups=6.52, wpb=666.4, bsz=666.4, num_updates=12250, lr=0.000262, gnorm=5.241, clip=0, train_wall=7, gb_free=67.3, wall=2148 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:07:21]    INFO >> epoch 008 | loss 3.673 | wps 4157.1 | ups 5.83 | wpb 712.7 | bsz 712.7 | num_updates 12279 | lr 0.000262 | gnorm 6.04 | clip 0.2 | train_wall 229 | gb_free 74.8 | wall 2153 (progress_bar.py:267, print())[0m
[33m[2025-11-21 17:07:21] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 17:07:37]    INFO >> epoch 008 | valid on 'valid' subset | loss 3.823 | wps 10363.8 | wpb 5412.5 | bsz 5412.5 | num_updates 12279 | best_loss 5.057 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 17:07:38]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 17:07:38]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/checkpoints/checkpoint_last.pt (epoch 8 @ 12279 updates, score 3.823) (writing took 0.014110 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 17:07:38] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 17:07:41]    INFO >> epoch 009:     21 / 1539 loss=3.69, wps=1489.1, ups=2.23, wpb=668.9, bsz=668.9, num_updates=12300, lr=0.000227, gnorm=5.82, clip=0, train_wall=7, gb_free=73.1, wall=2171 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:07:49]    INFO >> epoch 009:     71 / 1539 loss=3.689, wps=4141, ups=6.24, wpb=663.6, bsz=663.6, num_updates=12350, lr=0.000227, gnorm=5.473, clip=0, train_wall=8, gb_free=73.5, wall=2179 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:07:57]    INFO >> epoch 009:    121 / 1539 loss=3.687, wps=4380.5, ups=6.36, wpb=688.4, bsz=688.4, num_updates=12400, lr=0.000227, gnorm=5.895, clip=0, train_wall=7, gb_free=73.7, wall=2187 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:08:06]    INFO >> epoch 009:    171 / 1539 loss=3.342, wps=5041, ups=5.98, wpb=842.4, bsz=842.4, num_updates=12450, lr=0.000227, gnorm=6.682, clip=0, train_wall=8, gb_free=71.8, wall=2195 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:08:14]    INFO >> epoch 009:    221 / 1539 loss=3.774, wps=4559.4, ups=6.59, wpb=691.5, bsz=691.5, num_updates=12500, lr=0.000227, gnorm=5.456, clip=0, train_wall=7, gb_free=71.5, wall=2203 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:08:21]    INFO >> epoch 009:    271 / 1539 loss=3.715, wps=4741.9, ups=6.74, wpb=703.6, bsz=703.6, num_updates=12550, lr=0.000227, gnorm=5.135, clip=0, train_wall=7, gb_free=70, wall=2210 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:08:30]    INFO >> epoch 009:    321 / 1539 loss=3.566, wps=4773.6, ups=6.08, wpb=784.5, bsz=784.5, num_updates=12600, lr=0.000227, gnorm=5.824, clip=0, train_wall=8, gb_free=66.3, wall=2218 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:08:40]    INFO >> epoch 009:    371 / 1539 loss=3.59, wps=4006.7, ups=5.72, wpb=701, bsz=701, num_updates=12650, lr=0.000227, gnorm=6.786, clip=0, train_wall=8, gb_free=65.8, wall=2227 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:08:40] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 215.25 MiB is free. Including non-PyTorch memory, this process has 78.91 GiB memory in use. Of the allocated memory 73.80 GiB is allocated by PyTorch, and 4.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:08:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:08:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:08:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 34           |        cudaMalloc retries: 66        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75050 MiB |  75993 MiB | 409585 GiB | 409512 GiB |
|       from large pool |  75037 MiB |  75980 MiB | 407262 GiB | 407189 GiB |
|       from small pool |     12 MiB |     16 MiB |   2323 GiB |   2323 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75050 MiB |  75993 MiB | 409585 GiB | 409512 GiB |
|       from large pool |  75037 MiB |  75980 MiB | 407262 GiB | 407189 GiB |
|       from small pool |     12 MiB |     16 MiB |   2323 GiB |   2323 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75035 MiB |  75977 MiB | 408722 GiB | 408649 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 406402 GiB | 406329 GiB |
|       from small pool |     12 MiB |     16 MiB |   2319 GiB |   2319 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80290 MiB |  80290 MiB |    979 GiB |    901 GiB |
|       from large pool |  80266 MiB |  80266 MiB |    974 GiB |    895 GiB |
|       from small pool |     24 MiB |    124 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5239 MiB |   9052 MiB | 391406 GiB | 391400 GiB |
|       from large pool |   5228 MiB |   9041 MiB | 388780 GiB | 388775 GiB |
|       from small pool |     11 MiB |     21 MiB |   2625 GiB |   2625 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   26995 K  |   26995 K  |
|       from large pool |     314    |     322    |   12839 K  |   12839 K  |
|       from small pool |     291    |     341    |   14155 K  |   14155 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   26995 K  |   26995 K  |
|       from large pool |     314    |     322    |   12839 K  |   12839 K  |
|       from small pool |     291    |     341    |   14155 K  |   14155 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     102    |     156    |    6256    |    6154    |
|       from large pool |      90    |      94    |    3450    |    3360    |
|       from small pool |      12    |      62    |    2806    |    2794    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      98    |      98    |   15263 K  |   15263 K  |
|       from large pool |      74    |      74    |    8430 K  |    8430 K  |
|       from small pool |      24    |      44    |    6833 K  |    6832 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:08:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:08:40] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:08:48]    INFO >> epoch 009:    422 / 1539 loss=3.73, wps=4108, ups=5.73, wpb=716.4, bsz=716.4, num_updates=12700, lr=0.000227, gnorm=5.927, clip=0, train_wall=8, gb_free=73.1, wall=2236 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:08:56]    INFO >> epoch 009:    472 / 1539 loss=3.592, wps=4457.6, ups=6.45, wpb=691.3, bsz=691.3, num_updates=12750, lr=0.000227, gnorm=6.129, clip=0, train_wall=7, gb_free=74, wall=2244 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:09:04]    INFO >> epoch 009:    522 / 1539 loss=3.551, wps=4724.4, ups=6.21, wpb=760.8, bsz=760.8, num_updates=12800, lr=0.000227, gnorm=7.059, clip=2, train_wall=8, gb_free=72.7, wall=2252 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:09:14]    INFO >> epoch 009:    572 / 1539 loss=3.76, wps=4506.3, ups=6, wpb=750.9, bsz=750.9, num_updates=12850, lr=0.000227, gnorm=5.868, clip=0, train_wall=8, gb_free=74.2, wall=2260 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:09:15] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 25.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.35 GiB is allocated by PyTorch, and 1.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:09:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:09:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:09:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 35           |        cudaMalloc retries: 67        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79147 MiB |  79207 MiB | 415679 GiB | 415602 GiB |
|       from large pool |  79057 MiB |  79117 MiB | 413324 GiB | 413247 GiB |
|       from small pool |     89 MiB |     91 MiB |   2355 GiB |   2354 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79147 MiB |  79207 MiB | 415679 GiB | 415602 GiB |
|       from large pool |  79057 MiB |  79117 MiB | 413324 GiB | 413247 GiB |
|       from small pool |     89 MiB |     91 MiB |   2355 GiB |   2354 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79123 MiB |  79183 MiB | 414803 GiB | 414726 GiB |
|       from large pool |  79034 MiB |  79093 MiB | 412452 GiB | 412375 GiB |
|       from small pool |     89 MiB |     90 MiB |   2351 GiB |   2351 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80480 MiB |  80480 MiB |    979 GiB |    901 GiB |
|       from large pool |  80386 MiB |  80386 MiB |    974 GiB |    895 GiB |
|       from small pool |     94 MiB |     94 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1272 MiB |   7349 MiB | 398135 GiB | 398134 GiB |
|       from large pool |   1268 MiB |   7339 MiB | 395473 GiB | 395472 GiB |
|       from small pool |      4 MiB |     25 MiB |   2661 GiB |   2661 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1961    |    1964    |   27385 K  |   27383 K  |
|       from large pool |     470    |     471    |   13039 K  |   13038 K  |
|       from small pool |    1491    |    1494    |   14346 K  |   14345 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1961    |    1964    |   27385 K  |   27383 K  |
|       from large pool |     470    |     471    |   13039 K  |   13038 K  |
|       from small pool |    1491    |    1494    |   14346 K  |   14345 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     139    |     139    |    6293    |    6154    |
|       from large pool |      92    |      92    |    3452    |    3360    |
|       from small pool |      47    |      47    |    2841    |    2794    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     126    |     127    |   15473 K  |   15473 K  |
|       from large pool |      78    |      79    |    8555 K  |    8555 K  |
|       from small pool |      48    |      53    |    6918 K  |    6918 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:09:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:09:15] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:09:23]    INFO >> epoch 009:    623 / 1539 loss=3.742, wps=4004.2, ups=5.64, wpb=709.4, bsz=709.4, num_updates=12900, lr=0.000227, gnorm=5.994, clip=0, train_wall=8, gb_free=65.9, wall=2269 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:09:30]    INFO >> epoch 009:    673 / 1539 loss=3.75, wps=4217.7, ups=6.4, wpb=658.7, bsz=658.7, num_updates=12950, lr=0.000227, gnorm=5.165, clip=0, train_wall=7, gb_free=71.5, wall=2277 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:09:38]    INFO >> epoch 009:    723 / 1539 loss=3.679, wps=4329.7, ups=6.45, wpb=671.1, bsz=671.1, num_updates=13000, lr=0.000227, gnorm=6.646, clip=0, train_wall=7, gb_free=71.2, wall=2284 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:09:48]    INFO >> epoch 009:    773 / 1539 loss=3.616, wps=4220.3, ups=6.08, wpb=693.9, bsz=693.9, num_updates=13050, lr=0.000227, gnorm=6.073, clip=0, train_wall=8, gb_free=57.9, wall=2293 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:09:56] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 155.25 MiB is free. Including non-PyTorch memory, this process has 78.96 GiB memory in use. Of the allocated memory 77.59 GiB is allocated by PyTorch, and 893.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:09:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:09:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:09:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 36           |        cudaMalloc retries: 68        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79058 MiB |  79456 MiB | 422841 GiB | 422763 GiB |
|       from large pool |  79039 MiB |  79437 MiB | 420449 GiB | 420372 GiB |
|       from small pool |     18 MiB |     19 MiB |   2391 GiB |   2391 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79058 MiB |  79456 MiB | 422841 GiB | 422763 GiB |
|       from large pool |  79039 MiB |  79437 MiB | 420449 GiB | 420372 GiB |
|       from small pool |     18 MiB |     19 MiB |   2391 GiB |   2391 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79031 MiB |  79429 MiB | 421950 GiB | 421873 GiB |
|       from large pool |  79012 MiB |  79410 MiB | 419562 GiB | 419485 GiB |
|       from small pool |     18 MiB |     19 MiB |   2388 GiB |   2388 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80350 MiB |  80420 MiB |    979 GiB |    901 GiB |
|       from large pool |  80326 MiB |  80326 MiB |    974 GiB |    895 GiB |
|       from small pool |     24 MiB |     94 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1291 MiB |   5527 MiB | 406017 GiB | 406015 GiB |
|       from large pool |   1286 MiB |   5521 MiB | 403312 GiB | 403311 GiB |
|       from small pool |      5 MiB |     23 MiB |   2704 GiB |   2704 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     647    |   27845 K  |   27844 K  |
|       from large pool |     341    |     347    |   13278 K  |   13278 K  |
|       from small pool |     300    |     348    |   14567 K  |   14566 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     647    |   27845 K  |   27844 K  |
|       from large pool |     341    |     347    |   13278 K  |   13278 K  |
|       from small pool |     300    |     348    |   14567 K  |   14566 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     103    |     138    |    6293    |    6190    |
|       from large pool |      91    |      91    |    3452    |    3361    |
|       from small pool |      12    |      47    |    2841    |    2829    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      96    |     102    |   15722 K  |   15721 K  |
|       from large pool |      70    |      76    |    8706 K  |    8706 K  |
|       from small pool |      26    |      52    |    7015 K  |    7015 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:09:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:09:56] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:09:56]    INFO >> epoch 009:    824 / 1539 loss=3.655, wps=4210.5, ups=5.85, wpb=719.6, bsz=719.6, num_updates=13100, lr=0.000227, gnorm=5.643, clip=0, train_wall=7, gb_free=70.5, wall=2301 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:10:04]    INFO >> epoch 009:    874 / 1539 loss=3.722, wps=4439.9, ups=6.89, wpb=644.4, bsz=644.4, num_updates=13150, lr=0.000227, gnorm=5.472, clip=0, train_wall=7, gb_free=74.8, wall=2308 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:10:11]    INFO >> epoch 009:    924 / 1539 loss=3.796, wps=4139.4, ups=6.69, wpb=618.7, bsz=618.7, num_updates=13200, lr=0.000227, gnorm=5.182, clip=0, train_wall=7, gb_free=73.5, wall=2316 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:10:22]    INFO >> epoch 009:    974 / 1539 loss=3.461, wps=4445.8, ups=5.17, wpb=859.7, bsz=859.7, num_updates=13250, lr=0.000227, gnorm=5.9, clip=0, train_wall=9, gb_free=70.8, wall=2326 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:10:30]    INFO >> epoch 009:   1024 / 1539 loss=3.702, wps=4495.7, ups=6.25, wpb=718.9, bsz=718.9, num_updates=13300, lr=0.000227, gnorm=5.336, clip=0, train_wall=7, gb_free=69.9, wall=2334 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:10:38]    INFO >> epoch 009:   1074 / 1539 loss=3.615, wps=4503, ups=6.2, wpb=725.8, bsz=725.8, num_updates=13350, lr=0.000227, gnorm=5.678, clip=0, train_wall=8, gb_free=65.3, wall=2342 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:10:46]    INFO >> epoch 009:   1124 / 1539 loss=3.602, wps=4702, ups=6.18, wpb=760.3, bsz=760.3, num_updates=13400, lr=0.000227, gnorm=6.254, clip=0, train_wall=8, gb_free=73.5, wall=2350 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:10:55]    INFO >> epoch 009:   1174 / 1539 loss=3.757, wps=4438.3, ups=6.63, wpb=669.6, bsz=669.6, num_updates=13450, lr=0.000227, gnorm=5.675, clip=0, train_wall=7, gb_free=72.7, wall=2357 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:11:04]    INFO >> epoch 009:   1224 / 1539 loss=3.291, wps=4907.7, ups=5.69, wpb=862, bsz=862, num_updates=13500, lr=0.000227, gnorm=5.746, clip=2, train_wall=8, gb_free=69, wall=2366 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:11:12]    INFO >> epoch 009:   1274 / 1539 loss=3.585, wps=4825.1, ups=6.22, wpb=776.3, bsz=776.3, num_updates=13550, lr=0.000227, gnorm=6.087, clip=0, train_wall=8, gb_free=72.6, wall=2374 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:11:20]    INFO >> epoch 009:   1324 / 1539 loss=3.754, wps=4256.4, ups=5.97, wpb=712.6, bsz=712.6, num_updates=13600, lr=0.000227, gnorm=5.597, clip=0, train_wall=8, gb_free=75.1, wall=2382 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:11:30]    INFO >> epoch 009:   1374 / 1539 loss=3.239, wps=4555.8, ups=6.08, wpb=749.6, bsz=749.6, num_updates=13650, lr=0.000227, gnorm=6.548, clip=2, train_wall=8, gb_free=69.9, wall=2391 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:11:37] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 37.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 76.24 GiB is allocated by PyTorch, and 2.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:11:37] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:11:37] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:11:37] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 37           |        cudaMalloc retries: 70        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78007 MiB |  78067 MiB | 440587 GiB | 440511 GiB |
|       from large pool |  77625 MiB |  77685 MiB | 438093 GiB | 438017 GiB |
|       from small pool |    382 MiB |    383 MiB |   2494 GiB |   2493 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78007 MiB |  78067 MiB | 440587 GiB | 440511 GiB |
|       from large pool |  77625 MiB |  77685 MiB | 438093 GiB | 438017 GiB |
|       from small pool |    382 MiB |    383 MiB |   2494 GiB |   2493 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77777 MiB |  77836 MiB | 439658 GiB | 439582 GiB |
|       from large pool |  77397 MiB |  77456 MiB | 437167 GiB | 437092 GiB |
|       from small pool |    380 MiB |    381 MiB |   2490 GiB |   2490 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80468 MiB |  80468 MiB |   1006 GiB |    927 GiB |
|       from large pool |  80046 MiB |  80046 MiB |   1000 GiB |    922 GiB |
|       from small pool |    422 MiB |    422 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2400 MiB |   7496 MiB | 423322 GiB | 423319 GiB |
|       from large pool |   2360 MiB |   7489 MiB | 420499 GiB | 420497 GiB |
|       from small pool |     39 MiB |     40 MiB |   2822 GiB |   2822 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7054    |    7057    |   29063 K  |   29056 K  |
|       from large pool |     911    |     912    |   13873 K  |   13872 K  |
|       from small pool |    6143    |    6146    |   15190 K  |   15184 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7054    |    7057    |   29063 K  |   29056 K  |
|       from large pool |     911    |     912    |   13873 K  |   13872 K  |
|       from small pool |    6143    |    6146    |   15190 K  |   15184 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     742    |     742    |    6942    |    6200    |
|       from large pool |     531    |     531    |    3902    |    3371    |
|       from small pool |     211    |     211    |    3040    |    2829    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     504    |     504    |   16405 K  |   16404 K  |
|       from large pool |     127    |     127    |    9092 K  |    9092 K  |
|       from small pool |     377    |     377    |    7312 K  |    7312 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:11:37] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:11:37] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:11:38]    INFO >> epoch 009:   1425 / 1539 loss=3.67, wps=4092.6, ups=6.05, wpb=676.7, bsz=676.7, num_updates=13700, lr=0.000227, gnorm=5.792, clip=0, train_wall=7, gb_free=72, wall=2399 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:11:46]    INFO >> epoch 009:   1475 / 1539 loss=3.771, wps=4082.1, ups=6.44, wpb=634.1, bsz=634.1, num_updates=13750, lr=0.000227, gnorm=5.217, clip=0, train_wall=7, gb_free=72.9, wall=2407 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:11:54]    INFO >> epoch 009:   1525 / 1539 loss=3.743, wps=4067.2, ups=6.53, wpb=623.3, bsz=623.3, num_updates=13800, lr=0.000227, gnorm=5.289, clip=0, train_wall=7, gb_free=67.9, wall=2414 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:11:56]    INFO >> epoch 009 | loss 3.633 | wps 4148 | ups 5.82 | wpb 712.7 | bsz 712.7 | num_updates 13814 | lr 0.000227 | gnorm 5.843 | clip 0.2 | train_wall 231 | gb_free 74.2 | wall 2417 (progress_bar.py:267, print())[0m
[33m[2025-11-21 17:11:56] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 17:12:12]    INFO >> epoch 009 | valid on 'valid' subset | loss 3.802 | wps 10032.1 | wpb 5412.5 | bsz 5412.5 | num_updates 13814 | best_loss 5.057 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 17:12:13]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 17:12:13]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/checkpoints/checkpoint_last.pt (epoch 9 @ 13814 updates, score 3.802) (writing took 0.014404 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 17:12:13] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 17:12:20]    INFO >> epoch 010:     36 / 1539 loss=3.668, wps=1401.9, ups=2, wpb=701.4, bsz=701.4, num_updates=13850, lr=0.000193, gnorm=5.347, clip=0, train_wall=9, gb_free=72.8, wall=2439 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:12:28]    INFO >> epoch 010:     86 / 1539 loss=3.605, wps=4532.7, ups=6.32, wpb=716.9, bsz=716.9, num_updates=13900, lr=0.000193, gnorm=5.925, clip=0, train_wall=7, gb_free=69.6, wall=2447 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:12:37]    INFO >> epoch 010:    136 / 1539 loss=3.664, wps=4054.2, ups=6.47, wpb=626.5, bsz=626.5, num_updates=13950, lr=0.000193, gnorm=5.264, clip=0, train_wall=7, gb_free=74.1, wall=2455 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:12:45]    INFO >> epoch 010:    186 / 1539 loss=3.517, wps=4386.2, ups=5.89, wpb=744.2, bsz=744.2, num_updates=14000, lr=0.000193, gnorm=6.72, clip=2, train_wall=8, gb_free=74.2, wall=2463 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:12:54]    INFO >> epoch 010:    236 / 1539 loss=3.227, wps=4845.9, ups=5.61, wpb=863.9, bsz=863.9, num_updates=14050, lr=0.000193, gnorm=6.197, clip=2, train_wall=8, gb_free=74.4, wall=2472 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:13:02]    INFO >> epoch 010:    286 / 1539 loss=3.795, wps=4174.4, ups=6.18, wpb=676, bsz=676, num_updates=14100, lr=0.000193, gnorm=5.257, clip=0, train_wall=8, gb_free=73.9, wall=2480 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:13:10] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 953.25 MiB is free. Including non-PyTorch memory, this process has 78.19 GiB memory in use. Of the allocated memory 74.10 GiB is allocated by PyTorch, and 3.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:13:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:13:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:13:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 38           |        cudaMalloc retries: 72        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72064 MiB |  75873 MiB | 458286 GiB | 458216 GiB |
|       from large pool |  72046 MiB |  75855 MiB | 455686 GiB | 455615 GiB |
|       from small pool |     17 MiB |     18 MiB |   2600 GiB |   2600 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72064 MiB |  75873 MiB | 458286 GiB | 458216 GiB |
|       from large pool |  72046 MiB |  75855 MiB | 455686 GiB | 455615 GiB |
|       from small pool |     17 MiB |     18 MiB |   2600 GiB |   2600 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  75852 MiB | 457323 GiB | 457252 GiB |
|       from large pool |  72027 MiB |  75835 MiB | 454726 GiB | 454656 GiB |
|       from small pool |     17 MiB |     18 MiB |   2596 GiB |   2596 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79552 MiB |  79754 MiB |   1029 GiB |    952 GiB |
|       from large pool |  79528 MiB |  79528 MiB |   1023 GiB |    946 GiB |
|       from small pool |     24 MiB |    226 MiB |      6 GiB |      6 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4759 MiB |   7038 MiB | 438508 GiB | 438503 GiB |
|       from large pool |   4753 MiB |   7031 MiB | 435569 GiB | 435564 GiB |
|       from small pool |      6 MiB |     23 MiB |   2939 GiB |   2939 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     615    |   30202 K  |   30201 K  |
|       from large pool |     308    |     317    |   14352 K  |   14352 K  |
|       from small pool |     298    |     342    |   15849 K  |   15849 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     615    |   30202 K  |   30201 K  |
|       from large pool |     308    |     317    |   14352 K  |   14352 K  |
|       from small pool |     298    |     342    |   15849 K  |   15849 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     149    |     250    |    7051    |    6902    |
|       from large pool |     137    |     137    |    3911    |    3774    |
|       from small pool |      12    |     113    |    3140    |    3128    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     140    |     141    |   17068 K  |   17068 K  |
|       from large pool |     113    |     114    |    9410 K  |    9410 K  |
|       from small pool |      27    |      49    |    7658 K  |    7658 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:13:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:13:10] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:13:12]    INFO >> epoch 010:    337 / 1539 loss=3.628, wps=4301, ups=6.11, wpb=703.9, bsz=703.9, num_updates=14150, lr=0.000193, gnorm=6.089, clip=0, train_wall=7, gb_free=67.2, wall=2489 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:13:20]    INFO >> epoch 010:    387 / 1539 loss=3.673, wps=4375.6, ups=6.05, wpb=723.5, bsz=723.5, num_updates=14200, lr=0.000193, gnorm=5.789, clip=0, train_wall=8, gb_free=73.3, wall=2497 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:13:28]    INFO >> epoch 010:    437 / 1539 loss=3.668, wps=4048.9, ups=6.41, wpb=631.4, bsz=631.4, num_updates=14250, lr=0.000193, gnorm=5.028, clip=0, train_wall=7, gb_free=72.6, wall=2505 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:13:36]    INFO >> epoch 010:    487 / 1539 loss=3.617, wps=4082.1, ups=6.11, wpb=668.5, bsz=668.5, num_updates=14300, lr=0.000193, gnorm=5.668, clip=0, train_wall=8, gb_free=72.5, wall=2513 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:13:45]    INFO >> epoch 010:    537 / 1539 loss=3.477, wps=4438.1, ups=6.87, wpb=645.7, bsz=645.7, num_updates=14350, lr=0.000193, gnorm=6.228, clip=2, train_wall=7, gb_free=75, wall=2520 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:13:53]    INFO >> epoch 010:    587 / 1539 loss=3.779, wps=4081.6, ups=6.14, wpb=665.2, bsz=665.2, num_updates=14400, lr=0.000193, gnorm=4.985, clip=0, train_wall=8, gb_free=71.4, wall=2528 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:14:01]    INFO >> epoch 010:    637 / 1539 loss=3.669, wps=4718.1, ups=6.56, wpb=719.7, bsz=719.7, num_updates=14450, lr=0.000193, gnorm=5.595, clip=0, train_wall=7, gb_free=69.1, wall=2536 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:14:08]    INFO >> epoch 010:    687 / 1539 loss=3.582, wps=4616.9, ups=6.35, wpb=726.6, bsz=726.6, num_updates=14500, lr=0.000193, gnorm=6.212, clip=0, train_wall=7, gb_free=73.3, wall=2544 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:14:12] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 41.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 76.29 GiB is allocated by PyTorch, and 2.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:14:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:14:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:14:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 39           |        cudaMalloc retries: 73        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78063 MiB |  78123 MiB | 469353 GiB | 469277 GiB |
|       from large pool |  77678 MiB |  77738 MiB | 466692 GiB | 466616 GiB |
|       from small pool |    384 MiB |    385 MiB |   2661 GiB |   2660 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78063 MiB |  78123 MiB | 469353 GiB | 469277 GiB |
|       from large pool |  77678 MiB |  77738 MiB | 466692 GiB | 466616 GiB |
|       from small pool |    384 MiB |    385 MiB |   2661 GiB |   2660 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78017 MiB |  78077 MiB | 468366 GiB | 468290 GiB |
|       from large pool |  77635 MiB |  77694 MiB | 465709 GiB | 465633 GiB |
|       from small pool |    382 MiB |    383 MiB |   2657 GiB |   2656 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80464 MiB |  80464 MiB |   1033 GiB |    954 GiB |
|       from large pool |  80040 MiB |  80040 MiB |   1026 GiB |    948 GiB |
|       from small pool |    424 MiB |    424 MiB |      6 GiB |      6 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2340 MiB |   6606 MiB | 449393 GiB | 449391 GiB |
|       from large pool |   2301 MiB |   6599 MiB | 446384 GiB | 446382 GiB |
|       from small pool |     39 MiB |     40 MiB |   3009 GiB |   3009 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7098    |    7101    |   30947 K  |   30940 K  |
|       from large pool |     915    |     916    |   14731 K  |   14730 K  |
|       from small pool |    6183    |    6186    |   16216 K  |   16209 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7098    |    7101    |   30947 K  |   30940 K  |
|       from large pool |     915    |     916    |   14731 K  |   14730 K  |
|       from small pool |    6183    |    6186    |   16216 K  |   16209 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     402    |     402    |    7305    |    6903    |
|       from large pool |     190    |     190    |    3965    |    3775    |
|       from small pool |     212    |     212    |    3340    |    3128    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     495    |     496    |   17481 K  |   17481 K  |
|       from large pool |     118    |     118    |    9658 K  |    9658 K  |
|       from small pool |     377    |     378    |    7823 K  |    7822 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:14:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:14:12] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:14:18]    INFO >> epoch 010:    738 / 1539 loss=3.752, wps=3947.9, ups=6.14, wpb=642.7, bsz=642.7, num_updates=14550, lr=0.000193, gnorm=5.533, clip=0, train_wall=7, gb_free=68.7, wall=2552 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:14:26]    INFO >> epoch 010:    788 / 1539 loss=3.567, wps=4394.7, ups=6.08, wpb=723.3, bsz=723.3, num_updates=14600, lr=0.000193, gnorm=5.339, clip=0, train_wall=8, gb_free=67.8, wall=2560 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:14:34]    INFO >> epoch 010:    838 / 1539 loss=3.635, wps=4098.8, ups=6.08, wpb=673.9, bsz=673.9, num_updates=14650, lr=0.000193, gnorm=5.554, clip=0, train_wall=8, gb_free=66.8, wall=2568 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:14:43]    INFO >> epoch 010:    888 / 1539 loss=3.412, wps=4842.5, ups=5.71, wpb=848.7, bsz=848.7, num_updates=14700, lr=0.000193, gnorm=5.559, clip=0, train_wall=8, gb_free=76.2, wall=2577 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:14:52]    INFO >> epoch 010:    938 / 1539 loss=3.474, wps=4575.2, ups=6.41, wpb=713.5, bsz=713.5, num_updates=14750, lr=0.000193, gnorm=6.887, clip=0, train_wall=7, gb_free=74.7, wall=2585 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:15:00]    INFO >> epoch 010:    988 / 1539 loss=3.682, wps=4530.9, ups=6.56, wpb=690.7, bsz=690.7, num_updates=14800, lr=0.000193, gnorm=6.602, clip=0, train_wall=7, gb_free=74.4, wall=2593 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:15:08]    INFO >> epoch 010:   1038 / 1539 loss=3.628, wps=4314.6, ups=6.42, wpb=671.7, bsz=671.7, num_updates=14850, lr=0.000193, gnorm=6.56, clip=0, train_wall=7, gb_free=70, wall=2600 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:15:16]    INFO >> epoch 010:   1088 / 1539 loss=3.652, wps=4969.4, ups=6.18, wpb=803.8, bsz=803.8, num_updates=14900, lr=0.000193, gnorm=6.156, clip=0, train_wall=8, gb_free=72.8, wall=2608 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:15:25]    INFO >> epoch 010:   1138 / 1539 loss=3.674, wps=3925.5, ups=6.41, wpb=612.7, bsz=612.7, num_updates=14950, lr=0.000193, gnorm=5.177, clip=0, train_wall=7, gb_free=71.8, wall=2616 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:15:33] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 17.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 76.47 GiB is allocated by PyTorch, and 2.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:15:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:15:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:15:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 40           |        cudaMalloc retries: 75        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78249 MiB |  78309 MiB | 483183 GiB | 483107 GiB |
|       from large pool |  78168 MiB |  78228 MiB | 480446 GiB | 480370 GiB |
|       from small pool |     80 MiB |     82 MiB |   2736 GiB |   2736 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78249 MiB |  78309 MiB | 483183 GiB | 483107 GiB |
|       from large pool |  78168 MiB |  78228 MiB | 480446 GiB | 480370 GiB |
|       from small pool |     80 MiB |     82 MiB |   2736 GiB |   2736 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78222 MiB |  78281 MiB | 482164 GiB | 482088 GiB |
|       from large pool |  78141 MiB |  78201 MiB | 479431 GiB | 479355 GiB |
|       from small pool |     80 MiB |     81 MiB |   2732 GiB |   2732 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80488 MiB |  80490 MiB |   1034 GiB |    955 GiB |
|       from large pool |  80404 MiB |  80404 MiB |   1027 GiB |    949 GiB |
|       from small pool |     84 MiB |    424 MiB |      6 GiB |      6 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2178 MiB |  10077 MiB | 462249 GiB | 462246 GiB |
|       from large pool |   2175 MiB |  10070 MiB | 459152 GiB | 459150 GiB |
|       from small pool |      3 MiB |     23 MiB |   3096 GiB |   3096 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1796    |    1799    |   31868 K  |   31866 K  |
|       from large pool |     455    |     456    |   15195 K  |   15195 K  |
|       from small pool |    1341    |    1344    |   16672 K  |   16671 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1796    |    1799    |   31868 K  |   31866 K  |
|       from large pool |     455    |     456    |   15195 K  |   15195 K  |
|       from small pool |    1341    |    1344    |   16672 K  |   16671 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     224    |     401    |    7338    |    7114    |
|       from large pool |     182    |     189    |    3967    |    3785    |
|       from small pool |      42    |     212    |    3371    |    3329    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     187    |     188    |   18000 K  |   18000 K  |
|       from large pool |     141    |     149    |    9967 K  |    9967 K  |
|       from small pool |      46    |      51    |    8033 K  |    8033 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:15:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:15:33] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:15:34]    INFO >> epoch 010:   1189 / 1539 loss=3.66, wps=4270.4, ups=5.42, wpb=787.9, bsz=787.9, num_updates=15000, lr=0.000193, gnorm=5.854, clip=0, train_wall=8, gb_free=70.1, wall=2626 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:15:43]    INFO >> epoch 010:   1239 / 1539 loss=3.639, wps=4684.6, ups=5.96, wpb=786.3, bsz=786.3, num_updates=15050, lr=0.000193, gnorm=5.832, clip=0, train_wall=8, gb_free=73.2, wall=2634 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:15:51]    INFO >> epoch 010:   1289 / 1539 loss=3.515, wps=4493.3, ups=5.86, wpb=766.5, bsz=766.5, num_updates=15100, lr=0.000193, gnorm=6.247, clip=0, train_wall=8, gb_free=72.7, wall=2642 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:16:00]    INFO >> epoch 010:   1339 / 1539 loss=3.657, wps=4375.6, ups=6.48, wpb=675.5, bsz=675.5, num_updates=15150, lr=0.000193, gnorm=6.618, clip=0, train_wall=7, gb_free=74.4, wall=2650 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:16:08]    INFO >> epoch 010:   1389 / 1539 loss=3.612, wps=4719, ups=6.32, wpb=746.1, bsz=746.1, num_updates=15200, lr=0.000193, gnorm=5.903, clip=0, train_wall=7, gb_free=73.6, wall=2658 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:16:16]    INFO >> epoch 010:   1439 / 1539 loss=3.67, wps=4463.1, ups=6.57, wpb=679.6, bsz=679.6, num_updates=15250, lr=0.000193, gnorm=5.873, clip=0, train_wall=7, gb_free=70.1, wall=2666 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:16:21] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 253.25 MiB is free. Including non-PyTorch memory, this process has 78.87 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 4.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:16:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:16:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:16:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 41           |        cudaMalloc retries: 78        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72822 MiB |  75426 MiB | 491922 GiB | 491850 GiB |
|       from large pool |  72809 MiB |  75413 MiB | 489138 GiB | 489066 GiB |
|       from small pool |     12 MiB |     24 MiB |   2784 GiB |   2784 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72822 MiB |  75426 MiB | 491922 GiB | 491850 GiB |
|       from large pool |  72809 MiB |  75413 MiB | 489138 GiB | 489066 GiB |
|       from small pool |     12 MiB |     24 MiB |   2784 GiB |   2784 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72801 MiB |  75403 MiB | 490883 GiB | 490812 GiB |
|       from large pool |  72788 MiB |  75391 MiB | 488103 GiB | 488032 GiB |
|       from small pool |     12 MiB |     24 MiB |   2780 GiB |   2780 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80252 MiB |  80252 MiB |   1073 GiB |    995 GiB |
|       from large pool |  80228 MiB |  80228 MiB |   1066 GiB |    988 GiB |
|       from small pool |     24 MiB |    210 MiB |      6 GiB |      6 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5151 MiB |   9501 MiB | 469960 GiB | 469955 GiB |
|       from large pool |   5140 MiB |   9488 MiB | 466809 GiB | 466804 GiB |
|       from small pool |     11 MiB |     25 MiB |   3150 GiB |   3150 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     603    |     612    |   32444 K  |   32444 K  |
|       from large pool |     312    |     320    |   15487 K  |   15486 K  |
|       from small pool |     291    |     356    |   16957 K  |   16957 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     603    |     612    |   32444 K  |   32444 K  |
|       from large pool |     312    |     320    |   15487 K  |   15486 K  |
|       from small pool |     291    |     356    |   16957 K  |   16957 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     184    |     325    |    7510    |    7326    |
|       from large pool |     172    |     220    |    4076    |    3904    |
|       from small pool |      12    |     105    |    3434    |    3422    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     169    |     170    |   18327 K  |   18326 K  |
|       from large pool |     142    |     143    |   10163 K  |   10163 K  |
|       from small pool |      27    |      56    |    8163 K  |    8163 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:16:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:16:21] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:16:24]    INFO >> epoch 010:   1490 / 1539 loss=3.741, wps=4132.8, ups=5.95, wpb=694.7, bsz=694.7, num_updates=15300, lr=0.000193, gnorm=5.877, clip=0, train_wall=7, gb_free=64.7, wall=2674 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:16:33]    INFO >> epoch 010 | loss 3.613 | wps 4124.3 | ups 5.79 | wpb 712.7 | bsz 712.7 | num_updates 15349 | lr 0.000193 | gnorm 5.861 | clip 0.2 | train_wall 232 | gb_free 70.8 | wall 2682 (progress_bar.py:267, print())[0m
[33m[2025-11-21 17:16:33] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 17:16:48]    INFO >> epoch 010 | valid on 'valid' subset | loss 3.784 | wps 10451.7 | wpb 5412.5 | bsz 5412.5 | num_updates 15349 | best_loss 5.057 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 17:16:48]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 17:16:48]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/checkpoints/checkpoint_last.pt (epoch 10 @ 15349 updates, score 3.784) (writing took 0.013408 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 17:16:48] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 17:16:48]    INFO >> epoch 011:      1 / 1539 loss=3.679, wps=1605, ups=2.2, wpb=728.6, bsz=728.6, num_updates=15350, lr=0.000161, gnorm=5.678, clip=0, train_wall=7, gb_free=68.7, wall=2697 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:16:56]    INFO >> epoch 011:     51 / 1539 loss=3.727, wps=4318.2, ups=6.67, wpb=647.3, bsz=647.3, num_updates=15400, lr=0.000161, gnorm=5.559, clip=0, train_wall=7, gb_free=75, wall=2704 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:17:05]    INFO >> epoch 011:    101 / 1539 loss=3.662, wps=4604.4, ups=6.31, wpb=729.7, bsz=729.7, num_updates=15450, lr=0.000161, gnorm=5.846, clip=0, train_wall=7, gb_free=74.4, wall=2712 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:17:12]    INFO >> epoch 011:    151 / 1539 loss=3.797, wps=4605, ups=6.91, wpb=666, bsz=666, num_updates=15500, lr=0.000161, gnorm=5.458, clip=0, train_wall=7, gb_free=71.4, wall=2719 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:17:20]    INFO >> epoch 011:    201 / 1539 loss=3.587, wps=4852.6, ups=6.31, wpb=769.4, bsz=769.4, num_updates=15550, lr=0.000161, gnorm=5.689, clip=0, train_wall=7, gb_free=71.9, wall=2727 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:17:28]    INFO >> epoch 011:    251 / 1539 loss=3.66, wps=4883.3, ups=6.69, wpb=729.4, bsz=729.4, num_updates=15600, lr=0.000161, gnorm=5.55, clip=0, train_wall=7, gb_free=73.3, wall=2735 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:17:37]    INFO >> epoch 011:    301 / 1539 loss=3.573, wps=3981.5, ups=6.39, wpb=623.3, bsz=623.3, num_updates=15650, lr=0.000161, gnorm=5.848, clip=0, train_wall=7, gb_free=74.6, wall=2743 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:17:45]    INFO >> epoch 011:    351 / 1539 loss=3.611, wps=4634.7, ups=6.17, wpb=751.3, bsz=751.3, num_updates=15700, lr=0.000161, gnorm=6.073, clip=0, train_wall=8, gb_free=72, wall=2751 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:17:52]    INFO >> epoch 011:    401 / 1539 loss=3.636, wps=4555.8, ups=6.72, wpb=677.5, bsz=677.5, num_updates=15750, lr=0.000161, gnorm=5.472, clip=0, train_wall=7, gb_free=72.3, wall=2758 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:18:00]    INFO >> epoch 011:    451 / 1539 loss=3.778, wps=4271.1, ups=6.84, wpb=624.1, bsz=624.1, num_updates=15800, lr=0.000161, gnorm=5.208, clip=0, train_wall=7, gb_free=72.8, wall=2766 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:18:08]    INFO >> epoch 011:    501 / 1539 loss=3.76, wps=3556, ups=5.86, wpb=606.7, bsz=606.7, num_updates=15850, lr=0.000161, gnorm=4.923, clip=0, train_wall=8, gb_free=75, wall=2774 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:18:17]    INFO >> epoch 011:    551 / 1539 loss=3.642, wps=4341.1, ups=6.72, wpb=645.8, bsz=645.8, num_updates=15900, lr=0.000161, gnorm=5.991, clip=2, train_wall=7, gb_free=73.9, wall=2781 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:18:18] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 31.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.39 GiB is allocated by PyTorch, and 2.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:18:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:18:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:18:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 42           |        cudaMalloc retries: 79        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78168 MiB |  78228 MiB | 514019 GiB | 513943 GiB |
|       from large pool |  77782 MiB |  77842 MiB | 511103 GiB | 511027 GiB |
|       from small pool |    385 MiB |    386 MiB |   2916 GiB |   2916 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78168 MiB |  78228 MiB | 514019 GiB | 513943 GiB |
|       from large pool |  77782 MiB |  77842 MiB | 511103 GiB | 511027 GiB |
|       from small pool |    385 MiB |    386 MiB |   2916 GiB |   2916 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78137 MiB |  78197 MiB | 512935 GiB | 512858 GiB |
|       from large pool |  77753 MiB |  77813 MiB | 510022 GiB | 509946 GiB |
|       from small pool |    383 MiB |    384 MiB |   2912 GiB |   2912 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80474 MiB |  80476 MiB |   1076 GiB |    997 GiB |
|       from large pool |  80050 MiB |  80050 MiB |   1068 GiB |    990 GiB |
|       from small pool |    424 MiB |    426 MiB |      7 GiB |      6 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2245 MiB |   6868 MiB | 488992 GiB | 488989 GiB |
|       from large pool |   2207 MiB |   6862 MiB | 485694 GiB | 485692 GiB |
|       from small pool |     38 MiB |     40 MiB |   3297 GiB |   3297 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7120    |    7123    |   33914 K  |   33907 K  |
|       from large pool |     917    |     918    |   16134 K  |   16134 K  |
|       from small pool |    6203    |    6206    |   17779 K  |   17773 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7120    |    7123    |   33914 K  |   33907 K  |
|       from large pool |     917    |     918    |   16134 K  |   16134 K  |
|       from small pool |    6203    |    6206    |   17779 K  |   17773 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     417    |     418    |    7746    |    7329    |
|       from large pool |     205    |     205    |    4111    |    3906    |
|       from small pool |     212    |     213    |    3635    |    3423    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     548    |     550    |   19178 K  |   19177 K  |
|       from large pool |     171    |     171    |   10601 K  |   10601 K  |
|       from small pool |     377    |     379    |    8576 K  |    8576 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:18:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:18:18] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:18:25]    INFO >> epoch 011:    602 / 1539 loss=3.646, wps=4129.8, ups=5.95, wpb=694.1, bsz=694.1, num_updates=15950, lr=0.000161, gnorm=5.856, clip=0, train_wall=7, gb_free=71.6, wall=2790 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:18:34]    INFO >> epoch 011:    652 / 1539 loss=3.593, wps=4815.2, ups=5.91, wpb=815.3, bsz=815.3, num_updates=16000, lr=0.000161, gnorm=5.461, clip=0, train_wall=8, gb_free=71.5, wall=2798 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:18:41]    INFO >> epoch 011:    702 / 1539 loss=3.79, wps=4367.4, ups=6.61, wpb=661.1, bsz=661.1, num_updates=16050, lr=0.000161, gnorm=5.148, clip=0, train_wall=7, gb_free=71, wall=2806 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:18:52]    INFO >> epoch 011:    752 / 1539 loss=3.598, wps=4108.1, ups=5.61, wpb=731.8, bsz=731.8, num_updates=16100, lr=0.000161, gnorm=5.395, clip=0, train_wall=8, gb_free=72.6, wall=2815 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:18:57] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 457.25 MiB is free. Including non-PyTorch memory, this process has 78.67 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 4.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:18:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:18:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:18:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 43           |        cudaMalloc retries: 81        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72821 MiB |  75424 MiB | 521027 GiB | 520956 GiB |
|       from large pool |  72808 MiB |  75411 MiB | 518073 GiB | 518002 GiB |
|       from small pool |     12 MiB |     21 MiB |   2953 GiB |   2953 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72821 MiB |  75424 MiB | 521027 GiB | 520956 GiB |
|       from large pool |  72808 MiB |  75411 MiB | 518073 GiB | 518002 GiB |
|       from small pool |     12 MiB |     21 MiB |   2953 GiB |   2953 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72801 MiB |  75403 MiB | 519925 GiB | 519854 GiB |
|       from large pool |  72788 MiB |  75391 MiB | 516976 GiB | 516905 GiB |
|       from small pool |     12 MiB |     21 MiB |   2949 GiB |   2949 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80048 MiB |  80424 MiB |   1078 GiB |   1000 GiB |
|       from large pool |  80024 MiB |  80400 MiB |   1071 GiB |    993 GiB |
|       from small pool |     24 MiB |    424 MiB |      7 GiB |      7 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   7226 MiB |  10655 MiB | 495363 GiB | 495356 GiB |
|       from large pool |   7215 MiB |  10644 MiB | 492022 GiB | 492015 GiB |
|       from small pool |     11 MiB |     29 MiB |   3341 GiB |   3341 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     603    |     612    |   34373 K  |   34372 K  |
|       from large pool |     312    |     320    |   16366 K  |   16366 K  |
|       from small pool |     291    |     356    |   18006 K  |   18006 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     603    |     612    |   34373 K  |   34372 K  |
|       from large pool |     312    |     320    |   16366 K  |   16366 K  |
|       from small pool |     291    |     356    |   18006 K  |   18006 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     182    |     416    |    7749    |    7567    |
|       from large pool |     170    |     204    |    4114    |    3944    |
|       from small pool |      12    |     212    |    3635    |    3623    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     176    |     177    |   19442 K  |   19441 K  |
|       from large pool |     147    |     148    |   10758 K  |   10758 K  |
|       from small pool |      29    |      61    |    8683 K  |    8683 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:18:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:18:57] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:18:59]    INFO >> epoch 011:    803 / 1539 loss=3.714, wps=3744, ups=6.39, wpb=586.3, bsz=586.3, num_updates=16150, lr=0.000161, gnorm=4.979, clip=0, train_wall=7, gb_free=62.8, wall=2823 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:19:07]    INFO >> epoch 011:    853 / 1539 loss=3.608, wps=4437.3, ups=6.75, wpb=657, bsz=657, num_updates=16200, lr=0.000161, gnorm=5.061, clip=0, train_wall=7, gb_free=74.8, wall=2830 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:19:14]    INFO >> epoch 011:    903 / 1539 loss=3.611, wps=5188.6, ups=6.65, wpb=779.8, bsz=779.8, num_updates=16250, lr=0.000161, gnorm=6.077, clip=0, train_wall=7, gb_free=72.7, wall=2838 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:19:24]    INFO >> epoch 011:    953 / 1539 loss=3.531, wps=4726.4, ups=6.34, wpb=746, bsz=746, num_updates=16300, lr=0.000161, gnorm=5.791, clip=2, train_wall=7, gb_free=72, wall=2845 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:19:32]    INFO >> epoch 011:   1003 / 1539 loss=3.625, wps=4648, ups=6.3, wpb=737.8, bsz=737.8, num_updates=16350, lr=0.000161, gnorm=5.425, clip=0, train_wall=7, gb_free=71.3, wall=2853 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:19:39]    INFO >> epoch 011:   1053 / 1539 loss=3.538, wps=4504, ups=6.5, wpb=692.9, bsz=692.9, num_updates=16400, lr=0.000161, gnorm=5.427, clip=0, train_wall=7, gb_free=73.7, wall=2861 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:19:44] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 2 has a total capacity of 79.14 GiB of which 457.25 MiB is free. Including non-PyTorch memory, this process has 78.67 GiB memory in use. Of the allocated memory 70.08 GiB is allocated by PyTorch, and 8.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:19:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:19:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:19:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 44           |        cudaMalloc retries: 82        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69005 MiB |  71762 MiB | 529586 GiB | 529518 GiB |
|       from large pool |  68989 MiB |  71745 MiB | 526586 GiB | 526519 GiB |
|       from small pool |     16 MiB |     17 MiB |   2999 GiB |   2999 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69005 MiB |  71762 MiB | 529586 GiB | 529518 GiB |
|       from large pool |  68989 MiB |  71745 MiB | 526586 GiB | 526519 GiB |
|       from small pool |     16 MiB |     17 MiB |   2999 GiB |   2999 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  68980 MiB |  71736 MiB | 528466 GiB | 528399 GiB |
|       from large pool |  68963 MiB |  71719 MiB | 525471 GiB | 525403 GiB |
|       from small pool |     16 MiB |     17 MiB |   2995 GiB |   2995 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80048 MiB |  80148 MiB |   1078 GiB |   1000 GiB |
|       from large pool |  80024 MiB |  80024 MiB |   1071 GiB |    993 GiB |
|       from small pool |     24 MiB |    124 MiB |      7 GiB |      7 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   8408 MiB |  12174 MiB | 503560 GiB | 503552 GiB |
|       from large pool |   8400 MiB |  12166 MiB | 500166 GiB | 500158 GiB |
|       from small pool |      7 MiB |     24 MiB |   3393 GiB |   3393 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     649    |   34930 K  |   34930 K  |
|       from large pool |     343    |     351    |   16651 K  |   16650 K  |
|       from small pool |     298    |     342    |   18279 K  |   18279 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     649    |   34930 K  |   34930 K  |
|       from large pool |     343    |     351    |   16651 K  |   16650 K  |
|       from small pool |     298    |     342    |   18279 K  |   18279 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     182    |     232    |    7799    |    7617    |
|       from large pool |     170    |     170    |    4114    |    3944    |
|       from small pool |      12    |      62    |    3685    |    3673    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     162    |     163    |   19754 K  |   19754 K  |
|       from large pool |     137    |     138    |   10947 K  |   10947 K  |
|       from small pool |      25    |      51    |    8806 K  |    8806 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:19:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:19:44] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:19:48]    INFO >> epoch 011:   1104 / 1539 loss=3.692, wps=4305.8, ups=5.64, wpb=764, bsz=764, num_updates=16450, lr=0.000161, gnorm=5.225, clip=0, train_wall=8, gb_free=75, wall=2870 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:19:58]    INFO >> epoch 011:   1154 / 1539 loss=3.48, wps=5420, ups=5.96, wpb=909, bsz=909, num_updates=16500, lr=0.000161, gnorm=6.183, clip=0, train_wall=8, gb_free=69.3, wall=2878 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:20:06]    INFO >> epoch 011:   1204 / 1539 loss=3.593, wps=3832.1, ups=6.3, wpb=608.2, bsz=608.2, num_updates=16550, lr=0.000161, gnorm=5.004, clip=0, train_wall=7, gb_free=68.8, wall=2886 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:20:15]    INFO >> epoch 011:   1254 / 1539 loss=3.45, wps=4475.6, ups=5.4, wpb=828.9, bsz=828.9, num_updates=16600, lr=0.000161, gnorm=6.582, clip=0, train_wall=9, gb_free=73.1, wall=2896 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:20:23]    INFO >> epoch 011:   1304 / 1539 loss=3.647, wps=4663.9, ups=6.31, wpb=739.1, bsz=739.1, num_updates=16650, lr=0.000161, gnorm=6.068, clip=0, train_wall=7, gb_free=71.9, wall=2903 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:20:27] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.06 GiB is free. Including non-PyTorch memory, this process has 78.06 GiB memory in use. Of the allocated memory 74.92 GiB is allocated by PyTorch, and 2.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:20:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:20:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:20:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 45           |        cudaMalloc retries: 85        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76320 MiB |  78596 MiB | 536717 GiB | 536643 GiB |
|       from large pool |  76301 MiB |  78578 MiB | 533679 GiB | 533604 GiB |
|       from small pool |     18 MiB |     23 MiB |   3038 GiB |   3038 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76320 MiB |  78596 MiB | 536717 GiB | 536643 GiB |
|       from large pool |  76301 MiB |  78578 MiB | 533679 GiB | 533604 GiB |
|       from small pool |     18 MiB |     23 MiB |   3038 GiB |   3038 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB | 535583 GiB | 535508 GiB |
|       from large pool |  76285 MiB |  78562 MiB | 532548 GiB | 532474 GiB |
|       from small pool |     18 MiB |     23 MiB |   3034 GiB |   3034 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79424 MiB |  79424 MiB |   1176 GiB |   1098 GiB |
|       from large pool |  79400 MiB |  79400 MiB |   1169 GiB |   1091 GiB |
|       from small pool |     24 MiB |     76 MiB |      7 GiB |      7 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3103 MiB |   4235 MiB | 510413 GiB | 510410 GiB |
|       from large pool |   3098 MiB |   4229 MiB | 506974 GiB | 506971 GiB |
|       from small pool |      5 MiB |     27 MiB |   3438 GiB |   3438 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     640    |     646    |   35395 K  |   35394 K  |
|       from large pool |     340    |     346    |   16881 K  |   16880 K  |
|       from small pool |     300    |     356    |   18513 K  |   18513 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     640    |     646    |   35395 K  |   35394 K  |
|       from large pool |     340    |     346    |   16881 K  |   16880 K  |
|       from small pool |     300    |     356    |   18513 K  |   18513 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     110    |     179    |    7988    |    7878    |
|       from large pool |      98    |     141    |    4184    |    4086    |
|       from small pool |      12    |      38    |    3804    |    3792    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     125    |     125    |   20016 K  |   20016 K  |
|       from large pool |     100    |     100    |   11099 K  |   11099 K  |
|       from small pool |      25    |      56    |    8917 K  |    8917 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:20:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:20:27] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:20:34]    INFO >> epoch 011:   1355 / 1539 loss=3.607, wps=3401.5, ups=4.94, wpb=689.1, bsz=689.1, num_updates=16700, lr=0.000161, gnorm=5.541, clip=0, train_wall=8, gb_free=56.8, wall=2914 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:20:42]    INFO >> epoch 011:   1405 / 1539 loss=3.816, wps=4938.3, ups=6.24, wpb=791.3, bsz=791.3, num_updates=16750, lr=0.000161, gnorm=5.826, clip=0, train_wall=8, gb_free=73.5, wall=2922 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:20:50]    INFO >> epoch 011:   1455 / 1539 loss=3.626, wps=4297.1, ups=6.46, wpb=665.3, bsz=665.3, num_updates=16800, lr=0.000161, gnorm=5.776, clip=0, train_wall=7, gb_free=65.3, wall=2929 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:21:00]    INFO >> epoch 011:   1505 / 1539 loss=3.243, wps=4982.4, ups=6.09, wpb=818.6, bsz=818.6, num_updates=16850, lr=0.000161, gnorm=6.517, clip=2, train_wall=8, gb_free=72.5, wall=2938 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:21:05]    INFO >> epoch 011 | loss 3.621 | wps 4190.1 | ups 5.88 | wpb 712.7 | bsz 712.7 | num_updates 16884 | lr 0.000161 | gnorm 5.642 | clip 0.2 | train_wall 228 | gb_free 73.9 | wall 2943 (progress_bar.py:267, print())[0m
[33m[2025-11-21 17:21:05] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 17:21:19]    INFO >> epoch 011 | valid on 'valid' subset | loss 3.789 | wps 10561.2 | wpb 5412.5 | bsz 5412.5 | num_updates 16884 | best_loss 5.057 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 17:21:20]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 17:21:20]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/checkpoints/checkpoint_last.pt (epoch 11 @ 16884 updates, score 3.789) (writing took 0.021312 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[32m[2025-11-21 17:21:20]    INFO >> Êó©ÂÅú: È™åËØÅÊÄßËÉΩÂ∑≤10ËΩÆÊú™ÊèêÂçá (train_enhanced.py:616, single_main())[0m
[32m[2025-11-21 17:21:20]    INFO >> ËÆ≠ÁªÉÂÆåÊàêÔºåÁî®Êó∂ 2889.0 Áßí (train_enhanced.py:626, single_main())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 17:21:20]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 17:21:20]    INFO >> ÊâÄÊúâÊó•ÂøóÂ∑≤‰øùÂ≠òÂà∞: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/logs (train_enhanced.py:631, single_main())[0m
[32m[2025-11-21 17:21:20]    INFO >> 
================================================================================ (train_enhanced.py:634, single_main())[0m
[32m[2025-11-21 17:21:20]    INFO >> ÂºÄÂßãÊµãËØï... (train_enhanced.py:635, single_main())[0m
[32m[2025-11-21 17:21:20]    INFO >> ================================================================================ (train_enhanced.py:636, single_main())[0m
[32m[2025-11-21 17:21:20]    INFO >> Âä†ËΩΩÊúÄ‰Ω≥checkpoint: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/checkpoints/checkpoint_best.pt (train_enhanced.py:50, run_test_after_training())[0m
[32m[2025-11-21 17:21:20]    INFO >> ÊµãËØïÈõÜ: test (train_enhanced.py:51, run_test_after_training())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/checkpoint_utils.py:212: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(
[32m[2025-11-21 17:22:12]    INFO >> 
================================================================================ (train_enhanced.py:168, run_test_after_training())[0m
[32m[2025-11-21 17:22:12]    INFO >> ÊµãËØïÁªìÊûú: (train_enhanced.py:169, run_test_after_training())[0m
[32m[2025-11-21 17:22:12]    INFO >> -------------------------------------------------------------------------------- (train_enhanced.py:170, run_test_after_training())[0m
[32m[2025-11-21 17:22:12]    INFO >> Âπ≥ÂùáLoss:      3.9355 (train_enhanced.py:171, run_test_after_training())[0m
[32m[2025-11-21 17:22:12]    INFO >> Acc@1:         25.51% (train_enhanced.py:172, run_test_after_training())[0m
[32m[2025-11-21 17:22:12]    INFO >> Acc@5:         59.62% (train_enhanced.py:173, run_test_after_training())[0m
[32m[2025-11-21 17:22:12]    INFO >> Acc@1 (Âê´any): 25.51% (train_enhanced.py:174, run_test_after_training())[0m
[32m[2025-11-21 17:22:12]    INFO >> Acc@5 (Âê´any): 59.62% (train_enhanced.py:175, run_test_after_training())[0m
[32m[2025-11-21 17:22:12]    INFO >> ================================================================================ (train_enhanced.py:176, run_test_after_training())[0m
[32m[2025-11-21 17:22:12]    INFO >> ÊµãËØïÁªìÊûúÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/checkpoints/res.txt (train_enhanced.py:187, run_test_after_training())[0m
[32m[2025-11-21 17:22:12]    INFO >> ËÆ≠ÁªÉÊó•ÂøóÂ∑≤Êõ¥Êñ∞: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/logs (train_enhanced.py:222, run_test_after_training())[0m
[TrainingLogger] Êó•ÂøóÁõÆÂΩï: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/logs
[TrainingLogger] ÂéüÂßãËæìÂá∫Â∞Ü‰øùÂ≠òÂà∞: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/logs/training_output.log
[TrainingLogger] Epoch 1 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/logs/metrics.json
[TrainingLogger] Epoch 2 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/logs/metrics.json
[TrainingLogger] Epoch 3 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/logs/metrics.json
[TrainingLogger] Epoch 4 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/logs/metrics.json
[TrainingLogger] Epoch 5 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/logs/metrics.json
[TrainingLogger] Epoch 6 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/logs/metrics.json
[TrainingLogger] Epoch 7 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/logs/metrics.json
[TrainingLogger] Epoch 8 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/logs/metrics.json
[TrainingLogger] Epoch 9 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/logs/metrics.json
[TrainingLogger] Epoch 10 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/logs/metrics.json
[TrainingLogger] Epoch 11 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_02/logs/metrics.json

‚úì dropout_02 ÊàêÂäü

Á≠âÂæÖ3Áßí...

ËøõÂ∫¶: 4/4

============================================================
ÂÆûÈ™å: dropout_03 - Dropout=0.3 (ÊûÅÂº∫Ê≠£ÂàôÂåñ)
Êó∂Èó¥: 2025-11-21 17:22:56
============================================================

[32m[2025-11-21 17:22:58]    INFO >> Âä†ËΩΩÈÖçÁΩÆ: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/config.yml (train_enhanced.py:666, cli_main())[0m
[32m[2025-11-21 17:22:58]    INFO >> ÂçïGPUËÆ≠ÁªÉ... (train_enhanced.py:694, cli_main())[0m
[32m[2025-11-21 17:22:58]    INFO >> ËÆ≠ÁªÉÊó•ÂøóÂ∞Ü‰øùÂ≠òÂà∞: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/logs (train_enhanced.py:561, single_main())[0m
[32m[2025-11-21 17:22:58]    INFO >> [nodes] dictionary: 9999 types (typilus.py:102, setup_task())[0m
[32m[2025-11-21 17:22:58]    INFO >> [edges] dictionary: 0 types (typilus.py:102, setup_task())[0m
[32m[2025-11-21 17:22:58]    INFO >> [supernodes.annotation] dictionary: 99 types (typilus.py:106, setup_task())[0m
[32m[2025-11-21 17:23:07]    INFO >> Typilus(
  (encoder): GGNNEncoder(
    (node_embedding): Embedding(9999, 64, padding_idx=0)
    (node_layer): Sequential(
      (0): Dropout(p=0.3, inplace=False)
      (1): Linear(in_features=64, out_features=64, bias=False)
      (2): Dropout(p=0.3, inplace=False)
    )
    (ggnns): ModuleList(
      (0): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=64, out_features=64, bias=False)
          (OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (NEXT): Linear(in_features=64, out_features=64, bias=False)
          (SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
          (_CHILD): Linear(in_features=64, out_features=64, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT): Linear(in_features=64, out_features=64, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (_COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (_RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
        )
        (rnn_cell): GRUCell(64, 64)
      )
      (1): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=64, out_features=64, bias=False)
          (OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (NEXT): Linear(in_features=64, out_features=64, bias=False)
          (SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
          (_CHILD): Linear(in_features=64, out_features=64, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT): Linear(in_features=64, out_features=64, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (_COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (_RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
        )
        (rnn_cell): GRUCell(128, 64)
      )
    )
  )
  (decoder): DenseDecoder(
    (cls_layers): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=False)
      (1): Dropout(p=0.3, inplace=False)
      (2): Linear(in_features=64, out_features=99, bias=True)
    )
  )
) (train_enhanced.py:568, single_main())[0m
[32m[2025-11-21 17:23:07]    INFO >> Ê®°Âûã: typilus, ÊçüÂ§±ÂáΩÊï∞: TypilusCriterion (train_enhanced.py:569, single_main())[0m
[32m[2025-11-21 17:23:07]    INFO >> Ê®°ÂûãÂèÇÊï∞: 847843 (ÂèØËÆ≠ÁªÉ: 847843) (train_enhanced.py:570, single_main())[0m
[32m[2025-11-21 17:23:07]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 17:23:07]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 80579 MB ; used memory = 1340 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 17:23:07]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 17:23:07]    INFO >> ‰ΩøÁî® 1 ‰∏™GPUËÆ≠ÁªÉ (train_enhanced.py:576, single_main())[0m
[32m[2025-11-21 17:23:07]    INFO >> no existing checkpoint found /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-21 17:23:07]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[32m[2025-11-21 17:24:15]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-21 17:24:15] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-21 17:24:24]    INFO >> epoch 001:     50 / 1539 loss=5.594, wps=5101.4, ups=7.06, wpb=720, bsz=720, num_updates=50, lr=0.0004, gnorm=8.449, clip=0, train_wall=7, gb_free=74.2, wall=72 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:24:25] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 43.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 73.64 GiB is allocated by PyTorch, and 4.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:24:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:24:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:24:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 2         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75347 MiB |  75407 MiB |   1756 GiB |   1682 GiB |
|       from large pool |  74991 MiB |  75051 MiB |   1743 GiB |   1670 GiB |
|       from small pool |    356 MiB |    357 MiB |     12 GiB |     12 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75347 MiB |  75407 MiB |   1756 GiB |   1682 GiB |
|       from large pool |  74991 MiB |  75051 MiB |   1743 GiB |   1670 GiB |
|       from small pool |    356 MiB |    357 MiB |     12 GiB |     12 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75195 MiB |  75254 MiB |   1750 GiB |   1676 GiB |
|       from large pool |  74841 MiB |  74900 MiB |   1737 GiB |   1664 GiB |
|       from small pool |    354 MiB |    355 MiB |     12 GiB |     12 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80462 MiB |  80498 MiB |  91994 MiB |  11532 MiB |
|       from large pool |  80070 MiB |  80136 MiB |  91590 MiB |  11520 MiB |
|       from small pool |    392 MiB |    394 MiB |    404 MiB |     12 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5054 MiB |   5960 MiB |    875 GiB |    870 GiB |
|       from large pool |   5018 MiB |   5948 MiB |    859 GiB |    855 GiB |
|       from small pool |     35 MiB |     37 MiB |     15 GiB |     14 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6579    |    6582    |  136626    |  130047    |
|       from large pool |     868    |     869    |   57719    |   56851    |
|       from small pool |    5711    |    5714    |   78907    |   73196    |
|---------------------------------------------------------------------------|
| Active allocs         |    6579    |    6582    |  136626    |  130047    |
|       from large pool |     868    |     869    |   57719    |   56851    |
|       from small pool |    5711    |    5714    |   78907    |   73196    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     758    |     793    |    1006    |     248    |
|       from large pool |     562    |     612    |     804    |     242    |
|       from small pool |     196    |     197    |     202    |       6    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     654    |     656    |   86039    |   85385    |
|       from large pool |     306    |     306    |   44256    |   43950    |
|       from small pool |     348    |     350    |   41783    |   41435    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:24:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:24:25] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:24:31]    INFO >> epoch 001:    101 / 1539 loss=5.936, wps=4510.1, ups=7.39, wpb=610.7, bsz=610.7, num_updates=100, lr=0.0004, gnorm=9.977, clip=0, train_wall=6, gb_free=75.6, wall=79 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:24:38]    INFO >> epoch 001:    151 / 1539 loss=6.193, wps=5510.3, ups=6.75, wpb=816.7, bsz=816.7, num_updates=150, lr=0.0004, gnorm=11.051, clip=0, train_wall=7, gb_free=74.2, wall=87 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:24:45]    INFO >> epoch 001:    201 / 1539 loss=6.395, wps=4849.8, ups=7.56, wpb=641.7, bsz=641.7, num_updates=200, lr=0.0004, gnorm=11.155, clip=0, train_wall=6, gb_free=74.8, wall=93 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:24:51]    INFO >> epoch 001:    251 / 1539 loss=6.39, wps=4734.5, ups=7.42, wpb=637.9, bsz=637.9, num_updates=250, lr=0.0004, gnorm=10.239, clip=2, train_wall=6, gb_free=71.5, wall=100 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:25:00]    INFO >> epoch 001:    301 / 1539 loss=6.102, wps=5193.4, ups=6.6, wpb=786.5, bsz=786.5, num_updates=300, lr=0.0004, gnorm=10.886, clip=0, train_wall=7, gb_free=73.8, wall=108 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:25:07]    INFO >> epoch 001:    351 / 1539 loss=6.262, wps=4829.7, ups=7.19, wpb=671.6, bsz=671.6, num_updates=350, lr=0.0004, gnorm=11.73, clip=0, train_wall=6, gb_free=72.4, wall=115 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:25:15]    INFO >> epoch 001:    401 / 1539 loss=6.135, wps=5848.7, ups=6.87, wpb=851.7, bsz=851.7, num_updates=400, lr=0.0004, gnorm=10.496, clip=0, train_wall=7, gb_free=73.2, wall=122 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:25:24] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 7.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.81 GiB is allocated by PyTorch, and 823.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:25:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:25:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:25:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 4         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79614 MiB |  79674 MiB |  12419 GiB |  12342 GiB |
|       from large pool |  79520 MiB |  79580 MiB |  12346 GiB |  12268 GiB |
|       from small pool |     94 MiB |     95 MiB |     73 GiB |     73 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79614 MiB |  79674 MiB |  12419 GiB |  12342 GiB |
|       from large pool |  79520 MiB |  79580 MiB |  12346 GiB |  12268 GiB |
|       from small pool |     94 MiB |     95 MiB |     73 GiB |     73 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79544 MiB |  79604 MiB |  12379 GiB |  12302 GiB |
|       from large pool |  79450 MiB |  79510 MiB |  12306 GiB |  12228 GiB |
|       from small pool |     93 MiB |     95 MiB |     73 GiB |     73 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80498 MiB |  80500 MiB | 166722 MiB |  86224 MiB |
|       from large pool |  80400 MiB |  80400 MiB | 166242 MiB |  85842 MiB |
|       from small pool |     98 MiB |    392 MiB |    480 MiB |    382 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    823 MiB |   3119 MiB |   6143 GiB |   6143 GiB |
|       from large pool |    819 MiB |   3112 MiB |   6057 GiB |   6057 GiB |
|       from small pool |      3 MiB |     23 MiB |     85 GiB |     85 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2036    |    2039    |     869 K  |     867 K  |
|       from large pool |     477    |     478    |     421 K  |     421 K  |
|       from small pool |    1559    |    1562    |     447 K  |     445 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2036    |    2039    |     869 K  |     867 K  |
|       from large pool |     477    |     478    |     421 K  |     421 K  |
|       from small pool |    1559    |    1562    |     447 K  |     445 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     251    |     757    |    1201    |     950    |
|       from large pool |     202    |     561    |     961    |     759    |
|       from small pool |      49    |     196    |     240    |     191    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     132    |     133    |  540020    |  539888    |
|       from large pool |      79    |      81    |  324754    |  324675    |
|       from small pool |      53    |      55    |  215266    |  215213    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:25:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:25:24] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:25:30]    INFO >> epoch 001:    452 / 1539 loss=6.204, wps=2299.9, ups=3.64, wpb=631.6, bsz=631.6, num_updates=450, lr=0.0004, gnorm=10.134, clip=0, train_wall=6, gb_free=71.8, wall=136 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:25:40]    INFO >> epoch 001:    502 / 1539 loss=5.939, wps=3763.8, ups=5.07, wpb=743.1, bsz=743.1, num_updates=500, lr=0.0004, gnorm=11.089, clip=2, train_wall=9, gb_free=72.6, wall=145 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:25:47]    INFO >> epoch 001:    552 / 1539 loss=6.168, wps=4591.4, ups=6.99, wpb=657, bsz=657, num_updates=550, lr=0.0004, gnorm=10.43, clip=2, train_wall=6, gb_free=65.5, wall=153 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:25:54]    INFO >> epoch 001:    602 / 1539 loss=6.009, wps=4554, ups=6.81, wpb=668.6, bsz=668.6, num_updates=600, lr=0.0004, gnorm=10.407, clip=0, train_wall=7, gb_free=73.2, wall=160 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:26:03]    INFO >> epoch 001:    652 / 1539 loss=5.974, wps=4581.4, ups=6.43, wpb=712.2, bsz=712.2, num_updates=650, lr=0.0004, gnorm=10.083, clip=0, train_wall=7, gb_free=73.5, wall=168 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:26:12]    INFO >> epoch 001:    702 / 1539 loss=6.002, wps=4010.2, ups=5.96, wpb=672.5, bsz=672.5, num_updates=700, lr=0.0004, gnorm=9.689, clip=0, train_wall=8, gb_free=74.1, wall=176 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:26:19]    INFO >> epoch 001:    752 / 1539 loss=5.77, wps=4989, ups=6.59, wpb=757.4, bsz=757.4, num_updates=750, lr=0.0004, gnorm=11.007, clip=2, train_wall=7, gb_free=73.7, wall=184 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:26:26]    INFO >> epoch 001:    802 / 1539 loss=5.849, wps=5080, ups=7.01, wpb=725.2, bsz=725.2, num_updates=800, lr=0.0004, gnorm=10.774, clip=2, train_wall=7, gb_free=73.4, wall=191 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:26:34]    INFO >> epoch 001:    852 / 1539 loss=5.847, wps=4205.9, ups=6.56, wpb=641.1, bsz=641.1, num_updates=850, lr=0.0004, gnorm=10.78, clip=0, train_wall=7, gb_free=71.8, wall=198 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:26:42]    INFO >> epoch 001:    902 / 1539 loss=5.845, wps=4583.7, ups=6.94, wpb=660.1, bsz=660.1, num_updates=900, lr=0.0004, gnorm=10.278, clip=0, train_wall=7, gb_free=72.1, wall=206 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:26:50]    INFO >> epoch 001:    952 / 1539 loss=5.828, wps=4848.6, ups=6.8, wpb=713.2, bsz=713.2, num_updates=950, lr=0.0004, gnorm=10.356, clip=0, train_wall=7, gb_free=71.8, wall=213 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:26:53] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 901.25 MiB is free. Including non-PyTorch memory, this process has 78.24 GiB memory in use. Of the allocated memory 74.92 GiB is allocated by PyTorch, and 2.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:26:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:26:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:26:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 3            |        cudaMalloc retries: 7         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76322 MiB |  78597 MiB |  28415 GiB |  28340 GiB |
|       from large pool |  76303 MiB |  78579 MiB |  28261 GiB |  28187 GiB |
|       from small pool |     18 MiB |     19 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76322 MiB |  78597 MiB |  28415 GiB |  28340 GiB |
|       from large pool |  76303 MiB |  78579 MiB |  28261 GiB |  28187 GiB |
|       from small pool |     18 MiB |     19 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB |  28342 GiB |  28268 GiB |
|       from large pool |  76285 MiB |  78562 MiB |  28189 GiB |  28114 GiB |
|       from small pool |     18 MiB |     19 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79604 MiB |  79604 MiB | 305536 MiB | 225932 MiB |
|       from large pool |  79580 MiB |  79580 MiB | 304982 MiB | 225402 MiB |
|       from small pool |     24 MiB |     98 MiB |    554 MiB |    530 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3281 MiB |   4789 MiB |  24274 GiB |  24271 GiB |
|       from large pool |   3276 MiB |   4783 MiB |  24097 GiB |  24094 GiB |
|       from small pool |      5 MiB |     27 MiB |    177 GiB |    177 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     638    |     644    |    1881 K  |    1880 K  |
|       from large pool |     340    |     346    |     949 K  |     948 K  |
|       from small pool |     298    |     354    |     931 K  |     931 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     638    |     644    |    1881 K  |    1880 K  |
|       from large pool |     340    |     346    |     949 K  |     948 K  |
|       from small pool |     298    |     354    |     931 K  |     931 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      96    |     123    |    1327    |    1231    |
|       from large pool |      84    |      84    |    1050    |     966    |
|       from small pool |      12    |      49    |     277    |     265    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     115    |     117    |    1087 K  |    1087 K  |
|       from large pool |      93    |      95    |     661 K  |     661 K  |
|       from small pool |      22    |      56    |     426 K  |     426 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:26:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:26:53] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:26:59]    INFO >> epoch 001:   1003 / 1539 loss=5.702, wps=3425.5, ups=5.27, wpb=649.4, bsz=649.4, num_updates=1000, lr=0.0004, gnorm=11.099, clip=0, train_wall=7, gb_free=72.1, wall=222 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:27:07]    INFO >> epoch 001:   1053 / 1539 loss=5.692, wps=5179.4, ups=6.3, wpb=822.6, bsz=822.6, num_updates=1050, lr=0.0004, gnorm=10.28, clip=2, train_wall=7, gb_free=68, wall=230 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:27:17] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 197.25 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 75.66 GiB is allocated by PyTorch, and 2.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:27:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:27:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:27:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 4            |        cudaMalloc retries: 9         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76955 MiB |  77477 MiB |  32758 GiB |  32683 GiB |
|       from large pool |  76942 MiB |  77464 MiB |  32579 GiB |  32504 GiB |
|       from small pool |     12 MiB |     15 MiB |    178 GiB |    178 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76955 MiB |  77477 MiB |  32758 GiB |  32683 GiB |
|       from large pool |  76942 MiB |  77464 MiB |  32579 GiB |  32504 GiB |
|       from small pool |     12 MiB |     15 MiB |    178 GiB |    178 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76936 MiB |  77458 MiB |  32677 GiB |  32602 GiB |
|       from large pool |  76923 MiB |  77445 MiB |  32498 GiB |  32423 GiB |
|       from small pool |     12 MiB |     15 MiB |    178 GiB |    178 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80308 MiB |  80308 MiB | 310972 MiB | 230664 MiB |
|       from large pool |  80284 MiB |  80284 MiB | 310232 MiB | 229948 MiB |
|       from small pool |     24 MiB |    210 MiB |    740 MiB |    716 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3352 MiB |   9143 MiB |  29231 GiB |  29228 GiB |
|       from large pool |   3341 MiB |   9131 MiB |  29025 GiB |  29022 GiB |
|       from small pool |     11 MiB |     23 MiB |    206 GiB |    206 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     604    |     611    |    2166 K  |    2165 K  |
|       from large pool |     315    |     322    |    1085 K  |    1085 K  |
|       from small pool |     289    |     354    |    1080 K  |    1079 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     604    |     611    |    2166 K  |    2165 K  |
|       from large pool |     315    |     322    |    1085 K  |    1085 K  |
|       from small pool |     289    |     354    |    1080 K  |    1079 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      88    |     189    |    1425    |    1337    |
|       from large pool |      76    |      84    |    1055    |     979    |
|       from small pool |      12    |     105    |     370    |     358    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      84    |      84    |    1245 K  |    1245 K  |
|       from large pool |      60    |      60    |     746 K  |     746 K  |
|       from small pool |      24    |      53    |     498 K  |     498 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:27:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:27:17] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:27:18]    INFO >> epoch 001:   1104 / 1539 loss=5.527, wps=5061.5, ups=5.44, wpb=929.6, bsz=929.6, num_updates=1100, lr=0.0004, gnorm=11.9, clip=4, train_wall=8, gb_free=72.8, wall=240 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:27:25]    INFO >> epoch 001:   1154 / 1539 loss=5.656, wps=4648.6, ups=6.71, wpb=692.6, bsz=692.6, num_updates=1150, lr=0.0004, gnorm=11.146, clip=2, train_wall=7, gb_free=73.1, wall=247 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:27:32]    INFO >> epoch 001:   1204 / 1539 loss=5.773, wps=4667.7, ups=6.88, wpb=678.4, bsz=678.4, num_updates=1200, lr=0.0004, gnorm=9.757, clip=0, train_wall=7, gb_free=71.2, wall=254 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:27:40]    INFO >> epoch 001:   1254 / 1539 loss=5.621, wps=4854.4, ups=6.64, wpb=730.9, bsz=730.9, num_updates=1250, lr=0.0004, gnorm=10.315, clip=0, train_wall=7, gb_free=71.3, wall=262 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:27:49]    INFO >> epoch 001:   1304 / 1539 loss=5.479, wps=4694.7, ups=6.38, wpb=735.4, bsz=735.4, num_updates=1300, lr=0.0004, gnorm=8.519, clip=0, train_wall=7, gb_free=74.3, wall=270 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:27:57]    INFO >> epoch 001:   1354 / 1539 loss=5.4, wps=4462.4, ups=6.79, wpb=657.2, bsz=657.2, num_updates=1350, lr=0.0004, gnorm=9.819, clip=0, train_wall=7, gb_free=73.4, wall=277 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:28:04]    INFO >> epoch 001:   1404 / 1539 loss=5.492, wps=4602.3, ups=6.46, wpb=712.5, bsz=712.5, num_updates=1400, lr=0.0004, gnorm=9.371, clip=2, train_wall=7, gb_free=73.3, wall=285 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:28:12]    INFO >> epoch 001:   1454 / 1539 loss=5.48, wps=4749.4, ups=6.76, wpb=702.2, bsz=702.2, num_updates=1450, lr=0.0004, gnorm=9.449, clip=0, train_wall=7, gb_free=71.6, wall=292 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:28:21]    INFO >> epoch 001:   1504 / 1539 loss=5.368, wps=4644.8, ups=6.67, wpb=695.9, bsz=695.9, num_updates=1500, lr=0.0004, gnorm=9.208, clip=2, train_wall=7, gb_free=70.8, wall=300 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:28:26]    INFO >> epoch 001 | loss 5.833 | wps 4567.1 | ups 6.41 | wpb 712.7 | bsz 712.7 | num_updates 1535 | lr 0.0004 | gnorm 10.339 | clip 0.8 | train_wall 209 | gb_free 76.6 | wall 305 (progress_bar.py:267, print())[0m
[33m[2025-11-21 17:28:26] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 17:28:40]    INFO >> epoch 001 | valid on 'valid' subset | loss 5.157 | wps 10682.5 | wpb 5412.5 | bsz 5412.5 | num_updates 1535 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
[32m[2025-11-21 17:28:41]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 17:28:41]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/checkpoints/checkpoint_best.pt (epoch 1 @ 1535 updates, score 5.157) (writing took 0.015801 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 17:28:41] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-21 17:28:43]    INFO >> epoch 002:     15 / 1539 loss=5.255, wps=1651, ups=2.26, wpb=731.3, bsz=731.3, num_updates=1550, lr=0.0004, gnorm=10.089, clip=2, train_wall=7, gb_free=74.4, wall=322 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:28:51]    INFO >> epoch 002:     65 / 1539 loss=5.247, wps=4723.3, ups=7.17, wpb=658.8, bsz=658.8, num_updates=1600, lr=0.0004, gnorm=9.61, clip=0, train_wall=7, gb_free=73.4, wall=329 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:28:58]    INFO >> epoch 002:    115 / 1539 loss=5.271, wps=4802.2, ups=6.72, wpb=714.6, bsz=714.6, num_updates=1650, lr=0.0004, gnorm=9.578, clip=0, train_wall=7, gb_free=65.7, wall=336 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:29:06]    INFO >> epoch 002:    165 / 1539 loss=5.027, wps=4895.5, ups=6.63, wpb=738.8, bsz=738.8, num_updates=1700, lr=0.0004, gnorm=9.24, clip=0, train_wall=7, gb_free=73.6, wall=344 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:29:14]    INFO >> epoch 002:    215 / 1539 loss=5.059, wps=4642.1, ups=6.59, wpb=704.2, bsz=704.2, num_updates=1750, lr=0.0004, gnorm=9.832, clip=0, train_wall=7, gb_free=71.2, wall=351 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:29:22]    INFO >> epoch 002:    265 / 1539 loss=4.782, wps=5270.3, ups=6.03, wpb=873.5, bsz=873.5, num_updates=1800, lr=0.0004, gnorm=9.988, clip=0, train_wall=8, gb_free=74.7, wall=360 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:29:30]    INFO >> epoch 002:    315 / 1539 loss=5.147, wps=4525.1, ups=7.01, wpb=645.1, bsz=645.1, num_updates=1850, lr=0.0004, gnorm=9.498, clip=0, train_wall=7, gb_free=71.9, wall=367 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:29:38]    INFO >> epoch 002:    365 / 1539 loss=4.88, wps=4524.9, ups=6.9, wpb=655.8, bsz=655.8, num_updates=1900, lr=0.0004, gnorm=9.754, clip=0, train_wall=7, gb_free=74, wall=374 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:29:45]    INFO >> epoch 002:    415 / 1539 loss=4.731, wps=4501.5, ups=6.31, wpb=713.2, bsz=713.2, num_updates=1950, lr=0.0004, gnorm=9.441, clip=0, train_wall=7, gb_free=76.2, wall=382 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:29:46] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 197.25 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 74.93 GiB is allocated by PyTorch, and 3.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:29:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:29:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:29:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 5            |        cudaMalloc retries: 10        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76327 MiB |  78599 MiB |  61960 GiB |  61886 GiB |
|       from large pool |  76308 MiB |  78581 MiB |  61612 GiB |  61537 GiB |
|       from small pool |     18 MiB |     21 MiB |    348 GiB |    348 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76327 MiB |  78599 MiB |  61960 GiB |  61886 GiB |
|       from large pool |  76308 MiB |  78581 MiB |  61612 GiB |  61537 GiB |
|       from small pool |     18 MiB |     21 MiB |    348 GiB |    348 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB |  61824 GiB |  61750 GiB |
|       from large pool |  76285 MiB |  78562 MiB |  61476 GiB |  61402 GiB |
|       from small pool |     18 MiB |     21 MiB |    348 GiB |    348 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80308 MiB |  80494 MiB | 311158 MiB | 230850 MiB |
|       from large pool |  80284 MiB |  80284 MiB | 310232 MiB | 229948 MiB |
|       from small pool |     24 MiB |    210 MiB |    926 MiB |    902 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3980 MiB |   5490 MiB |  61023 GiB |  61019 GiB |
|       from large pool |   3975 MiB |   5484 MiB |  60627 GiB |  60623 GiB |
|       from small pool |      5 MiB |     27 MiB |    395 GiB |    395 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     640    |     646    |    4090 K  |    4090 K  |
|       from large pool |     340    |     346    |    1969 K  |    1969 K  |
|       from small pool |     300    |     355    |    2121 K  |    2120 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     640    |     646    |    4090 K  |    4090 K  |
|       from large pool |     340    |     346    |    1969 K  |    1969 K  |
|       from small pool |     300    |     355    |    2121 K  |    2120 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      88    |     181    |    1518    |    1430    |
|       from large pool |      76    |      76    |    1055    |     979    |
|       from small pool |      12    |     105    |     463    |     451    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      93    |      99    |    2309 K  |    2309 K  |
|       from large pool |      70    |      76    |    1305 K  |    1305 K  |
|       from small pool |      23    |      51    |    1004 K  |    1004 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:29:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:29:46] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:29:54]    INFO >> epoch 002:    466 / 1539 loss=4.774, wps=4485.7, ups=6.15, wpb=729.4, bsz=729.4, num_updates=2000, lr=0.0004, gnorm=9.307, clip=0, train_wall=7, gb_free=71.6, wall=390 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:30:02]    INFO >> epoch 002:    516 / 1539 loss=4.774, wps=4699.9, ups=6.63, wpb=708.4, bsz=708.4, num_updates=2050, lr=0.0004, gnorm=9.817, clip=0, train_wall=7, gb_free=71.2, wall=398 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:30:10]    INFO >> epoch 002:    566 / 1539 loss=4.735, wps=4360.7, ups=7.01, wpb=622.3, bsz=622.3, num_updates=2100, lr=0.0004, gnorm=9.293, clip=0, train_wall=7, gb_free=74, wall=405 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:30:18]    INFO >> epoch 002:    616 / 1539 loss=4.659, wps=5450.8, ups=6.26, wpb=871.1, bsz=871.1, num_updates=2150, lr=0.0004, gnorm=10.306, clip=2, train_wall=8, gb_free=68.6, wall=413 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:30:25]    INFO >> epoch 002:    666 / 1539 loss=4.431, wps=4938.4, ups=6.39, wpb=772.9, bsz=772.9, num_updates=2200, lr=0.0004, gnorm=11.523, clip=4, train_wall=7, gb_free=68.4, wall=421 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:30:34]    INFO >> epoch 002:    716 / 1539 loss=4.664, wps=4731.4, ups=6.68, wpb=708.6, bsz=708.6, num_updates=2250, lr=0.0004, gnorm=9.575, clip=0, train_wall=7, gb_free=73.1, wall=428 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:30:42]    INFO >> epoch 002:    766 / 1539 loss=4.783, wps=4353.2, ups=6.54, wpb=665.2, bsz=665.2, num_updates=2300, lr=0.0004, gnorm=9.035, clip=0, train_wall=7, gb_free=75.7, wall=436 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:30:50]    INFO >> epoch 002:    816 / 1539 loss=4.707, wps=4110.2, ups=6.27, wpb=656, bsz=656, num_updates=2350, lr=0.0004, gnorm=9.648, clip=0, train_wall=8, gb_free=72.2, wall=444 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:30:58]    INFO >> epoch 002:    866 / 1539 loss=4.705, wps=4709.6, ups=6.54, wpb=719.8, bsz=719.8, num_updates=2400, lr=0.0004, gnorm=8.897, clip=0, train_wall=7, gb_free=75.7, wall=451 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:31:06]    INFO >> epoch 002:    916 / 1539 loss=4.623, wps=4529.5, ups=6.75, wpb=671.5, bsz=671.5, num_updates=2450, lr=0.0004, gnorm=9.129, clip=0, train_wall=7, gb_free=73.1, wall=459 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:31:14]    INFO >> epoch 002:    966 / 1539 loss=4.495, wps=4364.2, ups=6.84, wpb=637.9, bsz=637.9, num_updates=2500, lr=0.0004, gnorm=10.374, clip=2, train_wall=7, gb_free=69.2, wall=466 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:31:15] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 37.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 77.54 GiB is allocated by PyTorch, and 1.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:31:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:31:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:31:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 6            |        cudaMalloc retries: 12        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79337 MiB |  79397 MiB |  78153 GiB |  78075 GiB |
|       from large pool |  78941 MiB |  79001 MiB |  77714 GiB |  77637 GiB |
|       from small pool |    395 MiB |    396 MiB |    438 GiB |    437 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79337 MiB |  79397 MiB |  78153 GiB |  78075 GiB |
|       from large pool |  78941 MiB |  79001 MiB |  77714 GiB |  77637 GiB |
|       from small pool |    395 MiB |    396 MiB |    438 GiB |    437 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79098 MiB |  79157 MiB |  77983 GiB |  77905 GiB |
|       from large pool |  78705 MiB |  78764 MiB |  77545 GiB |  77468 GiB |
|       from small pool |    393 MiB |    394 MiB |    437 GiB |    437 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80468 MiB |  80470 MiB | 335872 MiB | 255404 MiB |
|       from large pool |  80032 MiB |  80032 MiB | 334532 MiB | 254500 MiB |
|       from small pool |    436 MiB |    438 MiB |   1340 MiB |    904 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1070 MiB |   4610 MiB |  79115 GiB |  79114 GiB |
|       from large pool |   1030 MiB |   4603 MiB |  78616 GiB |  78615 GiB |
|       from small pool |     40 MiB |     41 MiB |    498 GiB |    498 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7296    |    7299    |    5179 K  |    5172 K  |
|       from large pool |     933    |     934    |    2516 K  |    2515 K  |
|       from small pool |    6363    |    6366    |    2663 K  |    2656 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7296    |    7299    |    5179 K  |    5172 K  |
|       from large pool |     933    |     934    |    2516 K  |    2515 K  |
|       from small pool |    6363    |    6366    |    2663 K  |    2656 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     690    |     691    |    2130    |    1440    |
|       from large pool |     472    |     472    |    1460    |     988    |
|       from small pool |     218    |     219    |     670    |     452    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     454    |     454    |    2903 K  |    2902 K  |
|       from large pool |      63    |      63    |    1651 K  |    1651 K  |
|       from small pool |     391    |     391    |    1251 K  |    1251 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:31:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:31:15] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 17:31:20] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 1.25 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 76.95 GiB is allocated by PyTorch, and 1.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:31:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:31:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:31:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 7            |        cudaMalloc retries: 15        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78678 MiB |  78798 MiB |  79117 GiB |  79040 GiB |
|       from large pool |  78593 MiB |  78713 MiB |  78673 GiB |  78596 GiB |
|       from small pool |     85 MiB |     86 MiB |    443 GiB |    443 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78678 MiB |  78798 MiB |  79117 GiB |  79040 GiB |
|       from large pool |  78593 MiB |  78713 MiB |  78673 GiB |  78596 GiB |
|       from small pool |     85 MiB |     86 MiB |    443 GiB |    443 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78642 MiB |  78762 MiB |  78944 GiB |  78868 GiB |
|       from large pool |  78558 MiB |  78677 MiB |  78501 GiB |  78425 GiB |
|       from small pool |     84 MiB |     85 MiB |    443 GiB |    442 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80504 MiB |  80504 MiB | 356340 MiB | 275836 MiB |
|       from large pool |  80416 MiB |  80416 MiB | 354934 MiB | 274518 MiB |
|       from small pool |     88 MiB |    436 MiB |   1406 MiB |   1318 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1705 MiB |   8357 MiB |  79995 GiB |  79994 GiB |
|       from large pool |   1702 MiB |   8349 MiB |  79489 GiB |  79488 GiB |
|       from small pool |      2 MiB |     29 MiB |    505 GiB |    505 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1873    |    1876    |    5242 K  |    5240 K  |
|       from large pool |     462    |     464    |    2546 K  |    2545 K  |
|       from small pool |    1411    |    1414    |    2696 K  |    2694 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1873    |    1876    |    5242 K  |    5240 K  |
|       from large pool |     462    |     464    |    2546 K  |    2545 K  |
|       from small pool |    1411    |    1414    |    2696 K  |    2694 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     221    |     689    |    2198    |    1977    |
|       from large pool |     177    |     471    |    1495    |    1318    |
|       from small pool |      44    |     218    |     703    |     659    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     167    |     168    |    2940 K  |    2940 K  |
|       from large pool |     120    |     125    |    1671 K  |    1671 K  |
|       from small pool |      47    |      59    |    1269 K  |    1269 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:31:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:31:20] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:31:22]    INFO >> epoch 002:   1018 / 1539 loss=4.559, wps=3959.9, ups=5.76, wpb=687.9, bsz=687.9, num_updates=2550, lr=0.0004, gnorm=10.063, clip=2, train_wall=7, gb_free=72.9, wall=475 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:31:30]    INFO >> epoch 002:   1068 / 1539 loss=4.44, wps=4989.6, ups=6.53, wpb=763.7, bsz=763.7, num_updates=2600, lr=0.0004, gnorm=9.342, clip=2, train_wall=7, gb_free=73.5, wall=482 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:31:39]    INFO >> epoch 002:   1118 / 1539 loss=4.466, wps=4966.9, ups=6.27, wpb=791.9, bsz=791.9, num_updates=2650, lr=0.0004, gnorm=9.867, clip=0, train_wall=8, gb_free=69.2, wall=490 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:31:47]    INFO >> epoch 002:   1168 / 1539 loss=4.423, wps=5083.2, ups=6.6, wpb=770.1, bsz=770.1, num_updates=2700, lr=0.0004, gnorm=10.027, clip=4, train_wall=7, gb_free=72.3, wall=498 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:31:55]    INFO >> epoch 002:   1218 / 1539 loss=4.493, wps=4546.5, ups=6.4, wpb=710, bsz=710, num_updates=2750, lr=0.0004, gnorm=9.568, clip=0, train_wall=7, gb_free=70.9, wall=506 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:31:58] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 121.25 MiB is free. Including non-PyTorch memory, this process has 79.00 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 5.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:31:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:31:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:31:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 8            |        cudaMalloc retries: 18        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72821 MiB |  75425 MiB |  86354 GiB |  86283 GiB |
|       from large pool |  72808 MiB |  75412 MiB |  85870 GiB |  85799 GiB |
|       from small pool |     12 MiB |     14 MiB |    483 GiB |    483 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72821 MiB |  75425 MiB |  86354 GiB |  86283 GiB |
|       from large pool |  72808 MiB |  75412 MiB |  85870 GiB |  85799 GiB |
|       from small pool |     12 MiB |     14 MiB |    483 GiB |    483 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72801 MiB |  75403 MiB |  86167 GiB |  86096 GiB |
|       from large pool |  72788 MiB |  75391 MiB |  85684 GiB |  85613 GiB |
|       from small pool |     12 MiB |     14 MiB |    483 GiB |    483 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80384 MiB |  80384 MiB | 387082 MiB | 306698 MiB |
|       from large pool |  80360 MiB |  80360 MiB | 385538 MiB | 305178 MiB |
|       from small pool |     24 MiB |    226 MiB |   1544 MiB |   1520 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5660 MiB |  11732 MiB |  87437 GiB |  87432 GiB |
|       from large pool |   5649 MiB |  11720 MiB |  86885 GiB |  86880 GiB |
|       from small pool |     11 MiB |     25 MiB |    551 GiB |    551 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     603    |     612    |    5719 K  |    5718 K  |
|       from large pool |     312    |     320    |    2781 K  |    2781 K  |
|       from small pool |     291    |     356    |    2937 K  |    2937 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     603    |     612    |    5719 K  |    5718 K  |
|       from large pool |     312    |     320    |    2781 K  |    2781 K  |
|       from small pool |     291    |     356    |    2937 K  |    2937 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     120    |     229    |    2295    |    2175    |
|       from large pool |     108    |     116    |    1523    |    1415    |
|       from small pool |      12    |     113    |     772    |     760    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     118    |     118    |    3207 K  |    3206 K  |
|       from large pool |      91    |      91    |    1823 K  |    1823 K  |
|       from small pool |      27    |      54    |    1383 K  |    1383 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:31:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:31:58] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:32:03]    INFO >> epoch 002:   1269 / 1539 loss=4.429, wps=4635.4, ups=6.06, wpb=765.3, bsz=765.3, num_updates=2800, lr=0.0004, gnorm=9.761, clip=0, train_wall=7, gb_free=70.5, wall=514 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:32:11]    INFO >> epoch 002:   1319 / 1539 loss=4.437, wps=4609, ups=6.97, wpb=661.5, bsz=661.5, num_updates=2850, lr=0.0004, gnorm=10.217, clip=0, train_wall=7, gb_free=75, wall=521 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:32:19]    INFO >> epoch 002:   1369 / 1539 loss=4.411, wps=4844.5, ups=6.65, wpb=728.5, bsz=728.5, num_updates=2900, lr=0.0004, gnorm=8.606, clip=0, train_wall=7, gb_free=70.5, wall=529 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:32:26]    INFO >> epoch 002:   1419 / 1539 loss=4.562, wps=4418.1, ups=6.76, wpb=653.6, bsz=653.6, num_updates=2950, lr=0.0004, gnorm=9.37, clip=2, train_wall=7, gb_free=64.8, wall=536 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:32:36]    INFO >> epoch 002:   1469 / 1539 loss=4.428, wps=3677.1, ups=5.23, wpb=702.5, bsz=702.5, num_updates=3000, lr=0.0004, gnorm=9.944, clip=0, train_wall=9, gb_free=70.3, wall=546 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:32:45]    INFO >> epoch 002:   1519 / 1539 loss=4.479, wps=4335.5, ups=6.44, wpb=673.3, bsz=673.3, num_updates=3050, lr=0.0004, gnorm=9.926, clip=2, train_wall=7, gb_free=74.2, wall=553 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:32:48]    INFO >> epoch 002 | loss 4.683 | wps 4347.1 | ups 6.1 | wpb 712.7 | bsz 712.7 | num_updates 3070 | lr 0.0004 | gnorm 9.688 | clip 0.7 | train_wall 221 | gb_free 72.4 | wall 557 (progress_bar.py:267, print())[0m
[33m[2025-11-21 17:32:48] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 17:33:02]    INFO >> epoch 002 | valid on 'valid' subset | loss 4.102 | wps 11191.2 | wpb 5412.5 | bsz 5412.5 | num_updates 3070 | best_loss 5.157 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 17:33:02]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 17:33:02]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/checkpoints/checkpoint_last.pt (epoch 2 @ 3070 updates, score 4.102) (writing took 0.013851 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 17:33:02] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 17:33:06]    INFO >> epoch 003:     30 / 1539 loss=4.405, wps=1587.8, ups=2.33, wpb=681.1, bsz=681.1, num_updates=3100, lr=0.000392, gnorm=9.565, clip=0, train_wall=7, gb_free=70.7, wall=575 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:33:14]    INFO >> epoch 003:     80 / 1539 loss=4.203, wps=5023.8, ups=6.5, wpb=772.8, bsz=772.8, num_updates=3150, lr=0.000392, gnorm=9.789, clip=0, train_wall=7, gb_free=73.4, wall=583 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:33:19] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.08 GiB is free. Including non-PyTorch memory, this process has 78.04 GiB memory in use. Of the allocated memory 74.93 GiB is allocated by PyTorch, and 2.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:33:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:33:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:33:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 9            |        cudaMalloc retries: 21        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76326 MiB |  78600 MiB | 102451 GiB | 102377 GiB |
|       from large pool |  76307 MiB |  78583 MiB | 101871 GiB | 101797 GiB |
|       from small pool |     18 MiB |     19 MiB |    580 GiB |    580 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76326 MiB |  78600 MiB | 102451 GiB | 102377 GiB |
|       from large pool |  76307 MiB |  78583 MiB | 101871 GiB | 101797 GiB |
|       from small pool |     18 MiB |     19 MiB |    580 GiB |    580 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB | 102235 GiB | 102160 GiB |
|       from large pool |  76285 MiB |  78562 MiB | 101655 GiB | 101581 GiB |
|       from small pool |     18 MiB |     19 MiB |    579 GiB |    579 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79400 MiB |  79400 MiB | 483464 MiB | 404064 MiB |
|       from large pool |  79376 MiB |  79376 MiB | 481770 MiB | 402394 MiB |
|       from small pool |     24 MiB |     74 MiB |   1694 MiB |   1670 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3073 MiB |   4962 MiB | 103113 GiB | 103110 GiB |
|       from large pool |   3068 MiB |   4948 MiB | 102456 GiB | 102453 GiB |
|       from small pool |      5 MiB |     27 MiB |    657 GiB |    657 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     640    |     646    |    6746 K  |    6745 K  |
|       from large pool |     340    |     346    |    3209 K  |    3208 K  |
|       from small pool |     300    |     356    |    3536 K  |    3536 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     640    |     646    |    6746 K  |    6745 K  |
|       from large pool |     340    |     346    |    3209 K  |    3208 K  |
|       from small pool |     300    |     356    |    3536 K  |    3536 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     106    |     118    |    2420    |    2314    |
|       from large pool |      94    |      94    |    1573    |    1479    |
|       from small pool |      12    |      37    |     847    |     835    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     110    |     112    |    3792 K  |    3792 K  |
|       from large pool |      85    |      87    |    2095 K  |    2095 K  |
|       from small pool |      25    |      57    |    1696 K  |    1696 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:33:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:33:19] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:33:25]    INFO >> epoch 003:    131 / 1539 loss=4.271, wps=4618.3, ups=5.53, wpb=835, bsz=835, num_updates=3200, lr=0.000392, gnorm=10.288, clip=0, train_wall=7, gb_free=71.9, wall=592 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:33:32]    INFO >> epoch 003:    181 / 1539 loss=4.597, wps=4311.9, ups=6.57, wpb=656.8, bsz=656.8, num_updates=3250, lr=0.000392, gnorm=8.596, clip=0, train_wall=7, gb_free=73, wall=599 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:33:37] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 153.25 MiB is free. Including non-PyTorch memory, this process has 78.97 GiB memory in use. Of the allocated memory 75.67 GiB is allocated by PyTorch, and 2.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:33:37] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:33:37] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:33:37] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 10           |        cudaMalloc retries: 23        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76958 MiB |  77481 MiB | 105988 GiB | 105913 GiB |
|       from large pool |  76945 MiB |  77468 MiB | 105386 GiB | 105311 GiB |
|       from small pool |     12 MiB |     21 MiB |    601 GiB |    601 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76958 MiB |  77481 MiB | 105988 GiB | 105913 GiB |
|       from large pool |  76945 MiB |  77468 MiB | 105386 GiB | 105311 GiB |
|       from small pool |     12 MiB |     21 MiB |    601 GiB |    601 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76936 MiB |  77458 MiB | 105764 GiB | 105689 GiB |
|       from large pool |  76923 MiB |  77445 MiB | 105164 GiB | 105089 GiB |
|       from small pool |     12 MiB |     21 MiB |    600 GiB |    600 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80352 MiB |  80352 MiB | 489864 MiB | 409512 MiB |
|       from large pool |  80326 MiB |  80326 MiB | 487984 MiB | 407658 MiB |
|       from small pool |     26 MiB |    210 MiB |   1880 MiB |   1854 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3393 MiB |   8163 MiB | 107068 GiB | 107064 GiB |
|       from large pool |   3380 MiB |   8148 MiB | 106386 GiB | 106383 GiB |
|       from small pool |     13 MiB |     27 MiB |    681 GiB |    681 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |    6985 K  |    6984 K  |
|       from large pool |     315    |     322    |    3323 K  |    3323 K  |
|       from small pool |     291    |     356    |    3662 K  |    3661 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |    6985 K  |    6984 K  |
|       from large pool |     315    |     322    |    3323 K  |    3323 K  |
|       from small pool |     291    |     356    |    3662 K  |    3661 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     100    |     199    |    2520    |    2420    |
|       from large pool |      87    |      94    |    1580    |    1493    |
|       from small pool |      13    |     105    |     940    |     927    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      95    |      95    |    3925 K  |    3925 K  |
|       from large pool |      69    |      69    |    2166 K  |    2166 K  |
|       from small pool |      26    |      58    |    1758 K  |    1758 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:33:37] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:33:37] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:33:41]    INFO >> epoch 003:    232 / 1539 loss=4.406, wps=4479.1, ups=5.84, wpb=767.2, bsz=767.2, num_updates=3300, lr=0.000392, gnorm=9.626, clip=0, train_wall=7, gb_free=74, wall=608 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:33:48]    INFO >> epoch 003:    282 / 1539 loss=4.372, wps=5002.5, ups=6.65, wpb=751.9, bsz=751.9, num_updates=3350, lr=0.000392, gnorm=9.22, clip=0, train_wall=7, gb_free=70.6, wall=615 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:33:58]    INFO >> epoch 003:    332 / 1539 loss=4.436, wps=4729.3, ups=5.98, wpb=790.4, bsz=790.4, num_updates=3400, lr=0.000392, gnorm=8.606, clip=0, train_wall=8, gb_free=73.8, wall=624 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:34:06]    INFO >> epoch 003:    382 / 1539 loss=4.366, wps=4254.3, ups=6.18, wpb=688.4, bsz=688.4, num_updates=3450, lr=0.000392, gnorm=9.149, clip=0, train_wall=8, gb_free=72.5, wall=632 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:34:14]    INFO >> epoch 003:    432 / 1539 loss=4.345, wps=4341.2, ups=6.6, wpb=657.8, bsz=657.8, num_updates=3500, lr=0.000392, gnorm=9.002, clip=0, train_wall=7, gb_free=66.2, wall=639 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:34:21]    INFO >> epoch 003:    482 / 1539 loss=4.406, wps=4817.5, ups=6.39, wpb=754, bsz=754, num_updates=3550, lr=0.000392, gnorm=8.891, clip=2, train_wall=7, gb_free=73.1, wall=647 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:34:30]    INFO >> epoch 003:    532 / 1539 loss=4.205, wps=4990.9, ups=6.48, wpb=770.5, bsz=770.5, num_updates=3600, lr=0.000392, gnorm=11.085, clip=4, train_wall=7, gb_free=73.8, wall=655 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:34:38]    INFO >> epoch 003:    582 / 1539 loss=4.295, wps=4526.5, ups=6.29, wpb=720.1, bsz=720.1, num_updates=3650, lr=0.000392, gnorm=9.134, clip=0, train_wall=8, gb_free=71.6, wall=663 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:34:46]    INFO >> epoch 003:    632 / 1539 loss=4.355, wps=4725.1, ups=7.01, wpb=674.5, bsz=674.5, num_updates=3700, lr=0.000392, gnorm=9.521, clip=2, train_wall=7, gb_free=66.5, wall=670 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:34:53]    INFO >> epoch 003:    682 / 1539 loss=4.358, wps=4775.9, ups=6.31, wpb=756.5, bsz=756.5, num_updates=3750, lr=0.000392, gnorm=9.752, clip=2, train_wall=7, gb_free=75.1, wall=678 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:35:03]    INFO >> epoch 003:    732 / 1539 loss=4.095, wps=4706.1, ups=5.81, wpb=810.3, bsz=810.3, num_updates=3800, lr=0.000392, gnorm=9.345, clip=0, train_wall=8, gb_free=73.9, wall=686 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:35:11]    INFO >> epoch 003:    782 / 1539 loss=3.782, wps=5085.7, ups=6.4, wpb=795, bsz=795, num_updates=3850, lr=0.000392, gnorm=9.459, clip=2, train_wall=7, gb_free=73.7, wall=694 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:35:12] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 29.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.96 GiB is allocated by PyTorch, and 1.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:35:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:35:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:35:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 11           |        cudaMalloc retries: 26        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78744 MiB |  78804 MiB | 123508 GiB | 123431 GiB |
|       from large pool |  78658 MiB |  78718 MiB | 122810 GiB | 122733 GiB |
|       from small pool |     85 MiB |     86 MiB |    698 GiB |    697 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78744 MiB |  78804 MiB | 123508 GiB | 123431 GiB |
|       from large pool |  78658 MiB |  78718 MiB | 122810 GiB | 122733 GiB |
|       from small pool |     85 MiB |     86 MiB |    698 GiB |    697 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78703 MiB |  78762 MiB | 123247 GiB | 123170 GiB |
|       from large pool |  78617 MiB |  78677 MiB | 122550 GiB | 122473 GiB |
|       from small pool |     85 MiB |     86 MiB |    697 GiB |    696 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80476 MiB |  80476 MiB | 517426 MiB | 436950 MiB |
|       from large pool |  80386 MiB |  80386 MiB | 515324 MiB | 434938 MiB |
|       from small pool |     90 MiB |    246 MiB |   2102 MiB |   2012 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1671 MiB |   7867 MiB | 124794 GiB | 124792 GiB |
|       from large pool |   1667 MiB |   7857 MiB | 124001 GiB | 124000 GiB |
|       from small pool |      4 MiB |     27 MiB |    792 GiB |    792 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1884    |    1887    |    8145 K  |    8143 K  |
|       from large pool |     463    |     464    |    3900 K  |    3900 K  |
|       from small pool |    1421    |    1424    |    4244 K  |    4243 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1884    |    1887    |    8145 K  |    8143 K  |
|       from large pool |     463    |     464    |    3900 K  |    3900 K  |
|       from small pool |    1421    |    1424    |    4244 K  |    4243 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     225    |     300    |    2734    |    2509    |
|       from large pool |     180    |     180    |    1683    |    1503    |
|       from small pool |      45    |     123    |    1051    |    1006    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     169    |     169    |    4572 K  |    4572 K  |
|       from large pool |     121    |     124    |    2540 K  |    2540 K  |
|       from small pool |      48    |      51    |    2031 K  |    2031 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:35:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:35:12] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:35:19]    INFO >> epoch 003:    833 / 1539 loss=4.227, wps=4487.2, ups=6.1, wpb=736, bsz=736, num_updates=3900, lr=0.000392, gnorm=10.384, clip=2, train_wall=7, gb_free=73, wall=702 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:35:27]    INFO >> epoch 003:    883 / 1539 loss=4.412, wps=4606.6, ups=6.42, wpb=717, bsz=717, num_updates=3950, lr=0.000392, gnorm=9.161, clip=2, train_wall=7, gb_free=72.5, wall=710 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:35:35]    INFO >> epoch 003:    933 / 1539 loss=4.151, wps=4669.1, ups=6.4, wpb=729.5, bsz=729.5, num_updates=4000, lr=0.000392, gnorm=9.333, clip=0, train_wall=7, gb_free=67.3, wall=718 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:35:44]    INFO >> epoch 003:    983 / 1539 loss=4.357, wps=3783.9, ups=5.53, wpb=684.3, bsz=684.3, num_updates=4050, lr=0.000392, gnorm=9.47, clip=0, train_wall=8, gb_free=71.5, wall=727 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:35:52]    INFO >> epoch 003:   1033 / 1539 loss=4.371, wps=4231.1, ups=6.65, wpb=636.3, bsz=636.3, num_updates=4100, lr=0.000392, gnorm=8.613, clip=0, train_wall=7, gb_free=68.3, wall=735 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:35:59]    INFO >> epoch 003:   1083 / 1539 loss=4.215, wps=4752.4, ups=7.05, wpb=673.9, bsz=673.9, num_updates=4150, lr=0.000392, gnorm=9.607, clip=0, train_wall=7, gb_free=72.9, wall=742 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:36:09] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 39.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 76.52 GiB is allocated by PyTorch, and 2.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:36:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:36:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:36:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 12           |        cudaMalloc retries: 28        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78301 MiB |  78361 MiB | 133746 GiB | 133670 GiB |
|       from large pool |  77916 MiB |  77976 MiB | 132993 GiB | 132917 GiB |
|       from small pool |    385 MiB |    386 MiB |    753 GiB |    752 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78301 MiB |  78361 MiB | 133746 GiB | 133670 GiB |
|       from large pool |  77916 MiB |  77976 MiB | 132993 GiB | 132917 GiB |
|       from small pool |    385 MiB |    386 MiB |    753 GiB |    752 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78077 MiB |  78137 MiB | 133463 GiB | 133387 GiB |
|       from large pool |  77694 MiB |  77753 MiB | 132711 GiB | 132635 GiB |
|       from small pool |    383 MiB |    384 MiB |    752 GiB |    751 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80466 MiB |  80504 MiB | 539302 MiB | 458836 MiB |
|       from large pool |  80042 MiB |  80326 MiB | 536864 MiB | 456822 MiB |
|       from small pool |    424 MiB |    426 MiB |   2438 MiB |   2014 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2104 MiB |   6644 MiB | 134072 GiB | 134070 GiB |
|       from large pool |   2065 MiB |   6637 MiB | 133215 GiB | 133213 GiB |
|       from small pool |     38 MiB |     40 MiB |    856 GiB |    856 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7109    |    7112    |    8817 K  |    8810 K  |
|       from large pool |     916    |     917    |    4239 K  |    4238 K  |
|       from small pool |    6193    |    6196    |    4578 K  |    4572 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7109    |    7112    |    8817 K  |    8810 K  |
|       from large pool |     916    |     917    |    4239 K  |    4238 K  |
|       from small pool |    6193    |    6196    |    4578 K  |    4572 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     742    |     743    |    3261    |    2519    |
|       from large pool |     530    |     530    |    2042    |    1512    |
|       from small pool |     212    |     213    |    1219    |    1007    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     510    |     511    |    4950 K  |    4949 K  |
|       from large pool |     132    |     132    |    2765 K  |    2765 K  |
|       from small pool |     378    |     379    |    2185 K  |    2184 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:36:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:36:09] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:36:10]    INFO >> epoch 003:   1134 / 1539 loss=4.18, wps=4176.9, ups=6.1, wpb=684.8, bsz=684.8, num_updates=4200, lr=0.000392, gnorm=8.87, clip=0, train_wall=7, gb_free=73, wall=750 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:36:17]    INFO >> epoch 003:   1184 / 1539 loss=4.172, wps=4449, ups=6.61, wpb=673.6, bsz=673.6, num_updates=4250, lr=0.000392, gnorm=9.28, clip=2, train_wall=7, gb_free=73.5, wall=757 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:36:25]    INFO >> epoch 003:   1234 / 1539 loss=4.387, wps=4543, ups=6.33, wpb=718.1, bsz=718.1, num_updates=4300, lr=0.000392, gnorm=9.049, clip=2, train_wall=7, gb_free=70.5, wall=765 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:36:32]    INFO >> epoch 003:   1284 / 1539 loss=3.99, wps=4409.9, ups=7.12, wpb=619.7, bsz=619.7, num_updates=4350, lr=0.000392, gnorm=8.993, clip=0, train_wall=7, gb_free=73.5, wall=772 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:36:41]    INFO >> epoch 003:   1334 / 1539 loss=4.158, wps=4860.3, ups=6.87, wpb=708, bsz=708, num_updates=4400, lr=0.000392, gnorm=8.385, clip=0, train_wall=7, gb_free=72.1, wall=780 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:36:48]    INFO >> epoch 003:   1384 / 1539 loss=4.127, wps=4758.3, ups=6.87, wpb=692.7, bsz=692.7, num_updates=4450, lr=0.000392, gnorm=9.156, clip=0, train_wall=7, gb_free=74.2, wall=787 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:36:55]    INFO >> epoch 003:   1434 / 1539 loss=4.26, wps=4598.3, ups=7.06, wpb=651.7, bsz=651.7, num_updates=4500, lr=0.000392, gnorm=8.257, clip=0, train_wall=7, gb_free=72.1, wall=794 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:37:03]    INFO >> epoch 003:   1484 / 1539 loss=4.232, wps=4278.9, ups=6.49, wpb=658.9, bsz=658.9, num_updates=4550, lr=0.000392, gnorm=8.463, clip=0, train_wall=7, gb_free=72.4, wall=802 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:37:10]    INFO >> epoch 003:   1534 / 1539 loss=4.213, wps=4492.5, ups=6.78, wpb=662.9, bsz=662.9, num_updates=4600, lr=0.000392, gnorm=8.949, clip=2, train_wall=7, gb_free=72, wall=809 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:37:11]    INFO >> epoch 003 | loss 4.265 | wps 4319.7 | ups 6.06 | wpb 712.7 | bsz 712.7 | num_updates 4605 | lr 0.000392 | gnorm 9.252 | clip 0.7 | train_wall 222 | gb_free 74.4 | wall 810 (progress_bar.py:267, print())[0m
[33m[2025-11-21 17:37:11] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 17:37:26]    INFO >> epoch 003 | valid on 'valid' subset | loss 3.948 | wps 11236.4 | wpb 5412.5 | bsz 5412.5 | num_updates 4605 | best_loss 5.157 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 17:37:26]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 17:37:26]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/checkpoints/checkpoint_last.pt (epoch 3 @ 4605 updates, score 3.948) (writing took 0.017440 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 17:37:26] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 17:37:33]    INFO >> epoch 004:     45 / 1539 loss=4.077, wps=1742.1, ups=2.32, wpb=751.7, bsz=751.7, num_updates=4650, lr=0.000376, gnorm=10.041, clip=2, train_wall=7, gb_free=72.7, wall=831 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:37:41]    INFO >> epoch 004:     95 / 1539 loss=4.13, wps=4925.6, ups=6.52, wpb=756, bsz=756, num_updates=4700, lr=0.000376, gnorm=9.372, clip=0, train_wall=7, gb_free=67.8, wall=838 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:37:52]    INFO >> epoch 004:    145 / 1539 loss=3.391, wps=5290.2, ups=5.05, wpb=1048.6, bsz=1048.6, num_updates=4750, lr=0.000376, gnorm=9.919, clip=4, train_wall=9, gb_free=75.1, wall=848 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:37:59]    INFO >> epoch 004:    195 / 1539 loss=4.232, wps=4869, ups=6.94, wpb=701.3, bsz=701.3, num_updates=4800, lr=0.000376, gnorm=8.272, clip=0, train_wall=7, gb_free=71.9, wall=856 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:38:07]    INFO >> epoch 004:    245 / 1539 loss=4.064, wps=4815, ups=6.48, wpb=743.6, bsz=743.6, num_updates=4850, lr=0.000376, gnorm=9.867, clip=0, train_wall=7, gb_free=73.5, wall=863 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:38:14]    INFO >> epoch 004:    295 / 1539 loss=4.156, wps=4519.3, ups=6.98, wpb=647.4, bsz=647.4, num_updates=4900, lr=0.000376, gnorm=8.207, clip=0, train_wall=7, gb_free=74.6, wall=870 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:38:23]    INFO >> epoch 004:    345 / 1539 loss=4.211, wps=4509.2, ups=6.56, wpb=687.4, bsz=687.4, num_updates=4950, lr=0.000376, gnorm=8.306, clip=0, train_wall=7, gb_free=69.9, wall=878 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:38:31]    INFO >> epoch 004:    395 / 1539 loss=3.971, wps=4878.9, ups=6.41, wpb=761.1, bsz=761.1, num_updates=5000, lr=0.000376, gnorm=8.444, clip=0, train_wall=7, gb_free=67.8, wall=886 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:38:40]    INFO >> epoch 004:    445 / 1539 loss=4.128, wps=3634.7, ups=5.76, wpb=630.5, bsz=630.5, num_updates=5050, lr=0.000376, gnorm=9.22, clip=0, train_wall=8, gb_free=71.4, wall=895 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:38:47]    INFO >> epoch 004:    495 / 1539 loss=4.22, wps=4732.7, ups=6.57, wpb=720.1, bsz=720.1, num_updates=5100, lr=0.000376, gnorm=8.26, clip=0, train_wall=7, gb_free=74, wall=902 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:38:56]    INFO >> epoch 004:    545 / 1539 loss=4.12, wps=4474.8, ups=6.38, wpb=701.6, bsz=701.6, num_updates=5150, lr=0.000376, gnorm=9.289, clip=0, train_wall=7, gb_free=73.5, wall=910 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:39:04]    INFO >> epoch 004:    595 / 1539 loss=4.218, wps=4266.8, ups=6.78, wpb=629.8, bsz=629.8, num_updates=5200, lr=0.000376, gnorm=7.447, clip=0, train_wall=7, gb_free=71, wall=917 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:39:05] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 13.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 77.38 GiB is allocated by PyTorch, and 1.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:39:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:39:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:39:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 13           |        cudaMalloc retries: 30        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79173 MiB |  79233 MiB | 167559 GiB | 167482 GiB |
|       from large pool |  79083 MiB |  79143 MiB | 166611 GiB | 166534 GiB |
|       from small pool |     89 MiB |     91 MiB |    948 GiB |    948 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79173 MiB |  79233 MiB | 167559 GiB | 167482 GiB |
|       from large pool |  79083 MiB |  79143 MiB | 166611 GiB | 166534 GiB |
|       from small pool |     89 MiB |     91 MiB |    948 GiB |    948 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79123 MiB |  79183 MiB | 167207 GiB | 167130 GiB |
|       from large pool |  79034 MiB |  79093 MiB | 166260 GiB | 166183 GiB |
|       from small pool |     89 MiB |     90 MiB |    946 GiB |    946 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80492 MiB |  80492 MiB | 564646 MiB | 484154 MiB |
|       from large pool |  80398 MiB |  80398 MiB | 562140 MiB | 481742 MiB |
|       from small pool |     94 MiB |     94 MiB |   2506 MiB |   2412 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1258 MiB |   7162 MiB | 164246 GiB | 164244 GiB |
|       from large pool |   1254 MiB |   7152 MiB | 163171 GiB | 163170 GiB |
|       from small pool |      4 MiB |     27 MiB |   1074 GiB |   1074 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1961    |    1964    |   11044 K  |   11042 K  |
|       from large pool |     470    |     471    |    5271 K  |    5271 K  |
|       from small pool |    1491    |    1494    |    5772 K  |    5770 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1961    |    1964    |   11044 K  |   11042 K  |
|       from large pool |     470    |     471    |    5271 K  |    5271 K  |
|       from small pool |    1491    |    1494    |    5772 K  |    5770 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     223    |     223    |    3348    |    3125    |
|       from large pool |     176    |     176    |    2095    |    1919    |
|       from small pool |      47    |      47    |    1253    |    1206    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     140    |     140    |    6216 K  |    6216 K  |
|       from large pool |      92    |      96    |    3444 K  |    3444 K  |
|       from small pool |      48    |      58    |    2772 K  |    2772 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:39:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:39:05] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:39:11]    INFO >> epoch 004:    646 / 1539 loss=3.988, wps=4302.6, ups=6.41, wpb=671.1, bsz=671.1, num_updates=5250, lr=0.000376, gnorm=9.301, clip=4, train_wall=7, gb_free=68.7, wall=925 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:39:20] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 983.25 MiB is free. Including non-PyTorch memory, this process has 78.16 GiB memory in use. Of the allocated memory 74.92 GiB is allocated by PyTorch, and 2.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:39:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:39:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:39:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 14           |        cudaMalloc retries: 32        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76323 MiB |  78596 MiB | 170178 GiB | 170104 GiB |
|       from large pool |  76304 MiB |  78579 MiB | 169217 GiB | 169142 GiB |
|       from small pool |     18 MiB |     24 MiB |    961 GiB |    961 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76323 MiB |  78596 MiB | 170178 GiB | 170104 GiB |
|       from large pool |  76304 MiB |  78579 MiB | 169217 GiB | 169142 GiB |
|       from small pool |     18 MiB |     24 MiB |    961 GiB |    961 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB | 169821 GiB | 169746 GiB |
|       from large pool |  76285 MiB |  78562 MiB | 168861 GiB | 168786 GiB |
|       from small pool |     18 MiB |     24 MiB |    960 GiB |    959 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79522 MiB |  80432 MiB | 637044 MiB | 557522 MiB |
|       from large pool |  79496 MiB |  80338 MiB | 634538 MiB | 555042 MiB |
|       from small pool |     26 MiB |     94 MiB |   2506 MiB |   2480 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3198 MiB |   4143 MiB | 166742 GiB | 166739 GiB |
|       from large pool |   3191 MiB |   4135 MiB | 165652 GiB | 165649 GiB |
|       from small pool |      7 MiB |     23 MiB |   1089 GiB |   1089 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     640    |     646    |   11213 K  |   11212 K  |
|       from large pool |     340    |     346    |    5361 K  |    5361 K  |
|       from small pool |     300    |     356    |    5852 K  |    5851 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     640    |     646    |   11213 K  |   11212 K  |
|       from large pool |     340    |     346    |    5361 K  |    5361 K  |
|       from small pool |     300    |     356    |    5852 K  |    5851 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     107    |     222    |    3403    |    3296    |
|       from large pool |      94    |     175    |    2150    |    2056    |
|       from small pool |      13    |      47    |    1253    |    1240    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     126    |     128    |    6309 K  |    6309 K  |
|       from large pool |      98    |     100    |    3503 K  |    3503 K  |
|       from small pool |      28    |      51    |    2805 K  |    2805 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:39:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:39:20] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:39:21]    INFO >> epoch 004:    697 / 1539 loss=4.205, wps=3283.6, ups=5.16, wpb=636.6, bsz=636.6, num_updates=5300, lr=0.000376, gnorm=7.267, clip=0, train_wall=7, gb_free=67.8, wall=935 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:39:31]    INFO >> epoch 004:    747 / 1539 loss=4.076, wps=3911.3, ups=5.81, wpb=673.1, bsz=673.1, num_updates=5350, lr=0.000376, gnorm=8.106, clip=0, train_wall=8, gb_free=72.9, wall=943 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:39:39]    INFO >> epoch 004:    797 / 1539 loss=4.018, wps=4697.7, ups=6.62, wpb=710.1, bsz=710.1, num_updates=5400, lr=0.000376, gnorm=7.968, clip=0, train_wall=7, gb_free=72.3, wall=951 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:39:46]    INFO >> epoch 004:    847 / 1539 loss=3.902, wps=4882.6, ups=6.53, wpb=747.5, bsz=747.5, num_updates=5450, lr=0.000376, gnorm=10.011, clip=0, train_wall=7, gb_free=63.6, wall=959 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:39:55]    INFO >> epoch 004:    897 / 1539 loss=4.118, wps=4683.2, ups=6.02, wpb=778.3, bsz=778.3, num_updates=5500, lr=0.000376, gnorm=8.765, clip=0, train_wall=8, gb_free=70.2, wall=967 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:40:03]    INFO >> epoch 004:    947 / 1539 loss=4.089, wps=4322.8, ups=6.68, wpb=646.7, bsz=646.7, num_updates=5550, lr=0.000376, gnorm=8.066, clip=0, train_wall=7, gb_free=67.5, wall=974 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:40:12]    INFO >> epoch 004:    997 / 1539 loss=4.009, wps=4552.3, ups=5.95, wpb=764.9, bsz=764.9, num_updates=5600, lr=0.000376, gnorm=8.691, clip=0, train_wall=8, gb_free=75.5, wall=983 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:40:19]    INFO >> epoch 004:   1047 / 1539 loss=4.194, wps=4163.4, ups=6.62, wpb=628.6, bsz=628.6, num_updates=5650, lr=0.000376, gnorm=7.784, clip=0, train_wall=7, gb_free=72.5, wall=990 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:40:27]    INFO >> epoch 004:   1097 / 1539 loss=4.081, wps=4732.9, ups=6.49, wpb=729.6, bsz=729.6, num_updates=5700, lr=0.000376, gnorm=8.808, clip=4, train_wall=7, gb_free=71.1, wall=998 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:40:36]    INFO >> epoch 004:   1147 / 1539 loss=4.173, wps=4413.9, ups=6.53, wpb=676.4, bsz=676.4, num_updates=5750, lr=0.000376, gnorm=8.109, clip=0, train_wall=7, gb_free=56.5, wall=1006 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:40:44]    INFO >> epoch 004:   1197 / 1539 loss=4.013, wps=4718.4, ups=6.25, wpb=754.7, bsz=754.7, num_updates=5800, lr=0.000376, gnorm=9.096, clip=0, train_wall=7, gb_free=75.6, wall=1014 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:40:51]    INFO >> epoch 004:   1247 / 1539 loss=4.026, wps=4326.9, ups=6.89, wpb=627.8, bsz=627.8, num_updates=5850, lr=0.000376, gnorm=7.403, clip=0, train_wall=7, gb_free=75.6, wall=1021 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:40:59]    INFO >> epoch 004:   1297 / 1539 loss=3.976, wps=4056.7, ups=6.29, wpb=644.6, bsz=644.6, num_updates=5900, lr=0.000376, gnorm=8.286, clip=0, train_wall=7, gb_free=74.6, wall=1029 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:41:07]    INFO >> epoch 004:   1347 / 1539 loss=4.006, wps=5103.2, ups=6.37, wpb=801.1, bsz=801.1, num_updates=5950, lr=0.000376, gnorm=8.704, clip=2, train_wall=7, gb_free=66.8, wall=1037 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:41:16] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.29 GiB memory in use. Of the allocated memory 73.80 GiB is allocated by PyTorch, and 2.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:41:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:41:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:41:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 15           |        cudaMalloc retries: 34        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75053 MiB |  75996 MiB | 190873 GiB | 190800 GiB |
|       from large pool |  75040 MiB |  75983 MiB | 189802 GiB | 189729 GiB |
|       from small pool |     12 MiB |     13 MiB |   1070 GiB |   1070 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75053 MiB |  75996 MiB | 190873 GiB | 190800 GiB |
|       from large pool |  75040 MiB |  75983 MiB | 189802 GiB | 189729 GiB |
|       from small pool |     12 MiB |     13 MiB |   1070 GiB |   1070 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75035 MiB |  75977 MiB | 190472 GiB | 190399 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 189403 GiB | 189330 GiB |
|       from small pool |     12 MiB |     13 MiB |   1069 GiB |   1069 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78634 MiB |  79742 MiB | 645400 MiB | 566766 MiB |
|       from large pool |  78608 MiB |  79496 MiB | 642674 MiB | 564066 MiB |
|       from small pool |     26 MiB |    246 MiB |   2726 MiB |   2700 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3580 MiB |   7454 MiB | 189481 GiB | 189477 GiB |
|       from large pool |   3567 MiB |   7440 MiB | 188266 GiB | 188263 GiB |
|       from small pool |     13 MiB |     21 MiB |   1214 GiB |   1214 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   12564 K  |   12563 K  |
|       from large pool |     314    |     322    |    6052 K  |    6052 K  |
|       from small pool |     291    |     336    |    6511 K  |    6510 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   12564 K  |   12563 K  |
|       from large pool |     314    |     322    |    6052 K  |    6052 K  |
|       from small pool |     291    |     336    |    6511 K  |    6510 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      91    |     217    |    3521    |    3430    |
|       from large pool |      78    |      94    |    2158    |    2080    |
|       from small pool |      13    |     123    |    1363    |    1350    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      89    |      90    |    7038 K  |    7038 K  |
|       from large pool |      60    |      61    |    3937 K  |    3937 K  |
|       from small pool |      29    |      51    |    3100 K  |    3100 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:41:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:41:16] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:41:17]    INFO >> epoch 004:   1398 / 1539 loss=3.985, wps=4600.1, ups=5.81, wpb=791.5, bsz=791.5, num_updates=6000, lr=0.000376, gnorm=8.294, clip=2, train_wall=7, gb_free=71.6, wall=1045 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:41:25]    INFO >> epoch 004:   1448 / 1539 loss=4.03, wps=4786, ups=6.72, wpb=711.7, bsz=711.7, num_updates=6050, lr=0.000376, gnorm=8.939, clip=0, train_wall=7, gb_free=73.5, wall=1053 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:41:32]    INFO >> epoch 004:   1498 / 1539 loss=4.13, wps=4408.7, ups=6.46, wpb=682, bsz=682, num_updates=6100, lr=0.000376, gnorm=8.286, clip=2, train_wall=7, gb_free=74.2, wall=1061 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:41:35] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 25.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.10 GiB is allocated by PyTorch, and 1.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:41:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:41:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:41:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 16           |        cudaMalloc retries: 35        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78888 MiB |  78948 MiB | 194338 GiB | 194261 GiB |
|       from large pool |  78495 MiB |  78555 MiB | 193245 GiB | 193169 GiB |
|       from small pool |    393 MiB |    394 MiB |   1092 GiB |   1092 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78888 MiB |  78948 MiB | 194338 GiB | 194261 GiB |
|       from large pool |  78495 MiB |  78555 MiB | 193245 GiB | 193169 GiB |
|       from small pool |    393 MiB |    394 MiB |   1092 GiB |   1092 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78858 MiB |  78917 MiB | 193930 GiB | 193853 GiB |
|       from large pool |  78467 MiB |  78526 MiB | 192839 GiB | 192762 GiB |
|       from small pool |    390 MiB |    392 MiB |   1090 GiB |   1090 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80480 MiB |  80482 MiB | 647248 MiB | 566768 MiB |
|       from large pool |  80048 MiB |  80048 MiB | 644114 MiB | 564066 MiB |
|       from small pool |    432 MiB |    434 MiB |   3134 MiB |   2702 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1531 MiB |   4145 MiB | 193506 GiB | 193504 GiB |
|       from large pool |   1492 MiB |   4108 MiB | 192266 GiB | 192265 GiB |
|       from small pool |     38 MiB |     41 MiB |   1239 GiB |   1239 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7252    |    7255    |   12813 K  |   12806 K  |
|       from large pool |     929    |     930    |    6169 K  |    6168 K  |
|       from small pool |    6323    |    6326    |    6643 K  |    6637 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7252    |    7255    |   12813 K  |   12806 K  |
|       from large pool |     929    |     930    |    6169 K  |    6168 K  |
|       from small pool |    6323    |    6326    |    6643 K  |    6637 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     318    |     319    |    3749    |    3431    |
|       from large pool |     102    |     102    |    2182    |    2080    |
|       from small pool |     216    |     217    |    1567    |    1351    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     462    |     464    |    7175 K  |    7174 K  |
|       from large pool |      77    |      77    |    4009 K  |    4009 K  |
|       from small pool |     385    |     387    |    3165 K  |    3165 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:41:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:41:35] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:41:39]    INFO >> epoch 004 | loss 4.053 | wps 4255.8 | ups 5.97 | wpb 712.7 | bsz 712.7 | num_updates 6140 | lr 0.000376 | gnorm 8.581 | clip 0.7 | train_wall 224 | gb_free 70.1 | wall 1067 (progress_bar.py:267, print())[0m
[33m[2025-11-21 17:41:39] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 17:41:54]    INFO >> epoch 004 | valid on 'valid' subset | loss 3.885 | wps 10878.6 | wpb 5412.5 | bsz 5412.5 | num_updates 6140 | best_loss 5.157 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 17:41:54]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 17:41:54]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/checkpoints/checkpoint_last.pt (epoch 4 @ 6140 updates, score 3.885) (writing took 0.015298 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 17:41:54] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 17:41:56]    INFO >> epoch 005:     10 / 1539 loss=4.106, wps=1456.1, ups=2.26, wpb=643.2, bsz=643.2, num_updates=6150, lr=0.000354, gnorm=8.067, clip=0, train_wall=7, gb_free=72.1, wall=1083 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:42:04]    INFO >> epoch 005:     60 / 1539 loss=4.101, wps=4310.9, ups=6.12, wpb=704.5, bsz=704.5, num_updates=6200, lr=0.000354, gnorm=7.793, clip=0, train_wall=8, gb_free=73.4, wall=1091 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:42:11]    INFO >> epoch 005:    110 / 1539 loss=4.101, wps=4707.6, ups=6.67, wpb=705.4, bsz=705.4, num_updates=6250, lr=0.000354, gnorm=8.577, clip=0, train_wall=7, gb_free=71.5, wall=1098 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:42:20]    INFO >> epoch 005:    160 / 1539 loss=4.071, wps=4328.6, ups=6.77, wpb=639.6, bsz=639.6, num_updates=6300, lr=0.000354, gnorm=8.436, clip=0, train_wall=7, gb_free=74, wall=1106 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:42:28]    INFO >> epoch 005:    210 / 1539 loss=3.961, wps=4429.6, ups=6.6, wpb=671.6, bsz=671.6, num_updates=6350, lr=0.000354, gnorm=8.72, clip=0, train_wall=7, gb_free=71, wall=1113 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:42:36]    INFO >> epoch 005:    260 / 1539 loss=3.893, wps=5226.9, ups=5.84, wpb=895.2, bsz=895.2, num_updates=6400, lr=0.000354, gnorm=9.314, clip=0, train_wall=8, gb_free=67.8, wall=1122 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:42:44]    INFO >> epoch 005:    310 / 1539 loss=3.972, wps=4618, ups=6.55, wpb=704.8, bsz=704.8, num_updates=6450, lr=0.000354, gnorm=8.277, clip=2, train_wall=7, gb_free=71.8, wall=1130 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:42:53]    INFO >> epoch 005:    360 / 1539 loss=3.559, wps=4870.9, ups=6.3, wpb=772.8, bsz=772.8, num_updates=6500, lr=0.000354, gnorm=8.685, clip=2, train_wall=7, gb_free=69.5, wall=1137 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:43:00] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 27.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.85 GiB is allocated by PyTorch, and 1.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:43:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:43:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:43:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 17           |        cudaMalloc retries: 36        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78634 MiB |  78694 MiB | 211161 GiB | 211084 GiB |
|       from large pool |  78243 MiB |  78303 MiB | 209958 GiB | 209881 GiB |
|       from small pool |    390 MiB |    391 MiB |   1202 GiB |   1202 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78634 MiB |  78694 MiB | 211161 GiB | 211084 GiB |
|       from large pool |  78243 MiB |  78303 MiB | 209958 GiB | 209881 GiB |
|       from small pool |    390 MiB |    391 MiB |   1202 GiB |   1202 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78618 MiB |  78677 MiB | 210720 GiB | 210643 GiB |
|       from large pool |  78229 MiB |  78288 MiB | 209518 GiB | 209442 GiB |
|       from small pool |    388 MiB |    389 MiB |   1201 GiB |   1200 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80478 MiB |  80480 MiB | 647308 MiB | 566830 MiB |
|       from large pool |  80048 MiB |  80048 MiB | 644174 MiB | 564126 MiB |
|       from small pool |    430 MiB |    432 MiB |   3134 MiB |   2704 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1783 MiB |   5192 MiB | 209175 GiB | 209173 GiB |
|       from large pool |   1744 MiB |   5187 MiB | 207813 GiB | 207812 GiB |
|       from small pool |     39 MiB |     41 MiB |   1361 GiB |   1361 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7208    |    7211    |   13967 K  |   13960 K  |
|       from large pool |     925    |     926    |    6636 K  |    6635 K  |
|       from small pool |    6283    |    6286    |    7330 K  |    7324 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7208    |    7211    |   13967 K  |   13960 K  |
|       from large pool |     925    |     926    |    6636 K  |    6635 K  |
|       from small pool |    6283    |    6286    |    7330 K  |    7324 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     317    |     318    |    3750    |    3433    |
|       from large pool |     102    |     102    |    2183    |    2081    |
|       from small pool |     215    |     216    |    1567    |    1352    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     476    |     477    |    7847 K  |    7846 K  |
|       from large pool |      90    |      90    |    4312 K  |    4312 K  |
|       from small pool |     386    |     387    |    3534 K  |    3534 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:43:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:43:00] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:43:01]    INFO >> epoch 005:    411 / 1539 loss=4.097, wps=4302.5, ups=6.31, wpb=681.6, bsz=681.6, num_updates=6550, lr=0.000354, gnorm=8.25, clip=0, train_wall=7, gb_free=73.9, wall=1145 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:43:13]    INFO >> epoch 005:    461 / 1539 loss=4.087, wps=2551.6, ups=4.2, wpb=608, bsz=608, num_updates=6600, lr=0.000354, gnorm=7.802, clip=0, train_wall=11, gb_free=74.7, wall=1157 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:43:21]    INFO >> epoch 005:    511 / 1539 loss=3.825, wps=4611.1, ups=6.31, wpb=730.5, bsz=730.5, num_updates=6650, lr=0.000354, gnorm=8.535, clip=0, train_wall=7, gb_free=69.8, wall=1165 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:43:30]    INFO >> epoch 005:    561 / 1539 loss=4.165, wps=4175, ups=6.73, wpb=620.7, bsz=620.7, num_updates=6700, lr=0.000354, gnorm=8.055, clip=0, train_wall=7, gb_free=72.5, wall=1173 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:43:34] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 3.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.48 GiB is allocated by PyTorch, and 1.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:43:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:43:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:43:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 18           |        cudaMalloc retries: 38        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79276 MiB |  79336 MiB | 216573 GiB | 216496 GiB |
|       from large pool |  79185 MiB |  79245 MiB | 215342 GiB | 215265 GiB |
|       from small pool |     91 MiB |     92 MiB |   1230 GiB |   1230 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79276 MiB |  79336 MiB | 216573 GiB | 216496 GiB |
|       from large pool |  79185 MiB |  79245 MiB | 215342 GiB | 215265 GiB |
|       from small pool |     91 MiB |     92 MiB |   1230 GiB |   1230 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79244 MiB |  79303 MiB | 216121 GiB | 216043 GiB |
|       from large pool |  79153 MiB |  79212 MiB | 214892 GiB | 214815 GiB |
|       from small pool |     90 MiB |     92 MiB |   1228 GiB |   1228 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80502 MiB |  80504 MiB | 647732 MiB | 567230 MiB |
|       from large pool |  80408 MiB |  80408 MiB | 644594 MiB | 564186 MiB |
|       from small pool |     94 MiB |    430 MiB |   3138 MiB |   3044 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1165 MiB |   6539 MiB | 214885 GiB | 214884 GiB |
|       from large pool |   1162 MiB |   6529 MiB | 213491 GiB | 213490 GiB |
|       from small pool |      2 MiB |     25 MiB |   1393 GiB |   1393 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1983    |    1986    |   14316 K  |   14314 K  |
|       from large pool |     472    |     473    |    6818 K  |    6818 K  |
|       from small pool |    1511    |    1514    |    7497 K  |    7496 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1983    |    1986    |   14316 K  |   14314 K  |
|       from large pool |     472    |     473    |    6818 K  |    6818 K  |
|       from small pool |    1511    |    1514    |    7497 K  |    7496 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     155    |     317    |    3759    |    3604    |
|       from large pool |     108    |     108    |    2190    |    2082    |
|       from small pool |      47    |     215    |    1569    |    1522    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     138    |     140    |    8036 K  |    8036 K  |
|       from large pool |      88    |      91    |    4428 K  |    4428 K  |
|       from small pool |      50    |      58    |    3608 K  |    3608 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:43:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:43:34] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:43:38]    INFO >> epoch 005:    612 / 1539 loss=3.896, wps=4379.4, ups=5.79, wpb=756, bsz=756, num_updates=6750, lr=0.000354, gnorm=8.985, clip=0, train_wall=7, gb_free=70.1, wall=1181 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:43:46]    INFO >> epoch 005:    662 / 1539 loss=4.012, wps=4608.7, ups=6.26, wpb=736.8, bsz=736.8, num_updates=6800, lr=0.000354, gnorm=7.978, clip=0, train_wall=7, gb_free=74.2, wall=1189 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:43:54]    INFO >> epoch 005:    712 / 1539 loss=3.949, wps=4630.4, ups=6.35, wpb=729, bsz=729, num_updates=6850, lr=0.000354, gnorm=9.731, clip=0, train_wall=7, gb_free=71.4, wall=1197 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:44:03]    INFO >> epoch 005:    762 / 1539 loss=3.956, wps=4817.7, ups=6.72, wpb=717.1, bsz=717.1, num_updates=6900, lr=0.000354, gnorm=8.079, clip=0, train_wall=7, gb_free=72.9, wall=1205 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:44:12]    INFO >> epoch 005:    812 / 1539 loss=3.959, wps=3959.3, ups=5.88, wpb=673.7, bsz=673.7, num_updates=6950, lr=0.000354, gnorm=9.049, clip=2, train_wall=8, gb_free=69.7, wall=1213 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:44:19]    INFO >> epoch 005:    862 / 1539 loss=4.01, wps=4379.1, ups=6.53, wpb=670.9, bsz=670.9, num_updates=7000, lr=0.000354, gnorm=8.102, clip=0, train_wall=7, gb_free=66.5, wall=1221 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:44:27]    INFO >> epoch 005:    912 / 1539 loss=3.973, wps=4278.7, ups=6.42, wpb=667, bsz=667, num_updates=7050, lr=0.000354, gnorm=8.669, clip=0, train_wall=7, gb_free=71.7, wall=1229 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:44:35] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 131.25 MiB is free. Including non-PyTorch memory, this process has 78.99 GiB memory in use. Of the allocated memory 74.93 GiB is allocated by PyTorch, and 3.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:44:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:44:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:44:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 19           |        cudaMalloc retries: 39        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76325 MiB |  78599 MiB | 227233 GiB | 227158 GiB |
|       from large pool |  76306 MiB |  78582 MiB | 225947 GiB | 225873 GiB |
|       from small pool |     18 MiB |     24 MiB |   1285 GiB |   1285 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76325 MiB |  78599 MiB | 227233 GiB | 227158 GiB |
|       from large pool |  76306 MiB |  78582 MiB | 225947 GiB | 225873 GiB |
|       from small pool |     18 MiB |     24 MiB |   1285 GiB |   1285 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB | 226757 GiB | 226683 GiB |
|       from large pool |  76285 MiB |  78562 MiB | 225474 GiB | 225399 GiB |
|       from small pool |     18 MiB |     24 MiB |   1283 GiB |   1283 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80374 MiB |  80442 MiB | 647732 MiB | 567358 MiB |
|       from large pool |  80348 MiB |  80348 MiB | 644594 MiB | 564246 MiB |
|       from small pool |     26 MiB |     94 MiB |   3138 MiB |   3112 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4048 MiB |   5924 MiB | 225934 GiB | 225930 GiB |
|       from large pool |   4041 MiB |   5910 MiB | 224478 GiB | 224474 GiB |
|       from small pool |      7 MiB |     23 MiB |   1455 GiB |   1455 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     640    |     646    |   14999 K  |   14999 K  |
|       from large pool |     340    |     346    |    7173 K  |    7173 K  |
|       from small pool |     300    |     356    |    7826 K  |    7825 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     640    |     646    |   14999 K  |   14999 K  |
|       from large pool |     340    |     346    |    7173 K  |    7173 K  |
|       from small pool |     300    |     356    |    7826 K  |    7825 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     120    |     154    |    3759    |    3639    |
|       from large pool |     107    |     107    |    2190    |    2083    |
|       from small pool |      13    |      47    |    1569    |    1556    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     111    |     113    |    8408 K  |    8408 K  |
|       from large pool |      84    |      86    |    4655 K  |    4655 K  |
|       from small pool |      27    |      48    |    3752 K  |    3752 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:44:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:44:35] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:44:37]    INFO >> epoch 005:    963 / 1539 loss=4.065, wps=4080.9, ups=6.03, wpb=676.6, bsz=676.6, num_updates=7100, lr=0.000354, gnorm=7.874, clip=2, train_wall=7, gb_free=70.4, wall=1237 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:44:46]    INFO >> epoch 005:   1013 / 1539 loss=3.695, wps=5225.8, ups=5.48, wpb=954.2, bsz=954.2, num_updates=7150, lr=0.000354, gnorm=8.661, clip=0, train_wall=9, gb_free=70.3, wall=1246 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:44:54]    INFO >> epoch 005:   1063 / 1539 loss=3.698, wps=4659.3, ups=5.89, wpb=791.1, bsz=791.1, num_updates=7200, lr=0.000354, gnorm=8.412, clip=0, train_wall=8, gb_free=64.7, wall=1254 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:45:02]    INFO >> epoch 005:   1113 / 1539 loss=3.955, wps=4136.5, ups=6.61, wpb=626.1, bsz=626.1, num_updates=7250, lr=0.000354, gnorm=8.044, clip=0, train_wall=7, gb_free=74.2, wall=1262 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:45:06] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 199.25 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 4.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:45:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:45:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:45:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 20           |        cudaMalloc retries: 43        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72823 MiB |  75427 MiB | 232756 GiB | 232684 GiB |
|       from large pool |  72810 MiB |  75414 MiB | 231439 GiB | 231368 GiB |
|       from small pool |     12 MiB |     21 MiB |   1316 GiB |   1316 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72823 MiB |  75427 MiB | 232756 GiB | 232684 GiB |
|       from large pool |  72810 MiB |  75414 MiB | 231439 GiB | 231368 GiB |
|       from small pool |     12 MiB |     21 MiB |   1316 GiB |   1316 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72801 MiB |  75403 MiB | 232269 GiB | 232198 GiB |
|       from large pool |  72788 MiB |  75391 MiB | 230954 GiB | 230883 GiB |
|       from small pool |     12 MiB |     21 MiB |   1314 GiB |   1314 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80306 MiB |  80306 MiB | 688498 MiB | 608192 MiB |
|       from large pool |  80280 MiB |  80280 MiB | 685160 MiB | 604880 MiB |
|       from small pool |     26 MiB |    226 MiB |   3338 MiB |   3312 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5580 MiB |   9572 MiB | 230920 GiB | 230915 GiB |
|       from large pool |   5567 MiB |   9557 MiB | 229429 GiB | 229423 GiB |
|       from small pool |     13 MiB |     31 MiB |   1491 GiB |   1491 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     603    |     612    |   15362 K  |   15361 K  |
|       from large pool |     312    |     320    |    7349 K  |    7348 K  |
|       from small pool |     291    |     356    |    8012 K  |    8012 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     603    |     612    |   15362 K  |   15361 K  |
|       from large pool |     312    |     320    |    7349 K  |    7348 K  |
|       from small pool |     291    |     356    |    8012 K  |    8012 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     170    |     327    |    3991    |    3821    |
|       from large pool |     157    |     214    |    2322    |    2165    |
|       from small pool |      13    |     113    |    1669    |    1656    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     159    |     161    |    8616 K  |    8616 K  |
|       from large pool |     133    |     135    |    4773 K  |    4773 K  |
|       from small pool |      26    |      58    |    3842 K  |    3842 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:45:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:45:06] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:45:12]    INFO >> epoch 005:   1164 / 1539 loss=3.84, wps=4443.5, ups=5.69, wpb=780.4, bsz=780.4, num_updates=7300, lr=0.000354, gnorm=8.529, clip=2, train_wall=8, gb_free=69.6, wall=1271 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:45:20]    INFO >> epoch 005:   1214 / 1539 loss=3.996, wps=4210.3, ups=6.4, wpb=657.7, bsz=657.7, num_updates=7350, lr=0.000354, gnorm=7.237, clip=0, train_wall=7, gb_free=68.7, wall=1279 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:45:27]    INFO >> epoch 005:   1264 / 1539 loss=3.848, wps=4487.6, ups=7.07, wpb=635.1, bsz=635.1, num_updates=7400, lr=0.000354, gnorm=8.777, clip=0, train_wall=7, gb_free=73.5, wall=1286 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:45:35]    INFO >> epoch 005:   1314 / 1539 loss=4.045, wps=4136.4, ups=6.35, wpb=651.5, bsz=651.5, num_updates=7450, lr=0.000354, gnorm=7.884, clip=2, train_wall=7, gb_free=70, wall=1294 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:45:44]    INFO >> epoch 005:   1364 / 1539 loss=3.896, wps=4618.9, ups=6.37, wpb=725.6, bsz=725.6, num_updates=7500, lr=0.000354, gnorm=7.992, clip=0, train_wall=7, gb_free=74.7, wall=1301 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:45:51]    INFO >> epoch 005:   1414 / 1539 loss=4.091, wps=4180, ups=6.63, wpb=630.1, bsz=630.1, num_updates=7550, lr=0.000354, gnorm=7.9, clip=0, train_wall=7, gb_free=74.2, wall=1309 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:45:59]    INFO >> epoch 005:   1464 / 1539 loss=3.789, wps=4752, ups=6.22, wpb=763.7, bsz=763.7, num_updates=7600, lr=0.000354, gnorm=9.282, clip=4, train_wall=8, gb_free=48.5, wall=1317 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:46:08]    INFO >> epoch 005:   1514 / 1539 loss=3.922, wps=4874.9, ups=6.07, wpb=802.7, bsz=802.7, num_updates=7650, lr=0.000354, gnorm=7.916, clip=0, train_wall=8, gb_free=54.7, wall=1325 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:46:13]    INFO >> epoch 005 | loss 3.939 | wps 4169 | ups 5.85 | wpb 712.7 | bsz 712.7 | num_updates 7675 | lr 0.000354 | gnorm 8.394 | clip 0.5 | train_wall 230 | gb_free 63.5 | wall 1329 (progress_bar.py:267, print())[0m
[33m[2025-11-21 17:46:13] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 17:46:28]    INFO >> epoch 005 | valid on 'valid' subset | loss 3.857 | wps 10117.8 | wpb 5412.5 | bsz 5412.5 | num_updates 7675 | best_loss 5.157 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 17:46:28]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 17:46:28]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/checkpoints/checkpoint_last.pt (epoch 5 @ 7675 updates, score 3.857) (writing took 0.014713 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 17:46:28] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 17:46:32]    INFO >> epoch 006:     25 / 1539 loss=3.937, wps=1536.8, ups=2.17, wpb=708.8, bsz=708.8, num_updates=7700, lr=0.000327, gnorm=8.239, clip=0, train_wall=7, gb_free=74.9, wall=1348 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:46:40]    INFO >> epoch 006:     75 / 1539 loss=3.935, wps=5195.5, ups=6.02, wpb=863.7, bsz=863.7, num_updates=7750, lr=0.000327, gnorm=8.447, clip=2, train_wall=8, gb_free=75.2, wall=1357 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:46:49]    INFO >> epoch 006:    125 / 1539 loss=3.867, wps=4409.4, ups=6.53, wpb=675.3, bsz=675.3, num_updates=7800, lr=0.000327, gnorm=8.773, clip=0, train_wall=7, gb_free=70.3, wall=1364 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:46:57]    INFO >> epoch 006:    175 / 1539 loss=3.983, wps=4372, ups=6.35, wpb=689, bsz=689, num_updates=7850, lr=0.000327, gnorm=8.204, clip=0, train_wall=7, gb_free=74.3, wall=1372 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:47:00] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 535.25 MiB is free. Including non-PyTorch memory, this process has 78.59 GiB memory in use. Of the allocated memory 73.81 GiB is allocated by PyTorch, and 4.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:47:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:47:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:47:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 21           |        cudaMalloc retries: 44        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75055 MiB |  75998 MiB | 254753 GiB | 254679 GiB |
|       from large pool |  75042 MiB |  75985 MiB | 253307 GiB | 253233 GiB |
|       from small pool |     12 MiB |     15 MiB |   1446 GiB |   1446 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75055 MiB |  75998 MiB | 254753 GiB | 254679 GiB |
|       from large pool |  75042 MiB |  75985 MiB | 253307 GiB | 253233 GiB |
|       from small pool |     12 MiB |     15 MiB |   1446 GiB |   1446 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75035 MiB |  75977 MiB | 254221 GiB | 254148 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 252777 GiB | 252704 GiB |
|       from small pool |     12 MiB |     15 MiB |   1443 GiB |   1443 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79970 MiB |  80406 MiB | 690500 MiB | 610530 MiB |
|       from large pool |  79944 MiB |  80306 MiB | 687088 MiB | 607144 MiB |
|       from small pool |     26 MiB |    100 MiB |   3412 MiB |   3386 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4914 MiB |   8989 MiB | 249574 GiB | 249569 GiB |
|       from large pool |   4901 MiB |   8976 MiB | 247940 GiB | 247935 GiB |
|       from small pool |     13 MiB |     25 MiB |   1633 GiB |   1633 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   16790 K  |   16790 K  |
|       from large pool |     314    |     322    |    7980 K  |    7979 K  |
|       from small pool |     291    |     356    |    8810 K  |    8810 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   16790 K  |   16790 K  |
|       from large pool |     314    |     322    |    7980 K  |    7979 K  |
|       from small pool |     291    |     356    |    8810 K  |    8810 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     172    |     210    |    4032    |    3860    |
|       from large pool |     159    |     160    |    2326    |    2167    |
|       from small pool |      13    |      50    |    1706    |    1693    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     164    |     164    |    9442 K  |    9442 K  |
|       from large pool |     136    |     136    |    5197 K  |    5196 K  |
|       from small pool |      28    |      57    |    4245 K  |    4245 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:47:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:47:00] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 17:47:06] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 904.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 535.25 MiB is free. Including non-PyTorch memory, this process has 78.59 GiB memory in use. Of the allocated memory 72.94 GiB is allocated by PyTorch, and 5.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:47:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:47:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:47:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 22           |        cudaMalloc retries: 45        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69875 MiB |  74872 MiB | 255914 GiB | 255845 GiB |
|       from large pool |  69858 MiB |  74855 MiB | 254462 GiB | 254394 GiB |
|       from small pool |     17 MiB |     17 MiB |   1451 GiB |   1451 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69875 MiB |  74872 MiB | 255914 GiB | 255845 GiB |
|       from large pool |  69858 MiB |  74855 MiB | 254462 GiB | 254394 GiB |
|       from small pool |     17 MiB |     17 MiB |   1451 GiB |   1451 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  69846 MiB |  74842 MiB | 255380 GiB | 255311 GiB |
|       from large pool |  69829 MiB |  74825 MiB | 253930 GiB | 253862 GiB |
|       from small pool |     17 MiB |     17 MiB |   1449 GiB |   1449 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79970 MiB |  80006 MiB | 690536 MiB | 610566 MiB |
|       from large pool |  79944 MiB |  79944 MiB | 687088 MiB | 607144 MiB |
|       from small pool |     26 MiB |     62 MiB |   3448 MiB |   3422 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  10094 MiB |  11472 MiB | 250631 GiB | 250621 GiB |
|       from large pool |  10085 MiB |  11463 MiB | 248992 GiB | 248982 GiB |
|       from small pool |      8 MiB |     27 MiB |   1639 GiB |   1639 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     643    |     661    |   16854 K  |   16853 K  |
|       from large pool |     344    |     362    |    8013 K  |    8013 K  |
|       from small pool |     299    |     356    |    8840 K  |    8840 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     643    |     661    |   16854 K  |   16853 K  |
|       from large pool |     344    |     362    |    8013 K  |    8013 K  |
|       from small pool |     299    |     356    |    8840 K  |    8840 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     172    |     190    |    4050    |    3878    |
|       from large pool |     159    |     159    |    2326    |    2167    |
|       from small pool |      13    |      31    |    1724    |    1711    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     151    |     153    |    9477 K  |    9477 K  |
|       from large pool |     121    |     123    |    5219 K  |    5219 K  |
|       from small pool |      30    |      62    |    4258 K  |    4258 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:47:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:47:06] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:47:07]    INFO >> epoch 006:    227 / 1539 loss=3.938, wps=3729.2, ups=5.19, wpb=719.2, bsz=719.2, num_updates=7900, lr=0.000327, gnorm=7.87, clip=0, train_wall=8, gb_free=71.4, wall=1382 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:47:14]    INFO >> epoch 006:    277 / 1539 loss=4.017, wps=4165, ups=6.66, wpb=625.7, bsz=625.7, num_updates=7950, lr=0.000327, gnorm=6.758, clip=0, train_wall=7, gb_free=74.5, wall=1389 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:47:24]    INFO >> epoch 006:    327 / 1539 loss=3.646, wps=5030.3, ups=6.06, wpb=830.5, bsz=830.5, num_updates=8000, lr=0.000327, gnorm=8.953, clip=0, train_wall=8, gb_free=70.3, wall=1398 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:47:32]    INFO >> epoch 006:    377 / 1539 loss=4.098, wps=4132.1, ups=6.28, wpb=657.6, bsz=657.6, num_updates=8050, lr=0.000327, gnorm=7.039, clip=0, train_wall=7, gb_free=71.1, wall=1406 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:47:40]    INFO >> epoch 006:    427 / 1539 loss=3.713, wps=4963.7, ups=6.43, wpb=772.1, bsz=772.1, num_updates=8100, lr=0.000327, gnorm=9.171, clip=4, train_wall=7, gb_free=75, wall=1413 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:47:42] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 2.39 GiB is free. Including non-PyTorch memory, this process has 76.72 GiB memory in use. Of the allocated memory 70.76 GiB is allocated by PyTorch, and 5.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:47:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:47:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:47:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 23           |        cudaMalloc retries: 47        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72059 MiB |  72460 MiB | 262338 GiB | 262268 GiB |
|       from large pool |  72042 MiB |  72442 MiB | 260852 GiB | 260781 GiB |
|       from small pool |     17 MiB |     21 MiB |   1486 GiB |   1486 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72059 MiB |  72460 MiB | 262338 GiB | 262268 GiB |
|       from large pool |  72042 MiB |  72442 MiB | 260852 GiB | 260781 GiB |
|       from small pool |     17 MiB |     21 MiB |   1486 GiB |   1486 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  72444 MiB | 261790 GiB | 261719 GiB |
|       from large pool |  72027 MiB |  72426 MiB | 260305 GiB | 260235 GiB |
|       from small pool |     17 MiB |     21 MiB |   1484 GiB |   1484 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78056 MiB |  80154 MiB | 696552 MiB | 618496 MiB |
|       from large pool |  78026 MiB |  79944 MiB | 692920 MiB | 614894 MiB |
|       from small pool |     30 MiB |    210 MiB |   3632 MiB |   3602 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5996 MiB |   8377 MiB | 256697 GiB | 256691 GiB |
|       from large pool |   5983 MiB |   8364 MiB | 255017 GiB | 255011 GiB |
|       from small pool |     12 MiB |     31 MiB |   1680 GiB |   1680 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   17287 K  |   17286 K  |
|       from large pool |     308    |     315    |    8230 K  |    8230 K  |
|       from small pool |     298    |     356    |    9056 K  |    9056 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   17287 K  |   17286 K  |
|       from large pool |     308    |     315    |    8230 K  |    8230 K  |
|       from small pool |     298    |     356    |    9056 K  |    9056 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     172    |     264    |    4145    |    3973    |
|       from large pool |     157    |     159    |    2329    |    2172    |
|       from small pool |      15    |     105    |    1816    |    1801    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     169    |     169    |    9721 K  |    9721 K  |
|       from large pool |     137    |     137    |    5362 K  |    5362 K  |
|       from small pool |      32    |      55    |    4358 K  |    4358 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:47:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:47:42] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:47:48]    INFO >> epoch 006:    478 / 1539 loss=3.878, wps=4259.8, ups=6.04, wpb=705.8, bsz=705.8, num_updates=8150, lr=0.000327, gnorm=8.302, clip=0, train_wall=7, gb_free=65, wall=1422 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:47:58]    INFO >> epoch 006:    528 / 1539 loss=3.808, wps=4170.1, ups=5.89, wpb=707.6, bsz=707.6, num_updates=8200, lr=0.000327, gnorm=7.899, clip=0, train_wall=8, gb_free=72, wall=1430 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:48:06]    INFO >> epoch 006:    578 / 1539 loss=3.857, wps=4928.9, ups=6.18, wpb=797.6, bsz=797.6, num_updates=8250, lr=0.000327, gnorm=8.033, clip=0, train_wall=8, gb_free=74.1, wall=1438 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:48:13]    INFO >> epoch 006:    628 / 1539 loss=3.898, wps=4372.2, ups=6.78, wpb=644.9, bsz=644.9, num_updates=8300, lr=0.000327, gnorm=8.191, clip=0, train_wall=7, gb_free=70.5, wall=1446 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:48:22]    INFO >> epoch 006:    678 / 1539 loss=3.844, wps=4440.4, ups=6.08, wpb=730.2, bsz=730.2, num_updates=8350, lr=0.000327, gnorm=7.612, clip=0, train_wall=8, gb_free=73.6, wall=1454 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:48:30]    INFO >> epoch 006:    728 / 1539 loss=3.996, wps=4120.3, ups=6.69, wpb=616.1, bsz=616.1, num_updates=8400, lr=0.000327, gnorm=7.125, clip=0, train_wall=7, gb_free=62, wall=1461 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:48:31] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 17.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 76.16 GiB is allocated by PyTorch, and 2.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:48:31] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:48:31] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:48:31] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 24           |        cudaMalloc retries: 48        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77932 MiB |  77992 MiB | 270810 GiB | 270734 GiB |
|       from large pool |  77549 MiB |  77609 MiB | 269273 GiB | 269197 GiB |
|       from small pool |    383 MiB |    384 MiB |   1536 GiB |   1536 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77932 MiB |  77992 MiB | 270810 GiB | 270734 GiB |
|       from large pool |  77549 MiB |  77609 MiB | 269273 GiB | 269197 GiB |
|       from small pool |    383 MiB |    384 MiB |   1536 GiB |   1536 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77897 MiB |  77956 MiB | 270242 GiB | 270166 GiB |
|       from large pool |  77516 MiB |  77575 MiB | 268708 GiB | 268632 GiB |
|       from small pool |    381 MiB |    382 MiB |   1534 GiB |   1533 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80488 MiB |  80490 MiB | 698986 MiB | 618498 MiB |
|       from large pool |  80066 MiB |  80066 MiB | 694960 MiB | 614894 MiB |
|       from small pool |    422 MiB |    424 MiB |   4026 MiB |   3604 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2495 MiB |   6667 MiB | 264824 GiB | 264822 GiB |
|       from large pool |   2456 MiB |   6661 MiB | 263086 GiB | 263084 GiB |
|       from small pool |     38 MiB |     40 MiB |   1738 GiB |   1738 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7076    |    7079    |   17882 K  |   17874 K  |
|       from large pool |     913    |     914    |    8520 K  |    8519 K  |
|       from small pool |    6163    |    6166    |    9361 K  |    9355 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7076    |    7079    |   17882 K  |   17874 K  |
|       from large pool |     913    |     914    |    8520 K  |    8519 K  |
|       from small pool |    6163    |    6166    |    9361 K  |    9355 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     402    |     403    |    4376    |    3974    |
|       from large pool |     191    |     191    |    2363    |    2172    |
|       from small pool |     211    |     212    |    2013    |    1802    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     526    |     527    |   10055 K  |   10055 K  |
|       from large pool |     150    |     150    |    5552 K  |    5552 K  |
|       from small pool |     376    |     377    |    4503 K  |    4502 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:48:31] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:48:31] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:48:38]    INFO >> epoch 006:    779 / 1539 loss=3.926, wps=4114.7, ups=6.4, wpb=642.5, bsz=642.5, num_updates=8450, lr=0.000327, gnorm=7.689, clip=0, train_wall=7, gb_free=74.1, wall=1469 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:48:46]    INFO >> epoch 006:    829 / 1539 loss=3.925, wps=4318.2, ups=6.57, wpb=656.8, bsz=656.8, num_updates=8500, lr=0.000327, gnorm=8.07, clip=0, train_wall=7, gb_free=66.6, wall=1477 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:48:54]    INFO >> epoch 006:    879 / 1539 loss=3.867, wps=4589.1, ups=6.29, wpb=729.6, bsz=729.6, num_updates=8550, lr=0.000327, gnorm=8.015, clip=0, train_wall=7, gb_free=70.2, wall=1485 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:49:04]    INFO >> epoch 006:    929 / 1539 loss=4.003, wps=4760.1, ups=5.76, wpb=826.1, bsz=826.1, num_updates=8600, lr=0.000327, gnorm=8.997, clip=0, train_wall=8, gb_free=73.1, wall=1493 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:49:12]    INFO >> epoch 006:    979 / 1539 loss=3.865, wps=4441.7, ups=6.34, wpb=701.1, bsz=701.1, num_updates=8650, lr=0.000327, gnorm=7.632, clip=0, train_wall=7, gb_free=75.1, wall=1501 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:49:21] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.57 GiB. GPU 2 has a total capacity of 79.14 GiB of which 2.27 GiB is free. Including non-PyTorch memory, this process has 76.84 GiB memory in use. Of the allocated memory 66.03 GiB is allocated by PyTorch, and 10.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:49:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:49:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:49:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 25           |        cudaMalloc retries: 49        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  63499 MiB |  70130 MiB | 279629 GiB | 279567 GiB |
|       from large pool |  63479 MiB |  70110 MiB | 278045 GiB | 277983 GiB |
|       from small pool |     20 MiB |     60 MiB |   1584 GiB |   1584 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  63499 MiB |  70130 MiB | 279629 GiB | 279567 GiB |
|       from large pool |  63479 MiB |  70110 MiB | 278045 GiB | 277983 GiB |
|       from small pool |     20 MiB |     60 MiB |   1584 GiB |   1584 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  63481 MiB |  70111 MiB | 279042 GiB | 278980 GiB |
|       from large pool |  63461 MiB |  70091 MiB | 277460 GiB | 277398 GiB |
|       from small pool |     20 MiB |     60 MiB |   1582 GiB |   1582 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78178 MiB |  80428 MiB | 698986 MiB | 620808 MiB |
|       from large pool |  78146 MiB |  80006 MiB | 694960 MiB | 616814 MiB |
|       from small pool |     32 MiB |    422 MiB |   4026 MiB |   3994 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   7300 MiB |  10790 MiB | 272808 GiB | 272801 GiB |
|       from large pool |   7288 MiB |  10778 MiB | 271014 GiB | 271007 GiB |
|       from small pool |     11 MiB |     29 MiB |   1793 GiB |   1793 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     646    |    1326    |   18465 K  |   18465 K  |
|       from large pool |     330    |     403    |    8814 K  |    8814 K  |
|       from small pool |     316    |     924    |    9651 K  |    9650 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     646    |    1326    |   18465 K  |   18465 K  |
|       from large pool |     330    |     403    |    8814 K  |    8814 K  |
|       from small pool |     316    |     924    |    9651 K  |    9650 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     176    |     401    |    4376    |    4200    |
|       from large pool |     160    |     190    |    2363    |    2203    |
|       from small pool |      16    |     211    |    2013    |    1997    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     165    |     213    |   10388 K  |   10388 K  |
|       from large pool |     129    |     160    |    5749 K  |    5749 K  |
|       from small pool |      36    |      56    |    4638 K  |    4638 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:49:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:49:21] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:49:21]    INFO >> epoch 006:   1030 / 1539 loss=3.978, wps=3975.4, ups=5.45, wpb=730.1, bsz=730.1, num_updates=8700, lr=0.000327, gnorm=7.388, clip=0, train_wall=8, gb_free=11.5, wall=1510 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:49:29]    INFO >> epoch 006:   1080 / 1539 loss=3.885, wps=4501.9, ups=6.22, wpb=723.5, bsz=723.5, num_updates=8750, lr=0.000327, gnorm=8.243, clip=0, train_wall=8, gb_free=72.4, wall=1518 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:49:39]    INFO >> epoch 006:   1130 / 1539 loss=3.622, wps=4371.5, ups=5.77, wpb=757.6, bsz=757.6, num_updates=8800, lr=0.000327, gnorm=8.515, clip=2, train_wall=8, gb_free=72.1, wall=1527 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:49:47]    INFO >> epoch 006:   1180 / 1539 loss=3.966, wps=4598.2, ups=6.21, wpb=740.5, bsz=740.5, num_updates=8850, lr=0.000327, gnorm=8.882, clip=0, train_wall=8, gb_free=69, wall=1535 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:49:55]    INFO >> epoch 006:   1230 / 1539 loss=4.019, wps=4050.5, ups=6.14, wpb=659.3, bsz=659.3, num_updates=8900, lr=0.000327, gnorm=7.678, clip=0, train_wall=8, gb_free=73.8, wall=1543 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:50:03]    INFO >> epoch 006:   1280 / 1539 loss=3.989, wps=4271.7, ups=6.57, wpb=650.6, bsz=650.6, num_updates=8950, lr=0.000327, gnorm=8.025, clip=0, train_wall=7, gb_free=75.3, wall=1551 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:50:12]    INFO >> epoch 006:   1330 / 1539 loss=3.883, wps=4769.4, ups=6.44, wpb=741, bsz=741, num_updates=9000, lr=0.000327, gnorm=7.403, clip=0, train_wall=7, gb_free=71.3, wall=1559 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:50:19]    INFO >> epoch 006:   1380 / 1539 loss=3.997, wps=4203.1, ups=6.9, wpb=608.9, bsz=608.9, num_updates=9050, lr=0.000327, gnorm=6.718, clip=0, train_wall=7, gb_free=68.9, wall=1566 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:50:27]    INFO >> epoch 006:   1430 / 1539 loss=3.788, wps=4717.2, ups=6.27, wpb=751.8, bsz=751.8, num_updates=9100, lr=0.000327, gnorm=8.081, clip=0, train_wall=7, gb_free=70.5, wall=1574 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:50:35]    INFO >> epoch 006:   1480 / 1539 loss=3.681, wps=4357.5, ups=6.08, wpb=717, bsz=717, num_updates=9150, lr=0.000327, gnorm=7.928, clip=2, train_wall=8, gb_free=69.4, wall=1582 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:50:44]    INFO >> epoch 006:   1530 / 1539 loss=3.873, wps=4530.6, ups=6.72, wpb=674.1, bsz=674.1, num_updates=9200, lr=0.000327, gnorm=7.222, clip=0, train_wall=7, gb_free=74.6, wall=1590 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:50:46]    INFO >> epoch 006 | loss 3.889 | wps 4168.7 | ups 5.86 | wpb 711.1 | bsz 711.1 | num_updates 9209 | lr 0.000327 | gnorm 7.951 | clip 0.3 | train_wall 228 | gb_free 72.3 | wall 1591 (progress_bar.py:267, print())[0m
[33m[2025-11-21 17:50:46] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 17:51:01]    INFO >> epoch 006 | valid on 'valid' subset | loss 3.865 | wps 10059.2 | wpb 5412.5 | bsz 5412.5 | num_updates 9209 | best_loss 5.157 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 17:51:01]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 17:51:01]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/checkpoints/checkpoint_last.pt (epoch 6 @ 9209 updates, score 3.865) (writing took 0.014185 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 17:51:01] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 17:51:08]    INFO >> epoch 007:     41 / 1539 loss=3.999, wps=1455.2, ups=2.07, wpb=704.3, bsz=704.3, num_updates=9250, lr=0.000295, gnorm=7.411, clip=0, train_wall=8, gb_free=71.6, wall=1614 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:51:10] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.29 GiB memory in use. Of the allocated memory 70.77 GiB is allocated by PyTorch, and 6.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:51:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:51:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:51:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 26           |        cudaMalloc retries: 51        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72064 MiB |  72463 MiB | 300570 GiB | 300499 GiB |
|       from large pool |  72047 MiB |  72446 MiB | 298863 GiB | 298793 GiB |
|       from small pool |     17 MiB |     25 MiB |   1706 GiB |   1706 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72064 MiB |  72463 MiB | 300570 GiB | 300499 GiB |
|       from large pool |  72047 MiB |  72446 MiB | 298863 GiB | 298793 GiB |
|       from small pool |     17 MiB |     25 MiB |   1706 GiB |   1706 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  72444 MiB | 299940 GiB | 299870 GiB |
|       from large pool |  72027 MiB |  72426 MiB | 298236 GiB | 298165 GiB |
|       from small pool |     17 MiB |     25 MiB |   1704 GiB |   1704 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78634 MiB |  78794 MiB | 712436 MiB | 633802 MiB |
|       from large pool |  78610 MiB |  78670 MiB | 708318 MiB | 629708 MiB |
|       from small pool |     24 MiB |    124 MiB |   4118 MiB |   4094 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6569 MiB |   8371 MiB | 290286 GiB | 290279 GiB |
|       from large pool |   6562 MiB |   8363 MiB | 288359 GiB | 288352 GiB |
|       from small pool |      6 MiB |     33 MiB |   1927 GiB |   1927 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   19804 K  |   19803 K  |
|       from large pool |     308    |     315    |    9401 K  |    9401 K  |
|       from small pool |     298    |     356    |   10402 K  |   10402 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   19804 K  |   19803 K  |
|       from large pool |     308    |     315    |    9401 K  |    9401 K  |
|       from small pool |     298    |     356    |   10402 K  |   10402 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     168    |     222    |    4427    |    4259    |
|       from large pool |     156    |     160    |    2368    |    2212    |
|       from small pool |      12    |      62    |    2059    |    2047    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     162    |     163    |   11162 K  |   11162 K  |
|       from large pool |     137    |     138    |    6142 K  |    6142 K  |
|       from small pool |      25    |      62    |    5020 K  |    5020 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:51:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:51:10] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:51:18]    INFO >> epoch 007:     92 / 1539 loss=3.779, wps=4124.5, ups=6.11, wpb=674.7, bsz=674.7, num_updates=9300, lr=0.000295, gnorm=8.099, clip=0, train_wall=7, gb_free=74.6, wall=1622 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:51:26]    INFO >> epoch 007:    142 / 1539 loss=3.93, wps=4234.8, ups=6.39, wpb=663.1, bsz=663.1, num_updates=9350, lr=0.000295, gnorm=8.036, clip=0, train_wall=7, gb_free=73.4, wall=1630 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:51:34]    INFO >> epoch 007:    192 / 1539 loss=3.571, wps=5430.5, ups=6.14, wpb=885.1, bsz=885.1, num_updates=9400, lr=0.000295, gnorm=7.939, clip=0, train_wall=8, gb_free=67.7, wall=1638 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:51:42]    INFO >> epoch 007:    242 / 1539 loss=3.845, wps=4736.2, ups=5.96, wpb=794.3, bsz=794.3, num_updates=9450, lr=0.000295, gnorm=8.463, clip=0, train_wall=8, gb_free=70.7, wall=1646 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:51:51]    INFO >> epoch 007:    292 / 1539 loss=3.946, wps=4419.9, ups=6.32, wpb=699.6, bsz=699.6, num_updates=9500, lr=0.000295, gnorm=7.212, clip=0, train_wall=7, gb_free=71.4, wall=1654 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:51:57] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.29 GiB memory in use. Of the allocated memory 70.08 GiB is allocated by PyTorch, and 6.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:51:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:51:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:51:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 27           |        cudaMalloc retries: 52        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69006 MiB |  71763 MiB | 308769 GiB | 308702 GiB |
|       from large pool |  68989 MiB |  71746 MiB | 307018 GiB | 306950 GiB |
|       from small pool |     16 MiB |     18 MiB |   1751 GiB |   1751 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69006 MiB |  71763 MiB | 308769 GiB | 308702 GiB |
|       from large pool |  68989 MiB |  71746 MiB | 307018 GiB | 306950 GiB |
|       from small pool |     16 MiB |     18 MiB |   1751 GiB |   1751 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  68980 MiB |  71736 MiB | 308120 GiB | 308053 GiB |
|       from large pool |  68963 MiB |  71719 MiB | 306372 GiB | 306304 GiB |
|       from small pool |     16 MiB |     17 MiB |   1748 GiB |   1748 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78636 MiB |  78836 MiB | 712638 MiB | 634002 MiB |
|       from large pool |  78610 MiB |  78610 MiB | 708318 MiB | 629708 MiB |
|       from small pool |     26 MiB |    226 MiB |   4320 MiB |   4294 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6901 MiB |  10618 MiB | 298170 GiB | 298163 GiB |
|       from large pool |   6892 MiB |  10608 MiB | 296191 GiB | 296185 GiB |
|       from small pool |      9 MiB |     25 MiB |   1978 GiB |   1978 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     649    |   20344 K  |   20344 K  |
|       from large pool |     343    |     351    |    9674 K  |    9674 K  |
|       from small pool |     298    |     342    |   10670 K  |   10669 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     649    |   20344 K  |   20344 K  |
|       from large pool |     343    |     351    |    9674 K  |    9674 K  |
|       from small pool |     298    |     342    |   10670 K  |   10669 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     169    |     269    |    4528    |    4359    |
|       from large pool |     156    |     156    |    2368    |    2212    |
|       from small pool |      13    |     113    |    2160    |    2147    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     153    |     157    |   11466 K  |   11466 K  |
|       from large pool |     124    |     128    |    6323 K  |    6323 K  |
|       from small pool |      29    |      53    |    5142 K  |    5142 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:51:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:51:57] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:52:01]    INFO >> epoch 007:    343 / 1539 loss=3.793, wps=3685.4, ups=5.36, wpb=687.1, bsz=687.1, num_updates=9550, lr=0.000295, gnorm=7.99, clip=2, train_wall=8, gb_free=69.5, wall=1663 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:52:08]    INFO >> epoch 007:    393 / 1539 loss=4.076, wps=3981.3, ups=6.66, wpb=598.1, bsz=598.1, num_updates=9600, lr=0.000295, gnorm=7.142, clip=0, train_wall=7, gb_free=62, wall=1671 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:52:17]    INFO >> epoch 007:    443 / 1539 loss=3.776, wps=4112, ups=6.07, wpb=677.8, bsz=677.8, num_updates=9650, lr=0.000295, gnorm=7.257, clip=2, train_wall=8, gb_free=74.5, wall=1679 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:52:23] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 123.25 MiB is free. Including non-PyTorch memory, this process has 79.00 GiB memory in use. Of the allocated memory 75.66 GiB is allocated by PyTorch, and 2.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:52:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:52:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:52:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 28           |        cudaMalloc retries: 54        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76958 MiB |  77480 MiB | 313195 GiB | 313120 GiB |
|       from large pool |  76945 MiB |  77467 MiB | 311423 GiB | 311348 GiB |
|       from small pool |     12 MiB |     18 MiB |   1772 GiB |   1772 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76958 MiB |  77480 MiB | 313195 GiB | 313120 GiB |
|       from large pool |  76945 MiB |  77467 MiB | 311423 GiB | 311348 GiB |
|       from small pool |     12 MiB |     18 MiB |   1772 GiB |   1772 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76936 MiB |  77458 MiB | 312537 GiB | 312462 GiB |
|       from large pool |  76923 MiB |  77445 MiB | 310767 GiB | 310692 GiB |
|       from small pool |     12 MiB |     18 MiB |   1769 GiB |   1769 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80382 MiB |  80382 MiB | 719380 MiB | 638998 MiB |
|       from large pool |  80358 MiB |  80358 MiB | 715014 MiB | 634656 MiB |
|       from small pool |     24 MiB |     72 MiB |   4366 MiB |   4342 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3423 MiB |   7648 MiB | 302343 GiB | 302340 GiB |
|       from large pool |   3412 MiB |   7637 MiB | 300341 GiB | 300338 GiB |
|       from small pool |     11 MiB |     19 MiB |   2002 GiB |   2002 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   20614 K  |   20613 K  |
|       from large pool |     315    |     322    |    9816 K  |    9816 K  |
|       from small pool |     291    |     356    |   10797 K  |   10797 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   20614 K  |   20613 K  |
|       from large pool |     315    |     322    |    9816 K  |    9816 K  |
|       from small pool |     291    |     356    |   10797 K  |   10797 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     138    |     197    |    4559    |    4421    |
|       from large pool |     126    |     161    |    2376    |    2250    |
|       from small pool |      12    |      36    |    2183    |    2171    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     134    |     134    |   11614 K  |   11614 K  |
|       from large pool |     107    |     107    |    6417 K  |    6417 K  |
|       from small pool |      27    |      47    |    5197 K  |    5197 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:52:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:52:23] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:52:27]    INFO >> epoch 007:    494 / 1539 loss=4.041, wps=4166.9, ups=5.61, wpb=742.8, bsz=742.8, num_updates=9700, lr=0.000295, gnorm=7.3, clip=0, train_wall=8, gb_free=72.2, wall=1688 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:52:34]    INFO >> epoch 007:    544 / 1539 loss=4.04, wps=4121.1, ups=6.53, wpb=631.2, bsz=631.2, num_updates=9750, lr=0.000295, gnorm=6.978, clip=0, train_wall=7, gb_free=74.3, wall=1696 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:52:42]    INFO >> epoch 007:    594 / 1539 loss=3.926, wps=4395.5, ups=6.42, wpb=684.3, bsz=684.3, num_updates=9800, lr=0.000295, gnorm=7.419, clip=2, train_wall=7, gb_free=75.1, wall=1704 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:52:50]    INFO >> epoch 007:    644 / 1539 loss=3.69, wps=4324.5, ups=6.2, wpb=697.8, bsz=697.8, num_updates=9850, lr=0.000295, gnorm=8.52, clip=0, train_wall=8, gb_free=70.6, wall=1712 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:53:00]    INFO >> epoch 007:    694 / 1539 loss=3.655, wps=5063.8, ups=6.06, wpb=835.9, bsz=835.9, num_updates=9900, lr=0.000295, gnorm=8.029, clip=4, train_wall=8, gb_free=73.9, wall=1720 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:53:08]    INFO >> epoch 007:    744 / 1539 loss=3.973, wps=4612.8, ups=6.48, wpb=712, bsz=712, num_updates=9950, lr=0.000295, gnorm=7.619, clip=0, train_wall=7, gb_free=72.8, wall=1728 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:53:16]    INFO >> epoch 007:    794 / 1539 loss=3.938, wps=4080.4, ups=6.23, wpb=654.6, bsz=654.6, num_updates=10000, lr=0.000295, gnorm=7.278, clip=0, train_wall=8, gb_free=72.1, wall=1736 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:53:23]    INFO >> epoch 007:    844 / 1539 loss=3.777, wps=4599.4, ups=6.36, wpb=722.7, bsz=722.7, num_updates=10050, lr=0.000295, gnorm=7.848, clip=0, train_wall=7, gb_free=70.6, wall=1744 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:53:33]    INFO >> epoch 007:    894 / 1539 loss=3.945, wps=4318.2, ups=6.28, wpb=688.1, bsz=688.1, num_updates=10100, lr=0.000295, gnorm=7.442, clip=0, train_wall=8, gb_free=73.8, wall=1751 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:53:41]    INFO >> epoch 007:    944 / 1539 loss=3.84, wps=4471.5, ups=6.4, wpb=699, bsz=699, num_updates=10150, lr=0.000295, gnorm=7.803, clip=0, train_wall=7, gb_free=71.9, wall=1759 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:53:49]    INFO >> epoch 007:    994 / 1539 loss=3.966, wps=4528.2, ups=6.15, wpb=736.1, bsz=736.1, num_updates=10200, lr=0.000295, gnorm=6.625, clip=0, train_wall=8, gb_free=70.3, wall=1767 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:53:57]    INFO >> epoch 007:   1044 / 1539 loss=3.783, wps=4455.8, ups=6.31, wpb=705.7, bsz=705.7, num_updates=10250, lr=0.000295, gnorm=8.118, clip=0, train_wall=7, gb_free=73, wall=1775 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:54:06]    INFO >> epoch 007:   1094 / 1539 loss=3.885, wps=4359.9, ups=6.29, wpb=693.4, bsz=693.4, num_updates=10300, lr=0.000295, gnorm=7.622, clip=0, train_wall=7, gb_free=71.6, wall=1783 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:54:15]    INFO >> epoch 007:   1144 / 1539 loss=3.676, wps=4445.9, ups=5.66, wpb=784.9, bsz=784.9, num_updates=10350, lr=0.000295, gnorm=7.624, clip=0, train_wall=8, gb_free=74, wall=1792 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:54:23]    INFO >> epoch 007:   1194 / 1539 loss=3.709, wps=4849.5, ups=5.98, wpb=811.5, bsz=811.5, num_updates=10400, lr=0.000295, gnorm=9.03, clip=0, train_wall=8, gb_free=72.8, wall=1801 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:54:31]    INFO >> epoch 007:   1244 / 1539 loss=3.759, wps=4533.1, ups=6.61, wpb=686.2, bsz=686.2, num_updates=10450, lr=0.000295, gnorm=8.265, clip=0, train_wall=7, gb_free=68.3, wall=1808 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:54:40]    INFO >> epoch 007:   1294 / 1539 loss=3.914, wps=4087.1, ups=6.38, wpb=640.4, bsz=640.4, num_updates=10500, lr=0.000295, gnorm=6.736, clip=0, train_wall=7, gb_free=70.7, wall=1816 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:54:48]    INFO >> epoch 007:   1344 / 1539 loss=3.921, wps=4566.3, ups=6.55, wpb=697.4, bsz=697.4, num_updates=10550, lr=0.000295, gnorm=7.42, clip=0, train_wall=7, gb_free=70.4, wall=1824 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:54:56]    INFO >> epoch 007:   1394 / 1539 loss=3.724, wps=4300.7, ups=6.15, wpb=699.1, bsz=699.1, num_updates=10600, lr=0.000295, gnorm=7.122, clip=0, train_wall=8, gb_free=67.1, wall=1832 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:55:01] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 51.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 76.76 GiB is allocated by PyTorch, and 1.81 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:55:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:55:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:55:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 29           |        cudaMalloc retries: 56        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78538 MiB |  78598 MiB | 341065 GiB | 340988 GiB |
|       from large pool |  78151 MiB |  78211 MiB | 339139 GiB | 339063 GiB |
|       from small pool |    386 MiB |    388 MiB |   1925 GiB |   1925 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78538 MiB |  78598 MiB | 341065 GiB | 340988 GiB |
|       from large pool |  78151 MiB |  78211 MiB | 339139 GiB | 339063 GiB |
|       from small pool |    386 MiB |    388 MiB |   1925 GiB |   1925 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78257 MiB |  78317 MiB | 340344 GiB | 340268 GiB |
|       from large pool |  77872 MiB |  77932 MiB | 338421 GiB | 338345 GiB |
|       from small pool |    384 MiB |    386 MiB |   1922 GiB |   1922 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80454 MiB |  80456 MiB | 750444 MiB | 669990 MiB |
|       from large pool |  80028 MiB |  80028 MiB | 745674 MiB | 665646 MiB |
|       from small pool |    426 MiB |    428 MiB |   4770 MiB |   4344 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1855 MiB |   6794 MiB | 329939 GiB | 329938 GiB |
|       from large pool |   1816 MiB |   6788 MiB | 327761 GiB | 327759 GiB |
|       from small pool |     39 MiB |     41 MiB |   2178 GiB |   2178 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7142    |    7145    |   22485 K  |   22478 K  |
|       from large pool |     919    |     920    |   10761 K  |   10760 K  |
|       from small pool |    6223    |    6226    |   11723 K  |   11717 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7142    |    7145    |   22485 K  |   22478 K  |
|       from large pool |     919    |     920    |   10761 K  |   10760 K  |
|       from small pool |    6223    |    6226    |   11723 K  |   11717 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     838    |     839    |    5272    |    4434    |
|       from large pool |     625    |     625    |    2887    |    2262    |
|       from small pool |     213    |     214    |    2385    |    2172    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     519    |     520    |   12654 K  |   12653 K  |
|       from large pool |     138    |     138    |    7032 K  |    7032 K  |
|       from small pool |     381    |     382    |    5621 K  |    5621 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:55:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:55:01] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:55:04]    INFO >> epoch 007:   1445 / 1539 loss=3.835, wps=4126.1, ups=5.76, wpb=716, bsz=716, num_updates=10650, lr=0.000295, gnorm=7.595, clip=0, train_wall=8, gb_free=68.1, wall=1840 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:55:13]    INFO >> epoch 007:   1495 / 1539 loss=3.854, wps=4735.2, ups=6.64, wpb=713.5, bsz=713.5, num_updates=10700, lr=0.000295, gnorm=8.718, clip=0, train_wall=7, gb_free=70.4, wall=1848 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:55:21]    INFO >> epoch 007 | loss 3.834 | wps 4139.7 | ups 5.81 | wpb 712.7 | bsz 712.7 | num_updates 10744 | lr 0.000295 | gnorm 7.681 | clip 0.4 | train_wall 231 | gb_free 70.2 | wall 1855 (progress_bar.py:267, print())[0m
[33m[2025-11-21 17:55:21] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 17:55:36]    INFO >> epoch 007 | valid on 'valid' subset | loss 3.863 | wps 10011.1 | wpb 5412.5 | bsz 5412.5 | num_updates 10744 | best_loss 5.157 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 17:55:36]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 17:55:36]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/checkpoints/checkpoint_last.pt (epoch 7 @ 10744 updates, score 3.863) (writing took 0.014131 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 17:55:36] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 17:55:37]    INFO >> epoch 008:      6 / 1539 loss=3.54, wps=1537.5, ups=2.11, wpb=730.3, bsz=730.3, num_updates=10750, lr=0.000262, gnorm=7.393, clip=2, train_wall=8, gb_free=68.7, wall=1872 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:55:46]    INFO >> epoch 008:     56 / 1539 loss=3.552, wps=4655.3, ups=6.33, wpb=735.8, bsz=735.8, num_updates=10800, lr=0.000262, gnorm=7.612, clip=2, train_wall=7, gb_free=73.9, wall=1880 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:55:55]    INFO >> epoch 008:    106 / 1539 loss=3.874, wps=4574.5, ups=5.97, wpb=766.8, bsz=766.8, num_updates=10850, lr=0.000262, gnorm=7.084, clip=0, train_wall=8, gb_free=73.2, wall=1888 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:56:02]    INFO >> epoch 008:    156 / 1539 loss=3.879, wps=4202.2, ups=6.44, wpb=652.3, bsz=652.3, num_updates=10900, lr=0.000262, gnorm=7.015, clip=0, train_wall=7, gb_free=75.6, wall=1896 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:56:11]    INFO >> epoch 008:    206 / 1539 loss=3.697, wps=4908.1, ups=5.88, wpb=834.7, bsz=834.7, num_updates=10950, lr=0.000262, gnorm=7.794, clip=0, train_wall=8, gb_free=73.5, wall=1904 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:56:20]    INFO >> epoch 008:    256 / 1539 loss=3.898, wps=4536.6, ups=6.35, wpb=714.1, bsz=714.1, num_updates=11000, lr=0.000262, gnorm=7.357, clip=0, train_wall=7, gb_free=71.5, wall=1912 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:56:28]    INFO >> epoch 008:    306 / 1539 loss=3.871, wps=4487.3, ups=6.23, wpb=720.8, bsz=720.8, num_updates=11050, lr=0.000262, gnorm=8.249, clip=0, train_wall=8, gb_free=71.8, wall=1920 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:56:37]    INFO >> epoch 008:    356 / 1539 loss=3.802, wps=3917.2, ups=5.88, wpb=665.6, bsz=665.6, num_updates=11100, lr=0.000262, gnorm=7.924, clip=0, train_wall=8, gb_free=74.7, wall=1929 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:56:45]    INFO >> epoch 008:    406 / 1539 loss=3.867, wps=4480.2, ups=6.34, wpb=707, bsz=707, num_updates=11150, lr=0.000262, gnorm=7.48, clip=0, train_wall=7, gb_free=72.1, wall=1936 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:56:54]    INFO >> epoch 008:    456 / 1539 loss=3.721, wps=5312.9, ups=6.07, wpb=875.6, bsz=875.6, num_updates=11200, lr=0.000262, gnorm=8.254, clip=0, train_wall=8, gb_free=72.6, wall=1945 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:57:02]    INFO >> epoch 008:    506 / 1539 loss=3.862, wps=4808.6, ups=6.41, wpb=749.6, bsz=749.6, num_updates=11250, lr=0.000262, gnorm=8.12, clip=0, train_wall=7, gb_free=74.3, wall=1953 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:57:10]    INFO >> epoch 008:    556 / 1539 loss=3.917, wps=4086.9, ups=6.28, wpb=651.2, bsz=651.2, num_updates=11300, lr=0.000262, gnorm=6.806, clip=0, train_wall=7, gb_free=71.8, wall=1960 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:57:18]    INFO >> epoch 008:    606 / 1539 loss=3.901, wps=4323.3, ups=6.33, wpb=682.8, bsz=682.8, num_updates=11350, lr=0.000262, gnorm=7.162, clip=0, train_wall=7, gb_free=75.2, wall=1968 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:57:26]    INFO >> epoch 008:    656 / 1539 loss=3.898, wps=4305.4, ups=6.77, wpb=636.2, bsz=636.2, num_updates=11400, lr=0.000262, gnorm=7.416, clip=0, train_wall=7, gb_free=72.5, wall=1976 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:57:34]    INFO >> epoch 008:    706 / 1539 loss=3.733, wps=4392.7, ups=6.35, wpb=691.9, bsz=691.9, num_updates=11450, lr=0.000262, gnorm=7.163, clip=0, train_wall=7, gb_free=70, wall=1984 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:57:42]    INFO >> epoch 008:    756 / 1539 loss=3.788, wps=4483.9, ups=6.17, wpb=727, bsz=727, num_updates=11500, lr=0.000262, gnorm=8.52, clip=0, train_wall=8, gb_free=70.9, wall=1992 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:57:50]    INFO >> epoch 008:    806 / 1539 loss=3.757, wps=4413.2, ups=6.36, wpb=694.2, bsz=694.2, num_updates=11550, lr=0.000262, gnorm=8.199, clip=0, train_wall=7, gb_free=68.9, wall=2000 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:57:59]    INFO >> epoch 008:    856 / 1539 loss=3.848, wps=4106.1, ups=6.89, wpb=595.7, bsz=595.7, num_updates=11600, lr=0.000262, gnorm=7.645, clip=0, train_wall=7, gb_free=71.5, wall=2007 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:58:07]    INFO >> epoch 008:    906 / 1539 loss=3.708, wps=4874.5, ups=5.99, wpb=813.3, bsz=813.3, num_updates=11650, lr=0.000262, gnorm=7.533, clip=0, train_wall=8, gb_free=74, wall=2015 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:58:15]    INFO >> epoch 008:    956 / 1539 loss=3.848, wps=4518.3, ups=6.7, wpb=674.1, bsz=674.1, num_updates=11700, lr=0.000262, gnorm=7.734, clip=0, train_wall=7, gb_free=72.9, wall=2023 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:58:20] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 33.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 2.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:58:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:58:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:58:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 30           |        cudaMalloc retries: 59        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78274 MiB |  78334 MiB | 377371 GiB | 377294 GiB |
|       from large pool |  78193 MiB |  78253 MiB | 375231 GiB | 375154 GiB |
|       from small pool |     80 MiB |     82 MiB |   2139 GiB |   2139 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78274 MiB |  78334 MiB | 377371 GiB | 377294 GiB |
|       from large pool |  78193 MiB |  78253 MiB | 375231 GiB | 375154 GiB |
|       from small pool |     80 MiB |     82 MiB |   2139 GiB |   2139 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78222 MiB |  78281 MiB | 376569 GiB | 376492 GiB |
|       from large pool |  78141 MiB |  78201 MiB | 374432 GiB | 374356 GiB |
|       from small pool |     80 MiB |     81 MiB |   2136 GiB |   2136 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80472 MiB |  80498 MiB |    830 GiB | 770116 MiB |
|       from large pool |  80388 MiB |  80388 MiB |    825 GiB | 765152 MiB |
|       from small pool |     84 MiB |    246 MiB |      4 GiB |   4964 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2137 MiB |   7144 MiB | 358016 GiB | 358014 GiB |
|       from large pool |   2134 MiB |   7133 MiB | 355596 GiB | 355594 GiB |
|       from small pool |      3 MiB |     33 MiB |   2419 GiB |   2419 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1796    |    1799    |   24927 K  |   24926 K  |
|       from large pool |     455    |     456    |   11890 K  |   11890 K  |
|       from small pool |    1341    |    1344    |   13037 K  |   13035 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1796    |    1799    |   24927 K  |   24926 K  |
|       from large pool |     455    |     456    |   11890 K  |   11890 K  |
|       from small pool |    1341    |    1344    |   13037 K  |   13035 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     241    |     562    |    5866    |    5625    |
|       from large pool |     199    |     439    |    3342    |    3143    |
|       from small pool |      42    |     123    |    2524    |    2482    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     160    |     161    |   14099 K  |   14099 K  |
|       from large pool |     113    |     113    |    7829 K  |    7829 K  |
|       from small pool |      47    |      65    |    6269 K  |    6269 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:58:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:58:20] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:58:23]    INFO >> epoch 008:   1007 / 1539 loss=3.963, wps=3513.7, ups=5.83, wpb=602.8, bsz=602.8, num_updates=11750, lr=0.000262, gnorm=6.75, clip=0, train_wall=7, gb_free=70.6, wall=2031 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:58:35]    INFO >> epoch 008:   1057 / 1539 loss=3.748, wps=3863.8, ups=4.66, wpb=828.4, bsz=828.4, num_updates=11800, lr=0.000262, gnorm=7.857, clip=2, train_wall=10, gb_free=71.7, wall=2042 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:58:43]    INFO >> epoch 008:   1107 / 1539 loss=3.791, wps=4331.3, ups=6.55, wpb=661.2, bsz=661.2, num_updates=11850, lr=0.000262, gnorm=7.313, clip=0, train_wall=7, gb_free=72.8, wall=2050 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:58:50]    INFO >> epoch 008:   1157 / 1539 loss=4.103, wps=3783.1, ups=6.79, wpb=557.5, bsz=557.5, num_updates=11900, lr=0.000262, gnorm=6.733, clip=0, train_wall=7, gb_free=73.2, wall=2057 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:58:53] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 521.25 MiB is free. Including non-PyTorch memory, this process has 78.61 GiB memory in use. Of the allocated memory 75.66 GiB is allocated by PyTorch, and 2.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:58:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:58:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:58:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 31           |        cudaMalloc retries: 61        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76954 MiB |  77477 MiB | 382821 GiB | 382746 GiB |
|       from large pool |  76941 MiB |  77464 MiB | 380654 GiB | 380579 GiB |
|       from small pool |     12 MiB |     19 MiB |   2167 GiB |   2167 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76954 MiB |  77477 MiB | 382821 GiB | 382746 GiB |
|       from large pool |  76941 MiB |  77464 MiB | 380654 GiB | 380579 GiB |
|       from small pool |     12 MiB |     19 MiB |   2167 GiB |   2167 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76936 MiB |  77458 MiB | 382008 GiB | 381933 GiB |
|       from large pool |  76923 MiB |  77445 MiB | 379844 GiB | 379769 GiB |
|       from small pool |     12 MiB |     19 MiB |   2164 GiB |   2164 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79984 MiB |  80024 MiB |    903 GiB |    825 GiB |
|       from large pool |  79958 MiB |  79958 MiB |    898 GiB |    820 GiB |
|       from small pool |     26 MiB |     66 MiB |      4 GiB |      4 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3029 MiB |   8676 MiB | 363948 GiB | 363945 GiB |
|       from large pool |   3016 MiB |   8662 MiB | 361497 GiB | 361494 GiB |
|       from small pool |     13 MiB |     27 MiB |   2451 GiB |   2451 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   25272 K  |   25272 K  |
|       from large pool |     315    |     322    |   12070 K  |   12070 K  |
|       from small pool |     291    |     348    |   13202 K  |   13201 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   25272 K  |   25272 K  |
|       from large pool |     315    |     322    |   12070 K  |   12070 K  |
|       from small pool |     291    |     348    |   13202 K  |   13201 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      91    |     111    |    5928    |    5837    |
|       from large pool |      78    |      78    |    3385    |    3307    |
|       from small pool |      13    |      33    |    2543    |    2530    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      92    |      92    |   14285 K  |   14285 K  |
|       from large pool |      67    |      67    |    7943 K  |    7943 K  |
|       from small pool |      25    |      56    |    6342 K  |    6342 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:58:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:58:53] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:58:59]    INFO >> epoch 008:   1208 / 1539 loss=3.802, wps=4041.7, ups=5.65, wpb=715, bsz=715, num_updates=11950, lr=0.000262, gnorm=7.1, clip=0, train_wall=8, gb_free=76, wall=2066 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:59:06] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 47.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 77.61 GiB is allocated by PyTorch, and 983.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:59:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:59:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:59:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 32           |        cudaMalloc retries: 62        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79414 MiB |  79474 MiB | 385104 GiB | 385026 GiB |
|       from large pool |  79015 MiB |  79075 MiB | 382922 GiB | 382845 GiB |
|       from small pool |    398 MiB |    399 MiB |   2181 GiB |   2181 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79414 MiB |  79474 MiB | 385104 GiB | 385026 GiB |
|       from large pool |  79015 MiB |  79075 MiB | 382922 GiB | 382845 GiB |
|       from small pool |    398 MiB |    399 MiB |   2181 GiB |   2181 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79398 MiB |  79458 MiB | 384287 GiB | 384209 GiB |
|       from large pool |  79002 MiB |  79061 MiB | 382108 GiB | 382031 GiB |
|       from small pool |    396 MiB |    397 MiB |   2178 GiB |   2178 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80458 MiB |  80458 MiB |    903 GiB |    825 GiB |
|       from large pool |  80018 MiB |  80018 MiB |    898 GiB |    820 GiB |
|       from small pool |    440 MiB |    440 MiB |      5 GiB |      4 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    983 MiB |   5675 MiB | 366585 GiB | 366584 GiB |
|       from large pool |    942 MiB |   5668 MiB | 364118 GiB | 364117 GiB |
|       from small pool |     41 MiB |     42 MiB |   2467 GiB |   2467 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7351    |    7354    |   25429 K  |   25421 K  |
|       from large pool |     938    |     939    |   12139 K  |   12138 K  |
|       from small pool |    6413    |    6416    |   13290 K  |   13283 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7351    |    7354    |   25429 K  |   25421 K  |
|       from large pool |     938    |     939    |   12139 K  |   12138 K  |
|       from small pool |    6413    |    6416    |   13290 K  |   13283 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     299    |     299    |    6136    |    5837    |
|       from large pool |      79    |      79    |    3386    |    3307    |
|       from small pool |     220    |     220    |    2750    |    2530    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     473    |     473    |   14372 K  |   14372 K  |
|       from large pool |      80    |      80    |    7985 K  |    7985 K  |
|       from small pool |     393    |     393    |    6387 K  |    6386 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:59:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:59:06] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:59:10]    INFO >> epoch 008:   1259 / 1539 loss=3.76, wps=4731.6, ups=5.56, wpb=850.7, bsz=850.7, num_updates=12000, lr=0.000262, gnorm=8.566, clip=4, train_wall=8, gb_free=73.2, wall=2075 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:59:17]    INFO >> epoch 008:   1309 / 1539 loss=3.781, wps=4568.6, ups=6.49, wpb=704.3, bsz=704.3, num_updates=12050, lr=0.000262, gnorm=8.331, clip=0, train_wall=7, gb_free=72.7, wall=2083 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:59:25]    INFO >> epoch 008:   1359 / 1539 loss=3.856, wps=4206.5, ups=6.33, wpb=664.7, bsz=664.7, num_updates=12100, lr=0.000262, gnorm=7.945, clip=0, train_wall=7, gb_free=69.7, wall=2090 (progress_bar.py:258, log())[0m
[33m[2025-11-21 17:59:28] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.20 GiB is free. Including non-PyTorch memory, this process has 77.92 GiB memory in use. Of the allocated memory 74.92 GiB is allocated by PyTorch, and 2.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 17:59:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:59:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:59:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 33           |        cudaMalloc retries: 64        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76324 MiB |  78599 MiB | 388761 GiB | 388687 GiB |
|       from large pool |  76305 MiB |  78581 MiB | 386561 GiB | 386487 GiB |
|       from small pool |     18 MiB |     19 MiB |   2200 GiB |   2200 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76324 MiB |  78599 MiB | 388761 GiB | 388687 GiB |
|       from large pool |  76305 MiB |  78581 MiB | 386561 GiB | 386487 GiB |
|       from small pool |     18 MiB |     19 MiB |   2200 GiB |   2200 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB | 387938 GiB | 387863 GiB |
|       from large pool |  76285 MiB |  78562 MiB | 385741 GiB | 385666 GiB |
|       from small pool |     18 MiB |     19 MiB |   2197 GiB |   2197 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79276 MiB |  80398 MiB |    974 GiB |    897 GiB |
|       from large pool |  79252 MiB |  79958 MiB |    969 GiB |    892 GiB |
|       from small pool |     24 MiB |    440 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2951 MiB |   4272 MiB | 370700 GiB | 370697 GiB |
|       from large pool |   2946 MiB |   4266 MiB | 368211 GiB | 368208 GiB |
|       from small pool |      5 MiB |     27 MiB |   2489 GiB |   2489 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     637    |     643    |   25655 K  |   25655 K  |
|       from large pool |     340    |     346    |   12256 K  |   12256 K  |
|       from small pool |     297    |     348    |   13399 K  |   13398 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     637    |     643    |   25655 K  |   25655 K  |
|       from large pool |     340    |     346    |   12256 K  |   12256 K  |
|       from small pool |     297    |     348    |   13399 K  |   13398 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     106    |     298    |    6194    |    6088    |
|       from large pool |      94    |      95    |    3444    |    3350    |
|       from small pool |      12    |     220    |    2750    |    2738    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     112    |     114    |   14496 K  |   14496 K  |
|       from large pool |      89    |      91    |    8059 K  |    8058 K  |
|       from small pool |      23    |      59    |    6437 K  |    6437 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:59:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 17:59:28] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 17:59:36]    INFO >> epoch 008:   1410 / 1539 loss=3.917, wps=3878.5, ups=4.7, wpb=824.4, bsz=824.4, num_updates=12150, lr=0.000262, gnorm=7.609, clip=0, train_wall=8, gb_free=73, wall=2101 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:59:46]    INFO >> epoch 008:   1460 / 1539 loss=3.828, wps=4050.3, ups=5.66, wpb=715.2, bsz=715.2, num_updates=12200, lr=0.000262, gnorm=7.636, clip=2, train_wall=8, gb_free=75, wall=2110 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:59:54]    INFO >> epoch 008:   1510 / 1539 loss=3.915, wps=4209.6, ups=6.32, wpb=666.4, bsz=666.4, num_updates=12250, lr=0.000262, gnorm=7.571, clip=0, train_wall=7, gb_free=67.3, wall=2118 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:59:58]    INFO >> epoch 008 | loss 3.821 | wps 4094.8 | ups 5.75 | wpb 712.7 | bsz 712.7 | num_updates 12279 | lr 0.000262 | gnorm 7.612 | clip 0.3 | train_wall 232 | gb_free 74.8 | wall 2122 (progress_bar.py:267, print())[0m
[33m[2025-11-21 17:59:58] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 18:00:14]    INFO >> epoch 008 | valid on 'valid' subset | loss 3.863 | wps 10351.9 | wpb 5412.5 | bsz 5412.5 | num_updates 12279 | best_loss 5.157 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 18:00:15]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 18:00:15]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/checkpoints/checkpoint_last.pt (epoch 8 @ 12279 updates, score 3.863) (writing took 0.015021 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 18:00:15] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 18:00:18]    INFO >> epoch 009:     21 / 1539 loss=3.845, wps=1472.4, ups=2.2, wpb=668.9, bsz=668.9, num_updates=12300, lr=0.000227, gnorm=7.065, clip=0, train_wall=7, gb_free=73.1, wall=2141 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:00:26]    INFO >> epoch 009:     71 / 1539 loss=3.968, wps=4047.9, ups=6.1, wpb=663.6, bsz=663.6, num_updates=12350, lr=0.000227, gnorm=6.233, clip=0, train_wall=8, gb_free=73.5, wall=2149 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:00:34]    INFO >> epoch 009:    121 / 1539 loss=3.663, wps=4449, ups=6.46, wpb=688.4, bsz=688.4, num_updates=12400, lr=0.000227, gnorm=7.919, clip=2, train_wall=7, gb_free=73.7, wall=2156 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:00:42]    INFO >> epoch 009:    171 / 1539 loss=3.38, wps=5096.7, ups=6.05, wpb=842.4, bsz=842.4, num_updates=12450, lr=0.000227, gnorm=8.349, clip=2, train_wall=8, gb_free=71.8, wall=2165 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:00:51]    INFO >> epoch 009:    221 / 1539 loss=3.967, wps=4607.5, ups=6.66, wpb=691.5, bsz=691.5, num_updates=12500, lr=0.000227, gnorm=6.859, clip=0, train_wall=7, gb_free=71.5, wall=2172 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:00:59]    INFO >> epoch 009:    271 / 1539 loss=3.814, wps=4661.9, ups=6.63, wpb=703.6, bsz=703.6, num_updates=12550, lr=0.000227, gnorm=7.225, clip=0, train_wall=7, gb_free=70, wall=2180 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:01:07]    INFO >> epoch 009:    321 / 1539 loss=3.652, wps=4730, ups=6.03, wpb=784.5, bsz=784.5, num_updates=12600, lr=0.000227, gnorm=8.41, clip=0, train_wall=8, gb_free=66.3, wall=2188 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:01:16]    INFO >> epoch 009:    371 / 1539 loss=3.906, wps=4003.1, ups=5.71, wpb=701, bsz=701, num_updates=12650, lr=0.000227, gnorm=8.06, clip=0, train_wall=8, gb_free=65.8, wall=2197 (progress_bar.py:258, log())[0m
[33m[2025-11-21 18:01:16] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 213.25 MiB is free. Including non-PyTorch memory, this process has 78.91 GiB memory in use. Of the allocated memory 73.80 GiB is allocated by PyTorch, and 4.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 18:01:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:01:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:01:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 34           |        cudaMalloc retries: 66        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75049 MiB |  75992 MiB | 409585 GiB | 409512 GiB |
|       from large pool |  75037 MiB |  75979 MiB | 407262 GiB | 407188 GiB |
|       from small pool |     12 MiB |     16 MiB |   2323 GiB |   2323 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75049 MiB |  75992 MiB | 409585 GiB | 409512 GiB |
|       from large pool |  75037 MiB |  75979 MiB | 407262 GiB | 407188 GiB |
|       from small pool |     12 MiB |     16 MiB |   2323 GiB |   2323 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75035 MiB |  75977 MiB | 408722 GiB | 408649 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 406402 GiB | 406329 GiB |
|       from small pool |     12 MiB |     16 MiB |   2319 GiB |   2319 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80292 MiB |  80292 MiB |    979 GiB |    901 GiB |
|       from large pool |  80266 MiB |  80266 MiB |    974 GiB |    895 GiB |
|       from small pool |     26 MiB |    124 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5242 MiB |   8856 MiB | 391458 GiB | 391453 GiB |
|       from large pool |   5228 MiB |   8843 MiB | 388834 GiB | 388829 GiB |
|       from small pool |     13 MiB |     21 MiB |   2624 GiB |   2624 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   26995 K  |   26995 K  |
|       from large pool |     314    |     322    |   12839 K  |   12839 K  |
|       from small pool |     291    |     341    |   14155 K  |   14155 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   26995 K  |   26995 K  |
|       from large pool |     314    |     322    |   12839 K  |   12839 K  |
|       from small pool |     291    |     341    |   14155 K  |   14155 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     103    |     156    |    6251    |    6148    |
|       from large pool |      90    |      94    |    3451    |    3361    |
|       from small pool |      13    |      62    |    2800    |    2787    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      98    |      98    |   15252 K  |   15252 K  |
|       from large pool |      72    |      72    |    8430 K  |    8430 K  |
|       from small pool |      26    |      43    |    6822 K  |    6821 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:01:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:01:16] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 18:01:26]    INFO >> epoch 009:    422 / 1539 loss=3.857, wps=4107.9, ups=5.73, wpb=716.4, bsz=716.4, num_updates=12700, lr=0.000227, gnorm=6.787, clip=0, train_wall=8, gb_free=73.1, wall=2206 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:01:34]    INFO >> epoch 009:    472 / 1539 loss=3.745, wps=4411.3, ups=6.38, wpb=691.3, bsz=691.3, num_updates=12750, lr=0.000227, gnorm=7.722, clip=2, train_wall=7, gb_free=74, wall=2213 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:01:41]    INFO >> epoch 009:    522 / 1539 loss=3.792, wps=4761, ups=6.26, wpb=760.8, bsz=760.8, num_updates=12800, lr=0.000227, gnorm=8.036, clip=0, train_wall=8, gb_free=72.7, wall=2221 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:01:50]    INFO >> epoch 009:    572 / 1539 loss=3.918, wps=4427.7, ups=5.9, wpb=750.9, bsz=750.9, num_updates=12850, lr=0.000227, gnorm=7.39, clip=0, train_wall=8, gb_free=74.2, wall=2230 (progress_bar.py:258, log())[0m
[33m[2025-11-21 18:01:51] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 25.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.35 GiB is allocated by PyTorch, and 1.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 18:01:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:01:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:01:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 35           |        cudaMalloc retries: 67        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79147 MiB |  79207 MiB | 415679 GiB | 415602 GiB |
|       from large pool |  79057 MiB |  79117 MiB | 413324 GiB | 413247 GiB |
|       from small pool |     89 MiB |     91 MiB |   2355 GiB |   2354 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79147 MiB |  79207 MiB | 415679 GiB | 415602 GiB |
|       from large pool |  79057 MiB |  79117 MiB | 413324 GiB | 413247 GiB |
|       from small pool |     89 MiB |     91 MiB |   2355 GiB |   2354 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79123 MiB |  79183 MiB | 414803 GiB | 414726 GiB |
|       from large pool |  79034 MiB |  79093 MiB | 412452 GiB | 412375 GiB |
|       from small pool |     89 MiB |     90 MiB |   2351 GiB |   2351 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80480 MiB |  80482 MiB |    979 GiB |    901 GiB |
|       from large pool |  80386 MiB |  80386 MiB |    974 GiB |    895 GiB |
|       from small pool |     94 MiB |     96 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1272 MiB |   7349 MiB | 398196 GiB | 398195 GiB |
|       from large pool |   1268 MiB |   7339 MiB | 395535 GiB | 395534 GiB |
|       from small pool |      4 MiB |     23 MiB |   2660 GiB |   2660 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1961    |    1964    |   27385 K  |   27383 K  |
|       from large pool |     470    |     471    |   13039 K  |   13038 K  |
|       from small pool |    1491    |    1494    |   14346 K  |   14345 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1961    |    1964    |   27385 K  |   27383 K  |
|       from large pool |     470    |     471    |   13039 K  |   13038 K  |
|       from small pool |    1491    |    1494    |   14346 K  |   14345 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     139    |     140    |    6288    |    6149    |
|       from large pool |      92    |      92    |    3453    |    3361    |
|       from small pool |      47    |      48    |    2835    |    2788    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     130    |     131    |   15463 K  |   15462 K  |
|       from large pool |      78    |      79    |    8556 K  |    8556 K  |
|       from small pool |      52    |      54    |    6906 K  |    6906 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:01:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:01:51] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 18:02:00]    INFO >> epoch 009:    623 / 1539 loss=3.816, wps=4070.7, ups=5.74, wpb=709.4, bsz=709.4, num_updates=12900, lr=0.000227, gnorm=7.662, clip=0, train_wall=7, gb_free=65.9, wall=2239 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:02:08]    INFO >> epoch 009:    673 / 1539 loss=3.753, wps=4270.2, ups=6.48, wpb=658.7, bsz=658.7, num_updates=12950, lr=0.000227, gnorm=7.829, clip=0, train_wall=7, gb_free=71.5, wall=2246 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:02:15]    INFO >> epoch 009:    723 / 1539 loss=3.888, wps=4349.6, ups=6.48, wpb=671.1, bsz=671.1, num_updates=13000, lr=0.000227, gnorm=7.26, clip=0, train_wall=7, gb_free=71.2, wall=2254 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:02:24]    INFO >> epoch 009:    773 / 1539 loss=3.825, wps=4209.8, ups=6.07, wpb=693.9, bsz=693.9, num_updates=13050, lr=0.000227, gnorm=7.368, clip=0, train_wall=8, gb_free=57.9, wall=2262 (progress_bar.py:258, log())[0m
[33m[2025-11-21 18:02:33] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 151.25 MiB is free. Including non-PyTorch memory, this process has 78.97 GiB memory in use. Of the allocated memory 77.59 GiB is allocated by PyTorch, and 897.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 18:02:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:02:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:02:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 36           |        cudaMalloc retries: 68        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79058 MiB |  79456 MiB | 422841 GiB | 422763 GiB |
|       from large pool |  79039 MiB |  79437 MiB | 420449 GiB | 420372 GiB |
|       from small pool |     18 MiB |     19 MiB |   2391 GiB |   2391 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79058 MiB |  79456 MiB | 422841 GiB | 422763 GiB |
|       from large pool |  79039 MiB |  79437 MiB | 420449 GiB | 420372 GiB |
|       from small pool |     18 MiB |     19 MiB |   2391 GiB |   2391 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79031 MiB |  79429 MiB | 421950 GiB | 421873 GiB |
|       from large pool |  79012 MiB |  79410 MiB | 419562 GiB | 419485 GiB |
|       from small pool |     18 MiB |     19 MiB |   2388 GiB |   2388 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80354 MiB |  80420 MiB |    979 GiB |    901 GiB |
|       from large pool |  80326 MiB |  80326 MiB |    974 GiB |    895 GiB |
|       from small pool |     28 MiB |     94 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1295 MiB |   5531 MiB | 406074 GiB | 406073 GiB |
|       from large pool |   1286 MiB |   5521 MiB | 403372 GiB | 403371 GiB |
|       from small pool |      9 MiB |     25 MiB |   2702 GiB |   2702 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     647    |   27845 K  |   27844 K  |
|       from large pool |     341    |     347    |   13278 K  |   13278 K  |
|       from small pool |     300    |     348    |   14567 K  |   14566 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     647    |   27845 K  |   27844 K  |
|       from large pool |     341    |     347    |   13278 K  |   13278 K  |
|       from small pool |     300    |     348    |   14567 K  |   14566 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     105    |     138    |    6288    |    6183    |
|       from large pool |      91    |      91    |    3453    |    3362    |
|       from small pool |      14    |      47    |    2835    |    2821    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      97    |     103    |   15710 K  |   15710 K  |
|       from large pool |      70    |      76    |    8707 K  |    8707 K  |
|       from small pool |      27    |      49    |    7003 K  |    7003 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:02:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:02:33] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 18:02:34]    INFO >> epoch 009:    824 / 1539 loss=3.79, wps=4156.5, ups=5.78, wpb=719.6, bsz=719.6, num_updates=13100, lr=0.000227, gnorm=7.636, clip=0, train_wall=7, gb_free=70.5, wall=2271 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:02:41]    INFO >> epoch 009:    874 / 1539 loss=3.846, wps=4327.1, ups=6.72, wpb=644.4, bsz=644.4, num_updates=13150, lr=0.000227, gnorm=7.136, clip=0, train_wall=7, gb_free=74.8, wall=2278 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:02:49]    INFO >> epoch 009:    924 / 1539 loss=3.977, wps=3987.3, ups=6.44, wpb=618.7, bsz=618.7, num_updates=13200, lr=0.000227, gnorm=6.799, clip=0, train_wall=7, gb_free=73.5, wall=2286 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:02:59]    INFO >> epoch 009:    974 / 1539 loss=3.668, wps=4336.9, ups=5.04, wpb=859.7, bsz=859.7, num_updates=13250, lr=0.000227, gnorm=7.968, clip=0, train_wall=9, gb_free=70.8, wall=2296 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:03:08]    INFO >> epoch 009:   1024 / 1539 loss=3.791, wps=4358.3, ups=6.06, wpb=718.9, bsz=718.9, num_updates=13300, lr=0.000227, gnorm=7.554, clip=0, train_wall=8, gb_free=69.9, wall=2304 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:03:16]    INFO >> epoch 009:   1074 / 1539 loss=3.708, wps=4529.4, ups=6.24, wpb=725.8, bsz=725.8, num_updates=13350, lr=0.000227, gnorm=7.531, clip=0, train_wall=8, gb_free=65.3, wall=2312 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:03:25]    INFO >> epoch 009:   1124 / 1539 loss=3.865, wps=4588.4, ups=6.03, wpb=760.3, bsz=760.3, num_updates=13400, lr=0.000227, gnorm=7.286, clip=2, train_wall=8, gb_free=73.5, wall=2321 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:03:32]    INFO >> epoch 009:   1174 / 1539 loss=3.909, wps=4486, ups=6.7, wpb=669.6, bsz=669.6, num_updates=13450, lr=0.000227, gnorm=6.505, clip=0, train_wall=7, gb_free=72.7, wall=2328 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:03:43]    INFO >> epoch 009:   1224 / 1539 loss=3.396, wps=4767.3, ups=5.53, wpb=862, bsz=862, num_updates=13500, lr=0.000227, gnorm=7.437, clip=2, train_wall=9, gb_free=69, wall=2337 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:03:51]    INFO >> epoch 009:   1274 / 1539 loss=3.547, wps=4810.2, ups=6.2, wpb=776.3, bsz=776.3, num_updates=13550, lr=0.000227, gnorm=8.814, clip=0, train_wall=8, gb_free=72.6, wall=2345 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:03:59]    INFO >> epoch 009:   1324 / 1539 loss=3.947, wps=4179.8, ups=5.87, wpb=712.6, bsz=712.6, num_updates=13600, lr=0.000227, gnorm=7.341, clip=0, train_wall=8, gb_free=75.1, wall=2354 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:04:09]    INFO >> epoch 009:   1374 / 1539 loss=3.368, wps=4558.1, ups=6.08, wpb=749.6, bsz=749.6, num_updates=13650, lr=0.000227, gnorm=7.801, clip=2, train_wall=8, gb_free=69.9, wall=2362 (progress_bar.py:258, log())[0m
[33m[2025-11-21 18:04:15] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 37.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 76.24 GiB is allocated by PyTorch, and 2.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 18:04:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:04:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:04:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 37           |        cudaMalloc retries: 70        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78007 MiB |  78067 MiB | 440587 GiB | 440511 GiB |
|       from large pool |  77625 MiB |  77685 MiB | 438093 GiB | 438017 GiB |
|       from small pool |    382 MiB |    383 MiB |   2494 GiB |   2493 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78007 MiB |  78067 MiB | 440587 GiB | 440511 GiB |
|       from large pool |  77625 MiB |  77685 MiB | 438093 GiB | 438017 GiB |
|       from small pool |    382 MiB |    383 MiB |   2494 GiB |   2493 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77777 MiB |  77836 MiB | 439658 GiB | 439582 GiB |
|       from large pool |  77397 MiB |  77456 MiB | 437167 GiB | 437092 GiB |
|       from small pool |    380 MiB |    381 MiB |   2490 GiB |   2490 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80468 MiB |  80468 MiB |   1006 GiB |    927 GiB |
|       from large pool |  80046 MiB |  80046 MiB |   1000 GiB |    922 GiB |
|       from small pool |    422 MiB |    422 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2400 MiB |   7496 MiB | 423368 GiB | 423366 GiB |
|       from large pool |   2360 MiB |   7489 MiB | 420548 GiB | 420546 GiB |
|       from small pool |     39 MiB |     40 MiB |   2820 GiB |   2820 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7054    |    7057    |   29063 K  |   29056 K  |
|       from large pool |     911    |     912    |   13873 K  |   13872 K  |
|       from small pool |    6143    |    6146    |   15190 K  |   15184 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7054    |    7057    |   29063 K  |   29056 K  |
|       from large pool |     911    |     912    |   13873 K  |   13872 K  |
|       from small pool |    6143    |    6146    |   15190 K  |   15184 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     742    |     742    |    6935    |    6193    |
|       from large pool |     531    |     531    |    3903    |    3372    |
|       from small pool |     211    |     211    |    3032    |    2821    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     503    |     503    |   16394 K  |   16393 K  |
|       from large pool |     127    |     127    |    9094 K  |    9094 K  |
|       from small pool |     376    |     376    |    7300 K  |    7299 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:04:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:04:15] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 18:04:17]    INFO >> epoch 009:   1425 / 1539 loss=3.781, wps=4200.3, ups=6.21, wpb=676.7, bsz=676.7, num_updates=13700, lr=0.000227, gnorm=7.352, clip=0, train_wall=7, gb_free=72, wall=2370 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:04:25]    INFO >> epoch 009:   1475 / 1539 loss=3.869, wps=4019.9, ups=6.34, wpb=634.1, bsz=634.1, num_updates=13750, lr=0.000227, gnorm=6.926, clip=0, train_wall=7, gb_free=72.9, wall=2378 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:04:32]    INFO >> epoch 009:   1525 / 1539 loss=3.919, wps=4155.3, ups=6.67, wpb=623.3, bsz=623.3, num_updates=13800, lr=0.000227, gnorm=6.775, clip=0, train_wall=7, gb_free=67.9, wall=2385 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:04:34]    INFO >> epoch 009 | loss 3.772 | wps 4124.7 | ups 5.79 | wpb 712.7 | bsz 712.7 | num_updates 13814 | lr 0.000227 | gnorm 7.448 | clip 0.4 | train_wall 232 | gb_free 74.2 | wall 2388 (progress_bar.py:267, print())[0m
[33m[2025-11-21 18:04:34] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 18:04:50]    INFO >> epoch 009 | valid on 'valid' subset | loss 3.857 | wps 10374.3 | wpb 5412.5 | bsz 5412.5 | num_updates 13814 | best_loss 5.157 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 18:04:51]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 18:04:51]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/checkpoints/checkpoint_last.pt (epoch 9 @ 13814 updates, score 3.857) (writing took 0.016086 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 18:04:51] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 18:04:58]    INFO >> epoch 010:     36 / 1539 loss=3.883, wps=1430.7, ups=2.04, wpb=701.4, bsz=701.4, num_updates=13850, lr=0.000193, gnorm=6.358, clip=0, train_wall=9, gb_free=72.8, wall=2410 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:05:06]    INFO >> epoch 010:     86 / 1539 loss=3.689, wps=4569, ups=6.37, wpb=716.9, bsz=716.9, num_updates=13900, lr=0.000193, gnorm=7.849, clip=0, train_wall=7, gb_free=69.6, wall=2418 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:05:15]    INFO >> epoch 010:    136 / 1539 loss=3.842, wps=4057.4, ups=6.48, wpb=626.5, bsz=626.5, num_updates=13950, lr=0.000193, gnorm=6.37, clip=0, train_wall=7, gb_free=74.1, wall=2425 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:05:24]    INFO >> epoch 010:    186 / 1539 loss=3.699, wps=4281.6, ups=5.75, wpb=744.2, bsz=744.2, num_updates=14000, lr=0.000193, gnorm=7.242, clip=0, train_wall=8, gb_free=74.2, wall=2434 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:05:32]    INFO >> epoch 010:    236 / 1539 loss=3.366, wps=4930.1, ups=5.71, wpb=863.9, bsz=863.9, num_updates=14050, lr=0.000193, gnorm=7.598, clip=0, train_wall=8, gb_free=74.4, wall=2443 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:05:40]    INFO >> epoch 010:    286 / 1539 loss=3.847, wps=4274.1, ups=6.32, wpb=676, bsz=676, num_updates=14100, lr=0.000193, gnorm=6.778, clip=0, train_wall=7, gb_free=73.9, wall=2451 (progress_bar.py:258, log())[0m
[33m[2025-11-21 18:05:47] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 953.25 MiB is free. Including non-PyTorch memory, this process has 78.19 GiB memory in use. Of the allocated memory 74.10 GiB is allocated by PyTorch, and 3.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 18:05:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:05:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:05:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 38           |        cudaMalloc retries: 72        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72064 MiB |  75873 MiB | 458286 GiB | 458216 GiB |
|       from large pool |  72046 MiB |  75855 MiB | 455686 GiB | 455615 GiB |
|       from small pool |     17 MiB |     18 MiB |   2600 GiB |   2600 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72064 MiB |  75873 MiB | 458286 GiB | 458216 GiB |
|       from large pool |  72046 MiB |  75855 MiB | 455686 GiB | 455615 GiB |
|       from small pool |     17 MiB |     18 MiB |   2600 GiB |   2600 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  75852 MiB | 457323 GiB | 457252 GiB |
|       from large pool |  72027 MiB |  75835 MiB | 454726 GiB | 454656 GiB |
|       from small pool |     17 MiB |     18 MiB |   2596 GiB |   2596 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79552 MiB |  79754 MiB |   1029 GiB |    952 GiB |
|       from large pool |  79528 MiB |  79528 MiB |   1023 GiB |    946 GiB |
|       from small pool |     24 MiB |    226 MiB |      6 GiB |      6 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4759 MiB |   7038 MiB | 438548 GiB | 438544 GiB |
|       from large pool |   4753 MiB |   7031 MiB | 435612 GiB | 435607 GiB |
|       from small pool |      6 MiB |     25 MiB |   2936 GiB |   2936 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     615    |   30202 K  |   30201 K  |
|       from large pool |     308    |     317    |   14352 K  |   14352 K  |
|       from small pool |     298    |     342    |   15849 K  |   15849 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     615    |   30202 K  |   30201 K  |
|       from large pool |     308    |     317    |   14352 K  |   14352 K  |
|       from small pool |     298    |     342    |   15849 K  |   15849 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     149    |     250    |    7045    |    6896    |
|       from large pool |     137    |     137    |    3912    |    3775    |
|       from small pool |      12    |     113    |    3133    |    3121    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     141    |     142    |   17052 K  |   17052 K  |
|       from large pool |     113    |     114    |    9412 K  |    9412 K  |
|       from small pool |      28    |      52    |    7640 K  |    7640 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:05:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:05:47] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 18:05:50]    INFO >> epoch 010:    337 / 1539 loss=3.842, wps=4294.2, ups=6.1, wpb=703.9, bsz=703.9, num_updates=14150, lr=0.000193, gnorm=6.948, clip=0, train_wall=7, gb_free=67.2, wall=2459 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:05:58]    INFO >> epoch 010:    387 / 1539 loss=3.704, wps=4365.6, ups=6.03, wpb=723.5, bsz=723.5, num_updates=14200, lr=0.000193, gnorm=7.981, clip=0, train_wall=8, gb_free=73.3, wall=2467 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:06:06]    INFO >> epoch 010:    437 / 1539 loss=3.87, wps=4107.5, ups=6.51, wpb=631.4, bsz=631.4, num_updates=14250, lr=0.000193, gnorm=6.371, clip=0, train_wall=7, gb_free=72.6, wall=2475 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:06:14]    INFO >> epoch 010:    487 / 1539 loss=3.8, wps=4048.1, ups=6.06, wpb=668.5, bsz=668.5, num_updates=14300, lr=0.000193, gnorm=7.022, clip=0, train_wall=8, gb_free=72.5, wall=2483 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:06:23]    INFO >> epoch 010:    537 / 1539 loss=3.674, wps=4396.6, ups=6.81, wpb=645.7, bsz=645.7, num_updates=14350, lr=0.000193, gnorm=7.058, clip=0, train_wall=7, gb_free=75, wall=2491 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:06:31]    INFO >> epoch 010:    587 / 1539 loss=3.854, wps=4097.9, ups=6.16, wpb=665.2, bsz=665.2, num_updates=14400, lr=0.000193, gnorm=7.648, clip=0, train_wall=8, gb_free=71.4, wall=2499 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:06:39]    INFO >> epoch 010:    637 / 1539 loss=3.802, wps=4660, ups=6.47, wpb=719.7, bsz=719.7, num_updates=14450, lr=0.000193, gnorm=7.265, clip=0, train_wall=7, gb_free=69.1, wall=2506 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:06:46]    INFO >> epoch 010:    687 / 1539 loss=3.766, wps=4631.2, ups=6.37, wpb=726.6, bsz=726.6, num_updates=14500, lr=0.000193, gnorm=7.041, clip=0, train_wall=7, gb_free=73.3, wall=2514 (progress_bar.py:258, log())[0m
[33m[2025-11-21 18:06:50] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 41.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 76.29 GiB is allocated by PyTorch, and 2.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 18:06:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:06:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:06:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 39           |        cudaMalloc retries: 73        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78063 MiB |  78123 MiB | 469353 GiB | 469277 GiB |
|       from large pool |  77678 MiB |  77738 MiB | 466692 GiB | 466616 GiB |
|       from small pool |    384 MiB |    385 MiB |   2661 GiB |   2660 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78063 MiB |  78123 MiB | 469353 GiB | 469277 GiB |
|       from large pool |  77678 MiB |  77738 MiB | 466692 GiB | 466616 GiB |
|       from small pool |    384 MiB |    385 MiB |   2661 GiB |   2660 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78017 MiB |  78077 MiB | 468366 GiB | 468290 GiB |
|       from large pool |  77635 MiB |  77694 MiB | 465709 GiB | 465633 GiB |
|       from small pool |    382 MiB |    383 MiB |   2657 GiB |   2656 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80464 MiB |  80464 MiB |   1033 GiB |    954 GiB |
|       from large pool |  80040 MiB |  80040 MiB |   1027 GiB |    948 GiB |
|       from small pool |    424 MiB |    424 MiB |      6 GiB |      6 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2340 MiB |   6606 MiB | 449445 GiB | 449443 GiB |
|       from large pool |   2301 MiB |   6599 MiB | 446439 GiB | 446437 GiB |
|       from small pool |     39 MiB |     40 MiB |   3006 GiB |   3006 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7098    |    7101    |   30947 K  |   30940 K  |
|       from large pool |     915    |     916    |   14731 K  |   14730 K  |
|       from small pool |    6183    |    6186    |   16216 K  |   16209 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7098    |    7101    |   30947 K  |   30940 K  |
|       from large pool |     915    |     916    |   14731 K  |   14730 K  |
|       from small pool |    6183    |    6186    |   16216 K  |   16209 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     402    |     402    |    7299    |    6897    |
|       from large pool |     190    |     190    |    3966    |    3776    |
|       from small pool |     212    |     212    |    3333    |    3121    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     495    |     496    |   17467 K  |   17466 K  |
|       from large pool |     118    |     118    |    9660 K  |    9660 K  |
|       from small pool |     377    |     378    |    7806 K  |    7805 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:06:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:06:50] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 18:06:54]    INFO >> epoch 010:    738 / 1539 loss=4.016, wps=4033, ups=6.27, wpb=642.7, bsz=642.7, num_updates=14550, lr=0.000193, gnorm=6.394, clip=0, train_wall=7, gb_free=68.7, wall=2522 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:07:04]    INFO >> epoch 010:    788 / 1539 loss=3.788, wps=4423.8, ups=6.12, wpb=723.3, bsz=723.3, num_updates=14600, lr=0.000193, gnorm=7.196, clip=0, train_wall=8, gb_free=67.8, wall=2530 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:07:12]    INFO >> epoch 010:    838 / 1539 loss=3.754, wps=4220.3, ups=6.26, wpb=673.9, bsz=673.9, num_updates=14650, lr=0.000193, gnorm=6.796, clip=0, train_wall=7, gb_free=66.8, wall=2538 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:07:21]    INFO >> epoch 010:    888 / 1539 loss=3.418, wps=4894.2, ups=5.77, wpb=848.7, bsz=848.7, num_updates=14700, lr=0.000193, gnorm=7.333, clip=2, train_wall=8, gb_free=76.2, wall=2547 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:07:28]    INFO >> epoch 010:    938 / 1539 loss=3.74, wps=4668, ups=6.54, wpb=713.5, bsz=713.5, num_updates=14750, lr=0.000193, gnorm=8.009, clip=2, train_wall=7, gb_free=74.7, wall=2555 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:07:37]    INFO >> epoch 010:    988 / 1539 loss=3.903, wps=4641.6, ups=6.72, wpb=690.7, bsz=690.7, num_updates=14800, lr=0.000193, gnorm=7.373, clip=0, train_wall=7, gb_free=74.4, wall=2562 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:07:45]    INFO >> epoch 010:   1038 / 1539 loss=3.851, wps=4395.9, ups=6.54, wpb=671.7, bsz=671.7, num_updates=14850, lr=0.000193, gnorm=7.53, clip=0, train_wall=7, gb_free=70, wall=2570 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:07:53]    INFO >> epoch 010:   1088 / 1539 loss=3.711, wps=4950.5, ups=6.16, wpb=803.8, bsz=803.8, num_updates=14900, lr=0.000193, gnorm=7.161, clip=0, train_wall=8, gb_free=72.8, wall=2578 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:08:00]    INFO >> epoch 010:   1138 / 1539 loss=3.804, wps=4070.6, ups=6.64, wpb=612.7, bsz=612.7, num_updates=14950, lr=0.000193, gnorm=6.552, clip=0, train_wall=7, gb_free=71.8, wall=2585 (progress_bar.py:258, log())[0m
[33m[2025-11-21 18:08:09] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 17.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 76.47 GiB is allocated by PyTorch, and 2.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 18:08:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:08:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:08:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 40           |        cudaMalloc retries: 75        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78250 MiB |  78310 MiB | 483183 GiB | 483107 GiB |
|       from large pool |  78169 MiB |  78229 MiB | 480446 GiB | 480370 GiB |
|       from small pool |     80 MiB |     82 MiB |   2736 GiB |   2736 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78250 MiB |  78310 MiB | 483183 GiB | 483107 GiB |
|       from large pool |  78169 MiB |  78229 MiB | 480446 GiB | 480370 GiB |
|       from small pool |     80 MiB |     82 MiB |   2736 GiB |   2736 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78222 MiB |  78281 MiB | 482164 GiB | 482088 GiB |
|       from large pool |  78141 MiB |  78201 MiB | 479431 GiB | 479355 GiB |
|       from small pool |     80 MiB |     81 MiB |   2732 GiB |   2732 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80488 MiB |  80490 MiB |   1034 GiB |    955 GiB |
|       from large pool |  80404 MiB |  80404 MiB |   1028 GiB |    949 GiB |
|       from small pool |     84 MiB |    424 MiB |      6 GiB |      6 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2177 MiB |   9886 MiB | 462297 GiB | 462295 GiB |
|       from large pool |   2174 MiB |   9877 MiB | 459203 GiB | 459201 GiB |
|       from small pool |      3 MiB |     21 MiB |   3093 GiB |   3093 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1796    |    1799    |   31868 K  |   31866 K  |
|       from large pool |     455    |     456    |   15195 K  |   15195 K  |
|       from small pool |    1341    |    1344    |   16672 K  |   16671 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1796    |    1799    |   31868 K  |   31866 K  |
|       from large pool |     455    |     456    |   15195 K  |   15195 K  |
|       from small pool |    1341    |    1344    |   16672 K  |   16671 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     224    |     401    |    7332    |    7108    |
|       from large pool |     182    |     189    |    3969    |    3787    |
|       from small pool |      42    |     212    |    3363    |    3321    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     187    |     188    |   17986 K  |   17986 K  |
|       from large pool |     140    |     144    |    9970 K  |    9969 K  |
|       from small pool |      47    |      52    |    8016 K  |    8016 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:08:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:08:09] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 18:08:11]    INFO >> epoch 010:   1189 / 1539 loss=3.692, wps=4353.4, ups=5.53, wpb=787.9, bsz=787.9, num_updates=15000, lr=0.000193, gnorm=7.125, clip=0, train_wall=8, gb_free=70.1, wall=2594 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:08:19]    INFO >> epoch 010:   1239 / 1539 loss=3.666, wps=4756.5, ups=6.05, wpb=786.3, bsz=786.3, num_updates=15050, lr=0.000193, gnorm=7.545, clip=0, train_wall=8, gb_free=73.2, wall=2603 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:08:28]    INFO >> epoch 010:   1289 / 1539 loss=3.544, wps=4467.1, ups=5.83, wpb=766.5, bsz=766.5, num_updates=15100, lr=0.000193, gnorm=7.666, clip=0, train_wall=8, gb_free=72.7, wall=2611 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:08:35]    INFO >> epoch 010:   1339 / 1539 loss=3.816, wps=4360.3, ups=6.45, wpb=675.5, bsz=675.5, num_updates=15150, lr=0.000193, gnorm=7.726, clip=0, train_wall=7, gb_free=74.4, wall=2619 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:08:43]    INFO >> epoch 010:   1389 / 1539 loss=3.728, wps=4726.3, ups=6.33, wpb=746.1, bsz=746.1, num_updates=15200, lr=0.000193, gnorm=7.824, clip=0, train_wall=7, gb_free=73.6, wall=2627 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:08:51]    INFO >> epoch 010:   1439 / 1539 loss=3.721, wps=4310.9, ups=6.34, wpb=679.6, bsz=679.6, num_updates=15250, lr=0.000193, gnorm=7.65, clip=0, train_wall=7, gb_free=70.1, wall=2635 (progress_bar.py:258, log())[0m
[33m[2025-11-21 18:08:57] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 251.25 MiB is free. Including non-PyTorch memory, this process has 78.87 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 4.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 18:08:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:08:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:08:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 41           |        cudaMalloc retries: 78        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72822 MiB |  75426 MiB | 491921 GiB | 491850 GiB |
|       from large pool |  72809 MiB |  75413 MiB | 489137 GiB | 489066 GiB |
|       from small pool |     12 MiB |     24 MiB |   2784 GiB |   2784 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72822 MiB |  75426 MiB | 491921 GiB | 491850 GiB |
|       from large pool |  72809 MiB |  75413 MiB | 489137 GiB | 489066 GiB |
|       from small pool |     12 MiB |     24 MiB |   2784 GiB |   2784 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72801 MiB |  75403 MiB | 490883 GiB | 490812 GiB |
|       from large pool |  72788 MiB |  75391 MiB | 488103 GiB | 488032 GiB |
|       from small pool |     12 MiB |     24 MiB |   2780 GiB |   2780 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80254 MiB |  80254 MiB |   1073 GiB |    995 GiB |
|       from large pool |  80228 MiB |  80228 MiB |   1067 GiB |    988 GiB |
|       from small pool |     26 MiB |    210 MiB |      6 GiB |      6 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5153 MiB |   9503 MiB | 470004 GiB | 469999 GiB |
|       from large pool |   5140 MiB |   9488 MiB | 466856 GiB | 466851 GiB |
|       from small pool |     13 MiB |     33 MiB |   3148 GiB |   3148 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     603    |     612    |   32444 K  |   32444 K  |
|       from large pool |     312    |     320    |   15487 K  |   15486 K  |
|       from small pool |     291    |     356    |   16957 K  |   16957 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     603    |     612    |   32444 K  |   32444 K  |
|       from large pool |     312    |     320    |   15487 K  |   15486 K  |
|       from small pool |     291    |     356    |   16957 K  |   16957 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     185    |     325    |    7504    |    7319    |
|       from large pool |     172    |     220    |    4078    |    3906    |
|       from small pool |      13    |     105    |    3426    |    3413    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     168    |     169    |   18312 K  |   18312 K  |
|       from large pool |     142    |     143    |   10166 K  |   10165 K  |
|       from small pool |      26    |      63    |    8146 K  |    8146 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:08:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:08:57] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 18:09:00]    INFO >> epoch 010:   1490 / 1539 loss=3.891, wps=4105.7, ups=5.91, wpb=694.7, bsz=694.7, num_updates=15300, lr=0.000193, gnorm=7.418, clip=0, train_wall=7, gb_free=64.7, wall=2643 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:09:07]    INFO >> epoch 010 | loss 3.749 | wps 4150.8 | ups 5.82 | wpb 712.7 | bsz 712.7 | num_updates 15349 | lr 0.000193 | gnorm 7.221 | clip 0.1 | train_wall 231 | gb_free 70.8 | wall 2651 (progress_bar.py:267, print())[0m
[33m[2025-11-21 18:09:07] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 18:09:22]    INFO >> epoch 010 | valid on 'valid' subset | loss 3.849 | wps 10579 | wpb 5412.5 | bsz 5412.5 | num_updates 15349 | best_loss 5.157 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 18:09:22]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 18:09:22]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/checkpoints/checkpoint_last.pt (epoch 10 @ 15349 updates, score 3.849) (writing took 0.018694 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 18:09:22] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 18:09:22]    INFO >> epoch 011:      1 / 1539 loss=3.857, wps=1605.4, ups=2.2, wpb=728.6, bsz=728.6, num_updates=15350, lr=0.000161, gnorm=6.806, clip=0, train_wall=8, gb_free=68.7, wall=2666 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:09:30]    INFO >> epoch 011:     51 / 1539 loss=3.93, wps=4270.6, ups=6.6, wpb=647.3, bsz=647.3, num_updates=15400, lr=0.000161, gnorm=6.764, clip=0, train_wall=7, gb_free=75, wall=2674 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:09:38]    INFO >> epoch 011:    101 / 1539 loss=3.805, wps=4466.3, ups=6.12, wpb=729.7, bsz=729.7, num_updates=15450, lr=0.000161, gnorm=7.192, clip=0, train_wall=8, gb_free=74.4, wall=2682 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:09:45]    INFO >> epoch 011:    151 / 1539 loss=3.891, wps=4530.9, ups=6.8, wpb=666, bsz=666, num_updates=15500, lr=0.000161, gnorm=7.221, clip=0, train_wall=7, gb_free=71.4, wall=2689 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:09:53]    INFO >> epoch 011:    201 / 1539 loss=3.646, wps=4793.6, ups=6.23, wpb=769.4, bsz=769.4, num_updates=15550, lr=0.000161, gnorm=7.382, clip=2, train_wall=8, gb_free=71.9, wall=2697 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:10:01]    INFO >> epoch 011:    251 / 1539 loss=3.656, wps=4752.1, ups=6.51, wpb=729.4, bsz=729.4, num_updates=15600, lr=0.000161, gnorm=8.145, clip=2, train_wall=7, gb_free=73.3, wall=2705 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:10:09]    INFO >> epoch 011:    301 / 1539 loss=3.788, wps=3979.4, ups=6.38, wpb=623.3, bsz=623.3, num_updates=15650, lr=0.000161, gnorm=7.074, clip=0, train_wall=7, gb_free=74.6, wall=2713 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:10:22]    INFO >> epoch 011:    351 / 1539 loss=3.627, wps=4688.7, ups=6.24, wpb=751.3, bsz=751.3, num_updates=15700, lr=0.000161, gnorm=7.97, clip=0, train_wall=8, gb_free=72, wall=2721 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:10:30]    INFO >> epoch 011:    401 / 1539 loss=3.894, wps=4526.6, ups=6.68, wpb=677.5, bsz=677.5, num_updates=15750, lr=0.000161, gnorm=7.045, clip=2, train_wall=7, gb_free=72.3, wall=2728 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:10:37]    INFO >> epoch 011:    451 / 1539 loss=3.988, wps=4091.8, ups=6.56, wpb=624.1, bsz=624.1, num_updates=15800, lr=0.000161, gnorm=6.21, clip=0, train_wall=7, gb_free=72.8, wall=2736 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:10:46]    INFO >> epoch 011:    501 / 1539 loss=3.996, wps=3555.5, ups=5.86, wpb=606.7, bsz=606.7, num_updates=15850, lr=0.000161, gnorm=6.313, clip=0, train_wall=8, gb_free=75, wall=2744 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:10:55]    INFO >> epoch 011:    551 / 1539 loss=3.894, wps=4254.3, ups=6.59, wpb=645.8, bsz=645.8, num_updates=15900, lr=0.000161, gnorm=7.09, clip=0, train_wall=7, gb_free=73.9, wall=2752 (progress_bar.py:258, log())[0m
[33m[2025-11-21 18:10:56] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 31.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.39 GiB is allocated by PyTorch, and 2.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 18:10:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:10:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:10:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 42           |        cudaMalloc retries: 79        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78168 MiB |  78228 MiB | 514019 GiB | 513943 GiB |
|       from large pool |  77782 MiB |  77842 MiB | 511102 GiB | 511026 GiB |
|       from small pool |    385 MiB |    386 MiB |   2916 GiB |   2916 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78168 MiB |  78228 MiB | 514019 GiB | 513943 GiB |
|       from large pool |  77782 MiB |  77842 MiB | 511102 GiB | 511026 GiB |
|       from small pool |    385 MiB |    386 MiB |   2916 GiB |   2916 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78137 MiB |  78197 MiB | 512935 GiB | 512858 GiB |
|       from large pool |  77753 MiB |  77813 MiB | 510022 GiB | 509946 GiB |
|       from small pool |    383 MiB |    384 MiB |   2912 GiB |   2912 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80474 MiB |  80476 MiB |   1076 GiB |    997 GiB |
|       from large pool |  80050 MiB |  80050 MiB |   1069 GiB |    990 GiB |
|       from small pool |    424 MiB |    426 MiB |      7 GiB |      6 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2245 MiB |   6868 MiB | 489053 GiB | 489051 GiB |
|       from large pool |   2207 MiB |   6862 MiB | 485758 GiB | 485756 GiB |
|       from small pool |     38 MiB |     40 MiB |   3295 GiB |   3295 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7120    |    7123    |   33914 K  |   33907 K  |
|       from large pool |     917    |     918    |   16134 K  |   16134 K  |
|       from small pool |    6203    |    6206    |   17779 K  |   17773 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7120    |    7123    |   33914 K  |   33907 K  |
|       from large pool |     917    |     918    |   16134 K  |   16134 K  |
|       from small pool |    6203    |    6206    |   17779 K  |   17773 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     417    |     418    |    7739    |    7322    |
|       from large pool |     205    |     205    |    4113    |    3908    |
|       from small pool |     212    |     213    |    3626    |    3414    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     548    |     550    |   19159 K  |   19159 K  |
|       from large pool |     171    |     171    |   10604 K  |   10604 K  |
|       from small pool |     377    |     379    |    8555 K  |    8554 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:10:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:10:56] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 18:11:04]    INFO >> epoch 011:    602 / 1539 loss=3.712, wps=3985.8, ups=5.74, wpb=694.1, bsz=694.1, num_updates=15950, lr=0.000161, gnorm=7.684, clip=0, train_wall=8, gb_free=71.6, wall=2761 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:11:12]    INFO >> epoch 011:    652 / 1539 loss=3.748, wps=4845.9, ups=5.94, wpb=815.3, bsz=815.3, num_updates=16000, lr=0.000161, gnorm=7.325, clip=0, train_wall=8, gb_free=71.5, wall=2769 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:11:20]    INFO >> epoch 011:    702 / 1539 loss=3.84, wps=4263.7, ups=6.45, wpb=661.1, bsz=661.1, num_updates=16050, lr=0.000161, gnorm=7.343, clip=0, train_wall=7, gb_free=71, wall=2777 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:11:30]    INFO >> epoch 011:    752 / 1539 loss=3.724, wps=4214.1, ups=5.76, wpb=731.8, bsz=731.8, num_updates=16100, lr=0.000161, gnorm=7.172, clip=0, train_wall=8, gb_free=72.6, wall=2785 (progress_bar.py:258, log())[0m
[33m[2025-11-21 18:11:36] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 457.25 MiB is free. Including non-PyTorch memory, this process has 78.67 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 4.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 18:11:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:11:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:11:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 43           |        cudaMalloc retries: 81        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72821 MiB |  75424 MiB | 521026 GiB | 520955 GiB |
|       from large pool |  72808 MiB |  75411 MiB | 518072 GiB | 518001 GiB |
|       from small pool |     12 MiB |     21 MiB |   2953 GiB |   2953 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72821 MiB |  75424 MiB | 521026 GiB | 520955 GiB |
|       from large pool |  72808 MiB |  75411 MiB | 518072 GiB | 518001 GiB |
|       from small pool |     12 MiB |     21 MiB |   2953 GiB |   2953 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72801 MiB |  75403 MiB | 519925 GiB | 519854 GiB |
|       from large pool |  72788 MiB |  75391 MiB | 516976 GiB | 516905 GiB |
|       from small pool |     12 MiB |     21 MiB |   2949 GiB |   2949 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80048 MiB |  80424 MiB |   1078 GiB |   1000 GiB |
|       from large pool |  80024 MiB |  80400 MiB |   1071 GiB |    993 GiB |
|       from small pool |     24 MiB |    424 MiB |      7 GiB |      7 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   7226 MiB |  10655 MiB | 495422 GiB | 495415 GiB |
|       from large pool |   7215 MiB |  10644 MiB | 492083 GiB | 492076 GiB |
|       from small pool |     11 MiB |     27 MiB |   3338 GiB |   3338 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     603    |     612    |   34373 K  |   34372 K  |
|       from large pool |     312    |     320    |   16366 K  |   16366 K  |
|       from small pool |     291    |     356    |   18006 K  |   18006 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     603    |     612    |   34373 K  |   34372 K  |
|       from large pool |     312    |     320    |   16366 K  |   16366 K  |
|       from small pool |     291    |     356    |   18006 K  |   18006 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     182    |     416    |    7742    |    7560    |
|       from large pool |     170    |     204    |    4116    |    3946    |
|       from small pool |      12    |     212    |    3626    |    3614    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     175    |     176    |   19423 K  |   19423 K  |
|       from large pool |     147    |     148    |   10761 K  |   10761 K  |
|       from small pool |      28    |      63    |    8662 K  |    8662 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:11:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:11:36] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 18:11:38]    INFO >> epoch 011:    803 / 1539 loss=3.878, wps=3528.6, ups=6.02, wpb=586.3, bsz=586.3, num_updates=16150, lr=0.000161, gnorm=6.989, clip=0, train_wall=7, gb_free=62.8, wall=2794 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:11:46]    INFO >> epoch 011:    853 / 1539 loss=3.907, wps=4283.7, ups=6.52, wpb=657, bsz=657, num_updates=16200, lr=0.000161, gnorm=6.748, clip=0, train_wall=7, gb_free=74.8, wall=2801 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:11:53]    INFO >> epoch 011:    903 / 1539 loss=3.67, wps=5093.4, ups=6.53, wpb=779.8, bsz=779.8, num_updates=16250, lr=0.000161, gnorm=7.376, clip=0, train_wall=7, gb_free=72.7, wall=2809 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:12:03]    INFO >> epoch 011:    953 / 1539 loss=3.776, wps=4674.3, ups=6.27, wpb=746, bsz=746, num_updates=16300, lr=0.000161, gnorm=7.117, clip=0, train_wall=8, gb_free=72, wall=2817 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:12:11]    INFO >> epoch 011:   1003 / 1539 loss=3.778, wps=4601.3, ups=6.24, wpb=737.8, bsz=737.8, num_updates=16350, lr=0.000161, gnorm=7.759, clip=0, train_wall=8, gb_free=71.3, wall=2825 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:12:19]    INFO >> epoch 011:   1053 / 1539 loss=3.705, wps=4339.7, ups=6.26, wpb=692.9, bsz=692.9, num_updates=16400, lr=0.000161, gnorm=6.774, clip=0, train_wall=8, gb_free=73.7, wall=2833 (progress_bar.py:258, log())[0m
[33m[2025-11-21 18:12:24] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 2 has a total capacity of 79.14 GiB of which 457.25 MiB is free. Including non-PyTorch memory, this process has 78.67 GiB memory in use. Of the allocated memory 70.08 GiB is allocated by PyTorch, and 8.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 18:12:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:12:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:12:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 44           |        cudaMalloc retries: 82        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69006 MiB |  71763 MiB | 529586 GiB | 529518 GiB |
|       from large pool |  68989 MiB |  71746 MiB | 526586 GiB | 526518 GiB |
|       from small pool |     16 MiB |     17 MiB |   2999 GiB |   2999 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69006 MiB |  71763 MiB | 529586 GiB | 529518 GiB |
|       from large pool |  68989 MiB |  71746 MiB | 526586 GiB | 526518 GiB |
|       from small pool |     16 MiB |     17 MiB |   2999 GiB |   2999 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  68980 MiB |  71736 MiB | 528466 GiB | 528399 GiB |
|       from large pool |  68963 MiB |  71719 MiB | 525471 GiB | 525403 GiB |
|       from small pool |     16 MiB |     17 MiB |   2995 GiB |   2995 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80048 MiB |  80148 MiB |   1079 GiB |   1000 GiB |
|       from large pool |  80024 MiB |  80024 MiB |   1071 GiB |    993 GiB |
|       from small pool |     24 MiB |    124 MiB |      7 GiB |      7 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   8407 MiB |  12173 MiB | 503612 GiB | 503604 GiB |
|       from large pool |   8400 MiB |  12165 MiB | 500221 GiB | 500213 GiB |
|       from small pool |      7 MiB |     23 MiB |   3391 GiB |   3391 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     649    |   34930 K  |   34930 K  |
|       from large pool |     343    |     351    |   16651 K  |   16650 K  |
|       from small pool |     298    |     342    |   18279 K  |   18279 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     649    |   34930 K  |   34930 K  |
|       from large pool |     343    |     351    |   16651 K  |   16650 K  |
|       from small pool |     298    |     342    |   18279 K  |   18279 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     182    |     232    |    7792    |    7610    |
|       from large pool |     170    |     170    |    4116    |    3946    |
|       from small pool |      12    |      62    |    3676    |    3664    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     164    |     165    |   19736 K  |   19736 K  |
|       from large pool |     137    |     138    |   10951 K  |   10951 K  |
|       from small pool |      27    |      55    |    8785 K  |    8785 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:12:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:12:24] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 18:12:28]    INFO >> epoch 011:   1104 / 1539 loss=3.737, wps=4239.3, ups=5.55, wpb=764, bsz=764, num_updates=16450, lr=0.000161, gnorm=6.528, clip=0, train_wall=8, gb_free=75, wall=2842 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:12:37]    INFO >> epoch 011:   1154 / 1539 loss=3.538, wps=5394.4, ups=5.93, wpb=909, bsz=909, num_updates=16500, lr=0.000161, gnorm=7.917, clip=2, train_wall=8, gb_free=69.3, wall=2850 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:12:46]    INFO >> epoch 011:   1204 / 1539 loss=3.819, wps=3789.4, ups=6.23, wpb=608.2, bsz=608.2, num_updates=16550, lr=0.000161, gnorm=7.073, clip=0, train_wall=8, gb_free=68.8, wall=2858 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:12:55]    INFO >> epoch 011:   1254 / 1539 loss=3.657, wps=4409.3, ups=5.32, wpb=828.9, bsz=828.9, num_updates=16600, lr=0.000161, gnorm=7.676, clip=0, train_wall=9, gb_free=73.1, wall=2868 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:13:03]    INFO >> epoch 011:   1304 / 1539 loss=3.869, wps=4628.1, ups=6.26, wpb=739.1, bsz=739.1, num_updates=16650, lr=0.000161, gnorm=6.955, clip=0, train_wall=7, gb_free=71.9, wall=2876 (progress_bar.py:258, log())[0m
[33m[2025-11-21 18:13:07] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.06 GiB is free. Including non-PyTorch memory, this process has 78.06 GiB memory in use. Of the allocated memory 74.92 GiB is allocated by PyTorch, and 2.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 18:13:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:13:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:13:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 45           |        cudaMalloc retries: 85        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76320 MiB |  78596 MiB | 536717 GiB | 536643 GiB |
|       from large pool |  76301 MiB |  78578 MiB | 533678 GiB | 533604 GiB |
|       from small pool |     18 MiB |     23 MiB |   3038 GiB |   3038 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76320 MiB |  78596 MiB | 536717 GiB | 536643 GiB |
|       from large pool |  76301 MiB |  78578 MiB | 533678 GiB | 533604 GiB |
|       from small pool |     18 MiB |     23 MiB |   3038 GiB |   3038 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB | 535583 GiB | 535508 GiB |
|       from large pool |  76285 MiB |  78562 MiB | 532548 GiB | 532474 GiB |
|       from small pool |     18 MiB |     23 MiB |   3034 GiB |   3034 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79424 MiB |  79424 MiB |   1176 GiB |   1099 GiB |
|       from large pool |  79400 MiB |  79400 MiB |   1169 GiB |   1091 GiB |
|       from small pool |     24 MiB |     76 MiB |      7 GiB |      7 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3103 MiB |   4235 MiB | 510459 GiB | 510456 GiB |
|       from large pool |   3098 MiB |   4229 MiB | 507023 GiB | 507020 GiB |
|       from small pool |      5 MiB |     27 MiB |   3436 GiB |   3436 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     640    |     646    |   35395 K  |   35394 K  |
|       from large pool |     340    |     346    |   16881 K  |   16880 K  |
|       from small pool |     300    |     356    |   18513 K  |   18513 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     640    |     646    |   35395 K  |   35394 K  |
|       from large pool |     340    |     346    |   16881 K  |   16880 K  |
|       from small pool |     300    |     356    |   18513 K  |   18513 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     110    |     179    |    7980    |    7870    |
|       from large pool |      98    |     141    |    4186    |    4088    |
|       from small pool |      12    |      38    |    3794    |    3782    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     125    |     125    |   20000 K  |   20000 K  |
|       from large pool |     100    |     100    |   11103 K  |   11102 K  |
|       from small pool |      25    |      61    |    8897 K  |    8897 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:13:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 18:13:07] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 18:13:14]    INFO >> epoch 011:   1355 / 1539 loss=3.77, wps=3398.3, ups=4.93, wpb=689.1, bsz=689.1, num_updates=16700, lr=0.000161, gnorm=7.503, clip=0, train_wall=7, gb_free=56.8, wall=2886 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:13:23]    INFO >> epoch 011:   1405 / 1539 loss=3.867, wps=4757.3, ups=6.01, wpb=791.3, bsz=791.3, num_updates=16750, lr=0.000161, gnorm=7.788, clip=0, train_wall=8, gb_free=73.5, wall=2894 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:13:30]    INFO >> epoch 011:   1455 / 1539 loss=3.837, wps=4277.8, ups=6.43, wpb=665.3, bsz=665.3, num_updates=16800, lr=0.000161, gnorm=7.516, clip=0, train_wall=7, gb_free=65.3, wall=2902 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:13:40]    INFO >> epoch 011:   1505 / 1539 loss=3.509, wps=4779.9, ups=5.84, wpb=818.6, bsz=818.6, num_updates=16850, lr=0.000161, gnorm=7.634, clip=0, train_wall=8, gb_free=72.5, wall=2911 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:13:46]    INFO >> epoch 011 | loss 3.772 | wps 4128.4 | ups 5.79 | wpb 712.7 | bsz 712.7 | num_updates 16884 | lr 0.000161 | gnorm 7.246 | clip 0.3 | train_wall 231 | gb_free 73.9 | wall 2916 (progress_bar.py:267, print())[0m
[33m[2025-11-21 18:13:46] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 18:14:01]    INFO >> epoch 011 | valid on 'valid' subset | loss 3.833 | wps 10193.5 | wpb 5412.5 | bsz 5412.5 | num_updates 16884 | best_loss 5.157 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 18:14:01]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 18:14:01]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/checkpoints/checkpoint_last.pt (epoch 11 @ 16884 updates, score 3.833) (writing took 0.017568 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[32m[2025-11-21 18:14:01]    INFO >> Êó©ÂÅú: È™åËØÅÊÄßËÉΩÂ∑≤10ËΩÆÊú™ÊèêÂçá (train_enhanced.py:616, single_main())[0m
[32m[2025-11-21 18:14:01]    INFO >> ËÆ≠ÁªÉÂÆåÊàêÔºåÁî®Êó∂ 2866.3 Áßí (train_enhanced.py:626, single_main())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 18:14:01]    INFO >> ËÆ≠ÁªÉÊõ≤Á∫øÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 18:14:01]    INFO >> ÊâÄÊúâÊó•ÂøóÂ∑≤‰øùÂ≠òÂà∞: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/logs (train_enhanced.py:631, single_main())[0m
[32m[2025-11-21 18:14:01]    INFO >> 
================================================================================ (train_enhanced.py:634, single_main())[0m
[32m[2025-11-21 18:14:01]    INFO >> ÂºÄÂßãÊµãËØï... (train_enhanced.py:635, single_main())[0m
[32m[2025-11-21 18:14:01]    INFO >> ================================================================================ (train_enhanced.py:636, single_main())[0m
[32m[2025-11-21 18:14:01]    INFO >> Âä†ËΩΩÊúÄ‰Ω≥checkpoint: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/checkpoints/checkpoint_best.pt (train_enhanced.py:50, run_test_after_training())[0m
[32m[2025-11-21 18:14:01]    INFO >> ÊµãËØïÈõÜ: test (train_enhanced.py:51, run_test_after_training())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/checkpoint_utils.py:212: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(
[32m[2025-11-21 18:14:54]    INFO >> 
================================================================================ (train_enhanced.py:168, run_test_after_training())[0m
[32m[2025-11-21 18:14:54]    INFO >> ÊµãËØïÁªìÊûú: (train_enhanced.py:169, run_test_after_training())[0m
[32m[2025-11-21 18:14:54]    INFO >> -------------------------------------------------------------------------------- (train_enhanced.py:170, run_test_after_training())[0m
[32m[2025-11-21 18:14:54]    INFO >> Âπ≥ÂùáLoss:      3.9918 (train_enhanced.py:171, run_test_after_training())[0m
[32m[2025-11-21 18:14:54]    INFO >> Acc@1:         17.63% (train_enhanced.py:172, run_test_after_training())[0m
[32m[2025-11-21 18:14:54]    INFO >> Acc@5:         54.40% (train_enhanced.py:173, run_test_after_training())[0m
[32m[2025-11-21 18:14:54]    INFO >> Acc@1 (Âê´any): 17.63% (train_enhanced.py:174, run_test_after_training())[0m
[32m[2025-11-21 18:14:54]    INFO >> Acc@5 (Âê´any): 54.40% (train_enhanced.py:175, run_test_after_training())[0m
[32m[2025-11-21 18:14:54]    INFO >> ================================================================================ (train_enhanced.py:176, run_test_after_training())[0m
[32m[2025-11-21 18:14:54]    INFO >> ÊµãËØïÁªìÊûúÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/checkpoints/res.txt (train_enhanced.py:187, run_test_after_training())[0m
[32m[2025-11-21 18:14:54]    INFO >> ËÆ≠ÁªÉÊó•ÂøóÂ∑≤Êõ¥Êñ∞: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/logs (train_enhanced.py:222, run_test_after_training())[0m
[TrainingLogger] Êó•ÂøóÁõÆÂΩï: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/logs
[TrainingLogger] ÂéüÂßãËæìÂá∫Â∞Ü‰øùÂ≠òÂà∞: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/logs/training_output.log
[TrainingLogger] Epoch 1 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/logs/metrics.json
[TrainingLogger] Epoch 2 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/logs/metrics.json
[TrainingLogger] Epoch 3 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/logs/metrics.json
[TrainingLogger] Epoch 4 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/logs/metrics.json
[TrainingLogger] Epoch 5 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/logs/metrics.json
[TrainingLogger] Epoch 6 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/logs/metrics.json
[TrainingLogger] Epoch 7 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/logs/metrics.json
[TrainingLogger] Epoch 8 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/logs/metrics.json
[TrainingLogger] Epoch 9 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/logs/metrics.json
[TrainingLogger] Epoch 10 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/logs/metrics.json
[TrainingLogger] Epoch 11 ÊåáÊ†áÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_03/logs/metrics.json

‚úì dropout_03 ÊàêÂäü

============================================================
ÂÆûÈ™åÂÆåÊàê
============================================================
‚úì dropout_0
‚úì dropout_015
‚úì dropout_02
‚úì dropout_03
============================================================

Ê≠£Âú®ÂàÜÊûêÁªìÊûú...

================================================================================
ÂÆûÈ™åÁªìÊûúÂØπÊØî
================================================================================
ÂÆûÈ™åÂêçÁß∞                      ËΩÆÊï∞       ÊúÄÁªàLoss       ÊúÄ‰Ω≥È™åËØÅLoss       
--------------------------------------------------------------------------------
dropout_0                 18       3.1630       3.4660         
layers_1                  11       3.4400       3.6130         
embed_128                 11       3.3240       3.6830         
embed_96                  11       3.3540       3.6980         
embed_32                  11       3.6130       3.7080         
lr_1e-3                   4        3.5320       3.7180         
lr_1.5e-3                 4        3.5200       3.7210         
lr_1.25e-3                4        3.5090       3.7340         
lr_7.5e-4                 4        3.5530       3.7560         
dropout_015               11       3.5490       3.7610         
dropout_02                11       3.6210       3.7840         
lr_5e-4                   4        3.6480       3.7850         
dropout_03                11       3.7720       3.8330         
================================================================================
run_experiments.py:282: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.tight_layout()
run_experiments.py:282: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.tight_layout()
run_experiments.py:282: UserWarning: Glyph 23545 (\N{CJK UNIFIED IDEOGRAPH-5BF9}) missing from current font.
  plt.tight_layout()
run_experiments.py:282: UserWarning: Glyph 27604 (\N{CJK UNIFIED IDEOGRAPH-6BD4}) missing from current font.
  plt.tight_layout()
run_experiments.py:282: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.tight_layout()
run_experiments.py:282: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.tight_layout()
run_experiments.py:283: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.savefig(exp_base_dir / 'comparison.png', dpi=120, bbox_inches='tight')
run_experiments.py:283: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.savefig(exp_base_dir / 'comparison.png', dpi=120, bbox_inches='tight')
run_experiments.py:283: UserWarning: Glyph 23545 (\N{CJK UNIFIED IDEOGRAPH-5BF9}) missing from current font.
  plt.savefig(exp_base_dir / 'comparison.png', dpi=120, bbox_inches='tight')
run_experiments.py:283: UserWarning: Glyph 27604 (\N{CJK UNIFIED IDEOGRAPH-6BD4}) missing from current font.
  plt.savefig(exp_base_dir / 'comparison.png', dpi=120, bbox_inches='tight')
run_experiments.py:283: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.savefig(exp_base_dir / 'comparison.png', dpi=120, bbox_inches='tight')
run_experiments.py:283: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.savefig(exp_base_dir / 'comparison.png', dpi=120, bbox_inches='tight')

ÂØπÊØîÂõæÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/comparison.png
Êä•ÂëäÂ∑≤‰øùÂ≠ò: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/report.md

(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py 
ÂàÜÊûêÁõÆÂΩï: ../experiments

========================================================================================================================
Typilus Â≠¶‰π†ÁéáÂÆûÈ™åÁªìÊûúÂàÜÊûê
========================================================================================================================

## ËÆ≠ÁªÉÁªìÊûú
------------------------------------------------------------------------------------------------------------------------
+-------------+-------------+----------------+----------------+--------+----------------+----------------+
| Â≠¶‰π†Áéá      |   ÊúÄ‰Ω≥Epoch |   ÊúÄ‰Ω≥ËÆ≠ÁªÉLoss |   ÊúÄ‰Ω≥È™åËØÅLoss |    Gap |   ÊúÄÁªàËÆ≠ÁªÉLoss |   ÊúÄÁªàÈ™åËØÅLoss |
+=============+=============+================+================+========+================+================+
| dropout_0   |           2 |          3.248 |          3.466 | -0.218 |          3.163 |          3.498 |
+-------------+-------------+----------------+----------------+--------+----------------+----------------+
| dropout_015 |          10 |          3.556 |          3.761 | -0.205 |          3.549 |          3.766 |
+-------------+-------------+----------------+----------------+--------+----------------+----------------+
| dropout_02  |          10 |          3.613 |          3.784 | -0.171 |          3.621 |          3.789 |
+-------------+-------------+----------------+----------------+--------+----------------+----------------+
| dropout_03  |          11 |          3.772 |          3.833 | -0.061 |          3.772 |          3.833 |
+-------------+-------------+----------------+----------------+--------+----------------+----------------+
| embed_128   |          11 |          3.324 |          3.683 | -0.359 |          3.324 |          3.683 |
+-------------+-------------+----------------+----------------+--------+----------------+----------------+
| embed_32    |          10 |          3.634 |          3.708 | -0.074 |          3.613 |          3.818 |
+-------------+-------------+----------------+----------------+--------+----------------+----------------+
| embed_96    |          10 |          3.377 |          3.698 | -0.321 |          3.354 |          3.71  |
+-------------+-------------+----------------+----------------+--------+----------------+----------------+
| layers_1    |          11 |          3.44  |          3.613 | -0.173 |          3.44  |          3.613 |
+-------------+-------------+----------------+----------------+--------+----------------+----------------+
| 1.25e-3     |           3 |          3.573 |          3.734 | -0.161 |          3.509 |          3.738 |
+-------------+-------------+----------------+----------------+--------+----------------+----------------+
| 1.5e-3      |           2 |          3.712 |          3.721 | -0.009 |          3.52  |          3.74  |
+-------------+-------------+----------------+----------------+--------+----------------+----------------+
| 1e-3        |           4 |          3.532 |          3.718 | -0.186 |          3.532 |          3.718 |
+-------------+-------------+----------------+----------------+--------+----------------+----------------+
| 5e-4        |           4 |          3.648 |          3.785 | -0.137 |          3.648 |          3.785 |
+-------------+-------------+----------------+----------------+--------+----------------+----------------+
| 7.5e-4      |           4 |          3.553 |          3.756 | -0.203 |          3.553 |          3.756 |
+-------------+-------------+----------------+----------------+--------+----------------+----------------+

## ÊµãËØïÁªìÊûú
------------------------------------------------------------------------------------------------------------------------
+-------------+------------+---------+---------+----------------+----------------+
| Â≠¶‰π†Áéá      |   ÊµãËØïLoss | Acc@1   | Acc@5   | Acc@1(Âê´any)   | Acc@5(Âê´any)   |
+=============+============+=========+=========+================+================+
| dropout_0   |     7.939  | 0.78%   | 2.68%   | 0.78%          | 2.68%          |
+-------------+------------+---------+---------+----------------+----------------+
| dropout_015 |     3.8948 | 22.18%  | 60.94%  | 22.18%         | 60.94%         |
+-------------+------------+---------+---------+----------------+----------------+
| dropout_02  |     3.9355 | 25.51%  | 59.62%  | 25.51%         | 59.62%         |
+-------------+------------+---------+---------+----------------+----------------+
| dropout_03  |     3.9918 | 17.63%  | 54.40%  | 17.63%         | 54.40%         |
+-------------+------------+---------+---------+----------------+----------------+
| embed_128   |     4.1352 | 16.65%  | 40.32%  | 16.65%         | 40.32%         |
+-------------+------------+---------+---------+----------------+----------------+
| embed_32    |     3.9709 | 18.18%  | 51.89%  | 18.18%         | 51.89%         |
+-------------+------------+---------+---------+----------------+----------------+
| embed_96    |     4.0728 | 20.80%  | 41.85%  | 20.80%         | 41.85%         |
+-------------+------------+---------+---------+----------------+----------------+
| layers_1    |     3.9465 | 18.62%  | 59.31%  | 18.62%         | 59.31%         |
+-------------+------------+---------+---------+----------------+----------------+
| 1.25e-3     |     3.7413 | 13.48%  | 41.00%  | 13.48%         | 41.00%         |
+-------------+------------+---------+---------+----------------+----------------+
| 1.5e-3      |     3.8528 | 8.19%   | 32.78%  | 8.19%          | 32.78%         |
+-------------+------------+---------+---------+----------------+----------------+
| 1e-3        |     3.8048 | 11.45%  | 41.43%  | 11.45%         | 41.43%         |
+-------------+------------+---------+---------+----------------+----------------+
| 5e-4        |     3.8337 | 23.21%  | 53.33%  | 23.21%         | 53.33%         |
+-------------+------------+---------+---------+----------------+----------------+
| 7.5e-4      |     3.7879 | 20.18%  | 47.25%  | 20.18%         | 47.25%         |
+-------------+------------+---------+---------+----------------+----------------+

## ÊúÄ‰Ω≥ÈÖçÁΩÆ
------------------------------------------------------------------------------------------------------------------------

ÊúÄ‰Ω≥È™åËØÅLoss: dropout_0 (lr=dropout_0)
  - ÊúÄ‰Ω≥Epoch: 2
  - ËÆ≠ÁªÉLoss: 3.2480
  - È™åËØÅLoss: 3.4660
  - Gap: -0.2180
  - ÊµãËØïAcc@1: 0.78%
  - ÊµãËØïAcc@5: 2.68%

ÊúÄ‰Ω≥Ê≥õÂåñËÉΩÂäõ: embed_128 (Gap=-0.3590)
  - È™åËØÅLoss: 3.6830

ÊúÄ‰Ω≥ÊµãËØïÂáÜÁ°ÆÁéá: dropout_02
  - Acc@1: 25.51%
  - Acc@5: 59.62%

========================================================================================================================

ÂØπÊØîÂõæÂ∑≤‰øùÂ≠ò: ../experiments/comparison.png
Êä•ÂëäÂ∑≤‰øùÂ≠ò: ../experiments/analysis_report.md
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [C[C[C[C[C[C[C[C[C[C[C[31@run_experiments.py --config experiments_model.yml[C[C[C[C[C[C[C[C[C[C[C[C[31Panalyze_results.py[C[C[C[C[C[Kpython analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python analyze_results.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_too
ls[00m$ python analyze_results.py 