(base) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ conda activate naturalc

EnvironmentNameNotFound: Could not find conda environment: naturalc
You can list all discoverable environments with `conda info --envs`.


(base) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ conda activate naturalcc
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ git pull
^[[A^[[Aremote: Enumerating objects: 9, done.[K
remote: Counting objects:  11% (1/9)[Kremote: Counting objects:  22% (2/9)[Kremote: Counting objects:  33% (3/9)[Kremote: Counting objects:  44% (4/9)[Kremote: Counting objects:  55% (5/9)[Kremote: Counting objects:  66% (6/9)[Kremote: Counting objects:  77% (7/9)[Kremote: Counting objects:  88% (8/9)[Kremote: Counting objects: 100% (9/9)[Kremote: Counting objects: 100% (9/9), done.[K
remote: Compressing objects:  14% (1/7)[Kremote: Compressing objects:  28% (2/7)[Kremote: Compressing objects:  42% (3/7)[Kremote: Compressing objects:  57% (4/7)[Kremote: Compressing objects:  71% (5/7)[Kremote: Compressing objects:  85% (6/7)[Kremote: Compressing objects: 100% (7/7)[Kremote: Compressing objects: 100% (7/7), done.[K
Unpacking objects:  12% (1/8)Unpacking objects:  25% (2/8)Unpacking objects:  37% (3/8)Unpacking objects:  50% (4/8)Unpacking objects:  62% (5/8)remote: Total 8 (delta 1), reused 8 (delta 1), pack-reused 0 (from 0)[K
Unpacking objects:  75% (6/8)Unpacking objects:  87% (7/8)Unpacking objects: 100% (8/8)Unpacking objects: 100% (8/8), 17.14 KiB | 373.00 KiB/s, done.
From https://github.com/Cicecoo/Type-Prediction
   c7b013f..73a76cf  master     -> cicecoo/master
Updating c7b013f..73a76cf
Fast-forward
 EXPERIMENT_GUIDE.md          | 348 [32m++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++[m
 QUICKSTART.md                | 155 [32m++++++++++++++++++++++++++++++[m
 analyze_results.py           | 379 [32m++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++[m
 experiment_suite.sh          | 136 [32m+++++++++++++++++++++++++++[m
 generate_experiment_suite.py | 419 [32m++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++[m
 run_batch_experiments.py     | 290 [32m+++++++++++++++++++++++++++++++++++++++++++++++++++++++++[m
 6 files changed, 1727 insertions(+)
 create mode 100644 EXPERIMENT_GUIDE.md
 create mode 100644 QUICKSTART.md
 create mode 100644 analyze_results.py
 create mode 100644 experiment_suite.sh
 create mode 100644 generate_experiment_suite.py
 create mode 100644 run_batch_experiments.py
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ git pullconda activate naturalcc[16Pgit pull[Kpython generate_experiment_suite.py
============================================================
Generating Transformer Type Prediction Experiment Suite
============================================================

1. Baseline
âœ“ Created experiment: exp_baseline

2. Model Size Experiments
âœ“ Created experiment: exp_d_model_256
âœ“ Created experiment: exp_d_model_512
âœ“ Created experiment: exp_d_model_1024

3. Layer Number Experiments
âœ“ Created experiment: exp_layers_4
âœ“ Created experiment: exp_layers_6
âœ“ Created experiment: exp_layers_8

4. Learning Rate Experiments
âœ“ Created experiment: exp_lr_1e-04
âœ“ Created experiment: exp_lr_5e-04
âœ“ Created experiment: exp_lr_1e-03

5. Dropout Experiments
âœ“ Created experiment: exp_dropout_0.0
âœ“ Created experiment: exp_dropout_0.1
âœ“ Created experiment: exp_dropout_0.2

6. Encoder Type Experiments
âœ“ Created experiment: exp_encoder_transformer
âœ“ Created experiment: exp_encoder_lstm

7. Batch Size Experiments
âœ“ Created experiment: exp_batch_16
âœ“ Created experiment: exp_batch_32
âœ“ Created experiment: exp_batch_64

============================================================
Total experiments created: 18
Experiments directory: experiments/transformer_series
============================================================
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py --group baseline --gpus 0

============================================================
Batch Experiment Runner
Mode: serial
Experiments: 1
GPUs: [0]
============================================================


============================================================
Running: exp_baseline
GPU: 0
Time: 2025-11-20T00:10:47.539636
============================================================
usage: run_transformer_experiment.py [-h] --exp-name EXP_NAME
                                     [--base-dir BASE_DIR]
                                     [--data-dir DATA_DIR]
                                     [--config-template CONFIG_TEMPLATE]
                                     [--encoder-type {lstm,transformer}]
                                     [--encoder-layers ENCODER_LAYERS]
                                     [--encoder-embed-dim ENCODER_EMBED_DIM]
                                     [--dropout DROPOUT] [--lr LR]
                                     [--batch-size BATCH_SIZE]
                                     [--max-epoch MAX_EPOCH]
                                     [--warmup-updates WARMUP_UPDATES]
                                     [--skip-train] [--skip-eval]
run_transformer_experiment.py: error: unrecognized arguments: --language python --d-model 512 --n-layers 6

âŒ exp_baseline failed with code 2

============================================================
Batch Run Summary
============================================================
Total: 1
Success: 0
Failed: 1
Success Rate: 0.0%

Failed experiments:
  - exp_baseline
============================================================

(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py --group baseline --gpus 0[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [C[C[C[C[C[C[C[3Pgenerate_experiment_suite.py
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cgit pull[Kconda activate naturalcc[16Pgit pullpython generate_experiment_suite.pyrun_batch_experiments.py --group baseline --gpus 0[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cpython run_batch_experiments.py --group baseline --gpus 3

============================================================
Batch Experiment Runner
Mode: serial
Experiments: 1
GPUs: [3]
============================================================


============================================================
Running: exp_baseline
GPU: 3
Time: 2025-11-20T00:11:14.767556
============================================================
usage: run_transformer_experiment.py [-h] --exp-name EXP_NAME
                                     [--base-dir BASE_DIR]
                                     [--data-dir DATA_DIR]
                                     [--config-template CONFIG_TEMPLATE]
                                     [--encoder-type {lstm,transformer}]
                                     [--encoder-layers ENCODER_LAYERS]
                                     [--encoder-embed-dim ENCODER_EMBED_DIM]
                                     [--dropout DROPOUT] [--lr LR]
                                     [--batch-size BATCH_SIZE]
                                     [--max-epoch MAX_EPOCH]
                                     [--warmup-updates WARMUP_UPDATES]
                                     [--skip-train] [--skip-eval]
run_transformer_experiment.py: error: unrecognized arguments: --language python --d-model 512 --n-layers 6

âŒ exp_baseline failed with code 2

============================================================
Batch Run Summary
============================================================
Total: 1
Success: 0
Failed: 1
Success Rate: 0.0%

Failed experiments:
  - exp_baseline
============================================================

(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py --group baseline --gpus 3[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py --group baseline --gpus 30[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [C[C[C[C[C[C[Cgenerate_experiment_suite.py[K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ git pull[K
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C
remote: Enumerating objects: 5, done.[K
remote: Counting objects:  20% (1/5)[Kremote: Counting objects:  40% (2/5)[Kremote: Counting objects:  60% (3/5)[Kremote: Counting objects:  80% (4/5)[Kremote: Counting objects: 100% (5/5)[Kremote: Counting objects: 100% (5/5), done.[K
remote: Compressing objects: 100% (1/1)[Kremote: Compressing objects: 100% (1/1), done.[K
remote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0 (from 0)[K
Unpacking objects:  33% (1/3)Unpacking objects:  66% (2/3)Unpacking objects: 100% (3/3)Unpacking objects: 100% (3/3), 956 bytes | 478.00 KiB/s, done.
From https://github.com/Cicecoo/Type-Prediction
   73a76cf..efbbb91  master     -> cicecoo/master
Updating 73a76cf..efbbb91
Fast-forward
 run_batch_experiments.py | 67 [32m+++++++++++++++++++++++++++++++++++++++++++++++++[m[31m------------------[m
 1 file changed, 49 insertions(+), 18 deletions(-)
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_transformer_experiment.py \
>   --exp-name baseline \
>   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer \
>   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer \
>   --max-epoch 5

============================================================
Setting up experiment: baseline
============================================================
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer/baseline
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer/baseline/checkpoints
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer/baseline/logs
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer/baseline/results
âœ“ Saved config to: /mnt/data1/zhaojunzhang/experiments/transformer/baseline/config.yml
âœ“ Also saved to: run/type_prediction/transformer/config/baseline.yml
âœ“ Saved experiment info to: /mnt/data1/zhaojunzhang/experiments/transformer/baseline/info.txt

============================================================
Starting training...
============================================================

Command: python run/type_prediction/transformer/train.py --language baseline 2>&1 | tee /mnt/data1/zhaojunzhang/experiments/transformer/baseline/logs/train.log

Note: Config file will be loaded from run/type_prediction/transformer/config/baseline.yml

[32m[2025-11-20 00:16:24]    INFO >> Load arguments in run/type_prediction/transformer/config/baseline.yml (train.py:300, cli_main())[0m
[32m[2025-11-20 00:16:24]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer/baseline/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 16, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 16, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 2, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 5, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer/baseline/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer/baseline/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:302, cli_main())[0m
[32m[2025-11-20 00:16:24]    INFO >> single GPU training... (train.py:331, cli_main())[0m
[32m[2025-11-20 00:16:24]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer/baseline/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 16, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 16, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 2, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 5, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer/baseline/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer/baseline/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:208, single_main())[0m
[32m[2025-11-20 00:16:26]    INFO >> TypeTransformer(
  (decoder): RobertaEncoder(
    (encoder): CodeEncoderLSTM(
      (embedding): Embedding(10006, 512)
      (pos_encoder): PositionalEncoding()
      (encoder): LSTM(512, 512, num_layers=6, bidirectional=True)
    )
    (output): Sequential(
      (0): Linear(in_features=1024, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=10006, bias=True)
    )
  )
  (classification_heads): ModuleDict()
) (train.py:220, single_main())[0m
[32m[2025-11-20 00:16:26]    INFO >> model typetransformer, criterion TypePredictionCrossEntropyCriterion (train.py:221, single_main())[0m
[32m[2025-11-20 00:16:26]    INFO >> num. model params: 46481686 (num. trained: 46481686) (train.py:222, single_main())[0m
[32m[2025-11-20 00:16:27]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 00:16:27]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 35002 MB ; used memory = 46917 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 00:16:27]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 00:16:27]    INFO >> training on 1 GPUs (train.py:229, single_main())[0m
[32m[2025-11-20 00:16:27]    INFO >> max tokens per GPU = None and max sentences per GPU = 16 (train.py:230, single_main())[0m
[32m[2025-11-20 00:16:27]    INFO >> no existing checkpoint found /mnt/data1/zhaojunzhang/experiments/transformer/baseline/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-20 00:16:27]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
^CTraceback (most recent call last):
  File "run_transformer_experiment.py", line 575, in <module>
    main()
  File "run_transformer_experiment.py", line 561, in main
    if not exp.train():
  File "run_transformer_experiment.py", line 361, in train
    result = subprocess.run(
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/subprocess.py", line 495, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/subprocess.py", line 1020, in communicate
    self.wait()
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/subprocess.py", line 1083, in wait
    return self._wait(timeout=timeout)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/subprocess.py", line 1822, in _wait
    (pid, sts) = self._try_wait(0)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/subprocess.py", line 1780, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
KeyboardInterrupt

(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ ^C
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python generate_experiment_suite.py \
>   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series \
>   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer
usage: generate_experiment_suite.py [-h] [--base-dir BASE_DIR]
                                    [--groups {baseline,model_size,layers,lr,dropout,encoder,batch_size,all} [{baseline,model_size,layers,lr,dropout,encoder,batch_size,all} ...]]
generate_experiment_suite.py: error: unrecognized arguments: --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ cat /mnt/data1/zhaojunzhang/experiments/transformer_series/experiment_index.json
cat: /mnt/data1/zhaojunzhang/experiments/transformer_series/experiment_index.json: No such file or directory
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python generate_experiment_suite.py \
>   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series \
>   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer
usage: generate_experiment_suite.py [-h] [--base-dir BASE_DIR]
                                    [--groups {baseline,model_size,layers,lr,dropout,encoder,batch_size,all} [{baseline,model_size,layers,lr,dropout,encoder,batch_size,all} ...]]
generate_experiment_suite.py: error: unrecognized arguments: --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ 
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python generate_experiment_suite.py   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer[A[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ cat /mnt/data1/zhaojunzhang/exper[65Piments/transformer_series/experiment_index.json
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python generate_experiment_suite.py   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer[A[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [C[C[C[C[C[C[Crun_transformer_experiment.py   --exp-name baseline   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer   --max-epoch 5[A[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ git pull[K
[K
[K[A[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C
remote: Enumerating objects: 5, done.[K
remote: Counting objects:  20% (1/5)[Kremote: Counting objects:  40% (2/5)[Kremote: Counting objects:  60% (3/5)[Kremote: Counting objects:  80% (4/5)[Kremote: Counting objects: 100% (5/5)[Kremote: Counting objects: 100% (5/5), done.[K
remote: Compressing objects: 100% (1/1)[Kremote: Compressing objects: 100% (1/1), done.[K
remote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0 (from 0)[K
Unpacking objects:  33% (1/3)Unpacking objects:  66% (2/3)Unpacking objects: 100% (3/3)Unpacking objects: 100% (3/3), 422 bytes | 422.00 KiB/s, done.
From https://github.com/Cicecoo/Type-Prediction
   efbbb91..999473b  master     -> cicecoo/master
Updating efbbb91..999473b
Fast-forward
 generate_experiment_suite.py | 3 [32m+++[m
 1 file changed, 3 insertions(+)
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python generate_experiment_suite.py \
>   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series \
>   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer
============================================================
Generating Transformer Type Prediction Experiment Suite
============================================================

1. Baseline
âœ“ Created experiment: exp_baseline

2. Model Size Experiments
âœ“ Created experiment: exp_d_model_256
âœ“ Created experiment: exp_d_model_512
âœ“ Created experiment: exp_d_model_1024

3. Layer Number Experiments
âœ“ Created experiment: exp_layers_4
âœ“ Created experiment: exp_layers_6
âœ“ Created experiment: exp_layers_8

4. Learning Rate Experiments
âœ“ Created experiment: exp_lr_1e-04
âœ“ Created experiment: exp_lr_5e-04
âœ“ Created experiment: exp_lr_1e-03

5. Dropout Experiments
âœ“ Created experiment: exp_dropout_0.0
âœ“ Created experiment: exp_dropout_0.1
âœ“ Created experiment: exp_dropout_0.2

6. Encoder Type Experiments
âœ“ Created experiment: exp_encoder_transformer
âœ“ Created experiment: exp_encoder_lstm

7. Batch Size Experiments
âœ“ Created experiment: exp_batch_16
âœ“ Created experiment: exp_batch_32
âœ“ Created experiment: exp_batch_64

============================================================
Total experiments created: 18
Experiments directory: /mnt/data1/zhaojunzhang/experiments/transformer_series
============================================================
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ cat /mnt/data1/zhaojunzhang/experiments/transformer_series/experiment_index.json
{
  "created": "2025-11-20T00:18:15.235011",
  "total_experiments": 18,
  "experiments": [
    {
      "name": "exp_baseline",
      "description": "Baseline configuration with default hyperparameters",
      "path": "exp_baseline"
    },
    {
      "name": "exp_d_model_256",
      "description": "Model dimension: 256",
      "path": "exp_d_model_256"
    },
    {
      "name": "exp_d_model_512",
      "description": "Model dimension: 512",
      "path": "exp_d_model_512"
    },
    {
      "name": "exp_d_model_1024",
      "description": "Model dimension: 1024",
      "path": "exp_d_model_1024"
    },
    {
      "name": "exp_layers_4",
      "description": "Number of encoder layers: 4",
      "path": "exp_layers_4"
    },
    {
      "name": "exp_layers_6",
      "description": "Number of encoder layers: 6",
      "path": "exp_layers_6"
    },
    {
      "name": "exp_layers_8",
      "description": "Number of encoder layers: 8",
      "path": "exp_layers_8"
    },
    {
      "name": "exp_lr_1e-04",
      "description": "Learning rate: 0.0001",
      "path": "exp_lr_1e-04"
    },
    {
      "name": "exp_lr_5e-04",
      "description": "Learning rate: 0.0005",
      "path": "exp_lr_5e-04"
    },
    {
      "name": "exp_lr_1e-03",
      "description": "Learning rate: 0.001",
      "path": "exp_lr_1e-03"
    },
    {
      "name": "exp_dropout_0.0",
      "description": "Dropout rate: 0.0",
      "path": "exp_dropout_0.0"
    },
    {
      "name": "exp_dropout_0.1",
      "description": "Dropout rate: 0.1",
      "path": "exp_dropout_0.1"
    },
    {
      "name": "exp_dropout_0.2",
      "description": "Dropout rate: 0.2",
      "path": "exp_dropout_0.2"
    },
    {
      "name": "exp_encoder_transformer",
      "description": "Encoder type: transformer",
      "path": "exp_encoder_transformer"
    },
    {
      "name": "exp_encoder_lstm",
      "description": "Encoder type: lstm",
      "path": "exp_encoder_lstm"
    },
    {
      "name": "exp_batch_16",
      "description": "Batch size: 16",
      "path": "exp_batch_16"
    },
    {
      "name": "exp_batch_32",
      "description": "Batch size: 32",
      "path": "exp_batch_32"
    },
    {
      "name": "exp_batch_64",
      "description": "Batch size: 64",
      "path": "exp_batch_64"
    }
  ]
}(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py \
>   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series \
>   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer \
>   --mode serial \
>   --gpu 0[K3
usage: run_batch_experiments.py [-h] [--exp-dir EXP_DIR]
                                [--group {baseline,model_size,layers,lr,dropout,encoder,batch_size}]
                                [--names NAMES [NAMES ...]] [--all] [--mode {serial,parallel}]
                                [--gpus GPUS [GPUS ...]] [--dry-run]
run_batch_experiments.py: error: unrecognized arguments: --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer   --mode serial   --gpu 3[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-da[1P[A[C[C[C[C[C[C-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-dat[1P[A[C[C[C[C[C-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data[1P[A[C[C[C[C-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data/[1P[A[C[C[Cexp-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-da[C[3@a/t[A[C[C

usage: run_batch_experiments.py [-h] [--exp-dir EXP_DIR]
                                [--group {baseline,model_size,layers,lr,dropout,encoder,batch_size}]
                                [--names NAMES [NAMES ...]] [--all] [--mode {serial,parallel}]
                                [--gpus GPUS [GPUS ...]] [--dry-run]
run_batch_experiments.py: error: unrecognized arguments: --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py   --exp-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer   --mode serial   --gpu 3[A[C[C[C[Cbase-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-d[1@a[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[A[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ cat /mnt/data1/zhaojunzhang/exper[65Piments/transformer_series/experiment_index.json
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python generate_experiment_suite.py   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer[A[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ git pull[K
[K
[K[A[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C
remote: Enumerating objects: 5, done.[K
remote: Counting objects:  20% (1/5)[Kremote: Counting objects:  40% (2/5)[Kremote: Counting objects:  60% (3/5)[Kremote: Counting objects:  80% (4/5)[Kremote: Counting objects: 100% (5/5)[Kremote: Counting objects: 100% (5/5), done.[K
remote: Compressing objects: 100% (1/1)[Kremote: Compressing objects: 100% (1/1), done.[K
remote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0 (from 0)[K
Unpacking objects:  33% (1/3)Unpacking objects:  66% (2/3)Unpacking objects: 100% (3/3)Unpacking objects: 100% (3/3), 520 bytes | 260.00 KiB/s, done.
From https://github.com/Cicecoo/Type-Prediction
   999473b..14faf10  master     -> cicecoo/master
Updating 999473b..14faf10
Fast-forward
 run_batch_experiments.py | 17 [32m+++++++++++++[m[31m----[m
 1 file changed, 13 insertions(+), 4 deletions(-)
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py \
>   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series \
>   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer \
>   --mode serial \
>   --gpu 3 \
>   --all

============================================================
Batch Experiment Runner
Mode: serial
Experiments: 18
GPUs: [3]
============================================================


============================================================
Running: exp_baseline
GPU: 3
Time: 2025-11-20T00:20:41.766711
============================================================
[32m[2025-11-20 00:20:43]    INFO >> Load arguments in run/type_prediction/transformer/config/exp_baseline.yml (train.py:300, cli_main())[0m
[32m[2025-11-20 00:20:43]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:302, cli_main())[0m
[32m[2025-11-20 00:20:43]    INFO >> single GPU training... (train.py:331, cli_main())[0m
[32m[2025-11-20 00:20:43]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:208, single_main())[0m
[32m[2025-11-20 00:20:45]    INFO >> TypeTransformer(
  (decoder): RobertaEncoder(
    (encoder): CodeEncoderLSTM(
      (embedding): Embedding(10006, 512)
      (pos_encoder): PositionalEncoding()
      (encoder): LSTM(512, 512, num_layers=6, bidirectional=True)
    )
    (output): Sequential(
      (0): Linear(in_features=1024, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=10006, bias=True)
    )
  )
  (classification_heads): ModuleDict()
) (train.py:220, single_main())[0m
[32m[2025-11-20 00:20:45]    INFO >> model typetransformer, criterion TypePredictionCrossEntropyCriterion (train.py:221, single_main())[0m
[32m[2025-11-20 00:20:45]    INFO >> num. model params: 46481686 (num. trained: 46481686) (train.py:222, single_main())[0m
[32m[2025-11-20 00:20:45]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 00:20:45]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 35819 MB ; used memory = 46100 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 00:20:45]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 00:20:45]    INFO >> training on 1 GPUs (train.py:229, single_main())[0m
[32m[2025-11-20 00:20:45]    INFO >> max tokens per GPU = None and max sentences per GPU = 32 (train.py:230, single_main())[0m
[32m[2025-11-20 00:20:45]    INFO >> no existing checkpoint found /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-20 00:20:45]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[33m[2025-11-20 00:20:59] WARNING >> 32222 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[0, 1, 2, 4, 6, 7, 8, 10, 14, 19] (data_utils.py:301, filter_by_size())[0m
[32m[2025-11-20 00:20:59]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-20 00:20:59] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:398: UserWarning: ntokens not found in Criterion logging outputs, cannot log wpb or wps
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:407: UserWarning: nsentences not found in Criterion logging outputs, cannot log bsz
  warnings.warn(
[32m[2025-11-20 00:21:44]    INFO >> epoch 001:    100 / 5058 loss=1.903, ups=2.35, num_updates=100, lr=1e-05, gnorm=0.104, clip=0, train_wall=40, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:22:33]    INFO >> epoch 001:    200 / 5058 loss=0.786, ups=2.15, num_updates=200, lr=2e-05, gnorm=0.068, clip=0, train_wall=43, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:23:38]    INFO >> epoch 001:    300 / 5058 loss=0.654, ups=1.61, num_updates=300, lr=3e-05, gnorm=0.053, clip=0, train_wall=58, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:24:42]    INFO >> epoch 001:    400 / 5058 loss=0.623, ups=1.58, num_updates=400, lr=4e-05, gnorm=2.819, clip=2, train_wall=59, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:25:46]    INFO >> epoch 001:    500 / 5058 loss=0.208, ups=1.65, num_updates=500, lr=5e-05, gnorm=4.476, clip=2, train_wall=56, gb_free=65.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:26:47]    INFO >> epoch 001:    600 / 5058 loss=0.079, ups=1.7, num_updates=600, lr=6e-05, gnorm=4.788, clip=8, train_wall=55, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:27:30]    INFO >> epoch 001:    700 / 5058 loss=0.174, ups=2.4, num_updates=700, lr=7e-05, gnorm=2.641, clip=4, train_wall=39, gb_free=69.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:28:14]    INFO >> epoch 001:    800 / 5058 loss=0.166, ups=2.41, num_updates=800, lr=8e-05, gnorm=3.325, clip=8, train_wall=39, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:29:22]    INFO >> epoch 001:    900 / 5058 loss=0.216, ups=1.55, num_updates=900, lr=9e-05, gnorm=6.91, clip=8, train_wall=60, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:30:27]    INFO >> epoch 001:   1000 / 5058 loss=0.142, ups=1.61, num_updates=1000, lr=0.0001, gnorm=1.451, clip=0, train_wall=58, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:31:25]    INFO >> epoch 001:   1100 / 5058 loss=0.058, ups=1.75, num_updates=1100, lr=0.0001, gnorm=2.202, clip=2, train_wall=53, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:32:30]    INFO >> epoch 001:   1200 / 5058 loss=0.035, ups=1.62, num_updates=1200, lr=0.0001, gnorm=0.871, clip=0, train_wall=57, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:33:35]    INFO >> epoch 001:   1300 / 5058 loss=0.087, ups=1.59, num_updates=1300, lr=9.9e-05, gnorm=1.877, clip=2, train_wall=58, gb_free=67.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:34:39]    INFO >> epoch 001:   1400 / 5058 loss=0.059, ups=1.63, num_updates=1400, lr=9.9e-05, gnorm=2.488, clip=4, train_wall=57, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:35:44]    INFO >> epoch 001:   1500 / 5058 loss=0.279, ups=1.6, num_updates=1500, lr=9.9e-05, gnorm=1.776, clip=2, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:36:46]    INFO >> epoch 001:   1600 / 5058 loss=0.11, ups=1.71, num_updates=1600, lr=9.9e-05, gnorm=3.243, clip=4, train_wall=54, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:37:52]    INFO >> epoch 001:   1700 / 5058 loss=0.113, ups=1.56, num_updates=1700, lr=9.9e-05, gnorm=2.319, clip=2, train_wall=59, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:38:48]    INFO >> epoch 001:   1800 / 5058 loss=0.12, ups=1.89, num_updates=1800, lr=9.8e-05, gnorm=3.115, clip=5, train_wall=49, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:39:27]    INFO >> epoch 001:   1900 / 5058 loss=0.153, ups=2.64, num_updates=1900, lr=9.8e-05, gnorm=1.952, clip=4, train_wall=36, gb_free=67.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:40:29]    INFO >> epoch 001:   2000 / 5058 loss=0.089, ups=1.68, num_updates=2000, lr=9.8e-05, gnorm=1.422, clip=1, train_wall=56, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:41:36]    INFO >> epoch 001:   2100 / 5058 loss=0.074, ups=1.56, num_updates=2100, lr=9.8e-05, gnorm=3.142, clip=2, train_wall=59, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:42:43]    INFO >> epoch 001:   2200 / 5058 loss=0.279, ups=1.56, num_updates=2200, lr=9.8e-05, gnorm=5.078, clip=6, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:43:50]    INFO >> epoch 001:   2300 / 5058 loss=0.068, ups=1.54, num_updates=2300, lr=9.7e-05, gnorm=2.452, clip=5, train_wall=60, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:44:56]    INFO >> epoch 001:   2400 / 5058 loss=0.063, ups=1.6, num_updates=2400, lr=9.7e-05, gnorm=0.773, clip=1, train_wall=58, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:46:06]    INFO >> epoch 001:   2500 / 5058 loss=0.149, ups=1.49, num_updates=2500, lr=9.7e-05, gnorm=3.718, clip=1, train_wall=62, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:47:02]    INFO >> epoch 001:   2600 / 5058 loss=0.128, ups=1.8, num_updates=2600, lr=9.7e-05, gnorm=2.608, clip=1, train_wall=52, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:48:03]    INFO >> epoch 001:   2700 / 5058 loss=0.076, ups=1.72, num_updates=2700, lr=9.7e-05, gnorm=1.387, clip=1, train_wall=54, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:49:06]    INFO >> epoch 001:   2800 / 5058 loss=0.081, ups=1.67, num_updates=2800, lr=9.6e-05, gnorm=1.32, clip=3, train_wall=56, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:49:46]    INFO >> epoch 001:   2900 / 5058 loss=0.274, ups=2.59, num_updates=2900, lr=9.6e-05, gnorm=2.005, clip=3, train_wall=36, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:50:52]    INFO >> epoch 001:   3000 / 5058 loss=0.145, ups=1.58, num_updates=3000, lr=9.6e-05, gnorm=3.103, clip=5, train_wall=59, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:51:47]    INFO >> epoch 001:   3100 / 5058 loss=0.058, ups=1.92, num_updates=3100, lr=9.6e-05, gnorm=0.282, clip=0, train_wall=49, gb_free=73.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:52:43]    INFO >> epoch 001:   3200 / 5058 loss=0.282, ups=1.8, num_updates=3200, lr=9.6e-05, gnorm=2.208, clip=2, train_wall=52, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:53:44]    INFO >> epoch 001:   3300 / 5058 loss=0.086, ups=1.72, num_updates=3300, lr=9.5e-05, gnorm=1.673, clip=1, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:54:56]    INFO >> epoch 001:   3400 / 5058 loss=0.225, ups=1.46, num_updates=3400, lr=9.5e-05, gnorm=5.264, clip=3, train_wall=64, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:56:02]    INFO >> epoch 001:   3500 / 5058 loss=0.131, ups=1.58, num_updates=3500, lr=9.5e-05, gnorm=1.53, clip=0, train_wall=59, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:57:08]    INFO >> epoch 001:   3600 / 5058 loss=0.095, ups=1.58, num_updates=3600, lr=9.5e-05, gnorm=2.575, clip=1, train_wall=59, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:58:09]    INFO >> epoch 001:   3700 / 5058 loss=0.137, ups=1.7, num_updates=3700, lr=9.4e-05, gnorm=1.854, clip=2, train_wall=55, gb_free=66.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:59:16]    INFO >> epoch 001:   3800 / 5058 loss=0.058, ups=1.54, num_updates=3800, lr=9.4e-05, gnorm=1.191, clip=1, train_wall=60, gb_free=68.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:00:07]    INFO >> epoch 001:   3900 / 5058 loss=0.127, ups=2.07, num_updates=3900, lr=9.4e-05, gnorm=1.978, clip=2, train_wall=45, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:01:09]    INFO >> epoch 001:   4000 / 5058 loss=0.131, ups=1.67, num_updates=4000, lr=9.4e-05, gnorm=1.731, clip=1, train_wall=56, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:02:12]    INFO >> epoch 001:   4100 / 5058 loss=0.92, ups=1.65, num_updates=4100, lr=9.4e-05, gnorm=0.902, clip=1, train_wall=57, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:03:19]    INFO >> epoch 001:   4200 / 5058 loss=0.149, ups=1.55, num_updates=4200, lr=9.3e-05, gnorm=1.845, clip=3, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:04:19]    INFO >> epoch 001:   4300 / 5058 loss=0.117, ups=1.75, num_updates=4300, lr=9.3e-05, gnorm=1.721, clip=1, train_wall=53, gb_free=73.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:05:21]    INFO >> epoch 001:   4400 / 5058 loss=0.082, ups=1.71, num_updates=4400, lr=9.3e-05, gnorm=1.476, clip=0, train_wall=55, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:06:24]    INFO >> epoch 001:   4500 / 5058 loss=0.072, ups=1.65, num_updates=4500, lr=9.3e-05, gnorm=1.371, clip=0, train_wall=57, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:07:31]    INFO >> epoch 001:   4600 / 5058 loss=0.241, ups=1.56, num_updates=4600, lr=9.3e-05, gnorm=1.805, clip=0, train_wall=59, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:08:42]    INFO >> epoch 001:   4700 / 5058 loss=0.215, ups=1.47, num_updates=4700, lr=9.2e-05, gnorm=3.803, clip=2, train_wall=63, gb_free=64.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:09:44]    INFO >> epoch 001:   4800 / 5058 loss=0.056, ups=1.69, num_updates=4800, lr=9.2e-05, gnorm=1.411, clip=0, train_wall=55, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:10:25]    INFO >> epoch 001:   4900 / 5058 loss=0.142, ups=2.52, num_updates=4900, lr=9.2e-05, gnorm=0.824, clip=0, train_wall=37, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:11:30]    INFO >> epoch 001:   5000 / 5058 loss=0.09, ups=1.61, num_updates=5000, lr=9.2e-05, gnorm=2.488, clip=0, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:12:09]    INFO >> epoch 001 | loss 0.105 | ups 1.72 | num_updates 5058 | lr 9.2e-05 | gnorm 2.248 | clip 2.1 | train_wall 2745 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-20 01:12:09] WARNING >> 4469 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[2, 4, 10, 12, 13, 14, 16, 18, 19, 23] (data_utils.py:301, filter_by_size())[0m
[33m[2025-11-20 01:12:09] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
CodeEncoderLSTM: Creating BiLSTM with 6 layers, 512 hidden and input size
Traceback (most recent call last):
  File "run/type_prediction/transformer/train.py", line 339, in <module>
    cli_main()
  File "run/type_prediction/transformer/train.py", line 332, in cli_main
    single_main(args)
  File "run/type_prediction/transformer/train.py", line 254, in single_main
    valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)
  File "run/type_prediction/transformer/train.py", line 132, in validate
    trainer.valid_step(sample)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/trainers/ncc_trainers.py", line 565, in valid_step
    _loss, sample_size, logging_output = self.task.valid_step(
TypeError: cannot unpack non-iterable NoneType object

============================================================
Setting up experiment: exp_baseline
============================================================
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/logs
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/results
âœ“ Saved config to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/config.yml
âœ“ Also saved to: run/type_prediction/transformer/config/exp_baseline.yml
âœ“ Saved experiment info to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/info.txt

============================================================
Starting training...
============================================================

Command: python run/type_prediction/transformer/train.py --language exp_baseline 2>&1 | tee /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/logs/train.log

Note: Config file will be loaded from run/type_prediction/transformer/config/exp_baseline.yml


âœ“ Training completed successfully!

============================================================
Evaluating model on test set...
============================================================

Warning: Best checkpoint not found, using last checkpoint
âœ— No checkpoint found for evaluation

============================================================
âœ“ Experiment completed: exp_baseline
âœ“ Results in: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline
============================================================


âœ“ exp_baseline completed in 0.86 hours

============================================================
Running: exp_d_model_256
GPU: 3
Time: 2025-11-20T01:12:12.775627
============================================================
[32m[2025-11-20 01:12:14]    INFO >> Load arguments in run/type_prediction/transformer/config/exp_d_model_256.yml (train.py:300, cli_main())[0m
[32m[2025-11-20 01:12:14]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 256, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:302, cli_main())[0m
[32m[2025-11-20 01:12:14]    INFO >> single GPU training... (train.py:331, cli_main())[0m
[32m[2025-11-20 01:12:14]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 256, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:208, single_main())[0m
[32m[2025-11-20 01:12:16]    INFO >> TypeTransformer(
  (decoder): RobertaEncoder(
    (encoder): CodeEncoderLSTM(
      (embedding): Embedding(10006, 512)
      (pos_encoder): PositionalEncoding()
      (encoder): LSTM(512, 512, num_layers=6, bidirectional=True)
    )
    (output): Sequential(
      (0): Linear(in_features=1024, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=10006, bias=True)
    )
  )
  (classification_heads): ModuleDict()
) (train.py:220, single_main())[0m
[32m[2025-11-20 01:12:16]    INFO >> model typetransformer, criterion TypePredictionCrossEntropyCriterion (train.py:221, single_main())[0m
[32m[2025-11-20 01:12:16]    INFO >> num. model params: 46481686 (num. trained: 46481686) (train.py:222, single_main())[0m
[32m[2025-11-20 01:12:16]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 01:12:16]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 35815 MB ; used memory = 46104 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 01:12:16]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 01:12:16]    INFO >> training on 1 GPUs (train.py:229, single_main())[0m
[32m[2025-11-20 01:12:16]    INFO >> max tokens per GPU = None and max sentences per GPU = 32 (train.py:230, single_main())[0m
[32m[2025-11-20 01:12:16]    INFO >> no existing checkpoint found /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-20 01:12:16]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[33m[2025-11-20 01:12:28] WARNING >> 32222 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[0, 1, 2, 4, 6, 7, 8, 10, 14, 19] (data_utils.py:301, filter_by_size())[0m
[32m[2025-11-20 01:12:28]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-20 01:12:29] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:398: UserWarning: ntokens not found in Criterion logging outputs, cannot log wpb or wps
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:407: UserWarning: nsentences not found in Criterion logging outputs, cannot log bsz
  warnings.warn(
[32m[2025-11-20 01:13:13]    INFO >> epoch 001:    100 / 5058 loss=1.903, ups=2.34, num_updates=100, lr=1e-05, gnorm=0.104, clip=0, train_wall=41, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:14:02]    INFO >> epoch 001:    200 / 5058 loss=0.786, ups=2.14, num_updates=200, lr=2e-05, gnorm=0.068, clip=0, train_wall=44, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:15:06]    INFO >> epoch 001:    300 / 5058 loss=0.654, ups=1.61, num_updates=300, lr=3e-05, gnorm=0.053, clip=0, train_wall=58, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:16:13]    INFO >> epoch 001:    400 / 5058 loss=0.623, ups=1.57, num_updates=400, lr=4e-05, gnorm=2.819, clip=2, train_wall=60, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:17:16]    INFO >> epoch 001:    500 / 5058 loss=0.208, ups=1.65, num_updates=500, lr=5e-05, gnorm=4.476, clip=2, train_wall=57, gb_free=65.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:18:18]    INFO >> epoch 001:    600 / 5058 loss=0.079, ups=1.69, num_updates=600, lr=6e-05, gnorm=4.788, clip=8, train_wall=55, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:19:01]    INFO >> epoch 001:    700 / 5058 loss=0.174, ups=2.4, num_updates=700, lr=7e-05, gnorm=2.641, clip=4, train_wall=39, gb_free=69.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:19:44]    INFO >> epoch 001:    800 / 5058 loss=0.166, ups=2.43, num_updates=800, lr=8e-05, gnorm=3.325, clip=8, train_wall=39, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:20:52]    INFO >> epoch 001:    900 / 5058 loss=0.216, ups=1.54, num_updates=900, lr=9e-05, gnorm=6.91, clip=8, train_wall=61, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:21:57]    INFO >> epoch 001:   1000 / 5058 loss=0.142, ups=1.6, num_updates=1000, lr=0.0001, gnorm=1.451, clip=0, train_wall=59, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:22:56]    INFO >> epoch 001:   1100 / 5058 loss=0.058, ups=1.74, num_updates=1100, lr=0.0001, gnorm=2.202, clip=2, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:24:00]    INFO >> epoch 001:   1200 / 5058 loss=0.035, ups=1.62, num_updates=1200, lr=0.0001, gnorm=0.871, clip=0, train_wall=58, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:25:06]    INFO >> epoch 001:   1300 / 5058 loss=0.087, ups=1.59, num_updates=1300, lr=9.9e-05, gnorm=1.877, clip=2, train_wall=59, gb_free=67.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:26:10]    INFO >> epoch 001:   1400 / 5058 loss=0.059, ups=1.63, num_updates=1400, lr=9.9e-05, gnorm=2.488, clip=4, train_wall=58, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:27:15]    INFO >> epoch 001:   1500 / 5058 loss=0.279, ups=1.6, num_updates=1500, lr=9.9e-05, gnorm=1.776, clip=2, train_wall=59, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:28:17]    INFO >> epoch 001:   1600 / 5058 loss=0.11, ups=1.71, num_updates=1600, lr=9.9e-05, gnorm=3.243, clip=4, train_wall=55, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:29:23]    INFO >> epoch 001:   1700 / 5058 loss=0.113, ups=1.56, num_updates=1700, lr=9.9e-05, gnorm=2.319, clip=2, train_wall=60, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:30:19]    INFO >> epoch 001:   1800 / 5058 loss=0.12, ups=1.89, num_updates=1800, lr=9.8e-05, gnorm=3.115, clip=5, train_wall=50, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:30:58]    INFO >> epoch 001:   1900 / 5058 loss=0.153, ups=2.63, num_updates=1900, lr=9.8e-05, gnorm=1.952, clip=4, train_wall=36, gb_free=67.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:31:59]    INFO >> epoch 001:   2000 / 5058 loss=0.089, ups=1.67, num_updates=2000, lr=9.8e-05, gnorm=1.422, clip=1, train_wall=56, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:33:07]    INFO >> epoch 001:   2100 / 5058 loss=0.074, ups=1.56, num_updates=2100, lr=9.8e-05, gnorm=3.142, clip=2, train_wall=60, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:34:14]    INFO >> epoch 001:   2200 / 5058 loss=0.279, ups=1.56, num_updates=2200, lr=9.8e-05, gnorm=5.078, clip=6, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:35:22]    INFO >> epoch 001:   2300 / 5058 loss=0.068, ups=1.54, num_updates=2300, lr=9.7e-05, gnorm=2.452, clip=5, train_wall=61, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:36:26]    INFO >> epoch 001:   2400 / 5058 loss=0.063, ups=1.6, num_updates=2400, lr=9.7e-05, gnorm=0.773, clip=1, train_wall=59, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:37:37]    INFO >> epoch 001:   2500 / 5058 loss=0.149, ups=1.48, num_updates=2500, lr=9.7e-05, gnorm=3.718, clip=1, train_wall=63, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:38:35]    INFO >> epoch 001:   2600 / 5058 loss=0.128, ups=1.8, num_updates=2600, lr=9.7e-05, gnorm=2.608, clip=1, train_wall=52, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:39:35]    INFO >> epoch 001:   2700 / 5058 loss=0.076, ups=1.72, num_updates=2700, lr=9.7e-05, gnorm=1.387, clip=1, train_wall=55, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:40:38]    INFO >> epoch 001:   2800 / 5058 loss=0.081, ups=1.67, num_updates=2800, lr=9.6e-05, gnorm=1.32, clip=3, train_wall=56, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:41:18]    INFO >> epoch 001:   2900 / 5058 loss=0.274, ups=2.58, num_updates=2900, lr=9.6e-05, gnorm=2.005, clip=3, train_wall=37, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:42:24]    INFO >> epoch 001:   3000 / 5058 loss=0.145, ups=1.57, num_updates=3000, lr=9.6e-05, gnorm=3.103, clip=5, train_wall=59, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:43:19]    INFO >> epoch 001:   3100 / 5058 loss=0.058, ups=1.91, num_updates=3100, lr=9.6e-05, gnorm=0.282, clip=0, train_wall=49, gb_free=73.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:44:16]    INFO >> epoch 001:   3200 / 5058 loss=0.282, ups=1.8, num_updates=3200, lr=9.6e-05, gnorm=2.208, clip=2, train_wall=52, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:45:17]    INFO >> epoch 001:   3300 / 5058 loss=0.086, ups=1.71, num_updates=3300, lr=9.5e-05, gnorm=1.673, clip=1, train_wall=55, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:46:29]    INFO >> epoch 001:   3400 / 5058 loss=0.225, ups=1.45, num_updates=3400, lr=9.5e-05, gnorm=5.264, clip=3, train_wall=64, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:47:35]    INFO >> epoch 001:   3500 / 5058 loss=0.131, ups=1.57, num_updates=3500, lr=9.5e-05, gnorm=1.53, clip=0, train_wall=60, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:48:42]    INFO >> epoch 001:   3600 / 5058 loss=0.095, ups=1.57, num_updates=3600, lr=9.5e-05, gnorm=2.575, clip=1, train_wall=60, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:49:44]    INFO >> epoch 001:   3700 / 5058 loss=0.137, ups=1.69, num_updates=3700, lr=9.4e-05, gnorm=1.854, clip=2, train_wall=55, gb_free=66.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:50:51]    INFO >> epoch 001:   3800 / 5058 loss=0.058, ups=1.55, num_updates=3800, lr=9.4e-05, gnorm=1.191, clip=1, train_wall=60, gb_free=68.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:51:42]    INFO >> epoch 001:   3900 / 5058 loss=0.127, ups=2.05, num_updates=3900, lr=9.4e-05, gnorm=1.978, clip=2, train_wall=46, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:52:45]    INFO >> epoch 001:   4000 / 5058 loss=0.131, ups=1.66, num_updates=4000, lr=9.4e-05, gnorm=1.731, clip=1, train_wall=56, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:53:48]    INFO >> epoch 001:   4100 / 5058 loss=0.92, ups=1.64, num_updates=4100, lr=9.4e-05, gnorm=0.902, clip=1, train_wall=57, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:54:55]    INFO >> epoch 001:   4200 / 5058 loss=0.149, ups=1.54, num_updates=4200, lr=9.3e-05, gnorm=1.845, clip=3, train_wall=61, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:55:56]    INFO >> epoch 001:   4300 / 5058 loss=0.117, ups=1.73, num_updates=4300, lr=9.3e-05, gnorm=1.721, clip=1, train_wall=54, gb_free=73.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:56:58]    INFO >> epoch 001:   4400 / 5058 loss=0.082, ups=1.69, num_updates=4400, lr=9.3e-05, gnorm=1.476, clip=0, train_wall=55, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:58:01]    INFO >> epoch 001:   4500 / 5058 loss=0.072, ups=1.63, num_updates=4500, lr=9.3e-05, gnorm=1.371, clip=0, train_wall=57, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:59:08]    INFO >> epoch 001:   4600 / 5058 loss=0.241, ups=1.55, num_updates=4600, lr=9.3e-05, gnorm=1.805, clip=0, train_wall=60, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:00:20]    INFO >> epoch 001:   4700 / 5058 loss=0.215, ups=1.45, num_updates=4700, lr=9.2e-05, gnorm=3.803, clip=2, train_wall=64, gb_free=64.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:01:22]    INFO >> epoch 001:   4800 / 5058 loss=0.056, ups=1.68, num_updates=4800, lr=9.2e-05, gnorm=1.411, clip=0, train_wall=55, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:02:03]    INFO >> epoch 001:   4900 / 5058 loss=0.142, ups=2.5, num_updates=4900, lr=9.2e-05, gnorm=0.824, clip=0, train_wall=38, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:03:09]    INFO >> epoch 001:   5000 / 5058 loss=0.09, ups=1.59, num_updates=5000, lr=9.2e-05, gnorm=2.488, clip=0, train_wall=59, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:03:49]    INFO >> epoch 001 | loss 0.105 | ups 1.71 | num_updates 5058 | lr 9.2e-05 | gnorm 2.248 | clip 2.1 | train_wall 2771 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-20 02:03:49] WARNING >> 4469 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[2, 4, 10, 12, 13, 14, 16, 18, 19, 23] (data_utils.py:301, filter_by_size())[0m
[33m[2025-11-20 02:03:49] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
CodeEncoderLSTM: Creating BiLSTM with 6 layers, 512 hidden and input size
Traceback (most recent call last):
  File "run/type_prediction/transformer/train.py", line 339, in <module>
    cli_main()
  File "run/type_prediction/transformer/train.py", line 332, in cli_main
    single_main(args)
  File "run/type_prediction/transformer/train.py", line 254, in single_main
    valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)
  File "run/type_prediction/transformer/train.py", line 132, in validate
    trainer.valid_step(sample)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/trainers/ncc_trainers.py", line 565, in valid_step
    _loss, sample_size, logging_output = self.task.valid_step(
TypeError: cannot unpack non-iterable NoneType object

============================================================
Setting up experiment: exp_d_model_256
============================================================
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256/checkpoints
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256/logs
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256/results
âœ“ Saved config to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256/config.yml
âœ“ Also saved to: run/type_prediction/transformer/config/exp_d_model_256.yml
âœ“ Saved experiment info to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256/info.txt

============================================================
Starting training...
============================================================

Command: python run/type_prediction/transformer/train.py --language exp_d_model_256 2>&1 | tee /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256/logs/train.log

Note: Config file will be loaded from run/type_prediction/transformer/config/exp_d_model_256.yml


âœ“ Training completed successfully!

============================================================
Evaluating model on test set...
============================================================

Warning: Best checkpoint not found, using last checkpoint
âœ— No checkpoint found for evaluation

============================================================
âœ“ Experiment completed: exp_d_model_256
âœ“ Results in: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256
============================================================


âœ“ exp_d_model_256 completed in 0.86 hours

============================================================
Running: exp_d_model_512
GPU: 3
Time: 2025-11-20T02:03:52.566098
============================================================
[32m[2025-11-20 02:03:54]    INFO >> Load arguments in run/type_prediction/transformer/config/exp_d_model_512.yml (train.py:300, cli_main())[0m
[32m[2025-11-20 02:03:54]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_512/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_512/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_512/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:302, cli_main())[0m
[32m[2025-11-20 02:03:54]    INFO >> single GPU training... (train.py:331, cli_main())[0m
[32m[2025-11-20 02:03:54]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_512/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_512/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_512/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:208, single_main())[0m
[32m[2025-11-20 02:03:57]    INFO >> TypeTransformer(
  (decoder): RobertaEncoder(
    (encoder): CodeEncoderLSTM(
      (embedding): Embedding(10006, 512)
      (pos_encoder): PositionalEncoding()
      (encoder): LSTM(512, 512, num_layers=6, bidirectional=True)
    )
    (output): Sequential(
      (0): Linear(in_features=1024, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=10006, bias=True)
    )
  )
  (classification_heads): ModuleDict()
) (train.py:220, single_main())[0m
[32m[2025-11-20 02:03:57]    INFO >> model typetransformer, criterion TypePredictionCrossEntropyCriterion (train.py:221, single_main())[0m
[32m[2025-11-20 02:03:57]    INFO >> num. model params: 46481686 (num. trained: 46481686) (train.py:222, single_main())[0m
[32m[2025-11-20 02:03:57]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 02:03:57]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 38699 MB ; used memory = 43220 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 02:03:57]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 02:03:57]    INFO >> training on 1 GPUs (train.py:229, single_main())[0m
[32m[2025-11-20 02:03:57]    INFO >> max tokens per GPU = None and max sentences per GPU = 32 (train.py:230, single_main())[0m
[32m[2025-11-20 02:03:57]    INFO >> no existing checkpoint found /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_512/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-20 02:03:57]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[33m[2025-11-20 02:04:07] WARNING >> 32222 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[0, 1, 2, 4, 6, 7, 8, 10, 14, 19] (data_utils.py:301, filter_by_size())[0m
[32m[2025-11-20 02:04:07]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-20 02:04:08] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:398: UserWarning: ntokens not found in Criterion logging outputs, cannot log wpb or wps
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:407: UserWarning: nsentences not found in Criterion logging outputs, cannot log bsz
  warnings.warn(
[32m[2025-11-20 02:04:52]    INFO >> epoch 001:    100 / 5058 loss=1.903, ups=2.37, num_updates=100, lr=1e-05, gnorm=0.104, clip=0, train_wall=40, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:05:41]    INFO >> epoch 001:    200 / 5058 loss=0.786, ups=2.16, num_updates=200, lr=2e-05, gnorm=0.068, clip=0, train_wall=44, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:06:45]    INFO >> epoch 001:    300 / 5058 loss=0.654, ups=1.63, num_updates=300, lr=3e-05, gnorm=0.053, clip=0, train_wall=58, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:07:48]    INFO >> epoch 001:    400 / 5058 loss=0.623, ups=1.59, num_updates=400, lr=4e-05, gnorm=2.819, clip=2, train_wall=59, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:08:51]    INFO >> epoch 001:    500 / 5058 loss=0.208, ups=1.67, num_updates=500, lr=5e-05, gnorm=4.476, clip=2, train_wall=56, gb_free=65.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:09:52]    INFO >> epoch 001:    600 / 5058 loss=0.079, ups=1.71, num_updates=600, lr=6e-05, gnorm=4.788, clip=8, train_wall=55, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:10:35]    INFO >> epoch 001:    700 / 5058 loss=0.174, ups=2.43, num_updates=700, lr=7e-05, gnorm=2.641, clip=4, train_wall=39, gb_free=69.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:11:18]    INFO >> epoch 001:    800 / 5058 loss=0.166, ups=2.46, num_updates=800, lr=8e-05, gnorm=3.325, clip=8, train_wall=39, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:12:25]    INFO >> epoch 001:    900 / 5058 loss=0.216, ups=1.56, num_updates=900, lr=9e-05, gnorm=6.91, clip=8, train_wall=60, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:13:29]    INFO >> epoch 001:   1000 / 5058 loss=0.142, ups=1.62, num_updates=1000, lr=0.0001, gnorm=1.451, clip=0, train_wall=58, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:14:28]    INFO >> epoch 001:   1100 / 5058 loss=0.058, ups=1.76, num_updates=1100, lr=0.0001, gnorm=2.202, clip=2, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:15:31]    INFO >> epoch 001:   1200 / 5058 loss=0.035, ups=1.64, num_updates=1200, lr=0.0001, gnorm=0.871, clip=0, train_wall=57, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:16:36]    INFO >> epoch 001:   1300 / 5058 loss=0.087, ups=1.61, num_updates=1300, lr=9.9e-05, gnorm=1.877, clip=2, train_wall=58, gb_free=67.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:17:39]    INFO >> epoch 001:   1400 / 5058 loss=0.059, ups=1.65, num_updates=1400, lr=9.9e-05, gnorm=2.488, clip=4, train_wall=57, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:18:44]    INFO >> epoch 001:   1500 / 5058 loss=0.279, ups=1.62, num_updates=1500, lr=9.9e-05, gnorm=1.776, clip=2, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:19:45]    INFO >> epoch 001:   1600 / 5058 loss=0.11, ups=1.73, num_updates=1600, lr=9.9e-05, gnorm=3.243, clip=4, train_wall=55, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:20:51]    INFO >> epoch 001:   1700 / 5058 loss=0.113, ups=1.58, num_updates=1700, lr=9.9e-05, gnorm=2.319, clip=2, train_wall=59, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:21:44]    INFO >> epoch 001:   1800 / 5058 loss=0.12, ups=1.91, num_updates=1800, lr=9.8e-05, gnorm=3.115, clip=5, train_wall=49, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:22:25]    INFO >> epoch 001:   1900 / 5058 loss=0.153, ups=2.66, num_updates=1900, lr=9.8e-05, gnorm=1.952, clip=4, train_wall=36, gb_free=67.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:23:25]    INFO >> epoch 001:   2000 / 5058 loss=0.089, ups=1.69, num_updates=2000, lr=9.8e-05, gnorm=1.422, clip=1, train_wall=56, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:24:31]    INFO >> epoch 001:   2100 / 5058 loss=0.074, ups=1.58, num_updates=2100, lr=9.8e-05, gnorm=3.142, clip=2, train_wall=60, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:25:37]    INFO >> epoch 001:   2200 / 5058 loss=0.279, ups=1.57, num_updates=2200, lr=9.8e-05, gnorm=5.078, clip=6, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:26:44]    INFO >> epoch 001:   2300 / 5058 loss=0.068, ups=1.56, num_updates=2300, lr=9.7e-05, gnorm=2.452, clip=5, train_wall=60, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:27:49]    INFO >> epoch 001:   2400 / 5058 loss=0.063, ups=1.61, num_updates=2400, lr=9.7e-05, gnorm=0.773, clip=1, train_wall=58, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:28:58]    INFO >> epoch 001:   2500 / 5058 loss=0.149, ups=1.5, num_updates=2500, lr=9.7e-05, gnorm=3.718, clip=1, train_wall=63, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:29:56]    INFO >> epoch 001:   2600 / 5058 loss=0.128, ups=1.82, num_updates=2600, lr=9.7e-05, gnorm=2.608, clip=1, train_wall=52, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:30:56]    INFO >> epoch 001:   2700 / 5058 loss=0.076, ups=1.74, num_updates=2700, lr=9.7e-05, gnorm=1.387, clip=1, train_wall=54, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:31:58]    INFO >> epoch 001:   2800 / 5058 loss=0.081, ups=1.69, num_updates=2800, lr=9.6e-05, gnorm=1.32, clip=3, train_wall=56, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:32:38]    INFO >> epoch 001:   2900 / 5058 loss=0.274, ups=2.61, num_updates=2900, lr=9.6e-05, gnorm=2.005, clip=3, train_wall=36, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:33:43]    INFO >> epoch 001:   3000 / 5058 loss=0.145, ups=1.59, num_updates=3000, lr=9.6e-05, gnorm=3.103, clip=5, train_wall=59, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:34:36]    INFO >> epoch 001:   3100 / 5058 loss=0.058, ups=1.94, num_updates=3100, lr=9.6e-05, gnorm=0.282, clip=0, train_wall=49, gb_free=73.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:35:34]    INFO >> epoch 001:   3200 / 5058 loss=0.282, ups=1.82, num_updates=3200, lr=9.6e-05, gnorm=2.208, clip=2, train_wall=52, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:36:34]    INFO >> epoch 001:   3300 / 5058 loss=0.086, ups=1.74, num_updates=3300, lr=9.5e-05, gnorm=1.673, clip=1, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:37:45]    INFO >> epoch 001:   3400 / 5058 loss=0.225, ups=1.47, num_updates=3400, lr=9.5e-05, gnorm=5.264, clip=3, train_wall=64, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:38:50]    INFO >> epoch 001:   3500 / 5058 loss=0.131, ups=1.59, num_updates=3500, lr=9.5e-05, gnorm=1.53, clip=0, train_wall=59, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:39:55]    INFO >> epoch 001:   3600 / 5058 loss=0.095, ups=1.59, num_updates=3600, lr=9.5e-05, gnorm=2.575, clip=1, train_wall=59, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:40:56]    INFO >> epoch 001:   3700 / 5058 loss=0.137, ups=1.72, num_updates=3700, lr=9.4e-05, gnorm=1.854, clip=2, train_wall=55, gb_free=66.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:42:03]    INFO >> epoch 001:   3800 / 5058 loss=0.058, ups=1.57, num_updates=3800, lr=9.4e-05, gnorm=1.191, clip=1, train_wall=60, gb_free=68.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:42:52]    INFO >> epoch 001:   3900 / 5058 loss=0.127, ups=2.08, num_updates=3900, lr=9.4e-05, gnorm=1.978, clip=2, train_wall=45, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:43:54]    INFO >> epoch 001:   4000 / 5058 loss=0.131, ups=1.69, num_updates=4000, lr=9.4e-05, gnorm=1.731, clip=1, train_wall=56, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:44:57]    INFO >> epoch 001:   4100 / 5058 loss=0.92, ups=1.66, num_updates=4100, lr=9.4e-05, gnorm=0.902, clip=1, train_wall=57, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:46:02]    INFO >> epoch 001:   4200 / 5058 loss=0.149, ups=1.57, num_updates=4200, lr=9.3e-05, gnorm=1.845, clip=3, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:47:01]    INFO >> epoch 001:   4300 / 5058 loss=0.117, ups=1.77, num_updates=4300, lr=9.3e-05, gnorm=1.721, clip=1, train_wall=54, gb_free=73.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:48:02]    INFO >> epoch 001:   4400 / 5058 loss=0.082, ups=1.72, num_updates=4400, lr=9.3e-05, gnorm=1.476, clip=0, train_wall=55, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:49:05]    INFO >> epoch 001:   4500 / 5058 loss=0.072, ups=1.66, num_updates=4500, lr=9.3e-05, gnorm=1.371, clip=0, train_wall=57, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:50:11]    INFO >> epoch 001:   4600 / 5058 loss=0.241, ups=1.58, num_updates=4600, lr=9.3e-05, gnorm=1.805, clip=0, train_wall=60, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:51:21]    INFO >> epoch 001:   4700 / 5058 loss=0.215, ups=1.48, num_updates=4700, lr=9.2e-05, gnorm=3.803, clip=2, train_wall=64, gb_free=64.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:52:22]    INFO >> epoch 001:   4800 / 5058 loss=0.056, ups=1.71, num_updates=4800, lr=9.2e-05, gnorm=1.411, clip=0, train_wall=55, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:53:03]    INFO >> epoch 001:   4900 / 5058 loss=0.142, ups=2.54, num_updates=4900, lr=9.2e-05, gnorm=0.824, clip=0, train_wall=37, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:54:07]    INFO >> epoch 001:   5000 / 5058 loss=0.09, ups=1.62, num_updates=5000, lr=9.2e-05, gnorm=2.488, clip=0, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:54:47]    INFO >> epoch 001 | loss 0.105 | ups 1.73 | num_updates 5058 | lr 9.2e-05 | gnorm 2.248 | clip 2.1 | train_wall 2753 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-20 02:54:47] WARNING >> 4469 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[2, 4, 10, 12, 13, 14, 16, 18, 19, 23] (data_utils.py:301, filter_by_size())[0m
[33m[2025-11-20 02:54:47] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
CodeEncoderLSTM: Creating BiLSTM with 6 layers, 512 hidden and input size
Traceback (most recent call last):
  File "run/type_prediction/transformer/train.py", line 339, in <module>
    cli_main()
  File "run/type_prediction/transformer/train.py", line 332, in cli_main
    single_main(args)
  File "run/type_prediction/transformer/train.py", line 254, in single_main
    valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)
  File "run/type_prediction/transformer/train.py", line 132, in validate
    trainer.valid_step(sample)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/trainers/ncc_trainers.py", line 565, in valid_step
    _loss, sample_size, logging_output = self.task.valid_step(
TypeError: cannot unpack non-iterable NoneType object

============================================================
Setting up experiment: exp_d_model_512
============================================================
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_512
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_512/checkpoints
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_512/logs
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_512/results
âœ“ Saved config to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_512/config.yml
âœ“ Also saved to: run/type_prediction/transformer/config/exp_d_model_512.yml
âœ“ Saved experiment info to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_512/info.txt

============================================================
Starting training...
============================================================

Command: python run/type_prediction/transformer/train.py --language exp_d_model_512 2>&1 | tee /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_512/logs/train.log

Note: Config file will be loaded from run/type_prediction/transformer/config/exp_d_model_512.yml


âœ“ Training completed successfully!

============================================================
Evaluating model on test set...
============================================================

Warning: Best checkpoint not found, using last checkpoint
âœ— No checkpoint found for evaluation

============================================================
âœ“ Experiment completed: exp_d_model_512
âœ“ Results in: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_512
============================================================


âœ“ exp_d_model_512 completed in 0.85 hours

============================================================
Running: exp_d_model_1024
GPU: 3
Time: 2025-11-20T02:54:50.448474
============================================================
[32m[2025-11-20 02:54:52]    INFO >> Load arguments in run/type_prediction/transformer/config/exp_d_model_1024.yml (train.py:300, cli_main())[0m
[32m[2025-11-20 02:54:52]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_1024/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_1024/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_1024/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:302, cli_main())[0m
[32m[2025-11-20 02:54:52]    INFO >> single GPU training... (train.py:331, cli_main())[0m
[32m[2025-11-20 02:54:53]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_1024/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_1024/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_1024/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:208, single_main())[0m
[32m[2025-11-20 02:54:55]    INFO >> TypeTransformer(
  (decoder): RobertaEncoder(
    (encoder): CodeEncoderLSTM(
      (embedding): Embedding(10006, 512)
      (pos_encoder): PositionalEncoding()
      (encoder): LSTM(512, 512, num_layers=6, bidirectional=True)
    )
    (output): Sequential(
      (0): Linear(in_features=1024, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=10006, bias=True)
    )
  )
  (classification_heads): ModuleDict()
) (train.py:220, single_main())[0m
[32m[2025-11-20 02:54:55]    INFO >> model typetransformer, criterion TypePredictionCrossEntropyCriterion (train.py:221, single_main())[0m
[32m[2025-11-20 02:54:55]    INFO >> num. model params: 46481686 (num. trained: 46481686) (train.py:222, single_main())[0m
[32m[2025-11-20 02:54:55]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 02:54:55]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 38699 MB ; used memory = 43220 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 02:54:55]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 02:54:55]    INFO >> training on 1 GPUs (train.py:229, single_main())[0m
[32m[2025-11-20 02:54:55]    INFO >> max tokens per GPU = None and max sentences per GPU = 32 (train.py:230, single_main())[0m
[32m[2025-11-20 02:54:55]    INFO >> no existing checkpoint found /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_1024/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-20 02:54:55]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[33m[2025-11-20 02:55:05] WARNING >> 32222 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[0, 1, 2, 4, 6, 7, 8, 10, 14, 19] (data_utils.py:301, filter_by_size())[0m
[32m[2025-11-20 02:55:05]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-20 02:55:05] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:398: UserWarning: ntokens not found in Criterion logging outputs, cannot log wpb or wps
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:407: UserWarning: nsentences not found in Criterion logging outputs, cannot log bsz
  warnings.warn(
[32m[2025-11-20 02:55:50]    INFO >> epoch 001:    100 / 5058 loss=1.903, ups=2.37, num_updates=100, lr=1e-05, gnorm=0.104, clip=0, train_wall=40, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:56:38]    INFO >> epoch 001:    200 / 5058 loss=0.786, ups=2.16, num_updates=200, lr=2e-05, gnorm=0.068, clip=0, train_wall=44, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:57:43]    INFO >> epoch 001:    300 / 5058 loss=0.654, ups=1.63, num_updates=300, lr=3e-05, gnorm=0.053, clip=0, train_wall=58, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:58:48]    INFO >> epoch 001:    400 / 5058 loss=0.623, ups=1.59, num_updates=400, lr=4e-05, gnorm=2.819, clip=2, train_wall=59, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:59:49]    INFO >> epoch 001:    500 / 5058 loss=0.208, ups=1.67, num_updates=500, lr=5e-05, gnorm=4.476, clip=2, train_wall=56, gb_free=65.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:00:50]    INFO >> epoch 001:    600 / 5058 loss=0.079, ups=1.71, num_updates=600, lr=6e-05, gnorm=4.788, clip=8, train_wall=55, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:01:33]    INFO >> epoch 001:    700 / 5058 loss=0.174, ups=2.43, num_updates=700, lr=7e-05, gnorm=2.641, clip=4, train_wall=39, gb_free=69.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:02:16]    INFO >> epoch 001:    800 / 5058 loss=0.166, ups=2.46, num_updates=800, lr=8e-05, gnorm=3.325, clip=8, train_wall=39, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:03:23]    INFO >> epoch 001:    900 / 5058 loss=0.216, ups=1.56, num_updates=900, lr=9e-05, gnorm=6.91, clip=8, train_wall=60, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:04:27]    INFO >> epoch 001:   1000 / 5058 loss=0.142, ups=1.62, num_updates=1000, lr=0.0001, gnorm=1.451, clip=0, train_wall=58, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:05:26]    INFO >> epoch 001:   1100 / 5058 loss=0.058, ups=1.76, num_updates=1100, lr=0.0001, gnorm=2.202, clip=2, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:06:29]    INFO >> epoch 001:   1200 / 5058 loss=0.035, ups=1.64, num_updates=1200, lr=0.0001, gnorm=0.871, clip=0, train_wall=57, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:07:34]    INFO >> epoch 001:   1300 / 5058 loss=0.087, ups=1.61, num_updates=1300, lr=9.9e-05, gnorm=1.877, clip=2, train_wall=58, gb_free=67.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:08:38]    INFO >> epoch 001:   1400 / 5058 loss=0.059, ups=1.65, num_updates=1400, lr=9.9e-05, gnorm=2.488, clip=4, train_wall=57, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:09:42]    INFO >> epoch 001:   1500 / 5058 loss=0.279, ups=1.61, num_updates=1500, lr=9.9e-05, gnorm=1.776, clip=2, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:10:43]    INFO >> epoch 001:   1600 / 5058 loss=0.11, ups=1.73, num_updates=1600, lr=9.9e-05, gnorm=3.243, clip=4, train_wall=55, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:11:49]    INFO >> epoch 001:   1700 / 5058 loss=0.113, ups=1.58, num_updates=1700, lr=9.9e-05, gnorm=2.319, clip=2, train_wall=59, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:12:42]    INFO >> epoch 001:   1800 / 5058 loss=0.12, ups=1.91, num_updates=1800, lr=9.8e-05, gnorm=3.115, clip=5, train_wall=49, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:13:21]    INFO >> epoch 001:   1900 / 5058 loss=0.153, ups=2.66, num_updates=1900, lr=9.8e-05, gnorm=1.952, clip=4, train_wall=36, gb_free=67.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:14:23]    INFO >> epoch 001:   2000 / 5058 loss=0.089, ups=1.69, num_updates=2000, lr=9.8e-05, gnorm=1.422, clip=1, train_wall=56, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:15:29]    INFO >> epoch 001:   2100 / 5058 loss=0.074, ups=1.58, num_updates=2100, lr=9.8e-05, gnorm=3.142, clip=2, train_wall=59, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:16:35]    INFO >> epoch 001:   2200 / 5058 loss=0.279, ups=1.58, num_updates=2200, lr=9.8e-05, gnorm=5.078, clip=6, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:17:42]    INFO >> epoch 001:   2300 / 5058 loss=0.068, ups=1.56, num_updates=2300, lr=9.7e-05, gnorm=2.452, clip=5, train_wall=60, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:18:47]    INFO >> epoch 001:   2400 / 5058 loss=0.063, ups=1.62, num_updates=2400, lr=9.7e-05, gnorm=0.773, clip=1, train_wall=58, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:19:56]    INFO >> epoch 001:   2500 / 5058 loss=0.149, ups=1.5, num_updates=2500, lr=9.7e-05, gnorm=3.718, clip=1, train_wall=62, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:20:54]    INFO >> epoch 001:   2600 / 5058 loss=0.128, ups=1.81, num_updates=2600, lr=9.7e-05, gnorm=2.608, clip=1, train_wall=52, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:21:54]    INFO >> epoch 001:   2700 / 5058 loss=0.076, ups=1.74, num_updates=2700, lr=9.7e-05, gnorm=1.387, clip=1, train_wall=54, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:22:56]    INFO >> epoch 001:   2800 / 5058 loss=0.081, ups=1.69, num_updates=2800, lr=9.6e-05, gnorm=1.32, clip=3, train_wall=56, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:23:36]    INFO >> epoch 001:   2900 / 5058 loss=0.274, ups=2.61, num_updates=2900, lr=9.6e-05, gnorm=2.005, clip=3, train_wall=36, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:24:41]    INFO >> epoch 001:   3000 / 5058 loss=0.145, ups=1.59, num_updates=3000, lr=9.6e-05, gnorm=3.103, clip=5, train_wall=59, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:25:34]    INFO >> epoch 001:   3100 / 5058 loss=0.058, ups=1.94, num_updates=3100, lr=9.6e-05, gnorm=0.282, clip=0, train_wall=49, gb_free=73.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:26:31]    INFO >> epoch 001:   3200 / 5058 loss=0.282, ups=1.82, num_updates=3200, lr=9.6e-05, gnorm=2.208, clip=2, train_wall=52, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:27:32]    INFO >> epoch 001:   3300 / 5058 loss=0.086, ups=1.74, num_updates=3300, lr=9.5e-05, gnorm=1.673, clip=1, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:28:42]    INFO >> epoch 001:   3400 / 5058 loss=0.225, ups=1.47, num_updates=3400, lr=9.5e-05, gnorm=5.264, clip=3, train_wall=64, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:29:48]    INFO >> epoch 001:   3500 / 5058 loss=0.131, ups=1.6, num_updates=3500, lr=9.5e-05, gnorm=1.53, clip=0, train_wall=59, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:30:53]    INFO >> epoch 001:   3600 / 5058 loss=0.095, ups=1.6, num_updates=3600, lr=9.5e-05, gnorm=2.575, clip=1, train_wall=59, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:31:54]    INFO >> epoch 001:   3700 / 5058 loss=0.137, ups=1.73, num_updates=3700, lr=9.4e-05, gnorm=1.854, clip=2, train_wall=55, gb_free=66.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:32:59]    INFO >> epoch 001:   3800 / 5058 loss=0.058, ups=1.57, num_updates=3800, lr=9.4e-05, gnorm=1.191, clip=1, train_wall=60, gb_free=68.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:33:50]    INFO >> epoch 001:   3900 / 5058 loss=0.127, ups=2.08, num_updates=3900, lr=9.4e-05, gnorm=1.978, clip=2, train_wall=45, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:34:51]    INFO >> epoch 001:   4000 / 5058 loss=0.131, ups=1.69, num_updates=4000, lr=9.4e-05, gnorm=1.731, clip=1, train_wall=56, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:35:54]    INFO >> epoch 001:   4100 / 5058 loss=0.92, ups=1.66, num_updates=4100, lr=9.4e-05, gnorm=0.902, clip=1, train_wall=57, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:37:01]    INFO >> epoch 001:   4200 / 5058 loss=0.149, ups=1.56, num_updates=4200, lr=9.3e-05, gnorm=1.845, clip=3, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:38:00]    INFO >> epoch 001:   4300 / 5058 loss=0.117, ups=1.74, num_updates=4300, lr=9.3e-05, gnorm=1.721, clip=1, train_wall=53, gb_free=73.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:39:01]    INFO >> epoch 001:   4400 / 5058 loss=0.082, ups=1.7, num_updates=4400, lr=9.3e-05, gnorm=1.476, clip=0, train_wall=55, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:40:05]    INFO >> epoch 001:   4500 / 5058 loss=0.072, ups=1.64, num_updates=4500, lr=9.3e-05, gnorm=1.371, clip=0, train_wall=57, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:41:11]    INFO >> epoch 001:   4600 / 5058 loss=0.241, ups=1.56, num_updates=4600, lr=9.3e-05, gnorm=1.805, clip=0, train_wall=59, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:42:23]    INFO >> epoch 001:   4700 / 5058 loss=0.215, ups=1.45, num_updates=4700, lr=9.2e-05, gnorm=3.803, clip=2, train_wall=63, gb_free=64.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:43:26]    INFO >> epoch 001:   4800 / 5058 loss=0.056, ups=1.67, num_updates=4800, lr=9.2e-05, gnorm=1.411, clip=0, train_wall=55, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:44:07]    INFO >> epoch 001:   4900 / 5058 loss=0.142, ups=2.48, num_updates=4900, lr=9.2e-05, gnorm=0.824, clip=0, train_wall=37, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:45:13]    INFO >> epoch 001:   5000 / 5058 loss=0.09, ups=1.58, num_updates=5000, lr=9.2e-05, gnorm=2.488, clip=0, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:45:55]    INFO >> epoch 001 | loss 0.105 | ups 1.73 | num_updates 5058 | lr 9.2e-05 | gnorm 2.248 | clip 2.1 | train_wall 2748 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-20 03:45:55] WARNING >> 4469 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[2, 4, 10, 12, 13, 14, 16, 18, 19, 23] (data_utils.py:301, filter_by_size())[0m
[33m[2025-11-20 03:45:55] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
CodeEncoderLSTM: Creating BiLSTM with 6 layers, 512 hidden and input size
Traceback (most recent call last):
  File "run/type_prediction/transformer/train.py", line 339, in <module>
    cli_main()
  File "run/type_prediction/transformer/train.py", line 332, in cli_main
    single_main(args)
  File "run/type_prediction/transformer/train.py", line 254, in single_main
    valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)
  File "run/type_prediction/transformer/train.py", line 132, in validate
    trainer.valid_step(sample)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/trainers/ncc_trainers.py", line 565, in valid_step
    _loss, sample_size, logging_output = self.task.valid_step(
TypeError: cannot unpack non-iterable NoneType object

============================================================
Setting up experiment: exp_d_model_1024
============================================================
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_1024
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_1024/checkpoints
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_1024/logs
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_1024/results
âœ“ Saved config to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_1024/config.yml
âœ“ Also saved to: run/type_prediction/transformer/config/exp_d_model_1024.yml
âœ“ Saved experiment info to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_1024/info.txt

============================================================
Starting training...
============================================================

Command: python run/type_prediction/transformer/train.py --language exp_d_model_1024 2>&1 | tee /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_1024/logs/train.log

Note: Config file will be loaded from run/type_prediction/transformer/config/exp_d_model_1024.yml


âœ“ Training completed successfully!

============================================================
Evaluating model on test set...
============================================================

Warning: Best checkpoint not found, using last checkpoint
âœ— No checkpoint found for evaluation

============================================================
âœ“ Experiment completed: exp_d_model_1024
âœ“ Results in: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_1024
============================================================


âœ“ exp_d_model_1024 completed in 0.85 hours

============================================================
Running: exp_layers_4
GPU: 3
Time: 2025-11-20T03:45:58.786184
============================================================
[32m[2025-11-20 03:46:00]    INFO >> Load arguments in run/type_prediction/transformer/config/exp_layers_4.yml (train.py:300, cli_main())[0m
[32m[2025-11-20 03:46:00]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_4/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 4, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_4/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_4/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:302, cli_main())[0m
[32m[2025-11-20 03:46:00]    INFO >> single GPU training... (train.py:331, cli_main())[0m
[32m[2025-11-20 03:46:00]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_4/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 4, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_4/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_4/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:208, single_main())[0m
[32m[2025-11-20 03:46:02]    INFO >> TypeTransformer(
  (decoder): RobertaEncoder(
    (encoder): CodeEncoderLSTM(
      (embedding): Embedding(10006, 512)
      (pos_encoder): PositionalEncoding()
      (encoder): LSTM(512, 512, num_layers=6, bidirectional=True)
    )
    (output): Sequential(
      (0): Linear(in_features=1024, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=10006, bias=True)
    )
  )
  (classification_heads): ModuleDict()
) (train.py:220, single_main())[0m
[32m[2025-11-20 03:46:02]    INFO >> model typetransformer, criterion TypePredictionCrossEntropyCriterion (train.py:221, single_main())[0m
[32m[2025-11-20 03:46:02]    INFO >> num. model params: 46481686 (num. trained: 46481686) (train.py:222, single_main())[0m
[32m[2025-11-20 03:46:02]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 03:46:02]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 38699 MB ; used memory = 43220 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 03:46:02]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 03:46:02]    INFO >> training on 1 GPUs (train.py:229, single_main())[0m
[32m[2025-11-20 03:46:02]    INFO >> max tokens per GPU = None and max sentences per GPU = 32 (train.py:230, single_main())[0m
[32m[2025-11-20 03:46:02]    INFO >> no existing checkpoint found /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_4/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-20 03:46:02]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[33m[2025-11-20 03:46:14] WARNING >> 32222 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[0, 1, 2, 4, 6, 7, 8, 10, 14, 19] (data_utils.py:301, filter_by_size())[0m
[32m[2025-11-20 03:46:14]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-20 03:46:15] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:398: UserWarning: ntokens not found in Criterion logging outputs, cannot log wpb or wps
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:407: UserWarning: nsentences not found in Criterion logging outputs, cannot log bsz
  warnings.warn(
[32m[2025-11-20 03:46:59]    INFO >> epoch 001:    100 / 5058 loss=1.903, ups=2.35, num_updates=100, lr=1e-05, gnorm=0.104, clip=0, train_wall=40, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:47:49]    INFO >> epoch 001:    200 / 5058 loss=0.786, ups=2.14, num_updates=200, lr=2e-05, gnorm=0.068, clip=0, train_wall=44, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:48:53]    INFO >> epoch 001:    300 / 5058 loss=0.654, ups=1.62, num_updates=300, lr=3e-05, gnorm=0.053, clip=0, train_wall=58, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:49:59]    INFO >> epoch 001:    400 / 5058 loss=0.623, ups=1.58, num_updates=400, lr=4e-05, gnorm=2.819, clip=2, train_wall=59, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:51:02]    INFO >> epoch 001:    500 / 5058 loss=0.208, ups=1.66, num_updates=500, lr=5e-05, gnorm=4.476, clip=2, train_wall=56, gb_free=65.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:52:04]    INFO >> epoch 001:    600 / 5058 loss=0.079, ups=1.69, num_updates=600, lr=6e-05, gnorm=4.788, clip=8, train_wall=55, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:52:47]    INFO >> epoch 001:    700 / 5058 loss=0.174, ups=2.4, num_updates=700, lr=7e-05, gnorm=2.641, clip=4, train_wall=39, gb_free=69.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:53:29]    INFO >> epoch 001:    800 / 5058 loss=0.166, ups=2.44, num_updates=800, lr=8e-05, gnorm=3.325, clip=8, train_wall=39, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:54:37]    INFO >> epoch 001:    900 / 5058 loss=0.216, ups=1.54, num_updates=900, lr=9e-05, gnorm=6.91, clip=8, train_wall=60, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:55:42]    INFO >> epoch 001:   1000 / 5058 loss=0.142, ups=1.6, num_updates=1000, lr=0.0001, gnorm=1.451, clip=0, train_wall=58, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:56:42]    INFO >> epoch 001:   1100 / 5058 loss=0.058, ups=1.74, num_updates=1100, lr=0.0001, gnorm=2.202, clip=2, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:57:46]    INFO >> epoch 001:   1200 / 5058 loss=0.035, ups=1.62, num_updates=1200, lr=0.0001, gnorm=0.871, clip=0, train_wall=58, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:58:52]    INFO >> epoch 001:   1300 / 5058 loss=0.087, ups=1.59, num_updates=1300, lr=9.9e-05, gnorm=1.877, clip=2, train_wall=59, gb_free=67.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:59:56]    INFO >> epoch 001:   1400 / 5058 loss=0.059, ups=1.62, num_updates=1400, lr=9.9e-05, gnorm=2.488, clip=4, train_wall=57, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:01:01]    INFO >> epoch 001:   1500 / 5058 loss=0.279, ups=1.6, num_updates=1500, lr=9.9e-05, gnorm=1.776, clip=2, train_wall=59, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:02:01]    INFO >> epoch 001:   1600 / 5058 loss=0.11, ups=1.71, num_updates=1600, lr=9.9e-05, gnorm=3.243, clip=4, train_wall=55, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:03:08]    INFO >> epoch 001:   1700 / 5058 loss=0.113, ups=1.57, num_updates=1700, lr=9.9e-05, gnorm=2.319, clip=2, train_wall=60, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:04:03]    INFO >> epoch 001:   1800 / 5058 loss=0.12, ups=1.89, num_updates=1800, lr=9.8e-05, gnorm=3.115, clip=5, train_wall=50, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:04:43]    INFO >> epoch 001:   1900 / 5058 loss=0.153, ups=2.63, num_updates=1900, lr=9.8e-05, gnorm=1.952, clip=4, train_wall=36, gb_free=67.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:05:45]    INFO >> epoch 001:   2000 / 5058 loss=0.089, ups=1.67, num_updates=2000, lr=9.8e-05, gnorm=1.422, clip=1, train_wall=56, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:06:52]    INFO >> epoch 001:   2100 / 5058 loss=0.074, ups=1.55, num_updates=2100, lr=9.8e-05, gnorm=3.142, clip=2, train_wall=60, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:08:00]    INFO >> epoch 001:   2200 / 5058 loss=0.279, ups=1.55, num_updates=2200, lr=9.8e-05, gnorm=5.078, clip=6, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:09:08]    INFO >> epoch 001:   2300 / 5058 loss=0.068, ups=1.53, num_updates=2300, lr=9.7e-05, gnorm=2.452, clip=5, train_wall=61, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:10:13]    INFO >> epoch 001:   2400 / 5058 loss=0.063, ups=1.59, num_updates=2400, lr=9.7e-05, gnorm=0.773, clip=1, train_wall=59, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:11:24]    INFO >> epoch 001:   2500 / 5058 loss=0.149, ups=1.48, num_updates=2500, lr=9.7e-05, gnorm=3.718, clip=1, train_wall=63, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:12:22]    INFO >> epoch 001:   2600 / 5058 loss=0.128, ups=1.79, num_updates=2600, lr=9.7e-05, gnorm=2.608, clip=1, train_wall=52, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:13:23]    INFO >> epoch 001:   2700 / 5058 loss=0.076, ups=1.71, num_updates=2700, lr=9.7e-05, gnorm=1.387, clip=1, train_wall=54, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:14:25]    INFO >> epoch 001:   2800 / 5058 loss=0.081, ups=1.66, num_updates=2800, lr=9.6e-05, gnorm=1.32, clip=3, train_wall=56, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:15:07]    INFO >> epoch 001:   2900 / 5058 loss=0.274, ups=2.57, num_updates=2900, lr=9.6e-05, gnorm=2.005, clip=3, train_wall=37, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:16:13]    INFO >> epoch 001:   3000 / 5058 loss=0.145, ups=1.57, num_updates=3000, lr=9.6e-05, gnorm=3.103, clip=5, train_wall=59, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:17:07]    INFO >> epoch 001:   3100 / 5058 loss=0.058, ups=1.91, num_updates=3100, lr=9.6e-05, gnorm=0.282, clip=0, train_wall=49, gb_free=73.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:18:05]    INFO >> epoch 001:   3200 / 5058 loss=0.282, ups=1.79, num_updates=3200, lr=9.6e-05, gnorm=2.208, clip=2, train_wall=52, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:19:06]    INFO >> epoch 001:   3300 / 5058 loss=0.086, ups=1.71, num_updates=3300, lr=9.5e-05, gnorm=1.673, clip=1, train_wall=55, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:20:18]    INFO >> epoch 001:   3400 / 5058 loss=0.225, ups=1.44, num_updates=3400, lr=9.5e-05, gnorm=5.264, clip=3, train_wall=64, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:21:25]    INFO >> epoch 001:   3500 / 5058 loss=0.131, ups=1.57, num_updates=3500, lr=9.5e-05, gnorm=1.53, clip=0, train_wall=59, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:22:31]    INFO >> epoch 001:   3600 / 5058 loss=0.095, ups=1.57, num_updates=3600, lr=9.5e-05, gnorm=2.575, clip=1, train_wall=59, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:23:33]    INFO >> epoch 001:   3700 / 5058 loss=0.137, ups=1.7, num_updates=3700, lr=9.4e-05, gnorm=1.854, clip=2, train_wall=55, gb_free=66.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:24:40]    INFO >> epoch 001:   3800 / 5058 loss=0.058, ups=1.54, num_updates=3800, lr=9.4e-05, gnorm=1.191, clip=1, train_wall=60, gb_free=68.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:25:30]    INFO >> epoch 001:   3900 / 5058 loss=0.127, ups=2.06, num_updates=3900, lr=9.4e-05, gnorm=1.978, clip=2, train_wall=46, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:26:33]    INFO >> epoch 001:   4000 / 5058 loss=0.131, ups=1.66, num_updates=4000, lr=9.4e-05, gnorm=1.731, clip=1, train_wall=56, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:27:37]    INFO >> epoch 001:   4100 / 5058 loss=0.92, ups=1.64, num_updates=4100, lr=9.4e-05, gnorm=0.902, clip=1, train_wall=57, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:28:45]    INFO >> epoch 001:   4200 / 5058 loss=0.149, ups=1.54, num_updates=4200, lr=9.3e-05, gnorm=1.845, clip=3, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:29:45]    INFO >> epoch 001:   4300 / 5058 loss=0.117, ups=1.74, num_updates=4300, lr=9.3e-05, gnorm=1.721, clip=1, train_wall=54, gb_free=73.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:30:47]    INFO >> epoch 001:   4400 / 5058 loss=0.082, ups=1.69, num_updates=4400, lr=9.3e-05, gnorm=1.476, clip=0, train_wall=55, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:31:51]    INFO >> epoch 001:   4500 / 5058 loss=0.072, ups=1.63, num_updates=4500, lr=9.3e-05, gnorm=1.371, clip=0, train_wall=57, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:32:56]    INFO >> epoch 001:   4600 / 5058 loss=0.241, ups=1.55, num_updates=4600, lr=9.3e-05, gnorm=1.805, clip=0, train_wall=60, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:34:09]    INFO >> epoch 001:   4700 / 5058 loss=0.215, ups=1.45, num_updates=4700, lr=9.2e-05, gnorm=3.803, clip=2, train_wall=64, gb_free=64.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:35:10]    INFO >> epoch 001:   4800 / 5058 loss=0.056, ups=1.68, num_updates=4800, lr=9.2e-05, gnorm=1.411, clip=0, train_wall=55, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:35:52]    INFO >> epoch 001:   4900 / 5058 loss=0.142, ups=2.51, num_updates=4900, lr=9.2e-05, gnorm=0.824, clip=0, train_wall=38, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:36:58]    INFO >> epoch 001:   5000 / 5058 loss=0.09, ups=1.59, num_updates=5000, lr=9.2e-05, gnorm=2.488, clip=0, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:37:38]    INFO >> epoch 001 | loss 0.105 | ups 1.71 | num_updates 5058 | lr 9.2e-05 | gnorm 2.248 | clip 2.1 | train_wall 2762 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-20 04:37:38] WARNING >> 4469 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[2, 4, 10, 12, 13, 14, 16, 18, 19, 23] (data_utils.py:301, filter_by_size())[0m
[33m[2025-11-20 04:37:38] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
CodeEncoderLSTM: Creating BiLSTM with 6 layers, 512 hidden and input size
Traceback (most recent call last):
  File "run/type_prediction/transformer/train.py", line 339, in <module>
    cli_main()
  File "run/type_prediction/transformer/train.py", line 332, in cli_main
    single_main(args)
  File "run/type_prediction/transformer/train.py", line 254, in single_main
    valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)
  File "run/type_prediction/transformer/train.py", line 132, in validate
    trainer.valid_step(sample)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/trainers/ncc_trainers.py", line 565, in valid_step
    _loss, sample_size, logging_output = self.task.valid_step(
TypeError: cannot unpack non-iterable NoneType object

============================================================
Setting up experiment: exp_layers_4
============================================================
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_4
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_4/checkpoints
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_4/logs
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_4/results
âœ“ Saved config to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_4/config.yml
âœ“ Also saved to: run/type_prediction/transformer/config/exp_layers_4.yml
âœ“ Saved experiment info to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_4/info.txt

============================================================
Starting training...
============================================================

Command: python run/type_prediction/transformer/train.py --language exp_layers_4 2>&1 | tee /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_4/logs/train.log

Note: Config file will be loaded from run/type_prediction/transformer/config/exp_layers_4.yml


âœ“ Training completed successfully!

============================================================
Evaluating model on test set...
============================================================

Warning: Best checkpoint not found, using last checkpoint
âœ— No checkpoint found for evaluation

============================================================
âœ“ Experiment completed: exp_layers_4
âœ“ Results in: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_4
============================================================


âœ“ exp_layers_4 completed in 0.86 hours

============================================================
Running: exp_layers_6
GPU: 3
Time: 2025-11-20T04:37:41.527633
============================================================
[32m[2025-11-20 04:37:43]    INFO >> Load arguments in run/type_prediction/transformer/config/exp_layers_6.yml (train.py:300, cli_main())[0m
[32m[2025-11-20 04:37:43]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_6/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_6/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_6/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:302, cli_main())[0m
[32m[2025-11-20 04:37:43]    INFO >> single GPU training... (train.py:331, cli_main())[0m
[32m[2025-11-20 04:37:43]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_6/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_6/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_6/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:208, single_main())[0m
[32m[2025-11-20 04:37:44]    INFO >> TypeTransformer(
  (decoder): RobertaEncoder(
    (encoder): CodeEncoderLSTM(
      (embedding): Embedding(10006, 512)
      (pos_encoder): PositionalEncoding()
      (encoder): LSTM(512, 512, num_layers=6, bidirectional=True)
    )
    (output): Sequential(
      (0): Linear(in_features=1024, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=10006, bias=True)
    )
  )
  (classification_heads): ModuleDict()
) (train.py:220, single_main())[0m
[32m[2025-11-20 04:37:44]    INFO >> model typetransformer, criterion TypePredictionCrossEntropyCriterion (train.py:221, single_main())[0m
[32m[2025-11-20 04:37:44]    INFO >> num. model params: 46481686 (num. trained: 46481686) (train.py:222, single_main())[0m
[32m[2025-11-20 04:37:45]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 04:37:45]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 81024 MB ; used memory = 896 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 04:37:45]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 04:37:45]    INFO >> training on 1 GPUs (train.py:229, single_main())[0m
[32m[2025-11-20 04:37:45]    INFO >> max tokens per GPU = None and max sentences per GPU = 32 (train.py:230, single_main())[0m
[32m[2025-11-20 04:37:45]    INFO >> no existing checkpoint found /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_6/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-20 04:37:45]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[33m[2025-11-20 04:37:55] WARNING >> 32222 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[0, 1, 2, 4, 6, 7, 8, 10, 14, 19] (data_utils.py:301, filter_by_size())[0m
[32m[2025-11-20 04:37:55]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-20 04:37:55] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:398: UserWarning: ntokens not found in Criterion logging outputs, cannot log wpb or wps
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:407: UserWarning: nsentences not found in Criterion logging outputs, cannot log bsz
  warnings.warn(
[32m[2025-11-20 04:38:41]    INFO >> epoch 001:    100 / 5058 loss=1.903, ups=2.35, num_updates=100, lr=1e-05, gnorm=0.104, clip=0, train_wall=41, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:39:29]    INFO >> epoch 001:    200 / 5058 loss=0.786, ups=2.14, num_updates=200, lr=2e-05, gnorm=0.068, clip=0, train_wall=44, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:40:34]    INFO >> epoch 001:    300 / 5058 loss=0.654, ups=1.61, num_updates=300, lr=3e-05, gnorm=0.053, clip=0, train_wall=58, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:41:40]    INFO >> epoch 001:    400 / 5058 loss=0.623, ups=1.58, num_updates=400, lr=4e-05, gnorm=2.819, clip=2, train_wall=60, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:42:43]    INFO >> epoch 001:    500 / 5058 loss=0.208, ups=1.66, num_updates=500, lr=5e-05, gnorm=4.476, clip=2, train_wall=57, gb_free=65.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:43:44]    INFO >> epoch 001:    600 / 5058 loss=0.079, ups=1.7, num_updates=600, lr=6e-05, gnorm=4.788, clip=8, train_wall=56, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:44:27]    INFO >> epoch 001:    700 / 5058 loss=0.174, ups=2.4, num_updates=700, lr=7e-05, gnorm=2.641, clip=4, train_wall=39, gb_free=69.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:45:10]    INFO >> epoch 001:    800 / 5058 loss=0.166, ups=2.44, num_updates=800, lr=8e-05, gnorm=3.325, clip=8, train_wall=39, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:46:17]    INFO >> epoch 001:    900 / 5058 loss=0.216, ups=1.55, num_updates=900, lr=9e-05, gnorm=6.91, clip=8, train_wall=61, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:47:22]    INFO >> epoch 001:   1000 / 5058 loss=0.142, ups=1.6, num_updates=1000, lr=0.0001, gnorm=1.451, clip=0, train_wall=59, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:48:22]    INFO >> epoch 001:   1100 / 5058 loss=0.058, ups=1.74, num_updates=1100, lr=0.0001, gnorm=2.202, clip=2, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:49:27]    INFO >> epoch 001:   1200 / 5058 loss=0.035, ups=1.62, num_updates=1200, lr=0.0001, gnorm=0.871, clip=0, train_wall=58, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:50:32]    INFO >> epoch 001:   1300 / 5058 loss=0.087, ups=1.6, num_updates=1300, lr=9.9e-05, gnorm=1.877, clip=2, train_wall=59, gb_free=67.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:51:36]    INFO >> epoch 001:   1400 / 5058 loss=0.059, ups=1.63, num_updates=1400, lr=9.9e-05, gnorm=2.488, clip=4, train_wall=58, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:52:41]    INFO >> epoch 001:   1500 / 5058 loss=0.279, ups=1.6, num_updates=1500, lr=9.9e-05, gnorm=1.776, clip=2, train_wall=59, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:53:42]    INFO >> epoch 001:   1600 / 5058 loss=0.11, ups=1.71, num_updates=1600, lr=9.9e-05, gnorm=3.243, clip=4, train_wall=55, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:54:47]    INFO >> epoch 001:   1700 / 5058 loss=0.113, ups=1.57, num_updates=1700, lr=9.9e-05, gnorm=2.319, clip=2, train_wall=60, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:55:43]    INFO >> epoch 001:   1800 / 5058 loss=0.12, ups=1.89, num_updates=1800, lr=9.8e-05, gnorm=3.115, clip=5, train_wall=50, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:56:22]    INFO >> epoch 001:   1900 / 5058 loss=0.153, ups=2.64, num_updates=1900, lr=9.8e-05, gnorm=1.952, clip=4, train_wall=36, gb_free=67.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:57:24]    INFO >> epoch 001:   2000 / 5058 loss=0.089, ups=1.68, num_updates=2000, lr=9.8e-05, gnorm=1.422, clip=1, train_wall=56, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:58:31]    INFO >> epoch 001:   2100 / 5058 loss=0.074, ups=1.56, num_updates=2100, lr=9.8e-05, gnorm=3.142, clip=2, train_wall=60, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:59:38]    INFO >> epoch 001:   2200 / 5058 loss=0.279, ups=1.56, num_updates=2200, lr=9.8e-05, gnorm=5.078, clip=6, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:00:45]    INFO >> epoch 001:   2300 / 5058 loss=0.068, ups=1.54, num_updates=2300, lr=9.7e-05, gnorm=2.452, clip=5, train_wall=61, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:01:51]    INFO >> epoch 001:   2400 / 5058 loss=0.063, ups=1.6, num_updates=2400, lr=9.7e-05, gnorm=0.773, clip=1, train_wall=59, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:03:01]    INFO >> epoch 001:   2500 / 5058 loss=0.149, ups=1.49, num_updates=2500, lr=9.7e-05, gnorm=3.718, clip=1, train_wall=63, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:03:59]    INFO >> epoch 001:   2600 / 5058 loss=0.128, ups=1.8, num_updates=2600, lr=9.7e-05, gnorm=2.608, clip=1, train_wall=53, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:05:00]    INFO >> epoch 001:   2700 / 5058 loss=0.076, ups=1.72, num_updates=2700, lr=9.7e-05, gnorm=1.387, clip=1, train_wall=55, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:06:02]    INFO >> epoch 001:   2800 / 5058 loss=0.081, ups=1.67, num_updates=2800, lr=9.6e-05, gnorm=1.32, clip=3, train_wall=56, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:06:42]    INFO >> epoch 001:   2900 / 5058 loss=0.274, ups=2.58, num_updates=2900, lr=9.6e-05, gnorm=2.005, clip=3, train_wall=37, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:07:48]    INFO >> epoch 001:   3000 / 5058 loss=0.145, ups=1.58, num_updates=3000, lr=9.6e-05, gnorm=3.103, clip=5, train_wall=60, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:08:42]    INFO >> epoch 001:   3100 / 5058 loss=0.058, ups=1.92, num_updates=3100, lr=9.6e-05, gnorm=0.282, clip=0, train_wall=49, gb_free=73.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:09:40]    INFO >> epoch 001:   3200 / 5058 loss=0.282, ups=1.8, num_updates=3200, lr=9.6e-05, gnorm=2.208, clip=2, train_wall=52, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:10:40]    INFO >> epoch 001:   3300 / 5058 loss=0.086, ups=1.72, num_updates=3300, lr=9.5e-05, gnorm=1.673, clip=1, train_wall=55, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:11:52]    INFO >> epoch 001:   3400 / 5058 loss=0.225, ups=1.46, num_updates=3400, lr=9.5e-05, gnorm=5.264, clip=3, train_wall=64, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:12:58]    INFO >> epoch 001:   3500 / 5058 loss=0.131, ups=1.58, num_updates=3500, lr=9.5e-05, gnorm=1.53, clip=0, train_wall=60, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:14:04]    INFO >> epoch 001:   3600 / 5058 loss=0.095, ups=1.58, num_updates=3600, lr=9.5e-05, gnorm=2.575, clip=1, train_wall=60, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:15:05]    INFO >> epoch 001:   3700 / 5058 loss=0.137, ups=1.71, num_updates=3700, lr=9.4e-05, gnorm=1.854, clip=2, train_wall=55, gb_free=66.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:16:12]    INFO >> epoch 001:   3800 / 5058 loss=0.058, ups=1.55, num_updates=3800, lr=9.4e-05, gnorm=1.191, clip=1, train_wall=60, gb_free=68.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:17:02]    INFO >> epoch 001:   3900 / 5058 loss=0.127, ups=2.06, num_updates=3900, lr=9.4e-05, gnorm=1.978, clip=2, train_wall=46, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:18:04]    INFO >> epoch 001:   4000 / 5058 loss=0.131, ups=1.67, num_updates=4000, lr=9.4e-05, gnorm=1.731, clip=1, train_wall=56, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:19:08]    INFO >> epoch 001:   4100 / 5058 loss=0.92, ups=1.65, num_updates=4100, lr=9.4e-05, gnorm=0.902, clip=1, train_wall=57, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:20:15]    INFO >> epoch 001:   4200 / 5058 loss=0.149, ups=1.55, num_updates=4200, lr=9.3e-05, gnorm=1.845, clip=3, train_wall=61, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:21:15]    INFO >> epoch 001:   4300 / 5058 loss=0.117, ups=1.75, num_updates=4300, lr=9.3e-05, gnorm=1.721, clip=1, train_wall=54, gb_free=73.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:22:16]    INFO >> epoch 001:   4400 / 5058 loss=0.082, ups=1.7, num_updates=4400, lr=9.3e-05, gnorm=1.476, clip=0, train_wall=55, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:23:18]    INFO >> epoch 001:   4500 / 5058 loss=0.072, ups=1.65, num_updates=4500, lr=9.3e-05, gnorm=1.371, clip=0, train_wall=57, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:24:25]    INFO >> epoch 001:   4600 / 5058 loss=0.241, ups=1.57, num_updates=4600, lr=9.3e-05, gnorm=1.805, clip=0, train_wall=60, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:25:37]    INFO >> epoch 001:   4700 / 5058 loss=0.215, ups=1.46, num_updates=4700, lr=9.2e-05, gnorm=3.803, clip=2, train_wall=64, gb_free=64.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:26:37]    INFO >> epoch 001:   4800 / 5058 loss=0.056, ups=1.7, num_updates=4800, lr=9.2e-05, gnorm=1.411, clip=0, train_wall=56, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:27:20]    INFO >> epoch 001:   4900 / 5058 loss=0.142, ups=2.52, num_updates=4900, lr=9.2e-05, gnorm=0.824, clip=0, train_wall=38, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:28:25]    INFO >> epoch 001:   5000 / 5058 loss=0.09, ups=1.6, num_updates=5000, lr=9.2e-05, gnorm=2.488, clip=0, train_wall=59, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:29:05]    INFO >> epoch 001 | loss 0.105 | ups 1.72 | num_updates 5058 | lr 9.2e-05 | gnorm 2.248 | clip 2.1 | train_wall 2777 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-20 05:29:05] WARNING >> 4469 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[2, 4, 10, 12, 13, 14, 16, 18, 19, 23] (data_utils.py:301, filter_by_size())[0m
[33m[2025-11-20 05:29:05] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
CodeEncoderLSTM: Creating BiLSTM with 6 layers, 512 hidden and input size
Traceback (most recent call last):
  File "run/type_prediction/transformer/train.py", line 339, in <module>
    cli_main()
  File "run/type_prediction/transformer/train.py", line 332, in cli_main
    single_main(args)
  File "run/type_prediction/transformer/train.py", line 254, in single_main
    valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)
  File "run/type_prediction/transformer/train.py", line 132, in validate
    trainer.valid_step(sample)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/trainers/ncc_trainers.py", line 565, in valid_step
    _loss, sample_size, logging_output = self.task.valid_step(
TypeError: cannot unpack non-iterable NoneType object

============================================================
Setting up experiment: exp_layers_6
============================================================
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_6
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_6/checkpoints
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_6/logs
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_6/results
âœ“ Saved config to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_6/config.yml
âœ“ Also saved to: run/type_prediction/transformer/config/exp_layers_6.yml
âœ“ Saved experiment info to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_6/info.txt

============================================================
Starting training...
============================================================

Command: python run/type_prediction/transformer/train.py --language exp_layers_6 2>&1 | tee /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_6/logs/train.log

Note: Config file will be loaded from run/type_prediction/transformer/config/exp_layers_6.yml


âœ“ Training completed successfully!

============================================================
Evaluating model on test set...
============================================================

Warning: Best checkpoint not found, using last checkpoint
âœ— No checkpoint found for evaluation

============================================================
âœ“ Experiment completed: exp_layers_6
âœ“ Results in: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_6
============================================================


âœ“ exp_layers_6 completed in 0.86 hours

============================================================
Running: exp_layers_8
GPU: 3
Time: 2025-11-20T05:29:08.560213
============================================================
[32m[2025-11-20 05:29:10]    INFO >> Load arguments in run/type_prediction/transformer/config/exp_layers_8.yml (train.py:300, cli_main())[0m
[32m[2025-11-20 05:29:10]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_8/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 8, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_8/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_8/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:302, cli_main())[0m
[32m[2025-11-20 05:29:10]    INFO >> single GPU training... (train.py:331, cli_main())[0m
[32m[2025-11-20 05:29:10]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_8/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 8, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_8/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_8/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:208, single_main())[0m
[32m[2025-11-20 05:29:11]    INFO >> TypeTransformer(
  (decoder): RobertaEncoder(
    (encoder): CodeEncoderLSTM(
      (embedding): Embedding(10006, 512)
      (pos_encoder): PositionalEncoding()
      (encoder): LSTM(512, 512, num_layers=6, bidirectional=True)
    )
    (output): Sequential(
      (0): Linear(in_features=1024, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=10006, bias=True)
    )
  )
  (classification_heads): ModuleDict()
) (train.py:220, single_main())[0m
[32m[2025-11-20 05:29:11]    INFO >> model typetransformer, criterion TypePredictionCrossEntropyCriterion (train.py:221, single_main())[0m
[32m[2025-11-20 05:29:11]    INFO >> num. model params: 46481686 (num. trained: 46481686) (train.py:222, single_main())[0m
[32m[2025-11-20 05:29:12]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 05:29:12]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 81024 MB ; used memory = 896 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 05:29:12]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 05:29:12]    INFO >> training on 1 GPUs (train.py:229, single_main())[0m
[32m[2025-11-20 05:29:12]    INFO >> max tokens per GPU = None and max sentences per GPU = 32 (train.py:230, single_main())[0m
[32m[2025-11-20 05:29:12]    INFO >> no existing checkpoint found /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_8/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-20 05:29:12]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[33m[2025-11-20 05:29:22] WARNING >> 32222 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[0, 1, 2, 4, 6, 7, 8, 10, 14, 19] (data_utils.py:301, filter_by_size())[0m
[32m[2025-11-20 05:29:22]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-20 05:29:22] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:398: UserWarning: ntokens not found in Criterion logging outputs, cannot log wpb or wps
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:407: UserWarning: nsentences not found in Criterion logging outputs, cannot log bsz
  warnings.warn(
[32m[2025-11-20 05:30:08]    INFO >> epoch 001:    100 / 5058 loss=1.903, ups=2.35, num_updates=100, lr=1e-05, gnorm=0.104, clip=0, train_wall=41, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:30:56]    INFO >> epoch 001:    200 / 5058 loss=0.786, ups=2.14, num_updates=200, lr=2e-05, gnorm=0.068, clip=0, train_wall=44, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:32:01]    INFO >> epoch 001:    300 / 5058 loss=0.654, ups=1.61, num_updates=300, lr=3e-05, gnorm=0.053, clip=0, train_wall=58, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:33:07]    INFO >> epoch 001:    400 / 5058 loss=0.623, ups=1.58, num_updates=400, lr=4e-05, gnorm=2.819, clip=2, train_wall=60, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:34:10]    INFO >> epoch 001:    500 / 5058 loss=0.208, ups=1.66, num_updates=500, lr=5e-05, gnorm=4.476, clip=2, train_wall=57, gb_free=65.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:35:11]    INFO >> epoch 001:    600 / 5058 loss=0.079, ups=1.7, num_updates=600, lr=6e-05, gnorm=4.788, clip=8, train_wall=56, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:35:54]    INFO >> epoch 001:    700 / 5058 loss=0.174, ups=2.4, num_updates=700, lr=7e-05, gnorm=2.641, clip=4, train_wall=39, gb_free=69.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:36:37]    INFO >> epoch 001:    800 / 5058 loss=0.166, ups=2.44, num_updates=800, lr=8e-05, gnorm=3.325, clip=8, train_wall=39, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:37:44]    INFO >> epoch 001:    900 / 5058 loss=0.216, ups=1.55, num_updates=900, lr=9e-05, gnorm=6.91, clip=8, train_wall=61, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:38:49]    INFO >> epoch 001:   1000 / 5058 loss=0.142, ups=1.61, num_updates=1000, lr=0.0001, gnorm=1.451, clip=0, train_wall=59, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:39:49]    INFO >> epoch 001:   1100 / 5058 loss=0.058, ups=1.74, num_updates=1100, lr=0.0001, gnorm=2.202, clip=2, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:40:53]    INFO >> epoch 001:   1200 / 5058 loss=0.035, ups=1.62, num_updates=1200, lr=0.0001, gnorm=0.871, clip=0, train_wall=58, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:41:58]    INFO >> epoch 001:   1300 / 5058 loss=0.087, ups=1.6, num_updates=1300, lr=9.9e-05, gnorm=1.877, clip=2, train_wall=59, gb_free=67.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:43:02]    INFO >> epoch 001:   1400 / 5058 loss=0.059, ups=1.63, num_updates=1400, lr=9.9e-05, gnorm=2.488, clip=4, train_wall=58, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:44:08]    INFO >> epoch 001:   1500 / 5058 loss=0.279, ups=1.6, num_updates=1500, lr=9.9e-05, gnorm=1.776, clip=2, train_wall=59, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:45:07]    INFO >> epoch 001:   1600 / 5058 loss=0.11, ups=1.71, num_updates=1600, lr=9.9e-05, gnorm=3.243, clip=4, train_wall=55, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:46:14]    INFO >> epoch 001:   1700 / 5058 loss=0.113, ups=1.57, num_updates=1700, lr=9.9e-05, gnorm=2.319, clip=2, train_wall=60, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:47:09]    INFO >> epoch 001:   1800 / 5058 loss=0.12, ups=1.9, num_updates=1800, lr=9.8e-05, gnorm=3.115, clip=5, train_wall=50, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:47:49]    INFO >> epoch 001:   1900 / 5058 loss=0.153, ups=2.64, num_updates=1900, lr=9.8e-05, gnorm=1.952, clip=4, train_wall=36, gb_free=67.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:48:51]    INFO >> epoch 001:   2000 / 5058 loss=0.089, ups=1.68, num_updates=2000, lr=9.8e-05, gnorm=1.422, clip=1, train_wall=56, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:49:57]    INFO >> epoch 001:   2100 / 5058 loss=0.074, ups=1.56, num_updates=2100, lr=9.8e-05, gnorm=3.142, clip=2, train_wall=60, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:51:04]    INFO >> epoch 001:   2200 / 5058 loss=0.279, ups=1.56, num_updates=2200, lr=9.8e-05, gnorm=5.078, clip=6, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:52:12]    INFO >> epoch 001:   2300 / 5058 loss=0.068, ups=1.54, num_updates=2300, lr=9.7e-05, gnorm=2.452, clip=5, train_wall=61, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:53:17]    INFO >> epoch 001:   2400 / 5058 loss=0.063, ups=1.6, num_updates=2400, lr=9.7e-05, gnorm=0.773, clip=1, train_wall=59, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:54:27]    INFO >> epoch 001:   2500 / 5058 loss=0.149, ups=1.49, num_updates=2500, lr=9.7e-05, gnorm=3.718, clip=1, train_wall=63, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:55:25]    INFO >> epoch 001:   2600 / 5058 loss=0.128, ups=1.8, num_updates=2600, lr=9.7e-05, gnorm=2.608, clip=1, train_wall=53, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:56:26]    INFO >> epoch 001:   2700 / 5058 loss=0.076, ups=1.72, num_updates=2700, lr=9.7e-05, gnorm=1.387, clip=1, train_wall=55, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:57:27]    INFO >> epoch 001:   2800 / 5058 loss=0.081, ups=1.67, num_updates=2800, lr=9.6e-05, gnorm=1.32, clip=3, train_wall=56, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:58:08]    INFO >> epoch 001:   2900 / 5058 loss=0.274, ups=2.58, num_updates=2900, lr=9.6e-05, gnorm=2.005, clip=3, train_wall=37, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:59:14]    INFO >> epoch 001:   3000 / 5058 loss=0.145, ups=1.58, num_updates=3000, lr=9.6e-05, gnorm=3.103, clip=5, train_wall=60, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:00:07]    INFO >> epoch 001:   3100 / 5058 loss=0.058, ups=1.92, num_updates=3100, lr=9.6e-05, gnorm=0.282, clip=0, train_wall=49, gb_free=73.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:01:06]    INFO >> epoch 001:   3200 / 5058 loss=0.282, ups=1.8, num_updates=3200, lr=9.6e-05, gnorm=2.208, clip=2, train_wall=52, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:02:06]    INFO >> epoch 001:   3300 / 5058 loss=0.086, ups=1.72, num_updates=3300, lr=9.5e-05, gnorm=1.673, clip=1, train_wall=55, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:03:18]    INFO >> epoch 001:   3400 / 5058 loss=0.225, ups=1.46, num_updates=3400, lr=9.5e-05, gnorm=5.264, clip=3, train_wall=64, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:04:24]    INFO >> epoch 001:   3500 / 5058 loss=0.131, ups=1.58, num_updates=3500, lr=9.5e-05, gnorm=1.53, clip=0, train_wall=59, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:05:30]    INFO >> epoch 001:   3600 / 5058 loss=0.095, ups=1.58, num_updates=3600, lr=9.5e-05, gnorm=2.575, clip=1, train_wall=59, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:06:31]    INFO >> epoch 001:   3700 / 5058 loss=0.137, ups=1.71, num_updates=3700, lr=9.4e-05, gnorm=1.854, clip=2, train_wall=55, gb_free=66.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:07:38]    INFO >> epoch 001:   3800 / 5058 loss=0.058, ups=1.55, num_updates=3800, lr=9.4e-05, gnorm=1.191, clip=1, train_wall=60, gb_free=68.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:08:28]    INFO >> epoch 001:   3900 / 5058 loss=0.127, ups=2.06, num_updates=3900, lr=9.4e-05, gnorm=1.978, clip=2, train_wall=46, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:09:30]    INFO >> epoch 001:   4000 / 5058 loss=0.131, ups=1.67, num_updates=4000, lr=9.4e-05, gnorm=1.731, clip=1, train_wall=56, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:10:34]    INFO >> epoch 001:   4100 / 5058 loss=0.92, ups=1.64, num_updates=4100, lr=9.4e-05, gnorm=0.902, clip=1, train_wall=57, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:11:41]    INFO >> epoch 001:   4200 / 5058 loss=0.149, ups=1.55, num_updates=4200, lr=9.3e-05, gnorm=1.845, clip=3, train_wall=61, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:12:41]    INFO >> epoch 001:   4300 / 5058 loss=0.117, ups=1.74, num_updates=4300, lr=9.3e-05, gnorm=1.721, clip=1, train_wall=54, gb_free=73.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:13:41]    INFO >> epoch 001:   4400 / 5058 loss=0.082, ups=1.7, num_updates=4400, lr=9.3e-05, gnorm=1.476, clip=0, train_wall=55, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:14:45]    INFO >> epoch 001:   4500 / 5058 loss=0.072, ups=1.64, num_updates=4500, lr=9.3e-05, gnorm=1.371, clip=0, train_wall=57, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:15:51]    INFO >> epoch 001:   4600 / 5058 loss=0.241, ups=1.56, num_updates=4600, lr=9.3e-05, gnorm=1.805, clip=0, train_wall=60, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:17:03]    INFO >> epoch 001:   4700 / 5058 loss=0.215, ups=1.46, num_updates=4700, lr=9.2e-05, gnorm=3.803, clip=2, train_wall=64, gb_free=64.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:18:04]    INFO >> epoch 001:   4800 / 5058 loss=0.056, ups=1.69, num_updates=4800, lr=9.2e-05, gnorm=1.411, clip=0, train_wall=55, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:18:45]    INFO >> epoch 001:   4900 / 5058 loss=0.142, ups=2.51, num_updates=4900, lr=9.2e-05, gnorm=0.824, clip=0, train_wall=38, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:19:51]    INFO >> epoch 001:   5000 / 5058 loss=0.09, ups=1.6, num_updates=5000, lr=9.2e-05, gnorm=2.488, clip=0, train_wall=59, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:20:32]    INFO >> epoch 001 | loss 0.105 | ups 1.72 | num_updates 5058 | lr 9.2e-05 | gnorm 2.248 | clip 2.1 | train_wall 2774 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-20 06:20:32] WARNING >> 4469 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[2, 4, 10, 12, 13, 14, 16, 18, 19, 23] (data_utils.py:301, filter_by_size())[0m
[33m[2025-11-20 06:20:32] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
CodeEncoderLSTM: Creating BiLSTM with 6 layers, 512 hidden and input size
Traceback (most recent call last):
  File "run/type_prediction/transformer/train.py", line 339, in <module>
    cli_main()
  File "run/type_prediction/transformer/train.py", line 332, in cli_main
    single_main(args)
  File "run/type_prediction/transformer/train.py", line 254, in single_main
    valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)
  File "run/type_prediction/transformer/train.py", line 132, in validate
    trainer.valid_step(sample)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/trainers/ncc_trainers.py", line 565, in valid_step
    _loss, sample_size, logging_output = self.task.valid_step(
TypeError: cannot unpack non-iterable NoneType object

============================================================
Setting up experiment: exp_layers_8
============================================================
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_8
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_8/checkpoints
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_8/logs
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_8/results
âœ“ Saved config to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_8/config.yml
âœ“ Also saved to: run/type_prediction/transformer/config/exp_layers_8.yml
âœ“ Saved experiment info to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_8/info.txt

============================================================
Starting training...
============================================================

Command: python run/type_prediction/transformer/train.py --language exp_layers_8 2>&1 | tee /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_8/logs/train.log

Note: Config file will be loaded from run/type_prediction/transformer/config/exp_layers_8.yml


âœ“ Training completed successfully!

============================================================
Evaluating model on test set...
============================================================

Warning: Best checkpoint not found, using last checkpoint
âœ— No checkpoint found for evaluation

============================================================
âœ“ Experiment completed: exp_layers_8
âœ“ Results in: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_8
============================================================


âœ“ exp_layers_8 completed in 0.86 hours

============================================================
Running: exp_lr_1e-04
GPU: 3
Time: 2025-11-20T06:20:35.605291
============================================================
[32m[2025-11-20 06:20:37]    INFO >> Load arguments in run/type_prediction/transformer/config/exp_lr_1e-04.yml (train.py:300, cli_main())[0m
[32m[2025-11-20 06:20:37]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-04/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-04/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-04/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:302, cli_main())[0m
[32m[2025-11-20 06:20:37]    INFO >> single GPU training... (train.py:331, cli_main())[0m
[32m[2025-11-20 06:20:37]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-04/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-04/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-04/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:208, single_main())[0m
[32m[2025-11-20 06:20:39]    INFO >> TypeTransformer(
  (decoder): RobertaEncoder(
    (encoder): CodeEncoderLSTM(
      (embedding): Embedding(10006, 512)
      (pos_encoder): PositionalEncoding()
      (encoder): LSTM(512, 512, num_layers=6, bidirectional=True)
    )
    (output): Sequential(
      (0): Linear(in_features=1024, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=10006, bias=True)
    )
  )
  (classification_heads): ModuleDict()
) (train.py:220, single_main())[0m
[32m[2025-11-20 06:20:39]    INFO >> model typetransformer, criterion TypePredictionCrossEntropyCriterion (train.py:221, single_main())[0m
[32m[2025-11-20 06:20:39]    INFO >> num. model params: 46481686 (num. trained: 46481686) (train.py:222, single_main())[0m
[32m[2025-11-20 06:20:39]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 06:20:39]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 81024 MB ; used memory = 896 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 06:20:39]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 06:20:39]    INFO >> training on 1 GPUs (train.py:229, single_main())[0m
[32m[2025-11-20 06:20:39]    INFO >> max tokens per GPU = None and max sentences per GPU = 32 (train.py:230, single_main())[0m
[32m[2025-11-20 06:20:39]    INFO >> no existing checkpoint found /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-04/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-20 06:20:39]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[33m[2025-11-20 06:20:49] WARNING >> 32222 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[0, 1, 2, 4, 6, 7, 8, 10, 14, 19] (data_utils.py:301, filter_by_size())[0m
[32m[2025-11-20 06:20:49]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-20 06:20:49] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:398: UserWarning: ntokens not found in Criterion logging outputs, cannot log wpb or wps
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:407: UserWarning: nsentences not found in Criterion logging outputs, cannot log bsz
  warnings.warn(
[32m[2025-11-20 06:21:34]    INFO >> epoch 001:    100 / 5058 loss=1.903, ups=2.35, num_updates=100, lr=1e-05, gnorm=0.104, clip=0, train_wall=41, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:22:23]    INFO >> epoch 001:    200 / 5058 loss=0.786, ups=2.14, num_updates=200, lr=2e-05, gnorm=0.068, clip=0, train_wall=44, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:23:28]    INFO >> epoch 001:    300 / 5058 loss=0.654, ups=1.62, num_updates=300, lr=3e-05, gnorm=0.053, clip=0, train_wall=58, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:24:34]    INFO >> epoch 001:    400 / 5058 loss=0.623, ups=1.58, num_updates=400, lr=4e-05, gnorm=2.819, clip=2, train_wall=60, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:25:37]    INFO >> epoch 001:    500 / 5058 loss=0.208, ups=1.66, num_updates=500, lr=5e-05, gnorm=4.476, clip=2, train_wall=57, gb_free=65.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:26:37]    INFO >> epoch 001:    600 / 5058 loss=0.079, ups=1.7, num_updates=600, lr=6e-05, gnorm=4.788, clip=8, train_wall=56, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:27:21]    INFO >> epoch 001:    700 / 5058 loss=0.174, ups=2.4, num_updates=700, lr=7e-05, gnorm=2.641, clip=4, train_wall=39, gb_free=69.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:28:04]    INFO >> epoch 001:    800 / 5058 loss=0.166, ups=2.44, num_updates=800, lr=8e-05, gnorm=3.325, clip=8, train_wall=39, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:29:11]    INFO >> epoch 001:    900 / 5058 loss=0.216, ups=1.55, num_updates=900, lr=9e-05, gnorm=6.91, clip=8, train_wall=61, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:30:16]    INFO >> epoch 001:   1000 / 5058 loss=0.142, ups=1.6, num_updates=1000, lr=0.0001, gnorm=1.451, clip=0, train_wall=59, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:31:16]    INFO >> epoch 001:   1100 / 5058 loss=0.058, ups=1.74, num_updates=1100, lr=0.0001, gnorm=2.202, clip=2, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:32:20]    INFO >> epoch 001:   1200 / 5058 loss=0.035, ups=1.62, num_updates=1200, lr=0.0001, gnorm=0.871, clip=0, train_wall=58, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:33:25]    INFO >> epoch 001:   1300 / 5058 loss=0.087, ups=1.6, num_updates=1300, lr=9.9e-05, gnorm=1.877, clip=2, train_wall=59, gb_free=67.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:34:28]    INFO >> epoch 001:   1400 / 5058 loss=0.059, ups=1.63, num_updates=1400, lr=9.9e-05, gnorm=2.488, clip=4, train_wall=58, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:35:33]    INFO >> epoch 001:   1500 / 5058 loss=0.279, ups=1.6, num_updates=1500, lr=9.9e-05, gnorm=1.776, clip=2, train_wall=59, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:36:34]    INFO >> epoch 001:   1600 / 5058 loss=0.11, ups=1.71, num_updates=1600, lr=9.9e-05, gnorm=3.243, clip=4, train_wall=55, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:37:41]    INFO >> epoch 001:   1700 / 5058 loss=0.113, ups=1.57, num_updates=1700, lr=9.9e-05, gnorm=2.319, clip=2, train_wall=60, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:38:36]    INFO >> epoch 001:   1800 / 5058 loss=0.12, ups=1.89, num_updates=1800, lr=9.8e-05, gnorm=3.115, clip=5, train_wall=50, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:39:16]    INFO >> epoch 001:   1900 / 5058 loss=0.153, ups=2.64, num_updates=1900, lr=9.8e-05, gnorm=1.952, clip=4, train_wall=36, gb_free=67.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:40:18]    INFO >> epoch 001:   2000 / 5058 loss=0.089, ups=1.68, num_updates=2000, lr=9.8e-05, gnorm=1.422, clip=1, train_wall=56, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:41:25]    INFO >> epoch 001:   2100 / 5058 loss=0.074, ups=1.56, num_updates=2100, lr=9.8e-05, gnorm=3.142, clip=2, train_wall=60, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:42:31]    INFO >> epoch 001:   2200 / 5058 loss=0.279, ups=1.56, num_updates=2200, lr=9.8e-05, gnorm=5.078, clip=6, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:43:39]    INFO >> epoch 001:   2300 / 5058 loss=0.068, ups=1.54, num_updates=2300, lr=9.7e-05, gnorm=2.452, clip=5, train_wall=61, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:44:44]    INFO >> epoch 001:   2400 / 5058 loss=0.063, ups=1.6, num_updates=2400, lr=9.7e-05, gnorm=0.773, clip=1, train_wall=59, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:45:54]    INFO >> epoch 001:   2500 / 5058 loss=0.149, ups=1.49, num_updates=2500, lr=9.7e-05, gnorm=3.718, clip=1, train_wall=63, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:46:52]    INFO >> epoch 001:   2600 / 5058 loss=0.128, ups=1.8, num_updates=2600, lr=9.7e-05, gnorm=2.608, clip=1, train_wall=53, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:47:52]    INFO >> epoch 001:   2700 / 5058 loss=0.076, ups=1.72, num_updates=2700, lr=9.7e-05, gnorm=1.387, clip=1, train_wall=55, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:48:54]    INFO >> epoch 001:   2800 / 5058 loss=0.081, ups=1.68, num_updates=2800, lr=9.6e-05, gnorm=1.32, clip=3, train_wall=56, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:49:34]    INFO >> epoch 001:   2900 / 5058 loss=0.274, ups=2.59, num_updates=2900, lr=9.6e-05, gnorm=2.005, clip=3, train_wall=37, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:50:40]    INFO >> epoch 001:   3000 / 5058 loss=0.145, ups=1.58, num_updates=3000, lr=9.6e-05, gnorm=3.103, clip=5, train_wall=60, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:51:35]    INFO >> epoch 001:   3100 / 5058 loss=0.058, ups=1.92, num_updates=3100, lr=9.6e-05, gnorm=0.282, clip=0, train_wall=49, gb_free=73.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:52:33]    INFO >> epoch 001:   3200 / 5058 loss=0.282, ups=1.81, num_updates=3200, lr=9.6e-05, gnorm=2.208, clip=2, train_wall=52, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:53:32]    INFO >> epoch 001:   3300 / 5058 loss=0.086, ups=1.73, num_updates=3300, lr=9.5e-05, gnorm=1.673, clip=1, train_wall=55, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:54:45]    INFO >> epoch 001:   3400 / 5058 loss=0.225, ups=1.45, num_updates=3400, lr=9.5e-05, gnorm=5.264, clip=3, train_wall=64, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:55:51]    INFO >> epoch 001:   3500 / 5058 loss=0.131, ups=1.58, num_updates=3500, lr=9.5e-05, gnorm=1.53, clip=0, train_wall=59, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:56:57]    INFO >> epoch 001:   3600 / 5058 loss=0.095, ups=1.58, num_updates=3600, lr=9.5e-05, gnorm=2.575, clip=1, train_wall=59, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:57:56]    INFO >> epoch 001:   3700 / 5058 loss=0.137, ups=1.71, num_updates=3700, lr=9.4e-05, gnorm=1.854, clip=2, train_wall=55, gb_free=66.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:59:03]    INFO >> epoch 001:   3800 / 5058 loss=0.058, ups=1.56, num_updates=3800, lr=9.4e-05, gnorm=1.191, clip=1, train_wall=60, gb_free=68.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:59:54]    INFO >> epoch 001:   3900 / 5058 loss=0.127, ups=2.07, num_updates=3900, lr=9.4e-05, gnorm=1.978, clip=2, train_wall=46, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:00:57]    INFO >> epoch 001:   4000 / 5058 loss=0.131, ups=1.68, num_updates=4000, lr=9.4e-05, gnorm=1.731, clip=1, train_wall=56, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:02:00]    INFO >> epoch 001:   4100 / 5058 loss=0.92, ups=1.65, num_updates=4100, lr=9.4e-05, gnorm=0.902, clip=1, train_wall=57, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:03:07]    INFO >> epoch 001:   4200 / 5058 loss=0.149, ups=1.55, num_updates=4200, lr=9.3e-05, gnorm=1.845, clip=3, train_wall=61, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:04:06]    INFO >> epoch 001:   4300 / 5058 loss=0.117, ups=1.75, num_updates=4300, lr=9.3e-05, gnorm=1.721, clip=1, train_wall=54, gb_free=73.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:05:07]    INFO >> epoch 001:   4400 / 5058 loss=0.082, ups=1.71, num_updates=4400, lr=9.3e-05, gnorm=1.476, clip=0, train_wall=55, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:06:10]    INFO >> epoch 001:   4500 / 5058 loss=0.072, ups=1.65, num_updates=4500, lr=9.3e-05, gnorm=1.371, clip=0, train_wall=57, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:07:17]    INFO >> epoch 001:   4600 / 5058 loss=0.241, ups=1.57, num_updates=4600, lr=9.3e-05, gnorm=1.805, clip=0, train_wall=60, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:08:28]    INFO >> epoch 001:   4700 / 5058 loss=0.215, ups=1.47, num_updates=4700, lr=9.2e-05, gnorm=3.803, clip=2, train_wall=64, gb_free=64.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:09:29]    INFO >> epoch 001:   4800 / 5058 loss=0.056, ups=1.7, num_updates=4800, lr=9.2e-05, gnorm=1.411, clip=0, train_wall=55, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:10:10]    INFO >> epoch 001:   4900 / 5058 loss=0.142, ups=2.52, num_updates=4900, lr=9.2e-05, gnorm=0.824, clip=0, train_wall=38, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:11:15]    INFO >> epoch 001:   5000 / 5058 loss=0.09, ups=1.6, num_updates=5000, lr=9.2e-05, gnorm=2.488, clip=0, train_wall=59, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:11:55]    INFO >> epoch 001 | loss 0.105 | ups 1.72 | num_updates 5058 | lr 9.2e-05 | gnorm 2.248 | clip 2.1 | train_wall 2774 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-20 07:11:55] WARNING >> 4469 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[2, 4, 10, 12, 13, 14, 16, 18, 19, 23] (data_utils.py:301, filter_by_size())[0m
[33m[2025-11-20 07:11:55] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
CodeEncoderLSTM: Creating BiLSTM with 6 layers, 512 hidden and input size
Traceback (most recent call last):
  File "run/type_prediction/transformer/train.py", line 339, in <module>
    cli_main()
  File "run/type_prediction/transformer/train.py", line 332, in cli_main
    single_main(args)
  File "run/type_prediction/transformer/train.py", line 254, in single_main
    valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)
  File "run/type_prediction/transformer/train.py", line 132, in validate
    trainer.valid_step(sample)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/trainers/ncc_trainers.py", line 565, in valid_step
    _loss, sample_size, logging_output = self.task.valid_step(
TypeError: cannot unpack non-iterable NoneType object

============================================================
Setting up experiment: exp_lr_1e-04
============================================================
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-04
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-04/checkpoints
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-04/logs
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-04/results
âœ“ Saved config to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-04/config.yml
âœ“ Also saved to: run/type_prediction/transformer/config/exp_lr_1e-04.yml
âœ“ Saved experiment info to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-04/info.txt

============================================================
Starting training...
============================================================

Command: python run/type_prediction/transformer/train.py --language exp_lr_1e-04 2>&1 | tee /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-04/logs/train.log

Note: Config file will be loaded from run/type_prediction/transformer/config/exp_lr_1e-04.yml


âœ“ Training completed successfully!

============================================================
Evaluating model on test set...
============================================================

Warning: Best checkpoint not found, using last checkpoint
âœ— No checkpoint found for evaluation

============================================================
âœ“ Experiment completed: exp_lr_1e-04
âœ“ Results in: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-04
============================================================


âœ“ exp_lr_1e-04 completed in 0.86 hours

============================================================
Running: exp_lr_5e-04
GPU: 3
Time: 2025-11-20T07:11:58.762356
============================================================
[32m[2025-11-20 07:12:00]    INFO >> Load arguments in run/type_prediction/transformer/config/exp_lr_5e-04.yml (train.py:300, cli_main())[0m
[32m[2025-11-20 07:12:00]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_5e-04/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_5e-04/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_5e-04/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:302, cli_main())[0m
[32m[2025-11-20 07:12:00]    INFO >> single GPU training... (train.py:331, cli_main())[0m
[32m[2025-11-20 07:12:00]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_5e-04/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_5e-04/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_5e-04/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:208, single_main())[0m
[32m[2025-11-20 07:12:03]    INFO >> TypeTransformer(
  (decoder): RobertaEncoder(
    (encoder): CodeEncoderLSTM(
      (embedding): Embedding(10006, 512)
      (pos_encoder): PositionalEncoding()
      (encoder): LSTM(512, 512, num_layers=6, bidirectional=True)
    )
    (output): Sequential(
      (0): Linear(in_features=1024, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=10006, bias=True)
    )
  )
  (classification_heads): ModuleDict()
) (train.py:220, single_main())[0m
[32m[2025-11-20 07:12:03]    INFO >> model typetransformer, criterion TypePredictionCrossEntropyCriterion (train.py:221, single_main())[0m
[32m[2025-11-20 07:12:03]    INFO >> num. model params: 46481686 (num. trained: 46481686) (train.py:222, single_main())[0m
[32m[2025-11-20 07:12:03]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 07:12:03]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 81024 MB ; used memory = 896 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 07:12:03]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 07:12:03]    INFO >> training on 1 GPUs (train.py:229, single_main())[0m
[32m[2025-11-20 07:12:03]    INFO >> max tokens per GPU = None and max sentences per GPU = 32 (train.py:230, single_main())[0m
[32m[2025-11-20 07:12:03]    INFO >> no existing checkpoint found /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_5e-04/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-20 07:12:03]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[33m[2025-11-20 07:12:13] WARNING >> 32222 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[0, 1, 2, 4, 6, 7, 8, 10, 14, 19] (data_utils.py:301, filter_by_size())[0m
[32m[2025-11-20 07:12:13]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-20 07:12:14] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:398: UserWarning: ntokens not found in Criterion logging outputs, cannot log wpb or wps
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:407: UserWarning: nsentences not found in Criterion logging outputs, cannot log bsz
  warnings.warn(
[32m[2025-11-20 07:12:58]    INFO >> epoch 001:    100 / 5058 loss=1.903, ups=2.37, num_updates=100, lr=1e-05, gnorm=0.104, clip=0, train_wall=40, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:13:47]    INFO >> epoch 001:    200 / 5058 loss=0.786, ups=2.16, num_updates=200, lr=2e-05, gnorm=0.068, clip=0, train_wall=44, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:14:51]    INFO >> epoch 001:    300 / 5058 loss=0.654, ups=1.63, num_updates=300, lr=3e-05, gnorm=0.053, clip=0, train_wall=58, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:15:55]    INFO >> epoch 001:    400 / 5058 loss=0.623, ups=1.59, num_updates=400, lr=4e-05, gnorm=2.819, clip=2, train_wall=59, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:16:58]    INFO >> epoch 001:    500 / 5058 loss=0.208, ups=1.67, num_updates=500, lr=5e-05, gnorm=4.476, clip=2, train_wall=56, gb_free=65.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:17:59]    INFO >> epoch 001:    600 / 5058 loss=0.079, ups=1.71, num_updates=600, lr=6e-05, gnorm=4.788, clip=8, train_wall=55, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:18:41]    INFO >> epoch 001:    700 / 5058 loss=0.174, ups=2.42, num_updates=700, lr=7e-05, gnorm=2.641, clip=4, train_wall=39, gb_free=69.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:19:25]    INFO >> epoch 001:    800 / 5058 loss=0.166, ups=2.46, num_updates=800, lr=8e-05, gnorm=3.325, clip=8, train_wall=39, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:20:31]    INFO >> epoch 001:    900 / 5058 loss=0.216, ups=1.56, num_updates=900, lr=9e-05, gnorm=6.91, clip=8, train_wall=60, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:21:36]    INFO >> epoch 001:   1000 / 5058 loss=0.142, ups=1.62, num_updates=1000, lr=0.0001, gnorm=1.451, clip=0, train_wall=58, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:22:34]    INFO >> epoch 001:   1100 / 5058 loss=0.058, ups=1.76, num_updates=1100, lr=0.0001, gnorm=2.202, clip=2, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:23:38]    INFO >> epoch 001:   1200 / 5058 loss=0.035, ups=1.64, num_updates=1200, lr=0.0001, gnorm=0.871, clip=0, train_wall=57, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:24:43]    INFO >> epoch 001:   1300 / 5058 loss=0.087, ups=1.61, num_updates=1300, lr=9.9e-05, gnorm=1.877, clip=2, train_wall=58, gb_free=67.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:25:46]    INFO >> epoch 001:   1400 / 5058 loss=0.059, ups=1.64, num_updates=1400, lr=9.9e-05, gnorm=2.488, clip=4, train_wall=57, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:26:51]    INFO >> epoch 001:   1500 / 5058 loss=0.279, ups=1.61, num_updates=1500, lr=9.9e-05, gnorm=1.776, clip=2, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:27:51]    INFO >> epoch 001:   1600 / 5058 loss=0.11, ups=1.73, num_updates=1600, lr=9.9e-05, gnorm=3.243, clip=4, train_wall=55, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:28:57]    INFO >> epoch 001:   1700 / 5058 loss=0.113, ups=1.58, num_updates=1700, lr=9.9e-05, gnorm=2.319, clip=2, train_wall=59, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:29:51]    INFO >> epoch 001:   1800 / 5058 loss=0.12, ups=1.91, num_updates=1800, lr=9.8e-05, gnorm=3.115, clip=5, train_wall=49, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:30:30]    INFO >> epoch 001:   1900 / 5058 loss=0.153, ups=2.66, num_updates=1900, lr=9.8e-05, gnorm=1.952, clip=4, train_wall=36, gb_free=67.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:31:32]    INFO >> epoch 001:   2000 / 5058 loss=0.089, ups=1.69, num_updates=2000, lr=9.8e-05, gnorm=1.422, clip=1, train_wall=56, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:32:38]    INFO >> epoch 001:   2100 / 5058 loss=0.074, ups=1.58, num_updates=2100, lr=9.8e-05, gnorm=3.142, clip=2, train_wall=60, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:33:44]    INFO >> epoch 001:   2200 / 5058 loss=0.279, ups=1.57, num_updates=2200, lr=9.8e-05, gnorm=5.078, clip=6, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:34:51]    INFO >> epoch 001:   2300 / 5058 loss=0.068, ups=1.56, num_updates=2300, lr=9.7e-05, gnorm=2.452, clip=5, train_wall=60, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:35:56]    INFO >> epoch 001:   2400 / 5058 loss=0.063, ups=1.61, num_updates=2400, lr=9.7e-05, gnorm=0.773, clip=1, train_wall=58, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:37:05]    INFO >> epoch 001:   2500 / 5058 loss=0.149, ups=1.5, num_updates=2500, lr=9.7e-05, gnorm=3.718, clip=1, train_wall=63, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:38:03]    INFO >> epoch 001:   2600 / 5058 loss=0.128, ups=1.81, num_updates=2600, lr=9.7e-05, gnorm=2.608, clip=1, train_wall=52, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:39:03]    INFO >> epoch 001:   2700 / 5058 loss=0.076, ups=1.74, num_updates=2700, lr=9.7e-05, gnorm=1.387, clip=1, train_wall=54, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:40:05]    INFO >> epoch 001:   2800 / 5058 loss=0.081, ups=1.69, num_updates=2800, lr=9.6e-05, gnorm=1.32, clip=3, train_wall=56, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:40:45]    INFO >> epoch 001:   2900 / 5058 loss=0.274, ups=2.61, num_updates=2900, lr=9.6e-05, gnorm=2.005, clip=3, train_wall=36, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:41:50]    INFO >> epoch 001:   3000 / 5058 loss=0.145, ups=1.59, num_updates=3000, lr=9.6e-05, gnorm=3.103, clip=5, train_wall=59, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:42:43]    INFO >> epoch 001:   3100 / 5058 loss=0.058, ups=1.94, num_updates=3100, lr=9.6e-05, gnorm=0.282, clip=0, train_wall=49, gb_free=73.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:43:41]    INFO >> epoch 001:   3200 / 5058 loss=0.282, ups=1.82, num_updates=3200, lr=9.6e-05, gnorm=2.208, clip=2, train_wall=52, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:44:41]    INFO >> epoch 001:   3300 / 5058 loss=0.086, ups=1.74, num_updates=3300, lr=9.5e-05, gnorm=1.673, clip=1, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:45:52]    INFO >> epoch 001:   3400 / 5058 loss=0.225, ups=1.47, num_updates=3400, lr=9.5e-05, gnorm=5.264, clip=3, train_wall=64, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:46:57]    INFO >> epoch 001:   3500 / 5058 loss=0.131, ups=1.59, num_updates=3500, lr=9.5e-05, gnorm=1.53, clip=0, train_wall=59, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:48:03]    INFO >> epoch 001:   3600 / 5058 loss=0.095, ups=1.59, num_updates=3600, lr=9.5e-05, gnorm=2.575, clip=1, train_wall=59, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:49:03]    INFO >> epoch 001:   3700 / 5058 loss=0.137, ups=1.72, num_updates=3700, lr=9.4e-05, gnorm=1.854, clip=2, train_wall=55, gb_free=66.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:50:10]    INFO >> epoch 001:   3800 / 5058 loss=0.058, ups=1.57, num_updates=3800, lr=9.4e-05, gnorm=1.191, clip=1, train_wall=60, gb_free=68.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:50:59]    INFO >> epoch 001:   3900 / 5058 loss=0.127, ups=2.08, num_updates=3900, lr=9.4e-05, gnorm=1.978, clip=2, train_wall=45, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:52:01]    INFO >> epoch 001:   4000 / 5058 loss=0.131, ups=1.69, num_updates=4000, lr=9.4e-05, gnorm=1.731, clip=1, train_wall=56, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:53:04]    INFO >> epoch 001:   4100 / 5058 loss=0.92, ups=1.66, num_updates=4100, lr=9.4e-05, gnorm=0.902, clip=1, train_wall=57, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:54:11]    INFO >> epoch 001:   4200 / 5058 loss=0.149, ups=1.56, num_updates=4200, lr=9.3e-05, gnorm=1.845, clip=3, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:55:09]    INFO >> epoch 001:   4300 / 5058 loss=0.117, ups=1.76, num_updates=4300, lr=9.3e-05, gnorm=1.721, clip=1, train_wall=53, gb_free=73.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:56:09]    INFO >> epoch 001:   4400 / 5058 loss=0.082, ups=1.72, num_updates=4400, lr=9.3e-05, gnorm=1.476, clip=0, train_wall=55, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:57:12]    INFO >> epoch 001:   4500 / 5058 loss=0.072, ups=1.66, num_updates=4500, lr=9.3e-05, gnorm=1.371, clip=0, train_wall=57, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:58:18]    INFO >> epoch 001:   4600 / 5058 loss=0.241, ups=1.58, num_updates=4600, lr=9.3e-05, gnorm=1.805, clip=0, train_wall=60, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:59:29]    INFO >> epoch 001:   4700 / 5058 loss=0.215, ups=1.48, num_updates=4700, lr=9.2e-05, gnorm=3.803, clip=2, train_wall=63, gb_free=64.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:00:30]    INFO >> epoch 001:   4800 / 5058 loss=0.056, ups=1.71, num_updates=4800, lr=9.2e-05, gnorm=1.411, clip=0, train_wall=55, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:01:11]    INFO >> epoch 001:   4900 / 5058 loss=0.142, ups=2.54, num_updates=4900, lr=9.2e-05, gnorm=0.824, clip=0, train_wall=37, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:02:15]    INFO >> epoch 001:   5000 / 5058 loss=0.09, ups=1.61, num_updates=5000, lr=9.2e-05, gnorm=2.488, clip=0, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:02:55]    INFO >> epoch 001 | loss 0.105 | ups 1.73 | num_updates 5058 | lr 9.2e-05 | gnorm 2.248 | clip 2.1 | train_wall 2751 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-20 08:02:55] WARNING >> 4469 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[2, 4, 10, 12, 13, 14, 16, 18, 19, 23] (data_utils.py:301, filter_by_size())[0m
[33m[2025-11-20 08:02:55] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
CodeEncoderLSTM: Creating BiLSTM with 6 layers, 512 hidden and input size
Traceback (most recent call last):
  File "run/type_prediction/transformer/train.py", line 339, in <module>
    cli_main()
  File "run/type_prediction/transformer/train.py", line 332, in cli_main
    single_main(args)
  File "run/type_prediction/transformer/train.py", line 254, in single_main
    valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)
  File "run/type_prediction/transformer/train.py", line 132, in validate
    trainer.valid_step(sample)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/trainers/ncc_trainers.py", line 565, in valid_step
    _loss, sample_size, logging_output = self.task.valid_step(
TypeError: cannot unpack non-iterable NoneType object

============================================================
Setting up experiment: exp_lr_5e-04
============================================================
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_5e-04
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_5e-04/checkpoints
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_5e-04/logs
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_5e-04/results
âœ“ Saved config to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_5e-04/config.yml
âœ“ Also saved to: run/type_prediction/transformer/config/exp_lr_5e-04.yml
âœ“ Saved experiment info to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_5e-04/info.txt

============================================================
Starting training...
============================================================

Command: python run/type_prediction/transformer/train.py --language exp_lr_5e-04 2>&1 | tee /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_5e-04/logs/train.log

Note: Config file will be loaded from run/type_prediction/transformer/config/exp_lr_5e-04.yml


âœ“ Training completed successfully!

============================================================
Evaluating model on test set...
============================================================

Warning: Best checkpoint not found, using last checkpoint
âœ— No checkpoint found for evaluation

============================================================
âœ“ Experiment completed: exp_lr_5e-04
âœ“ Results in: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_5e-04
============================================================


âœ“ exp_lr_5e-04 completed in 0.85 hours

============================================================
Running: exp_lr_1e-03
GPU: 3
Time: 2025-11-20T08:02:58.312427
============================================================
[32m[2025-11-20 08:02:59]    INFO >> Load arguments in run/type_prediction/transformer/config/exp_lr_1e-03.yml (train.py:300, cli_main())[0m
[32m[2025-11-20 08:02:59]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-03/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-03/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-03/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:302, cli_main())[0m
[32m[2025-11-20 08:02:59]    INFO >> single GPU training... (train.py:331, cli_main())[0m
[32m[2025-11-20 08:03:00]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-03/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-03/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-03/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:208, single_main())[0m
[32m[2025-11-20 08:03:03]    INFO >> TypeTransformer(
  (decoder): RobertaEncoder(
    (encoder): CodeEncoderLSTM(
      (embedding): Embedding(10006, 512)
      (pos_encoder): PositionalEncoding()
      (encoder): LSTM(512, 512, num_layers=6, bidirectional=True)
    )
    (output): Sequential(
      (0): Linear(in_features=1024, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=10006, bias=True)
    )
  )
  (classification_heads): ModuleDict()
) (train.py:220, single_main())[0m
[32m[2025-11-20 08:03:03]    INFO >> model typetransformer, criterion TypePredictionCrossEntropyCriterion (train.py:221, single_main())[0m
[32m[2025-11-20 08:03:03]    INFO >> num. model params: 46481686 (num. trained: 46481686) (train.py:222, single_main())[0m
[32m[2025-11-20 08:03:03]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 08:03:03]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 81024 MB ; used memory = 896 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 08:03:03]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 08:03:03]    INFO >> training on 1 GPUs (train.py:229, single_main())[0m
[32m[2025-11-20 08:03:03]    INFO >> max tokens per GPU = None and max sentences per GPU = 32 (train.py:230, single_main())[0m
[32m[2025-11-20 08:03:03]    INFO >> no existing checkpoint found /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-03/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-20 08:03:03]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[33m[2025-11-20 08:03:13] WARNING >> 32222 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[0, 1, 2, 4, 6, 7, 8, 10, 14, 19] (data_utils.py:301, filter_by_size())[0m
[32m[2025-11-20 08:03:13]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-20 08:03:13] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:398: UserWarning: ntokens not found in Criterion logging outputs, cannot log wpb or wps
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:407: UserWarning: nsentences not found in Criterion logging outputs, cannot log bsz
  warnings.warn(
[32m[2025-11-20 08:03:57]    INFO >> epoch 001:    100 / 5058 loss=1.903, ups=2.36, num_updates=100, lr=1e-05, gnorm=0.104, clip=0, train_wall=40, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:04:46]    INFO >> epoch 001:    200 / 5058 loss=0.786, ups=2.16, num_updates=200, lr=2e-05, gnorm=0.068, clip=0, train_wall=44, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:05:51]    INFO >> epoch 001:    300 / 5058 loss=0.654, ups=1.63, num_updates=300, lr=3e-05, gnorm=0.053, clip=0, train_wall=58, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:06:55]    INFO >> epoch 001:    400 / 5058 loss=0.623, ups=1.59, num_updates=400, lr=4e-05, gnorm=2.819, clip=2, train_wall=59, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:07:57]    INFO >> epoch 001:    500 / 5058 loss=0.208, ups=1.67, num_updates=500, lr=5e-05, gnorm=4.476, clip=2, train_wall=56, gb_free=65.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:08:58]    INFO >> epoch 001:    600 / 5058 loss=0.079, ups=1.71, num_updates=600, lr=6e-05, gnorm=4.788, clip=8, train_wall=55, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:09:41]    INFO >> epoch 001:    700 / 5058 loss=0.174, ups=2.42, num_updates=700, lr=7e-05, gnorm=2.641, clip=4, train_wall=39, gb_free=69.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:10:24]    INFO >> epoch 001:    800 / 5058 loss=0.166, ups=2.46, num_updates=800, lr=8e-05, gnorm=3.325, clip=8, train_wall=39, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:11:31]    INFO >> epoch 001:    900 / 5058 loss=0.216, ups=1.56, num_updates=900, lr=9e-05, gnorm=6.91, clip=8, train_wall=60, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:12:36]    INFO >> epoch 001:   1000 / 5058 loss=0.142, ups=1.62, num_updates=1000, lr=0.0001, gnorm=1.451, clip=0, train_wall=58, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:13:34]    INFO >> epoch 001:   1100 / 5058 loss=0.058, ups=1.76, num_updates=1100, lr=0.0001, gnorm=2.202, clip=2, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:14:38]    INFO >> epoch 001:   1200 / 5058 loss=0.035, ups=1.64, num_updates=1200, lr=0.0001, gnorm=0.871, clip=0, train_wall=57, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:15:42]    INFO >> epoch 001:   1300 / 5058 loss=0.087, ups=1.61, num_updates=1300, lr=9.9e-05, gnorm=1.877, clip=2, train_wall=58, gb_free=67.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:16:46]    INFO >> epoch 001:   1400 / 5058 loss=0.059, ups=1.64, num_updates=1400, lr=9.9e-05, gnorm=2.488, clip=4, train_wall=57, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:17:51]    INFO >> epoch 001:   1500 / 5058 loss=0.279, ups=1.61, num_updates=1500, lr=9.9e-05, gnorm=1.776, clip=2, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:18:51]    INFO >> epoch 001:   1600 / 5058 loss=0.11, ups=1.73, num_updates=1600, lr=9.9e-05, gnorm=3.243, clip=4, train_wall=55, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:19:57]    INFO >> epoch 001:   1700 / 5058 loss=0.113, ups=1.58, num_updates=1700, lr=9.9e-05, gnorm=2.319, clip=2, train_wall=59, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:20:51]    INFO >> epoch 001:   1800 / 5058 loss=0.12, ups=1.91, num_updates=1800, lr=9.8e-05, gnorm=3.115, clip=5, train_wall=49, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:21:31]    INFO >> epoch 001:   1900 / 5058 loss=0.153, ups=2.66, num_updates=1900, lr=9.8e-05, gnorm=1.952, clip=4, train_wall=36, gb_free=67.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:22:32]    INFO >> epoch 001:   2000 / 5058 loss=0.089, ups=1.69, num_updates=2000, lr=9.8e-05, gnorm=1.422, clip=1, train_wall=56, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:23:38]    INFO >> epoch 001:   2100 / 5058 loss=0.074, ups=1.58, num_updates=2100, lr=9.8e-05, gnorm=3.142, clip=2, train_wall=60, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:24:44]    INFO >> epoch 001:   2200 / 5058 loss=0.279, ups=1.58, num_updates=2200, lr=9.8e-05, gnorm=5.078, clip=6, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:25:51]    INFO >> epoch 001:   2300 / 5058 loss=0.068, ups=1.56, num_updates=2300, lr=9.7e-05, gnorm=2.452, clip=5, train_wall=60, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:26:55]    INFO >> epoch 001:   2400 / 5058 loss=0.063, ups=1.61, num_updates=2400, lr=9.7e-05, gnorm=0.773, clip=1, train_wall=58, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:28:05]    INFO >> epoch 001:   2500 / 5058 loss=0.149, ups=1.5, num_updates=2500, lr=9.7e-05, gnorm=3.718, clip=1, train_wall=63, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:29:03]    INFO >> epoch 001:   2600 / 5058 loss=0.128, ups=1.81, num_updates=2600, lr=9.7e-05, gnorm=2.608, clip=1, train_wall=52, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:30:03]    INFO >> epoch 001:   2700 / 5058 loss=0.076, ups=1.74, num_updates=2700, lr=9.7e-05, gnorm=1.387, clip=1, train_wall=54, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:31:05]    INFO >> epoch 001:   2800 / 5058 loss=0.081, ups=1.69, num_updates=2800, lr=9.6e-05, gnorm=1.32, clip=3, train_wall=56, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:31:44]    INFO >> epoch 001:   2900 / 5058 loss=0.274, ups=2.61, num_updates=2900, lr=9.6e-05, gnorm=2.005, clip=3, train_wall=36, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:32:50]    INFO >> epoch 001:   3000 / 5058 loss=0.145, ups=1.59, num_updates=3000, lr=9.6e-05, gnorm=3.103, clip=5, train_wall=59, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:33:43]    INFO >> epoch 001:   3100 / 5058 loss=0.058, ups=1.94, num_updates=3100, lr=9.6e-05, gnorm=0.282, clip=0, train_wall=49, gb_free=73.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:34:41]    INFO >> epoch 001:   3200 / 5058 loss=0.282, ups=1.82, num_updates=3200, lr=9.6e-05, gnorm=2.208, clip=2, train_wall=52, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:35:41]    INFO >> epoch 001:   3300 / 5058 loss=0.086, ups=1.74, num_updates=3300, lr=9.5e-05, gnorm=1.673, clip=1, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:36:52]    INFO >> epoch 001:   3400 / 5058 loss=0.225, ups=1.47, num_updates=3400, lr=9.5e-05, gnorm=5.264, clip=3, train_wall=64, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:37:57]    INFO >> epoch 001:   3500 / 5058 loss=0.131, ups=1.59, num_updates=3500, lr=9.5e-05, gnorm=1.53, clip=0, train_wall=59, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:39:03]    INFO >> epoch 001:   3600 / 5058 loss=0.095, ups=1.59, num_updates=3600, lr=9.5e-05, gnorm=2.575, clip=1, train_wall=59, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:40:03]    INFO >> epoch 001:   3700 / 5058 loss=0.137, ups=1.72, num_updates=3700, lr=9.4e-05, gnorm=1.854, clip=2, train_wall=55, gb_free=66.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:41:10]    INFO >> epoch 001:   3800 / 5058 loss=0.058, ups=1.57, num_updates=3800, lr=9.4e-05, gnorm=1.191, clip=1, train_wall=60, gb_free=68.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:41:59]    INFO >> epoch 001:   3900 / 5058 loss=0.127, ups=2.08, num_updates=3900, lr=9.4e-05, gnorm=1.978, clip=2, train_wall=45, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:43:01]    INFO >> epoch 001:   4000 / 5058 loss=0.131, ups=1.69, num_updates=4000, lr=9.4e-05, gnorm=1.731, clip=1, train_wall=56, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:44:04]    INFO >> epoch 001:   4100 / 5058 loss=0.92, ups=1.66, num_updates=4100, lr=9.4e-05, gnorm=0.902, clip=1, train_wall=57, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:45:11]    INFO >> epoch 001:   4200 / 5058 loss=0.149, ups=1.56, num_updates=4200, lr=9.3e-05, gnorm=1.845, clip=3, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:46:10]    INFO >> epoch 001:   4300 / 5058 loss=0.117, ups=1.76, num_updates=4300, lr=9.3e-05, gnorm=1.721, clip=1, train_wall=53, gb_free=73.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:47:09]    INFO >> epoch 001:   4400 / 5058 loss=0.082, ups=1.72, num_updates=4400, lr=9.3e-05, gnorm=1.476, clip=0, train_wall=55, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:48:12]    INFO >> epoch 001:   4500 / 5058 loss=0.072, ups=1.66, num_updates=4500, lr=9.3e-05, gnorm=1.371, clip=0, train_wall=57, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:49:18]    INFO >> epoch 001:   4600 / 5058 loss=0.241, ups=1.58, num_updates=4600, lr=9.3e-05, gnorm=1.805, clip=0, train_wall=60, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:50:29]    INFO >> epoch 001:   4700 / 5058 loss=0.215, ups=1.48, num_updates=4700, lr=9.2e-05, gnorm=3.803, clip=2, train_wall=63, gb_free=64.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:51:30]    INFO >> epoch 001:   4800 / 5058 loss=0.056, ups=1.71, num_updates=4800, lr=9.2e-05, gnorm=1.411, clip=0, train_wall=55, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:52:10]    INFO >> epoch 001:   4900 / 5058 loss=0.142, ups=2.54, num_updates=4900, lr=9.2e-05, gnorm=0.824, clip=0, train_wall=37, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:53:15]    INFO >> epoch 001:   5000 / 5058 loss=0.09, ups=1.61, num_updates=5000, lr=9.2e-05, gnorm=2.488, clip=0, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:53:55]    INFO >> epoch 001 | loss 0.105 | ups 1.73 | num_updates 5058 | lr 9.2e-05 | gnorm 2.248 | clip 2.1 | train_wall 2750 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-20 08:53:55] WARNING >> 4469 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[2, 4, 10, 12, 13, 14, 16, 18, 19, 23] (data_utils.py:301, filter_by_size())[0m
[33m[2025-11-20 08:53:55] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
CodeEncoderLSTM: Creating BiLSTM with 6 layers, 512 hidden and input size
Traceback (most recent call last):
  File "run/type_prediction/transformer/train.py", line 339, in <module>
    cli_main()
  File "run/type_prediction/transformer/train.py", line 332, in cli_main
    single_main(args)
  File "run/type_prediction/transformer/train.py", line 254, in single_main
    valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)
  File "run/type_prediction/transformer/train.py", line 132, in validate
    trainer.valid_step(sample)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/trainers/ncc_trainers.py", line 565, in valid_step
    _loss, sample_size, logging_output = self.task.valid_step(
TypeError: cannot unpack non-iterable NoneType object

============================================================
Setting up experiment: exp_lr_1e-03
============================================================
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-03
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-03/checkpoints
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-03/logs
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-03/results
âœ“ Saved config to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-03/config.yml
âœ“ Also saved to: run/type_prediction/transformer/config/exp_lr_1e-03.yml
âœ“ Saved experiment info to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-03/info.txt

============================================================
Starting training...
============================================================

Command: python run/type_prediction/transformer/train.py --language exp_lr_1e-03 2>&1 | tee /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-03/logs/train.log

Note: Config file will be loaded from run/type_prediction/transformer/config/exp_lr_1e-03.yml


âœ“ Training completed successfully!

============================================================
Evaluating model on test set...
============================================================

Warning: Best checkpoint not found, using last checkpoint
âœ— No checkpoint found for evaluation

============================================================
âœ“ Experiment completed: exp_lr_1e-03
âœ“ Results in: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-03
============================================================


âœ“ exp_lr_1e-03 completed in 0.85 hours

============================================================
Running: exp_dropout_0.0
GPU: 3
Time: 2025-11-20T08:53:58.105704
============================================================
[32m[2025-11-20 08:54:01]    INFO >> Load arguments in run/type_prediction/transformer/config/exp_dropout_0.0.yml (train.py:300, cli_main())[0m
[32m[2025-11-20 08:54:01]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.0/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.0/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.0/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:302, cli_main())[0m
[32m[2025-11-20 08:54:01]    INFO >> single GPU training... (train.py:331, cli_main())[0m
[32m[2025-11-20 08:54:01]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.0/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.0/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.0/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:208, single_main())[0m
[32m[2025-11-20 08:54:02]    INFO >> TypeTransformer(
  (decoder): RobertaEncoder(
    (encoder): CodeEncoderLSTM(
      (embedding): Embedding(10006, 512)
      (pos_encoder): PositionalEncoding()
      (encoder): LSTM(512, 512, num_layers=6, bidirectional=True)
    )
    (output): Sequential(
      (0): Linear(in_features=1024, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=10006, bias=True)
    )
  )
  (classification_heads): ModuleDict()
) (train.py:220, single_main())[0m
[32m[2025-11-20 08:54:02]    INFO >> model typetransformer, criterion TypePredictionCrossEntropyCriterion (train.py:221, single_main())[0m
[32m[2025-11-20 08:54:02]    INFO >> num. model params: 46481686 (num. trained: 46481686) (train.py:222, single_main())[0m
[32m[2025-11-20 08:54:03]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 08:54:03]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 81024 MB ; used memory = 896 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 08:54:03]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 08:54:03]    INFO >> training on 1 GPUs (train.py:229, single_main())[0m
[32m[2025-11-20 08:54:03]    INFO >> max tokens per GPU = None and max sentences per GPU = 32 (train.py:230, single_main())[0m
[32m[2025-11-20 08:54:03]    INFO >> no existing checkpoint found /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.0/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-20 08:54:03]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[33m[2025-11-20 08:54:13] WARNING >> 32222 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[0, 1, 2, 4, 6, 7, 8, 10, 14, 19] (data_utils.py:301, filter_by_size())[0m
[32m[2025-11-20 08:54:13]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-20 08:54:13] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:398: UserWarning: ntokens not found in Criterion logging outputs, cannot log wpb or wps
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:407: UserWarning: nsentences not found in Criterion logging outputs, cannot log bsz
  warnings.warn(
[32m[2025-11-20 08:54:57]    INFO >> epoch 001:    100 / 5058 loss=1.903, ups=2.37, num_updates=100, lr=1e-05, gnorm=0.104, clip=0, train_wall=40, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:55:46]    INFO >> epoch 001:    200 / 5058 loss=0.786, ups=2.16, num_updates=200, lr=2e-05, gnorm=0.068, clip=0, train_wall=44, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:56:50]    INFO >> epoch 001:    300 / 5058 loss=0.654, ups=1.63, num_updates=300, lr=3e-05, gnorm=0.053, clip=0, train_wall=58, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:57:56]    INFO >> epoch 001:    400 / 5058 loss=0.623, ups=1.59, num_updates=400, lr=4e-05, gnorm=2.819, clip=2, train_wall=59, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:58:57]    INFO >> epoch 001:    500 / 5058 loss=0.208, ups=1.67, num_updates=500, lr=5e-05, gnorm=4.476, clip=2, train_wall=56, gb_free=65.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:59:58]    INFO >> epoch 001:    600 / 5058 loss=0.079, ups=1.71, num_updates=600, lr=6e-05, gnorm=4.788, clip=8, train_wall=55, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:00:41]    INFO >> epoch 001:    700 / 5058 loss=0.174, ups=2.43, num_updates=700, lr=7e-05, gnorm=2.641, clip=4, train_wall=39, gb_free=69.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:01:24]    INFO >> epoch 001:    800 / 5058 loss=0.166, ups=2.46, num_updates=800, lr=8e-05, gnorm=3.325, clip=8, train_wall=39, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:02:31]    INFO >> epoch 001:    900 / 5058 loss=0.216, ups=1.56, num_updates=900, lr=9e-05, gnorm=6.91, clip=8, train_wall=60, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:03:35]    INFO >> epoch 001:   1000 / 5058 loss=0.142, ups=1.62, num_updates=1000, lr=0.0001, gnorm=1.451, clip=0, train_wall=58, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:04:33]    INFO >> epoch 001:   1100 / 5058 loss=0.058, ups=1.76, num_updates=1100, lr=0.0001, gnorm=2.202, clip=2, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:05:37]    INFO >> epoch 001:   1200 / 5058 loss=0.035, ups=1.64, num_updates=1200, lr=0.0001, gnorm=0.871, clip=0, train_wall=57, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:06:42]    INFO >> epoch 001:   1300 / 5058 loss=0.087, ups=1.61, num_updates=1300, lr=9.9e-05, gnorm=1.877, clip=2, train_wall=59, gb_free=67.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:07:45]    INFO >> epoch 001:   1400 / 5058 loss=0.059, ups=1.65, num_updates=1400, lr=9.9e-05, gnorm=2.488, clip=4, train_wall=57, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:08:50]    INFO >> epoch 001:   1500 / 5058 loss=0.279, ups=1.61, num_updates=1500, lr=9.9e-05, gnorm=1.776, clip=2, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:09:51]    INFO >> epoch 001:   1600 / 5058 loss=0.11, ups=1.73, num_updates=1600, lr=9.9e-05, gnorm=3.243, clip=4, train_wall=55, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:10:57]    INFO >> epoch 001:   1700 / 5058 loss=0.113, ups=1.58, num_updates=1700, lr=9.9e-05, gnorm=2.319, clip=2, train_wall=59, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:11:50]    INFO >> epoch 001:   1800 / 5058 loss=0.12, ups=1.91, num_updates=1800, lr=9.8e-05, gnorm=3.115, clip=5, train_wall=49, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:12:31]    INFO >> epoch 001:   1900 / 5058 loss=0.153, ups=2.66, num_updates=1900, lr=9.8e-05, gnorm=1.952, clip=4, train_wall=36, gb_free=67.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:13:31]    INFO >> epoch 001:   2000 / 5058 loss=0.089, ups=1.69, num_updates=2000, lr=9.8e-05, gnorm=1.422, clip=1, train_wall=56, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:14:37]    INFO >> epoch 001:   2100 / 5058 loss=0.074, ups=1.58, num_updates=2100, lr=9.8e-05, gnorm=3.142, clip=2, train_wall=60, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:15:43]    INFO >> epoch 001:   2200 / 5058 loss=0.279, ups=1.57, num_updates=2200, lr=9.8e-05, gnorm=5.078, clip=6, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:16:50]    INFO >> epoch 001:   2300 / 5058 loss=0.068, ups=1.56, num_updates=2300, lr=9.7e-05, gnorm=2.452, clip=5, train_wall=60, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:17:55]    INFO >> epoch 001:   2400 / 5058 loss=0.063, ups=1.61, num_updates=2400, lr=9.7e-05, gnorm=0.773, clip=1, train_wall=58, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:19:04]    INFO >> epoch 001:   2500 / 5058 loss=0.149, ups=1.5, num_updates=2500, lr=9.7e-05, gnorm=3.718, clip=1, train_wall=63, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:20:02]    INFO >> epoch 001:   2600 / 5058 loss=0.128, ups=1.81, num_updates=2600, lr=9.7e-05, gnorm=2.608, clip=1, train_wall=52, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:21:02]    INFO >> epoch 001:   2700 / 5058 loss=0.076, ups=1.74, num_updates=2700, lr=9.7e-05, gnorm=1.387, clip=1, train_wall=54, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:22:04]    INFO >> epoch 001:   2800 / 5058 loss=0.081, ups=1.69, num_updates=2800, lr=9.6e-05, gnorm=1.32, clip=3, train_wall=56, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:22:44]    INFO >> epoch 001:   2900 / 5058 loss=0.274, ups=2.61, num_updates=2900, lr=9.6e-05, gnorm=2.005, clip=3, train_wall=36, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:23:49]    INFO >> epoch 001:   3000 / 5058 loss=0.145, ups=1.59, num_updates=3000, lr=9.6e-05, gnorm=3.103, clip=5, train_wall=59, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:24:42]    INFO >> epoch 001:   3100 / 5058 loss=0.058, ups=1.94, num_updates=3100, lr=9.6e-05, gnorm=0.282, clip=0, train_wall=49, gb_free=73.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:25:40]    INFO >> epoch 001:   3200 / 5058 loss=0.282, ups=1.82, num_updates=3200, lr=9.6e-05, gnorm=2.208, clip=2, train_wall=52, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:26:40]    INFO >> epoch 001:   3300 / 5058 loss=0.086, ups=1.74, num_updates=3300, lr=9.5e-05, gnorm=1.673, clip=1, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:27:51]    INFO >> epoch 001:   3400 / 5058 loss=0.225, ups=1.47, num_updates=3400, lr=9.5e-05, gnorm=5.264, clip=3, train_wall=64, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:28:56]    INFO >> epoch 001:   3500 / 5058 loss=0.131, ups=1.6, num_updates=3500, lr=9.5e-05, gnorm=1.53, clip=0, train_wall=59, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:30:02]    INFO >> epoch 001:   3600 / 5058 loss=0.095, ups=1.59, num_updates=3600, lr=9.5e-05, gnorm=2.575, clip=1, train_wall=59, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:31:02]    INFO >> epoch 001:   3700 / 5058 loss=0.137, ups=1.72, num_updates=3700, lr=9.4e-05, gnorm=1.854, clip=2, train_wall=55, gb_free=66.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:32:09]    INFO >> epoch 001:   3800 / 5058 loss=0.058, ups=1.57, num_updates=3800, lr=9.4e-05, gnorm=1.191, clip=1, train_wall=60, gb_free=68.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:32:58]    INFO >> epoch 001:   3900 / 5058 loss=0.127, ups=2.08, num_updates=3900, lr=9.4e-05, gnorm=1.978, clip=2, train_wall=45, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:34:00]    INFO >> epoch 001:   4000 / 5058 loss=0.131, ups=1.69, num_updates=4000, lr=9.4e-05, gnorm=1.731, clip=1, train_wall=56, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:35:03]    INFO >> epoch 001:   4100 / 5058 loss=0.92, ups=1.66, num_updates=4100, lr=9.4e-05, gnorm=0.902, clip=1, train_wall=57, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:36:10]    INFO >> epoch 001:   4200 / 5058 loss=0.149, ups=1.56, num_updates=4200, lr=9.3e-05, gnorm=1.845, clip=3, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:37:09]    INFO >> epoch 001:   4300 / 5058 loss=0.117, ups=1.76, num_updates=4300, lr=9.3e-05, gnorm=1.721, clip=1, train_wall=54, gb_free=73.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:38:08]    INFO >> epoch 001:   4400 / 5058 loss=0.082, ups=1.72, num_updates=4400, lr=9.3e-05, gnorm=1.476, clip=0, train_wall=55, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:39:11]    INFO >> epoch 001:   4500 / 5058 loss=0.072, ups=1.66, num_updates=4500, lr=9.3e-05, gnorm=1.371, clip=0, train_wall=57, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:40:17]    INFO >> epoch 001:   4600 / 5058 loss=0.241, ups=1.58, num_updates=4600, lr=9.3e-05, gnorm=1.805, clip=0, train_wall=60, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:41:28]    INFO >> epoch 001:   4700 / 5058 loss=0.215, ups=1.48, num_updates=4700, lr=9.2e-05, gnorm=3.803, clip=2, train_wall=64, gb_free=64.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:42:29]    INFO >> epoch 001:   4800 / 5058 loss=0.056, ups=1.71, num_updates=4800, lr=9.2e-05, gnorm=1.411, clip=0, train_wall=55, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:43:10]    INFO >> epoch 001:   4900 / 5058 loss=0.142, ups=2.54, num_updates=4900, lr=9.2e-05, gnorm=0.824, clip=0, train_wall=37, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:44:14]    INFO >> epoch 001:   5000 / 5058 loss=0.09, ups=1.62, num_updates=5000, lr=9.2e-05, gnorm=2.488, clip=0, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:44:54]    INFO >> epoch 001 | loss 0.105 | ups 1.73 | num_updates 5058 | lr 9.2e-05 | gnorm 2.248 | clip 2.1 | train_wall 2754 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-20 09:44:54] WARNING >> 4469 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[2, 4, 10, 12, 13, 14, 16, 18, 19, 23] (data_utils.py:301, filter_by_size())[0m
[33m[2025-11-20 09:44:54] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
CodeEncoderLSTM: Creating BiLSTM with 6 layers, 512 hidden and input size
Traceback (most recent call last):
  File "run/type_prediction/transformer/train.py", line 339, in <module>
    cli_main()
  File "run/type_prediction/transformer/train.py", line 332, in cli_main
    single_main(args)
  File "run/type_prediction/transformer/train.py", line 254, in single_main
    valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)
  File "run/type_prediction/transformer/train.py", line 132, in validate
    trainer.valid_step(sample)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/trainers/ncc_trainers.py", line 565, in valid_step
    _loss, sample_size, logging_output = self.task.valid_step(
TypeError: cannot unpack non-iterable NoneType object

============================================================
Setting up experiment: exp_dropout_0.0
============================================================
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.0
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.0/checkpoints
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.0/logs
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.0/results
âœ“ Saved config to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.0/config.yml
âœ“ Also saved to: run/type_prediction/transformer/config/exp_dropout_0.0.yml
âœ“ Saved experiment info to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.0/info.txt

============================================================
Starting training...
============================================================

Command: python run/type_prediction/transformer/train.py --language exp_dropout_0.0 2>&1 | tee /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.0/logs/train.log

Note: Config file will be loaded from run/type_prediction/transformer/config/exp_dropout_0.0.yml


âœ“ Training completed successfully!

============================================================
Evaluating model on test set...
============================================================

Warning: Best checkpoint not found, using last checkpoint
âœ— No checkpoint found for evaluation

============================================================
âœ“ Experiment completed: exp_dropout_0.0
âœ“ Results in: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.0
============================================================


âœ“ exp_dropout_0.0 completed in 0.85 hours

============================================================
Running: exp_dropout_0.1
GPU: 3
Time: 2025-11-20T09:44:57.142218
============================================================
[32m[2025-11-20 09:45:00]    INFO >> Load arguments in run/type_prediction/transformer/config/exp_dropout_0.1.yml (train.py:300, cli_main())[0m
[32m[2025-11-20 09:45:00]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.1/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.1/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.1/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:302, cli_main())[0m
[32m[2025-11-20 09:45:00]    INFO >> single GPU training... (train.py:331, cli_main())[0m
[32m[2025-11-20 09:45:00]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.1/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.1/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.1/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:208, single_main())[0m
[32m[2025-11-20 09:45:01]    INFO >> TypeTransformer(
  (decoder): RobertaEncoder(
    (encoder): CodeEncoderLSTM(
      (embedding): Embedding(10006, 512)
      (pos_encoder): PositionalEncoding()
      (encoder): LSTM(512, 512, num_layers=6, bidirectional=True)
    )
    (output): Sequential(
      (0): Linear(in_features=1024, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=10006, bias=True)
    )
  )
  (classification_heads): ModuleDict()
) (train.py:220, single_main())[0m
[32m[2025-11-20 09:45:01]    INFO >> model typetransformer, criterion TypePredictionCrossEntropyCriterion (train.py:221, single_main())[0m
[32m[2025-11-20 09:45:01]    INFO >> num. model params: 46481686 (num. trained: 46481686) (train.py:222, single_main())[0m
[32m[2025-11-20 09:45:02]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 09:45:02]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 81024 MB ; used memory = 896 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 09:45:02]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 09:45:02]    INFO >> training on 1 GPUs (train.py:229, single_main())[0m
[32m[2025-11-20 09:45:02]    INFO >> max tokens per GPU = None and max sentences per GPU = 32 (train.py:230, single_main())[0m
[32m[2025-11-20 09:45:02]    INFO >> no existing checkpoint found /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.1/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-20 09:45:02]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[33m[2025-11-20 09:45:12] WARNING >> 32222 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[0, 1, 2, 4, 6, 7, 8, 10, 14, 19] (data_utils.py:301, filter_by_size())[0m
[32m[2025-11-20 09:45:12]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-20 09:45:12] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:398: UserWarning: ntokens not found in Criterion logging outputs, cannot log wpb or wps
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:407: UserWarning: nsentences not found in Criterion logging outputs, cannot log bsz
  warnings.warn(
[32m[2025-11-20 09:45:56]    INFO >> epoch 001:    100 / 5058 loss=1.903, ups=2.37, num_updates=100, lr=1e-05, gnorm=0.104, clip=0, train_wall=40, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:46:45]    INFO >> epoch 001:    200 / 5058 loss=0.786, ups=2.16, num_updates=200, lr=2e-05, gnorm=0.068, clip=0, train_wall=44, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:47:49]    INFO >> epoch 001:    300 / 5058 loss=0.654, ups=1.63, num_updates=300, lr=3e-05, gnorm=0.053, clip=0, train_wall=58, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:48:55]    INFO >> epoch 001:    400 / 5058 loss=0.623, ups=1.59, num_updates=400, lr=4e-05, gnorm=2.819, clip=2, train_wall=59, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:49:56]    INFO >> epoch 001:    500 / 5058 loss=0.208, ups=1.67, num_updates=500, lr=5e-05, gnorm=4.476, clip=2, train_wall=56, gb_free=65.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:50:57]    INFO >> epoch 001:    600 / 5058 loss=0.079, ups=1.71, num_updates=600, lr=6e-05, gnorm=4.788, clip=8, train_wall=55, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:51:40]    INFO >> epoch 001:    700 / 5058 loss=0.174, ups=2.42, num_updates=700, lr=7e-05, gnorm=2.641, clip=4, train_wall=39, gb_free=69.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:52:23]    INFO >> epoch 001:    800 / 5058 loss=0.166, ups=2.46, num_updates=800, lr=8e-05, gnorm=3.325, clip=8, train_wall=39, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:53:30]    INFO >> epoch 001:    900 / 5058 loss=0.216, ups=1.56, num_updates=900, lr=9e-05, gnorm=6.91, clip=8, train_wall=60, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:54:33]    INFO >> epoch 001:   1000 / 5058 loss=0.142, ups=1.62, num_updates=1000, lr=0.0001, gnorm=1.451, clip=0, train_wall=58, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:55:32]    INFO >> epoch 001:   1100 / 5058 loss=0.058, ups=1.76, num_updates=1100, lr=0.0001, gnorm=2.202, clip=2, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:56:36]    INFO >> epoch 001:   1200 / 5058 loss=0.035, ups=1.64, num_updates=1200, lr=0.0001, gnorm=0.871, clip=0, train_wall=57, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:57:41]    INFO >> epoch 001:   1300 / 5058 loss=0.087, ups=1.61, num_updates=1300, lr=9.9e-05, gnorm=1.877, clip=2, train_wall=58, gb_free=67.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:58:44]    INFO >> epoch 001:   1400 / 5058 loss=0.059, ups=1.65, num_updates=1400, lr=9.9e-05, gnorm=2.488, clip=4, train_wall=57, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:59:49]    INFO >> epoch 001:   1500 / 5058 loss=0.279, ups=1.61, num_updates=1500, lr=9.9e-05, gnorm=1.776, clip=2, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:00:50]    INFO >> epoch 001:   1600 / 5058 loss=0.11, ups=1.73, num_updates=1600, lr=9.9e-05, gnorm=3.243, clip=4, train_wall=55, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:01:56]    INFO >> epoch 001:   1700 / 5058 loss=0.113, ups=1.58, num_updates=1700, lr=9.9e-05, gnorm=2.319, clip=2, train_wall=60, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:02:49]    INFO >> epoch 001:   1800 / 5058 loss=0.12, ups=1.91, num_updates=1800, lr=9.8e-05, gnorm=3.115, clip=5, train_wall=49, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:03:30]    INFO >> epoch 001:   1900 / 5058 loss=0.153, ups=2.66, num_updates=1900, lr=9.8e-05, gnorm=1.952, clip=4, train_wall=36, gb_free=67.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:04:30]    INFO >> epoch 001:   2000 / 5058 loss=0.089, ups=1.69, num_updates=2000, lr=9.8e-05, gnorm=1.422, clip=1, train_wall=56, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:05:36]    INFO >> epoch 001:   2100 / 5058 loss=0.074, ups=1.58, num_updates=2100, lr=9.8e-05, gnorm=3.142, clip=2, train_wall=60, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:06:42]    INFO >> epoch 001:   2200 / 5058 loss=0.279, ups=1.57, num_updates=2200, lr=9.8e-05, gnorm=5.078, clip=6, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:07:49]    INFO >> epoch 001:   2300 / 5058 loss=0.068, ups=1.56, num_updates=2300, lr=9.7e-05, gnorm=2.452, clip=5, train_wall=60, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:08:54]    INFO >> epoch 001:   2400 / 5058 loss=0.063, ups=1.61, num_updates=2400, lr=9.7e-05, gnorm=0.773, clip=1, train_wall=58, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:10:03]    INFO >> epoch 001:   2500 / 5058 loss=0.149, ups=1.5, num_updates=2500, lr=9.7e-05, gnorm=3.718, clip=1, train_wall=63, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:11:01]    INFO >> epoch 001:   2600 / 5058 loss=0.128, ups=1.82, num_updates=2600, lr=9.7e-05, gnorm=2.608, clip=1, train_wall=52, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:12:01]    INFO >> epoch 001:   2700 / 5058 loss=0.076, ups=1.74, num_updates=2700, lr=9.7e-05, gnorm=1.387, clip=1, train_wall=54, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:13:03]    INFO >> epoch 001:   2800 / 5058 loss=0.081, ups=1.69, num_updates=2800, lr=9.6e-05, gnorm=1.32, clip=3, train_wall=56, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:13:43]    INFO >> epoch 001:   2900 / 5058 loss=0.274, ups=2.61, num_updates=2900, lr=9.6e-05, gnorm=2.005, clip=3, train_wall=36, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:14:48]    INFO >> epoch 001:   3000 / 5058 loss=0.145, ups=1.59, num_updates=3000, lr=9.6e-05, gnorm=3.103, clip=5, train_wall=59, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:15:41]    INFO >> epoch 001:   3100 / 5058 loss=0.058, ups=1.94, num_updates=3100, lr=9.6e-05, gnorm=0.282, clip=0, train_wall=49, gb_free=73.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:16:39]    INFO >> epoch 001:   3200 / 5058 loss=0.282, ups=1.82, num_updates=3200, lr=9.6e-05, gnorm=2.208, clip=2, train_wall=52, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:17:39]    INFO >> epoch 001:   3300 / 5058 loss=0.086, ups=1.74, num_updates=3300, lr=9.5e-05, gnorm=1.673, clip=1, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:18:50]    INFO >> epoch 001:   3400 / 5058 loss=0.225, ups=1.47, num_updates=3400, lr=9.5e-05, gnorm=5.264, clip=3, train_wall=64, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:19:55]    INFO >> epoch 001:   3500 / 5058 loss=0.131, ups=1.6, num_updates=3500, lr=9.5e-05, gnorm=1.53, clip=0, train_wall=59, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:21:01]    INFO >> epoch 001:   3600 / 5058 loss=0.095, ups=1.6, num_updates=3600, lr=9.5e-05, gnorm=2.575, clip=1, train_wall=59, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:22:01]    INFO >> epoch 001:   3700 / 5058 loss=0.137, ups=1.72, num_updates=3700, lr=9.4e-05, gnorm=1.854, clip=2, train_wall=55, gb_free=66.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:23:08]    INFO >> epoch 001:   3800 / 5058 loss=0.058, ups=1.57, num_updates=3800, lr=9.4e-05, gnorm=1.191, clip=1, train_wall=60, gb_free=68.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:23:57]    INFO >> epoch 001:   3900 / 5058 loss=0.127, ups=2.09, num_updates=3900, lr=9.4e-05, gnorm=1.978, clip=2, train_wall=45, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:24:59]    INFO >> epoch 001:   4000 / 5058 loss=0.131, ups=1.69, num_updates=4000, lr=9.4e-05, gnorm=1.731, clip=1, train_wall=56, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:26:02]    INFO >> epoch 001:   4100 / 5058 loss=0.92, ups=1.66, num_updates=4100, lr=9.4e-05, gnorm=0.902, clip=1, train_wall=57, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:27:08]    INFO >> epoch 001:   4200 / 5058 loss=0.149, ups=1.56, num_updates=4200, lr=9.3e-05, gnorm=1.845, clip=3, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:28:08]    INFO >> epoch 001:   4300 / 5058 loss=0.117, ups=1.76, num_updates=4300, lr=9.3e-05, gnorm=1.721, clip=1, train_wall=54, gb_free=73.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:29:07]    INFO >> epoch 001:   4400 / 5058 loss=0.082, ups=1.72, num_updates=4400, lr=9.3e-05, gnorm=1.476, clip=0, train_wall=55, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:30:10]    INFO >> epoch 001:   4500 / 5058 loss=0.072, ups=1.66, num_updates=4500, lr=9.3e-05, gnorm=1.371, clip=0, train_wall=57, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:31:16]    INFO >> epoch 001:   4600 / 5058 loss=0.241, ups=1.58, num_updates=4600, lr=9.3e-05, gnorm=1.805, clip=0, train_wall=60, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:32:26]    INFO >> epoch 001:   4700 / 5058 loss=0.215, ups=1.48, num_updates=4700, lr=9.2e-05, gnorm=3.803, clip=2, train_wall=63, gb_free=64.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:33:28]    INFO >> epoch 001:   4800 / 5058 loss=0.056, ups=1.71, num_updates=4800, lr=9.2e-05, gnorm=1.411, clip=0, train_wall=55, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:34:08]    INFO >> epoch 001:   4900 / 5058 loss=0.142, ups=2.54, num_updates=4900, lr=9.2e-05, gnorm=0.824, clip=0, train_wall=37, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:35:13]    INFO >> epoch 001:   5000 / 5058 loss=0.09, ups=1.62, num_updates=5000, lr=9.2e-05, gnorm=2.488, clip=0, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:35:52]    INFO >> epoch 001 | loss 0.105 | ups 1.73 | num_updates 5058 | lr 9.2e-05 | gnorm 2.248 | clip 2.1 | train_wall 2751 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-20 10:35:52] WARNING >> 4469 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[2, 4, 10, 12, 13, 14, 16, 18, 19, 23] (data_utils.py:301, filter_by_size())[0m
[33m[2025-11-20 10:35:52] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
CodeEncoderLSTM: Creating BiLSTM with 6 layers, 512 hidden and input size
Traceback (most recent call last):
  File "run/type_prediction/transformer/train.py", line 339, in <module>
    cli_main()
  File "run/type_prediction/transformer/train.py", line 332, in cli_main
    single_main(args)
  File "run/type_prediction/transformer/train.py", line 254, in single_main
    valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)
  File "run/type_prediction/transformer/train.py", line 132, in validate
    trainer.valid_step(sample)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/trainers/ncc_trainers.py", line 565, in valid_step
    _loss, sample_size, logging_output = self.task.valid_step(
TypeError: cannot unpack non-iterable NoneType object

============================================================
Setting up experiment: exp_dropout_0.1
============================================================
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.1
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.1/checkpoints
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.1/logs
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.1/results
âœ“ Saved config to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.1/config.yml
âœ“ Also saved to: run/type_prediction/transformer/config/exp_dropout_0.1.yml
âœ“ Saved experiment info to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.1/info.txt

============================================================
Starting training...
============================================================

Command: python run/type_prediction/transformer/train.py --language exp_dropout_0.1 2>&1 | tee /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.1/logs/train.log

Note: Config file will be loaded from run/type_prediction/transformer/config/exp_dropout_0.1.yml


âœ“ Training completed successfully!

============================================================
Evaluating model on test set...
============================================================

Warning: Best checkpoint not found, using last checkpoint
âœ— No checkpoint found for evaluation

============================================================
âœ“ Experiment completed: exp_dropout_0.1
âœ“ Results in: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.1
============================================================


âœ“ exp_dropout_0.1 completed in 0.85 hours

============================================================
Running: exp_dropout_0.2
GPU: 3
Time: 2025-11-20T10:35:55.990885
============================================================
[32m[2025-11-20 10:35:57]    INFO >> Load arguments in run/type_prediction/transformer/config/exp_dropout_0.2.yml (train.py:300, cli_main())[0m
[32m[2025-11-20 10:35:57]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.2/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.2, 'attention_dropout': 0.2, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.2/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.2/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:302, cli_main())[0m
[32m[2025-11-20 10:35:57]    INFO >> single GPU training... (train.py:331, cli_main())[0m
[32m[2025-11-20 10:35:57]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.2/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.2, 'attention_dropout': 0.2, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.2/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.2/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:208, single_main())[0m
[32m[2025-11-20 10:36:00]    INFO >> TypeTransformer(
  (decoder): RobertaEncoder(
    (encoder): CodeEncoderLSTM(
      (embedding): Embedding(10006, 512)
      (pos_encoder): PositionalEncoding()
      (encoder): LSTM(512, 512, num_layers=6, bidirectional=True)
    )
    (output): Sequential(
      (0): Linear(in_features=1024, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=10006, bias=True)
    )
  )
  (classification_heads): ModuleDict()
) (train.py:220, single_main())[0m
[32m[2025-11-20 10:36:00]    INFO >> model typetransformer, criterion TypePredictionCrossEntropyCriterion (train.py:221, single_main())[0m
[32m[2025-11-20 10:36:00]    INFO >> num. model params: 46481686 (num. trained: 46481686) (train.py:222, single_main())[0m
[32m[2025-11-20 10:36:00]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 10:36:00]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 81024 MB ; used memory = 896 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 10:36:00]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 10:36:00]    INFO >> training on 1 GPUs (train.py:229, single_main())[0m
[32m[2025-11-20 10:36:00]    INFO >> max tokens per GPU = None and max sentences per GPU = 32 (train.py:230, single_main())[0m
[32m[2025-11-20 10:36:00]    INFO >> no existing checkpoint found /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.2/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-20 10:36:00]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[33m[2025-11-20 10:36:11] WARNING >> 32222 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[0, 1, 2, 4, 6, 7, 8, 10, 14, 19] (data_utils.py:301, filter_by_size())[0m
[32m[2025-11-20 10:36:11]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-20 10:36:11] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:398: UserWarning: ntokens not found in Criterion logging outputs, cannot log wpb or wps
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:407: UserWarning: nsentences not found in Criterion logging outputs, cannot log bsz
  warnings.warn(
[32m[2025-11-20 10:36:55]    INFO >> epoch 001:    100 / 5058 loss=1.903, ups=2.36, num_updates=100, lr=1e-05, gnorm=0.104, clip=0, train_wall=40, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:37:44]    INFO >> epoch 001:    200 / 5058 loss=0.786, ups=2.16, num_updates=200, lr=2e-05, gnorm=0.068, clip=0, train_wall=44, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:38:48]    INFO >> epoch 001:    300 / 5058 loss=0.654, ups=1.63, num_updates=300, lr=3e-05, gnorm=0.053, clip=0, train_wall=58, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:39:51]    INFO >> epoch 001:    400 / 5058 loss=0.623, ups=1.59, num_updates=400, lr=4e-05, gnorm=2.819, clip=2, train_wall=59, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:40:55]    INFO >> epoch 001:    500 / 5058 loss=0.208, ups=1.67, num_updates=500, lr=5e-05, gnorm=4.476, clip=2, train_wall=56, gb_free=65.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:41:56]    INFO >> epoch 001:    600 / 5058 loss=0.079, ups=1.71, num_updates=600, lr=6e-05, gnorm=4.788, clip=8, train_wall=55, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:42:39]    INFO >> epoch 001:    700 / 5058 loss=0.174, ups=2.43, num_updates=700, lr=7e-05, gnorm=2.641, clip=4, train_wall=39, gb_free=69.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:43:22]    INFO >> epoch 001:    800 / 5058 loss=0.166, ups=2.46, num_updates=800, lr=8e-05, gnorm=3.325, clip=8, train_wall=39, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:44:29]    INFO >> epoch 001:    900 / 5058 loss=0.216, ups=1.56, num_updates=900, lr=9e-05, gnorm=6.91, clip=8, train_wall=60, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:45:33]    INFO >> epoch 001:   1000 / 5058 loss=0.142, ups=1.62, num_updates=1000, lr=0.0001, gnorm=1.451, clip=0, train_wall=58, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:46:31]    INFO >> epoch 001:   1100 / 5058 loss=0.058, ups=1.76, num_updates=1100, lr=0.0001, gnorm=2.202, clip=2, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:47:35]    INFO >> epoch 001:   1200 / 5058 loss=0.035, ups=1.64, num_updates=1200, lr=0.0001, gnorm=0.871, clip=0, train_wall=57, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:48:40]    INFO >> epoch 001:   1300 / 5058 loss=0.087, ups=1.61, num_updates=1300, lr=9.9e-05, gnorm=1.877, clip=2, train_wall=58, gb_free=67.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:49:44]    INFO >> epoch 001:   1400 / 5058 loss=0.059, ups=1.64, num_updates=1400, lr=9.9e-05, gnorm=2.488, clip=4, train_wall=57, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:50:48]    INFO >> epoch 001:   1500 / 5058 loss=0.279, ups=1.61, num_updates=1500, lr=9.9e-05, gnorm=1.776, clip=2, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:51:49]    INFO >> epoch 001:   1600 / 5058 loss=0.11, ups=1.73, num_updates=1600, lr=9.9e-05, gnorm=3.243, clip=4, train_wall=55, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:52:55]    INFO >> epoch 001:   1700 / 5058 loss=0.113, ups=1.58, num_updates=1700, lr=9.9e-05, gnorm=2.319, clip=2, train_wall=59, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:53:48]    INFO >> epoch 001:   1800 / 5058 loss=0.12, ups=1.91, num_updates=1800, lr=9.8e-05, gnorm=3.115, clip=5, train_wall=49, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:54:27]    INFO >> epoch 001:   1900 / 5058 loss=0.153, ups=2.66, num_updates=1900, lr=9.8e-05, gnorm=1.952, clip=4, train_wall=36, gb_free=67.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:55:29]    INFO >> epoch 001:   2000 / 5058 loss=0.089, ups=1.69, num_updates=2000, lr=9.8e-05, gnorm=1.422, clip=1, train_wall=56, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:56:35]    INFO >> epoch 001:   2100 / 5058 loss=0.074, ups=1.58, num_updates=2100, lr=9.8e-05, gnorm=3.142, clip=2, train_wall=60, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:57:41]    INFO >> epoch 001:   2200 / 5058 loss=0.279, ups=1.58, num_updates=2200, lr=9.8e-05, gnorm=5.078, clip=6, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:58:48]    INFO >> epoch 001:   2300 / 5058 loss=0.068, ups=1.55, num_updates=2300, lr=9.7e-05, gnorm=2.452, clip=5, train_wall=60, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:59:53]    INFO >> epoch 001:   2400 / 5058 loss=0.063, ups=1.61, num_updates=2400, lr=9.7e-05, gnorm=0.773, clip=1, train_wall=58, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:01:02]    INFO >> epoch 001:   2500 / 5058 loss=0.149, ups=1.5, num_updates=2500, lr=9.7e-05, gnorm=3.718, clip=1, train_wall=63, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:02:00]    INFO >> epoch 001:   2600 / 5058 loss=0.128, ups=1.81, num_updates=2600, lr=9.7e-05, gnorm=2.608, clip=1, train_wall=52, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:03:00]    INFO >> epoch 001:   2700 / 5058 loss=0.076, ups=1.74, num_updates=2700, lr=9.7e-05, gnorm=1.387, clip=1, train_wall=54, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:04:02]    INFO >> epoch 001:   2800 / 5058 loss=0.081, ups=1.69, num_updates=2800, lr=9.6e-05, gnorm=1.32, clip=3, train_wall=56, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:04:42]    INFO >> epoch 001:   2900 / 5058 loss=0.274, ups=2.61, num_updates=2900, lr=9.6e-05, gnorm=2.005, clip=3, train_wall=36, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:05:48]    INFO >> epoch 001:   3000 / 5058 loss=0.145, ups=1.59, num_updates=3000, lr=9.6e-05, gnorm=3.103, clip=5, train_wall=59, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:06:40]    INFO >> epoch 001:   3100 / 5058 loss=0.058, ups=1.94, num_updates=3100, lr=9.6e-05, gnorm=0.282, clip=0, train_wall=49, gb_free=73.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:07:38]    INFO >> epoch 001:   3200 / 5058 loss=0.282, ups=1.82, num_updates=3200, lr=9.6e-05, gnorm=2.208, clip=2, train_wall=52, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:08:38]    INFO >> epoch 001:   3300 / 5058 loss=0.086, ups=1.74, num_updates=3300, lr=9.5e-05, gnorm=1.673, clip=1, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:09:49]    INFO >> epoch 001:   3400 / 5058 loss=0.225, ups=1.47, num_updates=3400, lr=9.5e-05, gnorm=5.264, clip=3, train_wall=64, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:10:55]    INFO >> epoch 001:   3500 / 5058 loss=0.131, ups=1.59, num_updates=3500, lr=9.5e-05, gnorm=1.53, clip=0, train_wall=59, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:12:00]    INFO >> epoch 001:   3600 / 5058 loss=0.095, ups=1.59, num_updates=3600, lr=9.5e-05, gnorm=2.575, clip=1, train_wall=59, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:13:01]    INFO >> epoch 001:   3700 / 5058 loss=0.137, ups=1.72, num_updates=3700, lr=9.4e-05, gnorm=1.854, clip=2, train_wall=55, gb_free=66.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:14:07]    INFO >> epoch 001:   3800 / 5058 loss=0.058, ups=1.57, num_updates=3800, lr=9.4e-05, gnorm=1.191, clip=1, train_wall=60, gb_free=68.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:14:57]    INFO >> epoch 001:   3900 / 5058 loss=0.127, ups=2.09, num_updates=3900, lr=9.4e-05, gnorm=1.978, clip=2, train_wall=45, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:15:58]    INFO >> epoch 001:   4000 / 5058 loss=0.131, ups=1.69, num_updates=4000, lr=9.4e-05, gnorm=1.731, clip=1, train_wall=56, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:17:01]    INFO >> epoch 001:   4100 / 5058 loss=0.92, ups=1.66, num_updates=4100, lr=9.4e-05, gnorm=0.902, clip=1, train_wall=57, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:18:08]    INFO >> epoch 001:   4200 / 5058 loss=0.149, ups=1.56, num_updates=4200, lr=9.3e-05, gnorm=1.845, clip=3, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:19:06]    INFO >> epoch 001:   4300 / 5058 loss=0.117, ups=1.76, num_updates=4300, lr=9.3e-05, gnorm=1.721, clip=1, train_wall=54, gb_free=73.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:20:07]    INFO >> epoch 001:   4400 / 5058 loss=0.082, ups=1.72, num_updates=4400, lr=9.3e-05, gnorm=1.476, clip=0, train_wall=55, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:21:10]    INFO >> epoch 001:   4500 / 5058 loss=0.072, ups=1.66, num_updates=4500, lr=9.3e-05, gnorm=1.371, clip=0, train_wall=57, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:22:16]    INFO >> epoch 001:   4600 / 5058 loss=0.241, ups=1.58, num_updates=4600, lr=9.3e-05, gnorm=1.805, clip=0, train_wall=60, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:23:26]    INFO >> epoch 001:   4700 / 5058 loss=0.215, ups=1.48, num_updates=4700, lr=9.2e-05, gnorm=3.803, clip=2, train_wall=63, gb_free=64.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:24:27]    INFO >> epoch 001:   4800 / 5058 loss=0.056, ups=1.71, num_updates=4800, lr=9.2e-05, gnorm=1.411, clip=0, train_wall=55, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:25:08]    INFO >> epoch 001:   4900 / 5058 loss=0.142, ups=2.54, num_updates=4900, lr=9.2e-05, gnorm=0.824, clip=0, train_wall=37, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:26:13]    INFO >> epoch 001:   5000 / 5058 loss=0.09, ups=1.61, num_updates=5000, lr=9.2e-05, gnorm=2.488, clip=0, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:26:52]    INFO >> epoch 001 | loss 0.105 | ups 1.73 | num_updates 5058 | lr 9.2e-05 | gnorm 2.248 | clip 2.1 | train_wall 2751 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-20 11:26:52] WARNING >> 4469 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[2, 4, 10, 12, 13, 14, 16, 18, 19, 23] (data_utils.py:301, filter_by_size())[0m
[33m[2025-11-20 11:26:52] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
CodeEncoderLSTM: Creating BiLSTM with 6 layers, 512 hidden and input size
Traceback (most recent call last):
  File "run/type_prediction/transformer/train.py", line 339, in <module>
    cli_main()
  File "run/type_prediction/transformer/train.py", line 332, in cli_main
    single_main(args)
  File "run/type_prediction/transformer/train.py", line 254, in single_main
    valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)
  File "run/type_prediction/transformer/train.py", line 132, in validate
    trainer.valid_step(sample)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/trainers/ncc_trainers.py", line 565, in valid_step
    _loss, sample_size, logging_output = self.task.valid_step(
TypeError: cannot unpack non-iterable NoneType object

============================================================
Setting up experiment: exp_dropout_0.2
============================================================
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.2
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.2/checkpoints
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.2/logs
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.2/results
âœ“ Saved config to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.2/config.yml
âœ“ Also saved to: run/type_prediction/transformer/config/exp_dropout_0.2.yml
âœ“ Saved experiment info to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.2/info.txt

============================================================
Starting training...
============================================================

Command: python run/type_prediction/transformer/train.py --language exp_dropout_0.2 2>&1 | tee /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.2/logs/train.log

Note: Config file will be loaded from run/type_prediction/transformer/config/exp_dropout_0.2.yml


âœ“ Training completed successfully!

============================================================
Evaluating model on test set...
============================================================

Warning: Best checkpoint not found, using last checkpoint
âœ— No checkpoint found for evaluation

============================================================
âœ“ Experiment completed: exp_dropout_0.2
âœ“ Results in: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.2
============================================================


âœ“ exp_dropout_0.2 completed in 0.85 hours

============================================================
Running: exp_encoder_transformer
GPU: 3
Time: 2025-11-20T11:26:55.809963
============================================================
[32m[2025-11-20 11:26:57]    INFO >> Load arguments in run/type_prediction/transformer/config/exp_encoder_transformer.yml (train.py:300, cli_main())[0m
[32m[2025-11-20 11:26:57]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_encoder_transformer/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'transformer', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_encoder_transformer/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_encoder_transformer/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:302, cli_main())[0m
[32m[2025-11-20 11:26:57]    INFO >> single GPU training... (train.py:331, cli_main())[0m
[32m[2025-11-20 11:26:57]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_encoder_transformer/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'transformer', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_encoder_transformer/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_encoder_transformer/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:208, single_main())[0m
/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
[32m[2025-11-20 11:27:00]    INFO >> TypeTransformer(
  (decoder): RobertaEncoder(
    (encoder): CodeEncoder(
      (embedding): Embedding(10006, 512)
      (pos_encoder): PositionalEncoding()
      (encoder): TransformerEncoder(
        (layers): ModuleList(
          (0-5): 6 x TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=2048, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=2048, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.0, inplace=False)
          )
        )
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output): Sequential(
      (0): Linear(in_features=512, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=10006, bias=True)
    )
  )
  (classification_heads): ModuleDict()
) (train.py:220, single_main())[0m
[32m[2025-11-20 11:27:00]    INFO >> model typetransformer, criterion TypePredictionCrossEntropyCriterion (train.py:221, single_main())[0m
[32m[2025-11-20 11:27:00]    INFO >> num. model params: 29434134 (num. trained: 29434134) (train.py:222, single_main())[0m
[32m[2025-11-20 11:27:00]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 11:27:00]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 81024 MB ; used memory = 896 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 11:27:00]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 11:27:00]    INFO >> training on 1 GPUs (train.py:229, single_main())[0m
[32m[2025-11-20 11:27:00]    INFO >> max tokens per GPU = None and max sentences per GPU = 32 (train.py:230, single_main())[0m
[32m[2025-11-20 11:27:00]    INFO >> no existing checkpoint found /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_encoder_transformer/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-20 11:27:00]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[33m[2025-11-20 11:27:10] WARNING >> 32222 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[0, 1, 2, 4, 6, 7, 8, 10, 14, 19] (data_utils.py:301, filter_by_size())[0m
[32m[2025-11-20 11:27:10]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-20 11:27:11] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:398: UserWarning: ntokens not found in Criterion logging outputs, cannot log wpb or wps
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:407: UserWarning: nsentences not found in Criterion logging outputs, cannot log bsz
  warnings.warn(
[32m[2025-11-20 11:28:07]    INFO >> epoch 001:    100 / 5058 loss=1.891, ups=1.89, num_updates=100, lr=1e-05, gnorm=1.41, clip=0, train_wall=51, gb_free=65.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:29:06]    INFO >> epoch 001:    200 / 5058 loss=0.703, ups=1.71, num_updates=200, lr=2e-05, gnorm=1.933, clip=3, train_wall=55, gb_free=64.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:30:30]    INFO >> epoch 001:    300 / 5058 loss=0.524, ups=1.25, num_updates=300, lr=3e-05, gnorm=0.834, clip=1, train_wall=76, gb_free=64.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:31:55]    INFO >> epoch 001:    400 / 5058 loss=0.677, ups=1.22, num_updates=400, lr=4e-05, gnorm=3.253, clip=1, train_wall=78, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:33:16]    INFO >> epoch 001:    500 / 5058 loss=0.267, ups=1.3, num_updates=500, lr=5e-05, gnorm=3.47, clip=2, train_wall=73, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:34:34]    INFO >> epoch 001:    600 / 5058 loss=0.086, ups=1.34, num_updates=600, lr=6e-05, gnorm=3.56, clip=7, train_wall=71, gb_free=66.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:35:27]    INFO >> epoch 001:    700 / 5058 loss=0.184, ups=1.97, num_updates=700, lr=7e-05, gnorm=1.977, clip=3, train_wall=48, gb_free=69.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:36:18]    INFO >> epoch 001:    800 / 5058 loss=0.174, ups=2.02, num_updates=800, lr=8e-05, gnorm=2.539, clip=3, train_wall=47, gb_free=62.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:37:45]    INFO >> epoch 001:    900 / 5058 loss=0.216, ups=1.19, num_updates=900, lr=9e-05, gnorm=5.712, clip=6, train_wall=79, gb_free=64.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:39:08]    INFO >> epoch 001:   1000 / 5058 loss=0.14, ups=1.25, num_updates=1000, lr=0.0001, gnorm=1.203, clip=0, train_wall=76, gb_free=62.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:40:24]    INFO >> epoch 001:   1100 / 5058 loss=0.057, ups=1.37, num_updates=1100, lr=0.0001, gnorm=1.875, clip=2, train_wall=69, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:41:46]    INFO >> epoch 001:   1200 / 5058 loss=0.035, ups=1.26, num_updates=1200, lr=0.0001, gnorm=0.774, clip=0, train_wall=75, gb_free=64.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:43:10]    INFO >> epoch 001:   1300 / 5058 loss=0.086, ups=1.24, num_updates=1300, lr=9.9e-05, gnorm=1.662, clip=2, train_wall=76, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:44:33]    INFO >> epoch 001:   1400 / 5058 loss=0.058, ups=1.27, num_updates=1400, lr=9.9e-05, gnorm=2.258, clip=4, train_wall=75, gb_free=62.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:45:57]    INFO >> epoch 001:   1500 / 5058 loss=0.285, ups=1.24, num_updates=1500, lr=9.9e-05, gnorm=1.602, clip=2, train_wall=76, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:47:15]    INFO >> epoch 001:   1600 / 5058 loss=0.11, ups=1.35, num_updates=1600, lr=9.9e-05, gnorm=2.998, clip=4, train_wall=70, gb_free=62.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:48:40]    INFO >> epoch 001:   1700 / 5058 loss=0.113, ups=1.22, num_updates=1700, lr=9.9e-05, gnorm=2.135, clip=2, train_wall=78, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:49:49]    INFO >> epoch 001:   1800 / 5058 loss=0.121, ups=1.5, num_updates=1800, lr=9.8e-05, gnorm=2.905, clip=5, train_wall=63, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:50:36]    INFO >> epoch 001:   1900 / 5058 loss=0.151, ups=2.23, num_updates=1900, lr=9.8e-05, gnorm=1.779, clip=2, train_wall=43, gb_free=67 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:51:55]    INFO >> epoch 001:   2000 / 5058 loss=0.088, ups=1.31, num_updates=2000, lr=9.8e-05, gnorm=1.352, clip=1, train_wall=72, gb_free=63.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:53:22]    INFO >> epoch 001:   2100 / 5058 loss=0.074, ups=1.2, num_updates=2100, lr=9.8e-05, gnorm=3.039, clip=1, train_wall=78, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:54:48]    INFO >> epoch 001:   2200 / 5058 loss=0.278, ups=1.21, num_updates=2200, lr=9.8e-05, gnorm=5.071, clip=6, train_wall=79, gb_free=62.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:56:16]    INFO >> epoch 001:   2300 / 5058 loss=0.068, ups=1.19, num_updates=2300, lr=9.7e-05, gnorm=2.436, clip=5, train_wall=79, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:57:39]    INFO >> epoch 001:   2400 / 5058 loss=0.064, ups=1.25, num_updates=2400, lr=9.7e-05, gnorm=0.779, clip=1, train_wall=76, gb_free=67.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:59:11]    INFO >> epoch 001:   2500 / 5058 loss=0.149, ups=1.14, num_updates=2500, lr=9.7e-05, gnorm=3.688, clip=1, train_wall=83, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 12:00:24]    INFO >> epoch 001:   2600 / 5058 loss=0.128, ups=1.43, num_updates=2600, lr=9.7e-05, gnorm=2.595, clip=1, train_wall=66, gb_free=62.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 12:01:38]    INFO >> epoch 001:   2700 / 5058 loss=0.076, ups=1.36, num_updates=2700, lr=9.7e-05, gnorm=1.387, clip=1, train_wall=69, gb_free=65.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 12:03:00]    INFO >> epoch 001:   2800 / 5058 loss=0.081, ups=1.31, num_updates=2800, lr=9.6e-05, gnorm=1.31, clip=3, train_wall=72, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 12:03:48]    INFO >> epoch 001:   2900 / 5058 loss=0.276, ups=2.15, num_updates=2900, lr=9.6e-05, gnorm=1.993, clip=3, train_wall=44, gb_free=62.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 12:05:14]    INFO >> epoch 001:   3000 / 5058 loss=0.146, ups=1.22, num_updates=3000, lr=9.6e-05, gnorm=3.189, clip=5, train_wall=77, gb_free=66.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 12:06:20]    INFO >> epoch 001:   3100 / 5058 loss=0.057, ups=1.57, num_updates=3100, lr=9.6e-05, gnorm=0.282, clip=0, train_wall=61, gb_free=73.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 12:07:34]    INFO >> epoch 001:   3200 / 5058 loss=0.288, ups=1.41, num_updates=3200, lr=9.6e-05, gnorm=2.256, clip=1, train_wall=67, gb_free=67 (progress_bar.py:258, log())[0m
[32m[2025-11-20 12:08:51]    INFO >> epoch 001:   3300 / 5058 loss=0.086, ups=1.35, num_updates=3300, lr=9.5e-05, gnorm=1.74, clip=1, train_wall=70, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 12:10:25]    INFO >> epoch 001:   3400 / 5058 loss=0.226, ups=1.1, num_updates=3400, lr=9.5e-05, gnorm=5.527, clip=3, train_wall=86, gb_free=62.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 12:11:51]    INFO >> epoch 001:   3500 / 5058 loss=0.132, ups=1.22, num_updates=3500, lr=9.5e-05, gnorm=1.586, clip=0, train_wall=78, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 12:13:15]    INFO >> epoch 001:   3600 / 5058 loss=0.094, ups=1.23, num_updates=3600, lr=9.5e-05, gnorm=2.732, clip=2, train_wall=77, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 12:14:31]    INFO >> epoch 001:   3700 / 5058 loss=0.142, ups=1.35, num_updates=3700, lr=9.4e-05, gnorm=2.216, clip=3, train_wall=71, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 12:15:59]    INFO >> epoch 001:   3800 / 5058 loss=0.059, ups=1.21, num_updates=3800, lr=9.4e-05, gnorm=1.281, clip=1, train_wall=79, gb_free=68.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 12:17:02]    INFO >> epoch 001:   3900 / 5058 loss=0.129, ups=1.66, num_updates=3900, lr=9.4e-05, gnorm=2.12, clip=2, train_wall=57, gb_free=63.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 12:18:22]    INFO >> epoch 001:   4000 / 5058 loss=0.131, ups=1.3, num_updates=4000, lr=9.4e-05, gnorm=1.871, clip=3, train_wall=73, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 12:19:43]    INFO >> epoch 001:   4100 / 5058 loss=0.938, ups=1.29, num_updates=4100, lr=9.4e-05, gnorm=0.983, clip=1, train_wall=74, gb_free=63.4 (progress_bar.py:258, log())[0m
^CTraceback (most recent call last):
  File "run_batch_experiments.py", line 330, in <module>
    main()
  File "run_batch_experiments.py", line 327, in main
    runner.run_batch(experiments, mode=args.mode, gpus=args.gpus, dry_run=args.dry_run)
  File "run_batch_experiments.py", line 233, in run_batch
    success = self.run_experiment(exp['name'], gpu_id, dry_run)
  File "run_batch_experiments.py", line 103, in run_experiment
    for line in process.stdout:
KeyboardInterrupt

(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ ^C
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ ^C
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ ^C
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ ^C
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ ^C
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ ^C
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer   --mode serial   --gpu 3   --all[K[A[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer   --mode serial   --gpu 3   --all[K[A[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer   --mode serial   --gpu 3   --all[K[A[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer   --mode serial   --gpu 3   --all[K[A[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer   --mode serial   --gpu 3   --all[K[A[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer   --mode serial   --gpu 3   --all[K[A[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer   --mode serial   --gpu 3   --all[K[A[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer   --mode serial   --gpu 3   --all[K[A[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer   --mode serial   --gpu 3   --all[K[A[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer   --mode serial   --gpu 3   --all[K[A[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer   --mode serial   --gpu 3   --all[K[A[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer   --mode serial   --gpu 3   --all[K[A[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer   --mode serial   --gpu 3   --all[K[A[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer   --mode serial   --gpu 3   --all[K[A[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer   --mode serial   --gpu 3   --all[K[A[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer   --mode serial   --gpu 3   --all[K[A[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer   --mode serial   --gpu 3   --all[K[A[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer   --mode serial   --gpu 3   --all[K[A[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer   --mode serial   --gpu 3   --all[K[A[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer   --mode serial   --gpu 3   --all[K[A[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer   --mode serial   --gpu 3   --all[A[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ git pull[K
[K
[K[A[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cpython run_batch_experiments.py   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer   --mode serial   --gpu 3   --all

============================================================
Batch Experiment Runner
Mode: serial
Experiments: 18
GPUs: [3]
============================================================


============================================================
Running: exp_baseline
GPU: 3
Time: 2025-11-20T22:47:03.157582
============================================================
[32m[2025-11-20 22:47:05]    INFO >> Load arguments in run/type_prediction/transformer/config/exp_baseline.yml (train.py:300, cli_main())[0m
[32m[2025-11-20 22:47:05]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:302, cli_main())[0m
[32m[2025-11-20 22:47:05]    INFO >> single GPU training... (train.py:331, cli_main())[0m
[32m[2025-11-20 22:47:05]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:208, single_main())[0m
[32m[2025-11-20 22:47:07]    INFO >> TypeTransformer(
  (decoder): RobertaEncoder(
    (encoder): CodeEncoderLSTM(
      (embedding): Embedding(10006, 512)
      (pos_encoder): PositionalEncoding()
      (encoder): LSTM(512, 512, num_layers=6, bidirectional=True)
    )
    (output): Sequential(
      (0): Linear(in_features=1024, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=10006, bias=True)
    )
  )
  (classification_heads): ModuleDict()
) (train.py:220, single_main())[0m
[32m[2025-11-20 22:47:07]    INFO >> model typetransformer, criterion TypePredictionCrossEntropyCriterion (train.py:221, single_main())[0m
[32m[2025-11-20 22:47:07]    INFO >> num. model params: 46481686 (num. trained: 46481686) (train.py:222, single_main())[0m
[32m[2025-11-20 22:47:07]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 22:47:07]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 38699 MB ; used memory = 43220 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 22:47:07]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 22:47:07]    INFO >> training on 1 GPUs (train.py:229, single_main())[0m
[32m[2025-11-20 22:47:07]    INFO >> max tokens per GPU = None and max sentences per GPU = 32 (train.py:230, single_main())[0m
[32m[2025-11-20 22:47:07]    INFO >> no existing checkpoint found /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-20 22:47:07]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[33m[2025-11-20 22:47:23] WARNING >> 32222 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[0, 1, 2, 4, 6, 7, 8, 10, 14, 19] (data_utils.py:301, filter_by_size())[0m
[32m[2025-11-20 22:47:23]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-20 22:47:23] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:398: UserWarning: ntokens not found in Criterion logging outputs, cannot log wpb or wps
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:407: UserWarning: nsentences not found in Criterion logging outputs, cannot log bsz
  warnings.warn(
[32m[2025-11-20 22:48:08]    INFO >> epoch 001:    100 / 5058 loss=1.903, ups=2.33, num_updates=100, lr=1e-05, gnorm=0.104, clip=0, train_wall=41, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 22:48:56]    INFO >> epoch 001:    200 / 5058 loss=0.786, ups=2.12, num_updates=200, lr=2e-05, gnorm=0.068, clip=0, train_wall=44, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 22:50:02]    INFO >> epoch 001:    300 / 5058 loss=0.654, ups=1.6, num_updates=300, lr=3e-05, gnorm=0.053, clip=0, train_wall=58, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 22:51:07]    INFO >> epoch 001:    400 / 5058 loss=0.623, ups=1.56, num_updates=400, lr=4e-05, gnorm=2.819, clip=2, train_wall=60, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 22:52:08]    INFO >> epoch 001:    500 / 5058 loss=0.208, ups=1.63, num_updates=500, lr=5e-05, gnorm=4.476, clip=2, train_wall=57, gb_free=65.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 22:53:15]    INFO >> epoch 001:    600 / 5058 loss=0.079, ups=1.67, num_updates=600, lr=6e-05, gnorm=4.788, clip=8, train_wall=55, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 22:53:58]    INFO >> epoch 001:    700 / 5058 loss=0.174, ups=2.38, num_updates=700, lr=7e-05, gnorm=2.641, clip=4, train_wall=39, gb_free=69.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 22:54:42]    INFO >> epoch 001:    800 / 5058 loss=0.166, ups=2.42, num_updates=800, lr=8e-05, gnorm=3.325, clip=8, train_wall=39, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 22:55:51]    INFO >> epoch 001:    900 / 5058 loss=0.216, ups=1.52, num_updates=900, lr=9e-05, gnorm=6.91, clip=8, train_wall=60, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 22:56:57]    INFO >> epoch 001:   1000 / 5058 loss=0.142, ups=1.58, num_updates=1000, lr=0.0001, gnorm=1.451, clip=0, train_wall=59, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 22:57:56]    INFO >> epoch 001:   1100 / 5058 loss=0.058, ups=1.72, num_updates=1100, lr=0.0001, gnorm=2.202, clip=2, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 22:59:01]    INFO >> epoch 001:   1200 / 5058 loss=0.035, ups=1.6, num_updates=1200, lr=0.0001, gnorm=0.871, clip=0, train_wall=58, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:00:07]    INFO >> epoch 001:   1300 / 5058 loss=0.087, ups=1.57, num_updates=1300, lr=9.9e-05, gnorm=1.877, clip=2, train_wall=59, gb_free=67.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:01:12]    INFO >> epoch 001:   1400 / 5058 loss=0.059, ups=1.61, num_updates=1400, lr=9.9e-05, gnorm=2.488, clip=4, train_wall=58, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:02:17]    INFO >> epoch 001:   1500 / 5058 loss=0.279, ups=1.58, num_updates=1500, lr=9.9e-05, gnorm=1.776, clip=2, train_wall=59, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:03:20]    INFO >> epoch 001:   1600 / 5058 loss=0.11, ups=1.69, num_updates=1600, lr=9.9e-05, gnorm=3.243, clip=4, train_wall=55, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:04:28]    INFO >> epoch 001:   1700 / 5058 loss=0.113, ups=1.54, num_updates=1700, lr=9.9e-05, gnorm=2.319, clip=2, train_wall=60, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:05:24]    INFO >> epoch 001:   1800 / 5058 loss=0.12, ups=1.87, num_updates=1800, lr=9.8e-05, gnorm=3.115, clip=5, train_wall=50, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:06:04]    INFO >> epoch 001:   1900 / 5058 loss=0.153, ups=2.61, num_updates=1900, lr=9.8e-05, gnorm=1.952, clip=4, train_wall=36, gb_free=67.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:07:07]    INFO >> epoch 001:   2000 / 5058 loss=0.089, ups=1.65, num_updates=2000, lr=9.8e-05, gnorm=1.422, clip=1, train_wall=56, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:08:15]    INFO >> epoch 001:   2100 / 5058 loss=0.074, ups=1.54, num_updates=2100, lr=9.8e-05, gnorm=3.142, clip=2, train_wall=60, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:09:22]    INFO >> epoch 001:   2200 / 5058 loss=0.279, ups=1.54, num_updates=2200, lr=9.8e-05, gnorm=5.078, clip=6, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:10:31]    INFO >> epoch 001:   2300 / 5058 loss=0.068, ups=1.52, num_updates=2300, lr=9.7e-05, gnorm=2.452, clip=5, train_wall=61, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:11:36]    INFO >> epoch 001:   2400 / 5058 loss=0.063, ups=1.58, num_updates=2400, lr=9.7e-05, gnorm=0.773, clip=1, train_wall=59, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:12:48]    INFO >> epoch 001:   2500 / 5058 loss=0.149, ups=1.46, num_updates=2500, lr=9.7e-05, gnorm=3.718, clip=1, train_wall=63, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:13:47]    INFO >> epoch 001:   2600 / 5058 loss=0.128, ups=1.78, num_updates=2600, lr=9.7e-05, gnorm=2.608, clip=1, train_wall=52, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:14:49]    INFO >> epoch 001:   2700 / 5058 loss=0.076, ups=1.7, num_updates=2700, lr=9.7e-05, gnorm=1.387, clip=1, train_wall=55, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:15:50]    INFO >> epoch 001:   2800 / 5058 loss=0.081, ups=1.65, num_updates=2800, lr=9.6e-05, gnorm=1.32, clip=3, train_wall=56, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:16:32]    INFO >> epoch 001:   2900 / 5058 loss=0.274, ups=2.56, num_updates=2900, lr=9.6e-05, gnorm=2.005, clip=3, train_wall=37, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:17:39]    INFO >> epoch 001:   3000 / 5058 loss=0.145, ups=1.55, num_updates=3000, lr=9.6e-05, gnorm=3.103, clip=5, train_wall=60, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:18:33]    INFO >> epoch 001:   3100 / 5058 loss=0.058, ups=1.9, num_updates=3100, lr=9.6e-05, gnorm=0.282, clip=0, train_wall=49, gb_free=73.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:19:32]    INFO >> epoch 001:   3200 / 5058 loss=0.282, ups=1.77, num_updates=3200, lr=9.6e-05, gnorm=2.208, clip=2, train_wall=52, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:20:34]    INFO >> epoch 001:   3300 / 5058 loss=0.086, ups=1.7, num_updates=3300, lr=9.5e-05, gnorm=1.673, clip=1, train_wall=55, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:21:47]    INFO >> epoch 001:   3400 / 5058 loss=0.225, ups=1.43, num_updates=3400, lr=9.5e-05, gnorm=5.264, clip=3, train_wall=64, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:22:54]    INFO >> epoch 001:   3500 / 5058 loss=0.131, ups=1.55, num_updates=3500, lr=9.5e-05, gnorm=1.53, clip=0, train_wall=59, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:24:01]    INFO >> epoch 001:   3600 / 5058 loss=0.095, ups=1.55, num_updates=3600, lr=9.5e-05, gnorm=2.575, clip=1, train_wall=59, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:25:03]    INFO >> epoch 001:   3700 / 5058 loss=0.137, ups=1.68, num_updates=3700, lr=9.4e-05, gnorm=1.854, clip=2, train_wall=55, gb_free=66.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:26:12]    INFO >> epoch 001:   3800 / 5058 loss=0.058, ups=1.52, num_updates=3800, lr=9.4e-05, gnorm=1.191, clip=1, train_wall=60, gb_free=68.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:27:03]    INFO >> epoch 001:   3900 / 5058 loss=0.127, ups=2.02, num_updates=3900, lr=9.4e-05, gnorm=1.978, clip=2, train_wall=46, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:28:07]    INFO >> epoch 001:   4000 / 5058 loss=0.131, ups=1.63, num_updates=4000, lr=9.4e-05, gnorm=1.731, clip=1, train_wall=56, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:29:11]    INFO >> epoch 001:   4100 / 5058 loss=0.92, ups=1.62, num_updates=4100, lr=9.4e-05, gnorm=0.902, clip=1, train_wall=57, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:30:20]    INFO >> epoch 001:   4200 / 5058 loss=0.149, ups=1.52, num_updates=4200, lr=9.3e-05, gnorm=1.845, clip=3, train_wall=61, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:31:21]    INFO >> epoch 001:   4300 / 5058 loss=0.117, ups=1.72, num_updates=4300, lr=9.3e-05, gnorm=1.721, clip=1, train_wall=54, gb_free=73.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:32:22]    INFO >> epoch 001:   4400 / 5058 loss=0.082, ups=1.67, num_updates=4400, lr=9.3e-05, gnorm=1.476, clip=0, train_wall=55, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:33:28]    INFO >> epoch 001:   4500 / 5058 loss=0.072, ups=1.62, num_updates=4500, lr=9.3e-05, gnorm=1.371, clip=0, train_wall=57, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:34:35]    INFO >> epoch 001:   4600 / 5058 loss=0.241, ups=1.54, num_updates=4600, lr=9.3e-05, gnorm=1.805, clip=0, train_wall=60, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:35:45]    INFO >> epoch 001:   4700 / 5058 loss=0.215, ups=1.44, num_updates=4700, lr=9.2e-05, gnorm=3.803, clip=2, train_wall=64, gb_free=64.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:36:50]    INFO >> epoch 001:   4800 / 5058 loss=0.056, ups=1.67, num_updates=4800, lr=9.2e-05, gnorm=1.411, clip=0, train_wall=55, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:37:31]    INFO >> epoch 001:   4900 / 5058 loss=0.142, ups=2.49, num_updates=4900, lr=9.2e-05, gnorm=0.824, clip=0, train_wall=38, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:38:37]    INFO >> epoch 001:   5000 / 5058 loss=0.09, ups=1.58, num_updates=5000, lr=9.2e-05, gnorm=2.488, clip=0, train_wall=59, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:39:18]    INFO >> epoch 001 | loss 0.105 | ups 1.69 | num_updates 5058 | lr 9.2e-05 | gnorm 2.248 | clip 2.1 | train_wall 2770 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-20 23:39:18] WARNING >> 4469 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[2, 4, 10, 12, 13, 14, 16, 18, 19, 23] (data_utils.py:301, filter_by_size())[0m
[33m[2025-11-20 23:39:18] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-20 23:46:07]    INFO >> epoch 001 | valid on 'valid' subset | loss 0.138 | num_updates 5058 (progress_bar.py:267, print())[0m
[32m[2025-11-20 23:46:08]    INFO >> saved checkpoint /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints/checkpoint1.pt (epoch 1 @ 5058 updates, score 0.138) (writing took 1.233916 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-20 23:46:08] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-20 23:46:31]    INFO >> epoch 002:     42 / 5058 loss=0.24, ups=0.22, num_updates=5100, lr=9.2e-05, gnorm=3.337, clip=2, train_wall=56, gb_free=77.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:47:13]    INFO >> epoch 002:    142 / 5058 loss=0.134, ups=2.47, num_updates=5200, lr=9.1e-05, gnorm=0.773, clip=0, train_wall=38, gb_free=78.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:48:10]    INFO >> epoch 002:    242 / 5058 loss=0.233, ups=1.84, num_updates=5300, lr=9.1e-05, gnorm=0.782, clip=0, train_wall=51, gb_free=70.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:49:15]    INFO >> epoch 002:    342 / 5058 loss=0.135, ups=1.6, num_updates=5400, lr=9.1e-05, gnorm=1.255, clip=0, train_wall=58, gb_free=64.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:50:24]    INFO >> epoch 002:    442 / 5058 loss=0.248, ups=1.5, num_updates=5500, lr=9.1e-05, gnorm=3.406, clip=1, train_wall=62, gb_free=68.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:51:26]    INFO >> epoch 002:    542 / 5058 loss=0.08, ups=1.7, num_updates=5600, lr=9.1e-05, gnorm=1.697, clip=1, train_wall=55, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:52:22]    INFO >> epoch 002:    642 / 5058 loss=0.088, ups=1.87, num_updates=5700, lr=9e-05, gnorm=1.912, clip=0, train_wall=50, gb_free=67.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:53:01]    INFO >> epoch 002:    742 / 5058 loss=0.239, ups=2.64, num_updates=5800, lr=9e-05, gnorm=2.029, clip=0, train_wall=36, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:53:54]    INFO >> epoch 002:    842 / 5058 loss=0.144, ups=1.92, num_updates=5900, lr=9e-05, gnorm=1.643, clip=0, train_wall=48, gb_free=63.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:55:03]    INFO >> epoch 002:    942 / 5058 loss=0.159, ups=1.52, num_updates=6000, lr=9e-05, gnorm=2.681, clip=0, train_wall=61, gb_free=66.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:56:04]    INFO >> epoch 002:   1042 / 5058 loss=0.107, ups=1.72, num_updates=6100, lr=9e-05, gnorm=1.282, clip=0, train_wall=54, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:57:05]    INFO >> epoch 002:   1142 / 5058 loss=0.036, ups=1.7, num_updates=6200, lr=8.9e-05, gnorm=0.293, clip=0, train_wall=55, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:58:13]    INFO >> epoch 002:   1242 / 5058 loss=0.042, ups=1.53, num_updates=6300, lr=8.9e-05, gnorm=0.551, clip=0, train_wall=61, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 23:59:19]    INFO >> epoch 002:   1342 / 5058 loss=0.054, ups=1.59, num_updates=6400, lr=8.9e-05, gnorm=0.868, clip=0, train_wall=59, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:00:23]    INFO >> epoch 002:   1442 / 5058 loss=0.131, ups=1.63, num_updates=6500, lr=8.9e-05, gnorm=1.541, clip=0, train_wall=57, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:01:25]    INFO >> epoch 002:   1542 / 5058 loss=0.08, ups=1.63, num_updates=6600, lr=8.9e-05, gnorm=1.148, clip=0, train_wall=57, gb_free=69.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:02:30]    INFO >> epoch 002:   1642 / 5058 loss=0.137, ups=1.64, num_updates=6700, lr=8.8e-05, gnorm=1.963, clip=1, train_wall=57, gb_free=63.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:03:37]    INFO >> epoch 002:   1742 / 5058 loss=0.11, ups=1.57, num_updates=6800, lr=8.8e-05, gnorm=1.674, clip=0, train_wall=59, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:04:19]    INFO >> epoch 002:   1842 / 5058 loss=0.075, ups=2.43, num_updates=6900, lr=8.8e-05, gnorm=0.746, clip=0, train_wall=39, gb_free=72 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:05:11]    INFO >> epoch 002:   1942 / 5058 loss=0.116, ups=1.98, num_updates=7000, lr=8.8e-05, gnorm=1.846, clip=0, train_wall=47, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:06:14]    INFO >> epoch 002:   2042 / 5058 loss=0.068, ups=1.65, num_updates=7100, lr=8.8e-05, gnorm=0.476, clip=0, train_wall=56, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:07:24]    INFO >> epoch 002:   2142 / 5058 loss=0.103, ups=1.48, num_updates=7200, lr=8.7e-05, gnorm=3.36, clip=0, train_wall=62, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:08:31]    INFO >> epoch 002:   2242 / 5058 loss=0.08, ups=1.58, num_updates=7300, lr=8.7e-05, gnorm=1.933, clip=0, train_wall=59, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:09:40]    INFO >> epoch 002:   2342 / 5058 loss=0.088, ups=1.48, num_updates=7400, lr=8.7e-05, gnorm=1.022, clip=0, train_wall=63, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:10:55]    INFO >> epoch 002:   2442 / 5058 loss=0.078, ups=1.44, num_updates=7500, lr=8.7e-05, gnorm=0.795, clip=0, train_wall=65, gb_free=64.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:12:04]    INFO >> epoch 002:   2542 / 5058 loss=0.225, ups=1.5, num_updates=7600, lr=8.7e-05, gnorm=2.32, clip=0, train_wall=62, gb_free=72.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:12:59]    INFO >> epoch 002:   2642 / 5058 loss=0.074, ups=1.87, num_updates=7700, lr=8.6e-05, gnorm=1.402, clip=0, train_wall=50, gb_free=69.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:14:05]    INFO >> epoch 002:   2742 / 5058 loss=0.09, ups=1.59, num_updates=7800, lr=8.6e-05, gnorm=0.648, clip=0, train_wall=58, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:14:53]    INFO >> epoch 002:   2842 / 5058 loss=0.161, ups=2.2, num_updates=7900, lr=8.6e-05, gnorm=0.632, clip=0, train_wall=42, gb_free=78.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:15:50]    INFO >> epoch 002:   2942 / 5058 loss=0.203, ups=1.78, num_updates=8000, lr=8.6e-05, gnorm=2.179, clip=0, train_wall=52, gb_free=64.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:16:59]    INFO >> epoch 002:   3042 / 5058 loss=0.081, ups=1.52, num_updates=8100, lr=8.6e-05, gnorm=1.026, clip=0, train_wall=60, gb_free=72 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:18:01]    INFO >> epoch 002:   3142 / 5058 loss=0.066, ups=1.69, num_updates=8200, lr=8.5e-05, gnorm=0.713, clip=0, train_wall=54, gb_free=71.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:18:57]    INFO >> epoch 002:   3242 / 5058 loss=0.379, ups=1.82, num_updates=8300, lr=8.5e-05, gnorm=0.754, clip=0, train_wall=50, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:20:09]    INFO >> epoch 002:   3342 / 5058 loss=0.104, ups=1.46, num_updates=8400, lr=8.5e-05, gnorm=2.319, clip=0, train_wall=62, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:21:22]    INFO >> epoch 002:   3442 / 5058 loss=0.244, ups=1.4, num_updates=8500, lr=8.5e-05, gnorm=3.47, clip=0, train_wall=65, gb_free=66.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:22:33]    INFO >> epoch 002:   3542 / 5058 loss=0.085, ups=1.49, num_updates=8600, lr=8.4e-05, gnorm=0.775, clip=1, train_wall=61, gb_free=64.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:23:40]    INFO >> epoch 002:   3642 / 5058 loss=0.089, ups=1.53, num_updates=8700, lr=8.4e-05, gnorm=1.298, clip=0, train_wall=60, gb_free=64 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:24:48]    INFO >> epoch 002:   3742 / 5058 loss=0.065, ups=1.56, num_updates=8800, lr=8.4e-05, gnorm=1.631, clip=0, train_wall=59, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:25:53]    INFO >> epoch 002:   3842 / 5058 loss=0.072, ups=1.6, num_updates=8900, lr=8.4e-05, gnorm=0.598, clip=0, train_wall=57, gb_free=72.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:26:48]    INFO >> epoch 002:   3942 / 5058 loss=0.154, ups=1.92, num_updates=9000, lr=8.4e-05, gnorm=1.231, clip=0, train_wall=48, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:27:56]    INFO >> epoch 002:   4042 / 5058 loss=0.154, ups=1.54, num_updates=9100, lr=8.3e-05, gnorm=0.968, clip=0, train_wall=59, gb_free=66.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:29:06]    INFO >> epoch 002:   4142 / 5058 loss=0.141, ups=1.48, num_updates=9200, lr=8.3e-05, gnorm=1.037, clip=0, train_wall=62, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:30:13]    INFO >> epoch 002:   4242 / 5058 loss=0.15, ups=1.57, num_updates=9300, lr=8.3e-05, gnorm=2.156, clip=0, train_wall=58, gb_free=75.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:31:14]    INFO >> epoch 002:   4342 / 5058 loss=0.138, ups=1.7, num_updates=9400, lr=8.3e-05, gnorm=0.847, clip=0, train_wall=54, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:32:25]    INFO >> epoch 002:   4442 / 5058 loss=0.074, ups=1.46, num_updates=9500, lr=8.3e-05, gnorm=1.073, clip=0, train_wall=63, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:33:31]    INFO >> epoch 002:   4542 / 5058 loss=0.06, ups=1.59, num_updates=9600, lr=8.2e-05, gnorm=0.621, clip=0, train_wall=57, gb_free=68.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:34:46]    INFO >> epoch 002:   4642 / 5058 loss=0.237, ups=1.38, num_updates=9700, lr=8.2e-05, gnorm=2.74, clip=0, train_wall=66, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:35:56]    INFO >> epoch 002:   4742 / 5058 loss=0.103, ups=1.49, num_updates=9800, lr=8.2e-05, gnorm=2.258, clip=0, train_wall=61, gb_free=63.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:36:53]    INFO >> epoch 002:   4842 / 5058 loss=0.052, ups=1.85, num_updates=9900, lr=8.2e-05, gnorm=1.153, clip=0, train_wall=49, gb_free=75.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:37:48]    INFO >> epoch 002:   4942 / 5058 loss=0.077, ups=1.92, num_updates=10000, lr=8.2e-05, gnorm=0.928, clip=0, train_wall=48, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:39:00]    INFO >> epoch 002:   5042 / 5058 loss=0.188, ups=1.43, num_updates=10100, lr=8.1e-05, gnorm=2.802, clip=0, train_wall=64, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:39:12]    INFO >> epoch 002 | loss 0.092 | ups 1.47 | num_updates 10116 | lr 8.1e-05 | gnorm 1.476 | clip 0.1 | train_wall 2818 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-21 00:39:12] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:46:13]    INFO >> epoch 002 | valid on 'valid' subset | loss 0.13 | num_updates 10116 | best_loss 0.13 (progress_bar.py:267, print())[0m
[32m[2025-11-21 00:46:14]    INFO >> saved checkpoint /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints/checkpoint2.pt (epoch 2 @ 10116 updates, score 0.13) (writing took 1.740104 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 00:46:14] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:46:54]    INFO >> epoch 003:     84 / 5058 loss=0.234, ups=0.22, num_updates=10200, lr=8.1e-05, gnorm=1.535, clip=0, train_wall=46, gb_free=78.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:47:40]    INFO >> epoch 003:    184 / 5058 loss=0.142, ups=2.24, num_updates=10300, lr=8.1e-05, gnorm=0.779, clip=0, train_wall=41, gb_free=63.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:48:49]    INFO >> epoch 003:    284 / 5058 loss=0.155, ups=1.49, num_updates=10400, lr=8.1e-05, gnorm=0.584, clip=0, train_wall=61, gb_free=64.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:49:58]    INFO >> epoch 003:    384 / 5058 loss=0.22, ups=1.52, num_updates=10500, lr=8.1e-05, gnorm=1.383, clip=0, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:51:07]    INFO >> epoch 003:    484 / 5058 loss=0.242, ups=1.52, num_updates=10600, lr=8e-05, gnorm=2.534, clip=0, train_wall=60, gb_free=69.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:52:14]    INFO >> epoch 003:    584 / 5058 loss=0.067, ups=1.56, num_updates=10700, lr=8e-05, gnorm=1.996, clip=0, train_wall=58, gb_free=68.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:53:01]    INFO >> epoch 003:    684 / 5058 loss=0.157, ups=2.23, num_updates=10800, lr=8e-05, gnorm=0.771, clip=0, train_wall=41, gb_free=65.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:53:42]    INFO >> epoch 003:    784 / 5058 loss=0.143, ups=2.47, num_updates=10900, lr=8e-05, gnorm=1.298, clip=0, train_wall=38, gb_free=78.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:54:52]    INFO >> epoch 003:    884 / 5058 loss=0.175, ups=1.51, num_updates=11000, lr=8e-05, gnorm=2.819, clip=0, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:55:58]    INFO >> epoch 003:    984 / 5058 loss=0.115, ups=1.58, num_updates=11100, lr=7.9e-05, gnorm=0.744, clip=0, train_wall=59, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:57:00]    INFO >> epoch 003:   1084 / 5058 loss=0.055, ups=1.68, num_updates=11200, lr=7.9e-05, gnorm=0.939, clip=0, train_wall=55, gb_free=75.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:58:09]    INFO >> epoch 003:   1184 / 5058 loss=0.032, ups=1.52, num_updates=11300, lr=7.9e-05, gnorm=0.166, clip=0, train_wall=60, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:59:16]    INFO >> epoch 003:   1284 / 5058 loss=0.078, ups=1.55, num_updates=11400, lr=7.9e-05, gnorm=0.892, clip=0, train_wall=59, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:00:22]    INFO >> epoch 003:   1384 / 5058 loss=0.055, ups=1.57, num_updates=11500, lr=7.9e-05, gnorm=0.971, clip=0, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:01:32]    INFO >> epoch 003:   1484 / 5058 loss=0.243, ups=1.49, num_updates=11600, lr=7.8e-05, gnorm=0.685, clip=0, train_wall=62, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:02:36]    INFO >> epoch 003:   1584 / 5058 loss=0.082, ups=1.64, num_updates=11700, lr=7.8e-05, gnorm=1.377, clip=0, train_wall=56, gb_free=63.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:03:46]    INFO >> epoch 003:   1684 / 5058 loss=0.109, ups=1.48, num_updates=11800, lr=7.8e-05, gnorm=1.121, clip=0, train_wall=62, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:04:49]    INFO >> epoch 003:   1784 / 5058 loss=0.104, ups=1.64, num_updates=11900, lr=7.8e-05, gnorm=1.508, clip=0, train_wall=56, gb_free=78 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:05:28]    INFO >> epoch 003:   1884 / 5058 loss=0.127, ups=2.68, num_updates=12000, lr=7.8e-05, gnorm=0.499, clip=0, train_wall=34, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:06:31]    INFO >> epoch 003:   1984 / 5058 loss=0.115, ups=1.66, num_updates=12100, lr=7.7e-05, gnorm=1.153, clip=0, train_wall=55, gb_free=65.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:07:42]    INFO >> epoch 003:   2084 / 5058 loss=0.064, ups=1.45, num_updates=12200, lr=7.7e-05, gnorm=1.286, clip=0, train_wall=62, gb_free=63.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:08:52]    INFO >> epoch 003:   2184 / 5058 loss=0.281, ups=1.49, num_updates=12300, lr=7.7e-05, gnorm=3.057, clip=0, train_wall=61, gb_free=63.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:10:03]    INFO >> epoch 003:   2284 / 5058 loss=0.06, ups=1.46, num_updates=12400, lr=7.7e-05, gnorm=1.1, clip=0, train_wall=62, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:11:13]    INFO >> epoch 003:   2384 / 5058 loss=0.085, ups=1.49, num_updates=12500, lr=7.7e-05, gnorm=0.415, clip=0, train_wall=61, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:12:25]    INFO >> epoch 003:   2484 / 5058 loss=0.127, ups=1.44, num_updates=12600, lr=7.6e-05, gnorm=1.607, clip=0, train_wall=63, gb_free=65.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:13:31]    INFO >> epoch 003:   2584 / 5058 loss=0.117, ups=1.6, num_updates=12700, lr=7.6e-05, gnorm=1.524, clip=0, train_wall=57, gb_free=71.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:14:32]    INFO >> epoch 003:   2684 / 5058 loss=0.081, ups=1.69, num_updates=12800, lr=7.6e-05, gnorm=0.806, clip=0, train_wall=54, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:15:41]    INFO >> epoch 003:   2784 / 5058 loss=0.098, ups=1.53, num_updates=12900, lr=7.6e-05, gnorm=0.866, clip=0, train_wall=59, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:16:23]    INFO >> epoch 003:   2884 / 5058 loss=0.302, ups=2.43, num_updates=13000, lr=7.6e-05, gnorm=0.629, clip=0, train_wall=38, gb_free=64.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:17:33]    INFO >> epoch 003:   2984 / 5058 loss=0.155, ups=1.49, num_updates=13100, lr=7.5e-05, gnorm=1.844, clip=0, train_wall=61, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:18:34]    INFO >> epoch 003:   3084 / 5058 loss=0.058, ups=1.72, num_updates=13200, lr=7.5e-05, gnorm=0.284, clip=0, train_wall=53, gb_free=68.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:19:35]    INFO >> epoch 003:   3184 / 5058 loss=0.133, ups=1.71, num_updates=13300, lr=7.5e-05, gnorm=1.112, clip=0, train_wall=53, gb_free=78.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:20:36]    INFO >> epoch 003:   3284 / 5058 loss=0.073, ups=1.72, num_updates=13400, lr=7.5e-05, gnorm=0.518, clip=0, train_wall=53, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:21:50]    INFO >> epoch 003:   3384 / 5058 loss=0.206, ups=1.39, num_updates=13500, lr=7.4e-05, gnorm=2.783, clip=0, train_wall=65, gb_free=63.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:23:00]    INFO >> epoch 003:   3484 / 5058 loss=0.208, ups=1.48, num_updates=13600, lr=7.4e-05, gnorm=1.609, clip=0, train_wall=61, gb_free=65.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:24:11]    INFO >> epoch 003:   3584 / 5058 loss=0.092, ups=1.48, num_updates=13700, lr=7.4e-05, gnorm=1.517, clip=0, train_wall=61, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:25:16]    INFO >> epoch 003:   3684 / 5058 loss=0.095, ups=1.6, num_updates=13800, lr=7.4e-05, gnorm=1.164, clip=0, train_wall=57, gb_free=64.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:26:27]    INFO >> epoch 003:   3784 / 5058 loss=0.054, ups=1.47, num_updates=13900, lr=7.4e-05, gnorm=0.732, clip=0, train_wall=62, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:27:20]    INFO >> epoch 003:   3884 / 5058 loss=0.098, ups=1.98, num_updates=14000, lr=7.3e-05, gnorm=0.787, clip=0, train_wall=46, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:28:26]    INFO >> epoch 003:   3984 / 5058 loss=0.13, ups=1.58, num_updates=14100, lr=7.3e-05, gnorm=1.313, clip=0, train_wall=57, gb_free=63.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:29:33]    INFO >> epoch 003:   4084 / 5058 loss=0.249, ups=1.55, num_updates=14200, lr=7.3e-05, gnorm=0.512, clip=0, train_wall=59, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:30:44]    INFO >> epoch 003:   4184 / 5058 loss=0.152, ups=1.46, num_updates=14300, lr=7.3e-05, gnorm=1.216, clip=0, train_wall=62, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:31:48]    INFO >> epoch 003:   4284 / 5058 loss=0.099, ups=1.63, num_updates=14400, lr=7.3e-05, gnorm=1.055, clip=0, train_wall=56, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:32:52]    INFO >> epoch 003:   4384 / 5058 loss=0.085, ups=1.63, num_updates=14500, lr=7.2e-05, gnorm=1.117, clip=0, train_wall=56, gb_free=65.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:34:00]    INFO >> epoch 003:   4484 / 5058 loss=0.07, ups=1.53, num_updates=14600, lr=7.2e-05, gnorm=0.865, clip=0, train_wall=59, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:35:10]    INFO >> epoch 003:   4584 / 5058 loss=0.278, ups=1.5, num_updates=14700, lr=7.2e-05, gnorm=1.008, clip=0, train_wall=60, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:36:25]    INFO >> epoch 003:   4684 / 5058 loss=0.221, ups=1.39, num_updates=14800, lr=7.2e-05, gnorm=2.713, clip=0, train_wall=65, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:37:31]    INFO >> epoch 003:   4784 / 5058 loss=0.074, ups=1.57, num_updates=14900, lr=7.2e-05, gnorm=1.316, clip=0, train_wall=57, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:38:20]    INFO >> epoch 003:   4884 / 5058 loss=0.041, ups=2.17, num_updates=15000, lr=7.1e-05, gnorm=0.539, clip=0, train_wall=42, gb_free=64.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:39:23]    INFO >> epoch 003:   4984 / 5058 loss=0.079, ups=1.63, num_updates=15100, lr=7.1e-05, gnorm=1.345, clip=0, train_wall=56, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:40:18]    INFO >> epoch 003 | loss 0.09 | ups 1.44 | num_updates 15174 | lr 7.1e-05 | gnorm 1.23 | clip 0 | train_wall 2837 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-21 01:40:18] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:47:19]    INFO >> epoch 003 | valid on 'valid' subset | loss 0.125 | num_updates 15174 | best_loss 0.125 (progress_bar.py:267, print())[0m
[32m[2025-11-21 01:47:21]    INFO >> saved checkpoint /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints/checkpoint3.pt (epoch 3 @ 15174 updates, score 0.125) (writing took 1.792679 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 01:47:21] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:47:37]    INFO >> epoch 004:     26 / 5058 loss=0.224, ups=0.21, num_updates=15200, lr=7.1e-05, gnorm=2.724, clip=0, train_wall=62, gb_free=74 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:48:20]    INFO >> epoch 004:    126 / 5058 loss=0.567, ups=2.44, num_updates=15300, lr=7.1e-05, gnorm=0.702, clip=0, train_wall=38, gb_free=64 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:49:17]    INFO >> epoch 004:    226 / 5058 loss=0.131, ups=1.83, num_updates=15400, lr=7.1e-05, gnorm=0.572, clip=0, train_wall=50, gb_free=65.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:50:24]    INFO >> epoch 004:    326 / 5058 loss=0.122, ups=1.55, num_updates=15500, lr=7e-05, gnorm=0.613, clip=0, train_wall=59, gb_free=65.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:51:35]    INFO >> epoch 004:    426 / 5058 loss=0.236, ups=1.47, num_updates=15600, lr=7e-05, gnorm=2.37, clip=0, train_wall=62, gb_free=64.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:52:39]    INFO >> epoch 004:    526 / 5058 loss=0.115, ups=1.62, num_updates=15700, lr=7e-05, gnorm=1.536, clip=0, train_wall=56, gb_free=64.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:53:41]    INFO >> epoch 004:    626 / 5058 loss=0.073, ups=1.69, num_updates=15800, lr=7e-05, gnorm=1.434, clip=0, train_wall=54, gb_free=74.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:54:23]    INFO >> epoch 004:    726 / 5058 loss=0.197, ups=2.45, num_updates=15900, lr=7e-05, gnorm=1.383, clip=0, train_wall=37, gb_free=68.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:55:17]    INFO >> epoch 004:    826 / 5058 loss=0.118, ups=1.96, num_updates=16000, lr=6.9e-05, gnorm=0.473, clip=0, train_wall=46, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:56:28]    INFO >> epoch 004:    926 / 5058 loss=0.168, ups=1.46, num_updates=16100, lr=6.9e-05, gnorm=2.756, clip=0, train_wall=62, gb_free=64.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:57:33]    INFO >> epoch 004:   1026 / 5058 loss=0.102, ups=1.6, num_updates=16200, lr=6.9e-05, gnorm=0.567, clip=0, train_wall=58, gb_free=74.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:58:36]    INFO >> epoch 004:   1126 / 5058 loss=0.039, ups=1.67, num_updates=16300, lr=6.9e-05, gnorm=0.609, clip=0, train_wall=55, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:59:45]    INFO >> epoch 004:   1226 / 5058 loss=0.042, ups=1.5, num_updates=16400, lr=6.9e-05, gnorm=0.413, clip=0, train_wall=61, gb_free=64.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:00:53]    INFO >> epoch 004:   1326 / 5058 loss=0.071, ups=1.53, num_updates=16500, lr=6.8e-05, gnorm=0.637, clip=0, train_wall=60, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:01:59]    INFO >> epoch 004:   1426 / 5058 loss=0.067, ups=1.59, num_updates=16600, lr=6.8e-05, gnorm=1.003, clip=0, train_wall=58, gb_free=65.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:03:07]    INFO >> epoch 004:   1526 / 5058 loss=0.086, ups=1.53, num_updates=16700, lr=6.8e-05, gnorm=0.993, clip=0, train_wall=60, gb_free=71.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:04:11]    INFO >> epoch 004:   1626 / 5058 loss=0.104, ups=1.63, num_updates=16800, lr=6.8e-05, gnorm=1.123, clip=0, train_wall=57, gb_free=68.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:05:17]    INFO >> epoch 004:   1726 / 5058 loss=0.127, ups=1.57, num_updates=16900, lr=6.8e-05, gnorm=1.091, clip=0, train_wall=59, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:06:05]    INFO >> epoch 004:   1826 / 5058 loss=0.084, ups=2.16, num_updates=17000, lr=6.7e-05, gnorm=0.886, clip=0, train_wall=44, gb_free=78.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:06:54]    INFO >> epoch 004:   1926 / 5058 loss=0.114, ups=2.16, num_updates=17100, lr=6.7e-05, gnorm=1.393, clip=0, train_wall=43, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:07:59]    INFO >> epoch 004:   2026 / 5058 loss=0.084, ups=1.59, num_updates=17200, lr=6.7e-05, gnorm=0.347, clip=0, train_wall=58, gb_free=65.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:09:11]    INFO >> epoch 004:   2126 / 5058 loss=0.08, ups=1.46, num_updates=17300, lr=6.7e-05, gnorm=1.911, clip=0, train_wall=63, gb_free=75 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:10:21]    INFO >> epoch 004:   2226 / 5058 loss=0.108, ups=1.49, num_updates=17400, lr=6.7e-05, gnorm=1.966, clip=0, train_wall=62, gb_free=64.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:11:30]    INFO >> epoch 004:   2326 / 5058 loss=0.071, ups=1.51, num_updates=17500, lr=6.6e-05, gnorm=0.881, clip=0, train_wall=61, gb_free=64.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:12:38]    INFO >> epoch 004:   2426 / 5058 loss=0.078, ups=1.53, num_updates=17600, lr=6.6e-05, gnorm=0.544, clip=0, train_wall=60, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:13:51]    INFO >> epoch 004:   2526 / 5058 loss=0.187, ups=1.42, num_updates=17700, lr=6.6e-05, gnorm=2.131, clip=0, train_wall=65, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:14:50]    INFO >> epoch 004:   2626 / 5058 loss=0.085, ups=1.77, num_updates=17800, lr=6.6e-05, gnorm=1.149, clip=0, train_wall=52, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:15:55]    INFO >> epoch 004:   2726 / 5058 loss=0.088, ups=1.61, num_updates=17900, lr=6.6e-05, gnorm=0.55, clip=0, train_wall=57, gb_free=64.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:16:49]    INFO >> epoch 004:   2826 / 5058 loss=0.096, ups=1.9, num_updates=18000, lr=6.5e-05, gnorm=0.556, clip=0, train_wall=49, gb_free=74 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:17:41]    INFO >> epoch 004:   2926 / 5058 loss=0.207, ups=2.02, num_updates=18100, lr=6.5e-05, gnorm=1.397, clip=0, train_wall=46, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:18:49]    INFO >> epoch 004:   3026 / 5058 loss=0.107, ups=1.51, num_updates=18200, lr=6.5e-05, gnorm=1.137, clip=0, train_wall=61, gb_free=64 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:19:47]    INFO >> epoch 004:   3126 / 5058 loss=0.062, ups=1.81, num_updates=18300, lr=6.5e-05, gnorm=0.471, clip=0, train_wall=51, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:20:43]    INFO >> epoch 004:   3226 / 5058 loss=0.344, ups=1.86, num_updates=18400, lr=6.4e-05, gnorm=0.684, clip=0, train_wall=50, gb_free=75.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:21:50]    INFO >> epoch 004:   3326 / 5058 loss=0.098, ups=1.56, num_updates=18500, lr=6.4e-05, gnorm=1.466, clip=0, train_wall=59, gb_free=65.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:23:03]    INFO >> epoch 004:   3426 / 5058 loss=0.207, ups=1.42, num_updates=18600, lr=6.4e-05, gnorm=2.545, clip=0, train_wall=64, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:24:12]    INFO >> epoch 004:   3526 / 5058 loss=0.138, ups=1.5, num_updates=18700, lr=6.4e-05, gnorm=0.787, clip=0, train_wall=61, gb_free=64.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:25:20]    INFO >> epoch 004:   3626 / 5058 loss=0.072, ups=1.53, num_updates=18800, lr=6.4e-05, gnorm=1.285, clip=0, train_wall=60, gb_free=67.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:26:24]    INFO >> epoch 004:   3726 / 5058 loss=0.114, ups=1.63, num_updates=18900, lr=6.3e-05, gnorm=1.426, clip=0, train_wall=56, gb_free=65.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:27:34]    INFO >> epoch 004:   3826 / 5058 loss=0.051, ups=1.5, num_updates=19000, lr=6.3e-05, gnorm=0.437, clip=0, train_wall=61, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:28:23]    INFO >> epoch 004:   3926 / 5058 loss=0.124, ups=2.15, num_updates=19100, lr=6.3e-05, gnorm=1.078, clip=0, train_wall=43, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:29:32]    INFO >> epoch 004:   4026 / 5058 loss=0.153, ups=1.52, num_updates=19200, lr=6.3e-05, gnorm=0.766, clip=0, train_wall=60, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:30:41]    INFO >> epoch 004:   4126 / 5058 loss=0.141, ups=1.5, num_updates=19300, lr=6.3e-05, gnorm=0.588, clip=0, train_wall=61, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:31:48]    INFO >> epoch 004:   4226 / 5058 loss=0.139, ups=1.54, num_updates=19400, lr=6.2e-05, gnorm=1.677, clip=0, train_wall=59, gb_free=63.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:32:48]    INFO >> epoch 004:   4326 / 5058 loss=0.118, ups=1.76, num_updates=19500, lr=6.2e-05, gnorm=0.595, clip=0, train_wall=52, gb_free=72.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:33:58]    INFO >> epoch 004:   4426 / 5058 loss=0.073, ups=1.49, num_updates=19600, lr=6.2e-05, gnorm=0.902, clip=0, train_wall=62, gb_free=66.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:35:03]    INFO >> epoch 004:   4526 / 5058 loss=0.066, ups=1.61, num_updates=19700, lr=6.2e-05, gnorm=0.654, clip=0, train_wall=57, gb_free=67.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:36:15]    INFO >> epoch 004:   4626 / 5058 loss=0.227, ups=1.44, num_updates=19800, lr=6.2e-05, gnorm=1.763, clip=0, train_wall=63, gb_free=65.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:37:28]    INFO >> epoch 004:   4726 / 5058 loss=0.103, ups=1.43, num_updates=19900, lr=6.1e-05, gnorm=2.064, clip=0, train_wall=64, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:38:26]    INFO >> epoch 004:   4826 / 5058 loss=0.056, ups=1.79, num_updates=20000, lr=6.1e-05, gnorm=0.885, clip=0, train_wall=51, gb_free=70.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:39:15]    INFO >> epoch 004:   4926 / 5058 loss=0.099, ups=2.12, num_updates=20100, lr=6.1e-05, gnorm=0.71, clip=0, train_wall=43, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:40:25]    INFO >> epoch 004:   5026 / 5058 loss=0.111, ups=1.48, num_updates=20200, lr=6.1e-05, gnorm=1.945, clip=0, train_wall=62, gb_free=63.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:40:48]    INFO >> epoch 004 | loss 0.09 | ups 1.45 | num_updates 20232 | lr 6.1e-05 | gnorm 1.133 | clip 0 | train_wall 2829 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-21 02:40:48] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 02:47:44]    INFO >> epoch 004 | valid on 'valid' subset | loss 0.121 | num_updates 20232 | best_loss 0.121 (progress_bar.py:267, print())[0m
[32m[2025-11-21 02:47:47]    INFO >> saved checkpoint /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints/checkpoint4.pt (epoch 4 @ 20232 updates, score 0.121) (writing took 1.914312 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 02:47:47] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 02:48:21]    INFO >> epoch 005:     68 / 5058 loss=0.25, ups=0.22, num_updates=20300, lr=6.1e-05, gnorm=1.838, clip=0, train_wall=50, gb_free=69.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:49:06]    INFO >> epoch 005:    168 / 5058 loss=0.118, ups=2.3, num_updates=20400, lr=6e-05, gnorm=0.49, clip=0, train_wall=40, gb_free=78.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:50:10]    INFO >> epoch 005:    268 / 5058 loss=0.164, ups=1.62, num_updates=20500, lr=6e-05, gnorm=0.501, clip=0, train_wall=57, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:51:17]    INFO >> epoch 005:    368 / 5058 loss=0.215, ups=1.56, num_updates=20600, lr=6e-05, gnorm=0.804, clip=0, train_wall=59, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:52:25]    INFO >> epoch 005:    468 / 5058 loss=0.226, ups=1.54, num_updates=20700, lr=6e-05, gnorm=2.483, clip=0, train_wall=59, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:53:30]    INFO >> epoch 005:    568 / 5058 loss=0.071, ups=1.6, num_updates=20800, lr=6e-05, gnorm=1.5, clip=0, train_wall=57, gb_free=71.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:54:21]    INFO >> epoch 005:    668 / 5058 loss=0.092, ups=1.99, num_updates=20900, lr=5.9e-05, gnorm=0.742, clip=0, train_wall=46, gb_free=78.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:55:03]    INFO >> epoch 005:    768 / 5058 loss=0.155, ups=2.49, num_updates=21000, lr=5.9e-05, gnorm=1.254, clip=0, train_wall=38, gb_free=78.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:56:03]    INFO >> epoch 005:    868 / 5058 loss=0.158, ups=1.74, num_updates=21100, lr=5.9e-05, gnorm=1.947, clip=0, train_wall=54, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:57:09]    INFO >> epoch 005:    968 / 5058 loss=0.127, ups=1.59, num_updates=21200, lr=5.9e-05, gnorm=1.053, clip=0, train_wall=59, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:58:12]    INFO >> epoch 005:   1068 / 5058 loss=0.069, ups=1.64, num_updates=21300, lr=5.9e-05, gnorm=0.719, clip=0, train_wall=57, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:59:15]    INFO >> epoch 005:   1168 / 5058 loss=0.033, ups=1.67, num_updates=21400, lr=5.8e-05, gnorm=0.247, clip=0, train_wall=56, gb_free=64.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:00:21]    INFO >> epoch 005:   1268 / 5058 loss=0.079, ups=1.59, num_updates=21500, lr=5.8e-05, gnorm=0.778, clip=0, train_wall=59, gb_free=66.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:01:23]    INFO >> epoch 005:   1368 / 5058 loss=0.053, ups=1.67, num_updates=21600, lr=5.8e-05, gnorm=0.819, clip=0, train_wall=56, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:02:31]    INFO >> epoch 005:   1468 / 5058 loss=0.152, ups=1.53, num_updates=21700, lr=5.8e-05, gnorm=0.668, clip=0, train_wall=61, gb_free=66.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:03:33]    INFO >> epoch 005:   1568 / 5058 loss=0.072, ups=1.71, num_updates=21800, lr=5.8e-05, gnorm=0.828, clip=0, train_wall=55, gb_free=69.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:04:39]    INFO >> epoch 005:   1668 / 5058 loss=0.14, ups=1.58, num_updates=21900, lr=5.7e-05, gnorm=1.223, clip=0, train_wall=59, gb_free=63.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:05:41]    INFO >> epoch 005:   1768 / 5058 loss=0.102, ups=1.64, num_updates=22000, lr=5.7e-05, gnorm=1.317, clip=0, train_wall=57, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:06:22]    INFO >> epoch 005:   1868 / 5058 loss=0.081, ups=2.61, num_updates=22100, lr=5.7e-05, gnorm=0.372, clip=0, train_wall=36, gb_free=65.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:07:15]    INFO >> epoch 005:   1968 / 5058 loss=0.104, ups=1.94, num_updates=22200, lr=5.7e-05, gnorm=0.995, clip=0, train_wall=48, gb_free=70.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:08:21]    INFO >> epoch 005:   2068 / 5058 loss=0.059, ups=1.58, num_updates=22300, lr=5.7e-05, gnorm=0.523, clip=0, train_wall=59, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:09:31]    INFO >> epoch 005:   2168 / 5058 loss=0.204, ups=1.49, num_updates=22400, lr=5.6e-05, gnorm=2.975, clip=0, train_wall=62, gb_free=65.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:10:37]    INFO >> epoch 005:   2268 / 5058 loss=0.07, ups=1.57, num_updates=22500, lr=5.6e-05, gnorm=0.94, clip=0, train_wall=59, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:11:43]    INFO >> epoch 005:   2368 / 5058 loss=0.066, ups=1.56, num_updates=22600, lr=5.6e-05, gnorm=0.671, clip=0, train_wall=60, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:12:51]    INFO >> epoch 005:   2468 / 5058 loss=0.111, ups=1.53, num_updates=22700, lr=5.6e-05, gnorm=1.111, clip=0, train_wall=61, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:13:56]    INFO >> epoch 005:   2568 / 5058 loss=0.118, ups=1.61, num_updates=22800, lr=5.6e-05, gnorm=1.256, clip=0, train_wall=58, gb_free=68.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:14:53]    INFO >> epoch 005:   2668 / 5058 loss=0.082, ups=1.87, num_updates=22900, lr=5.5e-05, gnorm=1.013, clip=0, train_wall=50, gb_free=66.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:15:58]    INFO >> epoch 005:   2768 / 5058 loss=0.11, ups=1.59, num_updates=23000, lr=5.5e-05, gnorm=0.905, clip=0, train_wall=59, gb_free=71 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:16:40]    INFO >> epoch 005:   2868 / 5058 loss=0.424, ups=2.51, num_updates=23100, lr=5.5e-05, gnorm=0.352, clip=0, train_wall=38, gb_free=68.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:17:43]    INFO >> epoch 005:   2968 / 5058 loss=0.155, ups=1.64, num_updates=23200, lr=5.5e-05, gnorm=1.628, clip=0, train_wall=57, gb_free=64.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:18:44]    INFO >> epoch 005:   3068 / 5058 loss=0.066, ups=1.72, num_updates=23300, lr=5.4e-05, gnorm=0.462, clip=0, train_wall=54, gb_free=67.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:19:42]    INFO >> epoch 005:   3168 / 5058 loss=0.078, ups=1.78, num_updates=23400, lr=5.4e-05, gnorm=0.736, clip=0, train_wall=53, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:20:37]    INFO >> epoch 005:   3268 / 5058 loss=0.133, ups=1.9, num_updates=23500, lr=5.4e-05, gnorm=0.554, clip=0, train_wall=49, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:21:46]    INFO >> epoch 005:   3368 / 5058 loss=0.121, ups=1.5, num_updates=23600, lr=5.4e-05, gnorm=2.287, clip=0, train_wall=62, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:22:55]    INFO >> epoch 005:   3468 / 5058 loss=0.193, ups=1.52, num_updates=23700, lr=5.4e-05, gnorm=1.678, clip=0, train_wall=61, gb_free=63.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:24:00]    INFO >> epoch 005:   3568 / 5058 loss=0.103, ups=1.61, num_updates=23800, lr=5.3e-05, gnorm=1.343, clip=0, train_wall=58, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:25:03]    INFO >> epoch 005:   3668 / 5058 loss=0.075, ups=1.66, num_updates=23900, lr=5.3e-05, gnorm=1.034, clip=0, train_wall=56, gb_free=75.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:26:11]    INFO >> epoch 005:   3768 / 5058 loss=0.058, ups=1.53, num_updates=24000, lr=5.3e-05, gnorm=0.798, clip=0, train_wall=61, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:27:08]    INFO >> epoch 005:   3868 / 5058 loss=0.089, ups=1.83, num_updates=24100, lr=5.3e-05, gnorm=0.763, clip=0, train_wall=51, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:28:04]    INFO >> epoch 005:   3968 / 5058 loss=0.131, ups=1.83, num_updates=24200, lr=5.3e-05, gnorm=1.105, clip=0, train_wall=51, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:29:06]    INFO >> epoch 005:   4068 / 5058 loss=0.202, ups=1.68, num_updates=24300, lr=5.2e-05, gnorm=0.437, clip=0, train_wall=56, gb_free=65.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:30:15]    INFO >> epoch 005:   4168 / 5058 loss=0.139, ups=1.53, num_updates=24400, lr=5.2e-05, gnorm=0.983, clip=0, train_wall=61, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:31:17]    INFO >> epoch 005:   4268 / 5058 loss=0.11, ups=1.67, num_updates=24500, lr=5.2e-05, gnorm=1.105, clip=0, train_wall=56, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:32:17]    INFO >> epoch 005:   4368 / 5058 loss=0.082, ups=1.75, num_updates=24600, lr=5.2e-05, gnorm=0.967, clip=0, train_wall=54, gb_free=63.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:33:23]    INFO >> epoch 005:   4468 / 5058 loss=0.07, ups=1.59, num_updates=24700, lr=5.2e-05, gnorm=0.679, clip=0, train_wall=59, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:34:26]    INFO >> epoch 005:   4568 / 5058 loss=0.113, ups=1.64, num_updates=24800, lr=5.1e-05, gnorm=0.574, clip=0, train_wall=57, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:35:38]    INFO >> epoch 005:   4668 / 5058 loss=0.238, ups=1.46, num_updates=24900, lr=5.1e-05, gnorm=2.382, clip=0, train_wall=64, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:36:41]    INFO >> epoch 005:   4768 / 5058 loss=0.084, ups=1.65, num_updates=25000, lr=5.1e-05, gnorm=1.356, clip=0, train_wall=57, gb_free=72.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:37:29]    INFO >> epoch 005:   4868 / 5058 loss=0.039, ups=2.13, num_updates=25100, lr=5.1e-05, gnorm=0.605, clip=0, train_wall=44, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:38:27]    INFO >> epoch 005:   4968 / 5058 loss=0.078, ups=1.83, num_updates=25200, lr=5.1e-05, gnorm=0.813, clip=0, train_wall=51, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:39:31]    INFO >> epoch 005 | loss 0.089 | ups 1.5 | num_updates 25290 | lr 5e-05 | gnorm 1.069 | clip 0 | train_wall 2779 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-21 03:39:31] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 03:46:17]    INFO >> epoch 005 | valid on 'valid' subset | loss 0.122 | num_updates 25290 | best_loss 0.121 (progress_bar.py:267, print())[0m
[32m[2025-11-21 03:46:17]    INFO >> saved checkpoint /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints/checkpoint5.pt (epoch 5 @ 25290 updates, score 0.122) (writing took 0.806068 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 03:46:17] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 03:46:25]    INFO >> epoch 006:     10 / 5058 loss=0.171, ups=0.22, num_updates=25300, lr=5e-05, gnorm=2.507, clip=0, train_wall=63, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:47:07]    INFO >> epoch 006:    110 / 5058 loss=0.37, ups=2.45, num_updates=25400, lr=5e-05, gnorm=0.937, clip=0, train_wall=38, gb_free=64.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:47:58]    INFO >> epoch 006:    210 / 5058 loss=0.139, ups=2.04, num_updates=25500, lr=5e-05, gnorm=0.565, clip=0, train_wall=46, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:49:03]    INFO >> epoch 006:    310 / 5058 loss=0.155, ups=1.61, num_updates=25600, lr=5e-05, gnorm=0.399, clip=0, train_wall=58, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:50:10]    INFO >> epoch 006:    410 / 5058 loss=0.216, ups=1.56, num_updates=25700, lr=5e-05, gnorm=1.848, clip=0, train_wall=60, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:51:13]    INFO >> epoch 006:    510 / 5058 loss=0.151, ups=1.65, num_updates=25800, lr=4.9e-05, gnorm=1.791, clip=0, train_wall=57, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:52:14]    INFO >> epoch 006:    610 / 5058 loss=0.066, ups=1.71, num_updates=25900, lr=4.9e-05, gnorm=1.389, clip=0, train_wall=55, gb_free=69.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:52:55]    INFO >> epoch 006:    710 / 5058 loss=0.143, ups=2.54, num_updates=26000, lr=4.9e-05, gnorm=0.901, clip=0, train_wall=37, gb_free=77.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:53:40]    INFO >> epoch 006:    810 / 5058 loss=0.119, ups=2.32, num_updates=26100, lr=4.9e-05, gnorm=0.794, clip=0, train_wall=41, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:54:48]    INFO >> epoch 006:    910 / 5058 loss=0.183, ups=1.52, num_updates=26200, lr=4.9e-05, gnorm=2.492, clip=0, train_wall=61, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:55:54]    INFO >> epoch 006:   1010 / 5058 loss=0.097, ups=1.59, num_updates=26300, lr=4.8e-05, gnorm=0.283, clip=0, train_wall=59, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:56:53]    INFO >> epoch 006:   1110 / 5058 loss=0.044, ups=1.77, num_updates=26400, lr=4.8e-05, gnorm=0.834, clip=0, train_wall=53, gb_free=64 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:57:58]    INFO >> epoch 006:   1210 / 5058 loss=0.037, ups=1.6, num_updates=26500, lr=4.8e-05, gnorm=0.292, clip=0, train_wall=59, gb_free=64.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 03:59:04]    INFO >> epoch 006:   1310 / 5058 loss=0.073, ups=1.59, num_updates=26600, lr=4.8e-05, gnorm=0.593, clip=0, train_wall=59, gb_free=63.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:00:08]    INFO >> epoch 006:   1410 / 5058 loss=0.056, ups=1.63, num_updates=26700, lr=4.8e-05, gnorm=0.773, clip=0, train_wall=57, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:01:14]    INFO >> epoch 006:   1510 / 5058 loss=0.28, ups=1.58, num_updates=26800, lr=4.7e-05, gnorm=0.965, clip=0, train_wall=59, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:02:14]    INFO >> epoch 006:   1610 / 5058 loss=0.081, ups=1.71, num_updates=26900, lr=4.7e-05, gnorm=1.017, clip=0, train_wall=55, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:03:19]    INFO >> epoch 006:   1710 / 5058 loss=0.116, ups=1.59, num_updates=27000, lr=4.7e-05, gnorm=0.831, clip=0, train_wall=59, gb_free=73.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:04:12]    INFO >> epoch 006:   1810 / 5058 loss=0.094, ups=1.99, num_updates=27100, lr=4.7e-05, gnorm=1.027, clip=0, train_wall=47, gb_free=74.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:04:54]    INFO >> epoch 006:   1910 / 5058 loss=0.157, ups=2.45, num_updates=27200, lr=4.7e-05, gnorm=1.145, clip=0, train_wall=39, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:05:58]    INFO >> epoch 006:   2010 / 5058 loss=0.068, ups=1.64, num_updates=27300, lr=4.6e-05, gnorm=0.408, clip=0, train_wall=57, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:07:06]    INFO >> epoch 006:   2110 / 5058 loss=0.072, ups=1.54, num_updates=27400, lr=4.6e-05, gnorm=1.493, clip=0, train_wall=60, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:08:12]    INFO >> epoch 006:   2210 / 5058 loss=0.27, ups=1.56, num_updates=27500, lr=4.6e-05, gnorm=2.106, clip=0, train_wall=60, gb_free=65.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:09:20]    INFO >> epoch 006:   2310 / 5058 loss=0.06, ups=1.53, num_updates=27600, lr=4.6e-05, gnorm=0.839, clip=0, train_wall=61, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:10:26]    INFO >> epoch 006:   2410 / 5058 loss=0.076, ups=1.58, num_updates=27700, lr=4.6e-05, gnorm=0.377, clip=0, train_wall=59, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:11:38]    INFO >> epoch 006:   2510 / 5058 loss=0.142, ups=1.45, num_updates=27800, lr=4.5e-05, gnorm=1.796, clip=0, train_wall=64, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:12:37]    INFO >> epoch 006:   2610 / 5058 loss=0.114, ups=1.78, num_updates=27900, lr=4.5e-05, gnorm=1.211, clip=0, train_wall=52, gb_free=66.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:13:38]    INFO >> epoch 006:   2710 / 5058 loss=0.086, ups=1.71, num_updates=28000, lr=4.5e-05, gnorm=0.49, clip=0, train_wall=54, gb_free=68.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:14:38]    INFO >> epoch 006:   2810 / 5058 loss=0.07, ups=1.72, num_updates=28100, lr=4.5e-05, gnorm=0.522, clip=0, train_wall=54, gb_free=73.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:15:21]    INFO >> epoch 006:   2910 / 5058 loss=0.208, ups=2.42, num_updates=28200, lr=4.4e-05, gnorm=1.103, clip=0, train_wall=39, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:16:29]    INFO >> epoch 006:   3010 / 5058 loss=0.137, ups=1.54, num_updates=28300, lr=4.4e-05, gnorm=1.102, clip=0, train_wall=60, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:17:23]    INFO >> epoch 006:   3110 / 5058 loss=0.059, ups=1.88, num_updates=28400, lr=4.4e-05, gnorm=0.338, clip=0, train_wall=50, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:18:21]    INFO >> epoch 006:   3210 / 5058 loss=0.265, ups=1.81, num_updates=28500, lr=4.4e-05, gnorm=0.745, clip=0, train_wall=52, gb_free=64.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:19:24]    INFO >> epoch 006:   3310 / 5058 loss=0.085, ups=1.67, num_updates=28600, lr=4.4e-05, gnorm=0.857, clip=0, train_wall=56, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:20:37]    INFO >> epoch 006:   3410 / 5058 loss=0.236, ups=1.44, num_updates=28700, lr=4.3e-05, gnorm=2.655, clip=0, train_wall=64, gb_free=64 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:21:43]    INFO >> epoch 006:   3510 / 5058 loss=0.12, ups=1.58, num_updates=28800, lr=4.3e-05, gnorm=0.776, clip=0, train_wall=59, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:22:50]    INFO >> epoch 006:   3610 / 5058 loss=0.082, ups=1.54, num_updates=28900, lr=4.3e-05, gnorm=1.268, clip=0, train_wall=60, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:23:51]    INFO >> epoch 006:   3710 / 5058 loss=0.137, ups=1.71, num_updates=29000, lr=4.3e-05, gnorm=1.085, clip=0, train_wall=54, gb_free=69.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:25:00]    INFO >> epoch 006:   3810 / 5058 loss=0.054, ups=1.51, num_updates=29100, lr=4.3e-05, gnorm=0.612, clip=0, train_wall=61, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:25:47]    INFO >> epoch 006:   3910 / 5058 loss=0.099, ups=2.22, num_updates=29200, lr=4.2e-05, gnorm=0.995, clip=0, train_wall=42, gb_free=72.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:26:52]    INFO >> epoch 006:   4010 / 5058 loss=0.159, ups=1.59, num_updates=29300, lr=4.2e-05, gnorm=0.699, clip=0, train_wall=58, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:27:58]    INFO >> epoch 006:   4110 / 5058 loss=0.622, ups=1.58, num_updates=29400, lr=4.2e-05, gnorm=0.528, clip=0, train_wall=59, gb_free=67.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:29:05]    INFO >> epoch 006:   4210 / 5058 loss=0.127, ups=1.57, num_updates=29500, lr=4.2e-05, gnorm=1.185, clip=0, train_wall=59, gb_free=69.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:30:06]    INFO >> epoch 006:   4310 / 5058 loss=0.111, ups=1.69, num_updates=29600, lr=4.2e-05, gnorm=0.827, clip=0, train_wall=55, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:31:09]    INFO >> epoch 006:   4410 / 5058 loss=0.069, ups=1.68, num_updates=29700, lr=4.1e-05, gnorm=0.843, clip=0, train_wall=55, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:32:13]    INFO >> epoch 006:   4510 / 5058 loss=0.069, ups=1.63, num_updates=29800, lr=4.1e-05, gnorm=0.651, clip=0, train_wall=57, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:33:21]    INFO >> epoch 006:   4610 / 5058 loss=0.232, ups=1.53, num_updates=29900, lr=4.1e-05, gnorm=1.376, clip=0, train_wall=61, gb_free=65.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:34:33]    INFO >> epoch 006:   4710 / 5058 loss=0.216, ups=1.44, num_updates=30000, lr=4.1e-05, gnorm=2.107, clip=0, train_wall=64, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:35:34]    INFO >> epoch 006:   4810 / 5058 loss=0.049, ups=1.72, num_updates=30100, lr=4.1e-05, gnorm=0.721, clip=0, train_wall=54, gb_free=75.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:36:17]    INFO >> epoch 006:   4910 / 5058 loss=0.127, ups=2.4, num_updates=30200, lr=4e-05, gnorm=0.687, clip=0, train_wall=39, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:37:24]    INFO >> epoch 006:   5010 / 5058 loss=0.092, ups=1.55, num_updates=30300, lr=4e-05, gnorm=1.662, clip=0, train_wall=60, gb_free=64.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:37:58]    INFO >> epoch 006 | loss 0.089 | ups 1.5 | num_updates 30348 | lr 4e-05 | gnorm 1.031 | clip 0 | train_wall 2773 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-21 04:37:58] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 04:44:47]    INFO >> epoch 006 | valid on 'valid' subset | loss 0.118 | num_updates 30348 | best_loss 0.118 (progress_bar.py:267, print())[0m
[32m[2025-11-21 04:44:48]    INFO >> saved checkpoint /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints/checkpoint6.pt (epoch 6 @ 30348 updates, score 0.118) (writing took 1.569620 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 04:44:48] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 04:45:14]    INFO >> epoch 007:     52 / 5058 loss=0.205, ups=0.22, num_updates=30400, lr=4e-05, gnorm=1.649, clip=0, train_wall=53, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:46:00]    INFO >> epoch 007:    152 / 5058 loss=0.141, ups=2.3, num_updates=30500, lr=4e-05, gnorm=0.628, clip=0, train_wall=41, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:46:57]    INFO >> epoch 007:    252 / 5058 loss=0.258, ups=1.8, num_updates=30600, lr=4e-05, gnorm=0.319, clip=0, train_wall=52, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:48:02]    INFO >> epoch 007:    352 / 5058 loss=0.154, ups=1.62, num_updates=30700, lr=3.9e-05, gnorm=0.836, clip=0, train_wall=58, gb_free=63.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:49:11]    INFO >> epoch 007:    452 / 5058 loss=0.228, ups=1.51, num_updates=30800, lr=3.9e-05, gnorm=2.179, clip=0, train_wall=61, gb_free=73.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:50:13]    INFO >> epoch 007:    552 / 5058 loss=0.073, ups=1.67, num_updates=30900, lr=3.9e-05, gnorm=1.315, clip=0, train_wall=56, gb_free=69.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:51:09]    INFO >> epoch 007:    652 / 5058 loss=0.083, ups=1.89, num_updates=31000, lr=3.9e-05, gnorm=0.853, clip=0, train_wall=49, gb_free=63.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:51:47]    INFO >> epoch 007:    752 / 5058 loss=0.151, ups=2.7, num_updates=31100, lr=3.9e-05, gnorm=1.042, clip=0, train_wall=35, gb_free=67.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:52:44]    INFO >> epoch 007:    852 / 5058 loss=0.162, ups=1.84, num_updates=31200, lr=3.8e-05, gnorm=1.221, clip=0, train_wall=50, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:53:53]    INFO >> epoch 007:    952 / 5058 loss=0.13, ups=1.52, num_updates=31300, lr=3.8e-05, gnorm=1.381, clip=0, train_wall=61, gb_free=74.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:54:54]    INFO >> epoch 007:   1052 / 5058 loss=0.106, ups=1.69, num_updates=31400, lr=3.8e-05, gnorm=0.803, clip=0, train_wall=55, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:55:56]    INFO >> epoch 007:   1152 / 5058 loss=0.035, ups=1.68, num_updates=31500, lr=3.8e-05, gnorm=0.191, clip=0, train_wall=55, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:57:03]    INFO >> epoch 007:   1252 / 5058 loss=0.048, ups=1.55, num_updates=31600, lr=3.8e-05, gnorm=0.549, clip=0, train_wall=60, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:58:07]    INFO >> epoch 007:   1352 / 5058 loss=0.055, ups=1.62, num_updates=31700, lr=3.7e-05, gnorm=0.546, clip=0, train_wall=57, gb_free=74.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 04:59:14]    INFO >> epoch 007:   1452 / 5058 loss=0.206, ups=1.57, num_updates=31800, lr=3.7e-05, gnorm=0.981, clip=0, train_wall=59, gb_free=64.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:00:17]    INFO >> epoch 007:   1552 / 5058 loss=0.075, ups=1.64, num_updates=31900, lr=3.7e-05, gnorm=0.823, clip=0, train_wall=57, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:01:22]    INFO >> epoch 007:   1652 / 5058 loss=0.131, ups=1.62, num_updates=32000, lr=3.7e-05, gnorm=1.115, clip=0, train_wall=57, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:02:28]    INFO >> epoch 007:   1752 / 5058 loss=0.103, ups=1.58, num_updates=32100, lr=3.7e-05, gnorm=0.89, clip=0, train_wall=59, gb_free=64.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:03:09]    INFO >> epoch 007:   1852 / 5058 loss=0.088, ups=2.51, num_updates=32200, lr=3.6e-05, gnorm=0.679, clip=0, train_wall=37, gb_free=65.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:04:01]    INFO >> epoch 007:   1952 / 5058 loss=0.104, ups=2, num_updates=32300, lr=3.6e-05, gnorm=0.908, clip=0, train_wall=47, gb_free=65.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:05:08]    INFO >> epoch 007:   2052 / 5058 loss=0.061, ups=1.57, num_updates=32400, lr=3.6e-05, gnorm=0.358, clip=0, train_wall=59, gb_free=64.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:06:17]    INFO >> epoch 007:   2152 / 5058 loss=0.129, ups=1.49, num_updates=32500, lr=3.6e-05, gnorm=2.581, clip=0, train_wall=62, gb_free=66.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:07:24]    INFO >> epoch 007:   2252 / 5058 loss=0.072, ups=1.57, num_updates=32600, lr=3.6e-05, gnorm=1.019, clip=0, train_wall=59, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:08:31]    INFO >> epoch 007:   2352 / 5058 loss=0.073, ups=1.56, num_updates=32700, lr=3.5e-05, gnorm=0.825, clip=0, train_wall=59, gb_free=72.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:09:38]    INFO >> epoch 007:   2452 / 5058 loss=0.095, ups=1.54, num_updates=32800, lr=3.5e-05, gnorm=0.71, clip=0, train_wall=60, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:10:46]    INFO >> epoch 007:   2552 / 5058 loss=0.156, ups=1.54, num_updates=32900, lr=3.5e-05, gnorm=1.313, clip=0, train_wall=60, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:11:41]    INFO >> epoch 007:   2652 / 5058 loss=0.075, ups=1.86, num_updates=33000, lr=3.5e-05, gnorm=1.014, clip=0, train_wall=50, gb_free=70.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:12:46]    INFO >> epoch 007:   2752 / 5058 loss=0.107, ups=1.61, num_updates=33100, lr=3.4e-05, gnorm=0.814, clip=0, train_wall=58, gb_free=67 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:13:32]    INFO >> epoch 007:   2852 / 5058 loss=1.241, ups=2.29, num_updates=33200, lr=3.4e-05, gnorm=0.203, clip=0, train_wall=41, gb_free=77.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:14:31]    INFO >> epoch 007:   2952 / 5058 loss=0.188, ups=1.76, num_updates=33300, lr=3.4e-05, gnorm=1.412, clip=0, train_wall=53, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:15:34]    INFO >> epoch 007:   3052 / 5058 loss=0.07, ups=1.64, num_updates=33400, lr=3.4e-05, gnorm=0.588, clip=0, train_wall=57, gb_free=69.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:16:34]    INFO >> epoch 007:   3152 / 5058 loss=0.069, ups=1.77, num_updates=33500, lr=3.4e-05, gnorm=0.562, clip=0, train_wall=53, gb_free=65 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:17:28]    INFO >> epoch 007:   3252 / 5058 loss=0.248, ups=1.92, num_updates=33600, lr=3.3e-05, gnorm=0.531, clip=0, train_wall=49, gb_free=63.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:18:37]    INFO >> epoch 007:   3352 / 5058 loss=0.108, ups=1.52, num_updates=33700, lr=3.3e-05, gnorm=1.866, clip=0, train_wall=61, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:19:47]    INFO >> epoch 007:   3452 / 5058 loss=0.209, ups=1.49, num_updates=33800, lr=3.3e-05, gnorm=1.946, clip=0, train_wall=62, gb_free=64.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:20:52]    INFO >> epoch 007:   3552 / 5058 loss=0.09, ups=1.59, num_updates=33900, lr=3.3e-05, gnorm=0.903, clip=0, train_wall=58, gb_free=74 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:21:58]    INFO >> epoch 007:   3652 / 5058 loss=0.084, ups=1.6, num_updates=34000, lr=3.3e-05, gnorm=0.933, clip=0, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:23:03]    INFO >> epoch 007:   3752 / 5058 loss=0.058, ups=1.59, num_updates=34100, lr=3.2e-05, gnorm=0.971, clip=0, train_wall=58, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:24:02]    INFO >> epoch 007:   3852 / 5058 loss=0.089, ups=1.73, num_updates=34200, lr=3.2e-05, gnorm=0.557, clip=0, train_wall=54, gb_free=70.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:24:57]    INFO >> epoch 007:   3952 / 5058 loss=0.124, ups=1.92, num_updates=34300, lr=3.2e-05, gnorm=1.049, clip=0, train_wall=48, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:26:00]    INFO >> epoch 007:   4052 / 5058 loss=0.142, ups=1.66, num_updates=34400, lr=3.2e-05, gnorm=0.318, clip=0, train_wall=56, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:27:09]    INFO >> epoch 007:   4152 / 5058 loss=0.157, ups=1.51, num_updates=34500, lr=3.2e-05, gnorm=1.071, clip=0, train_wall=61, gb_free=65.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:28:12]    INFO >> epoch 007:   4252 / 5058 loss=0.122, ups=1.65, num_updates=34600, lr=3.1e-05, gnorm=1.169, clip=0, train_wall=56, gb_free=64.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:29:11]    INFO >> epoch 007:   4352 / 5058 loss=0.071, ups=1.77, num_updates=34700, lr=3.1e-05, gnorm=0.66, clip=0, train_wall=53, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:30:19]    INFO >> epoch 007:   4452 / 5058 loss=0.105, ups=1.53, num_updates=34800, lr=3.1e-05, gnorm=0.679, clip=0, train_wall=61, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:31:20]    INFO >> epoch 007:   4552 / 5058 loss=0.059, ups=1.68, num_updates=34900, lr=3.1e-05, gnorm=0.445, clip=0, train_wall=55, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:32:33]    INFO >> epoch 007:   4652 / 5058 loss=0.23, ups=1.45, num_updates=35000, lr=3.1e-05, gnorm=2.117, clip=0, train_wall=64, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:33:39]    INFO >> epoch 007:   4752 / 5058 loss=0.077, ups=1.57, num_updates=35100, lr=3e-05, gnorm=1.404, clip=0, train_wall=59, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:34:30]    INFO >> epoch 007:   4852 / 5058 loss=0.052, ups=2.02, num_updates=35200, lr=3e-05, gnorm=0.814, clip=0, train_wall=46, gb_free=78 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:35:24]    INFO >> epoch 007:   4952 / 5058 loss=0.077, ups=1.95, num_updates=35300, lr=3e-05, gnorm=0.851, clip=0, train_wall=48, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:36:35]    INFO >> epoch 007:   5052 / 5058 loss=0.177, ups=1.48, num_updates=35400, lr=3e-05, gnorm=2.288, clip=0, train_wall=62, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:36:39]    INFO >> epoch 007 | loss 0.089 | ups 1.5 | num_updates 35406 | lr 3e-05 | gnorm 0.986 | clip 0 | train_wall 2772 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-21 05:36:39] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 05:43:28]    INFO >> epoch 007 | valid on 'valid' subset | loss 0.118 | num_updates 35406 | best_loss 0.118 (progress_bar.py:267, print())[0m
[32m[2025-11-21 05:43:29]    INFO >> saved checkpoint /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints/checkpoint7.pt (epoch 7 @ 35406 updates, score 0.118) (writing took 1.597810 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 05:43:29] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 05:44:12]    INFO >> epoch 008:     94 / 5058 loss=0.229, ups=0.23, num_updates=35500, lr=3e-05, gnorm=1.019, clip=0, train_wall=42, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:45:00]    INFO >> epoch 008:    194 / 5058 loss=0.138, ups=2.21, num_updates=35600, lr=2.9e-05, gnorm=0.517, clip=0, train_wall=42, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:46:06]    INFO >> epoch 008:    294 / 5058 loss=0.161, ups=1.57, num_updates=35700, lr=2.9e-05, gnorm=0.472, clip=0, train_wall=59, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:47:12]    INFO >> epoch 008:    394 / 5058 loss=0.208, ups=1.58, num_updates=35800, lr=2.9e-05, gnorm=1.327, clip=0, train_wall=59, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:48:16]    INFO >> epoch 008:    494 / 5058 loss=0.174, ups=1.64, num_updates=35900, lr=2.9e-05, gnorm=1.576, clip=0, train_wall=57, gb_free=63.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:49:19]    INFO >> epoch 008:    594 / 5058 loss=0.069, ups=1.66, num_updates=36000, lr=2.9e-05, gnorm=1.49, clip=0, train_wall=56, gb_free=64.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:50:02]    INFO >> epoch 008:    694 / 5058 loss=0.127, ups=2.37, num_updates=36100, lr=2.8e-05, gnorm=0.645, clip=0, train_wall=39, gb_free=64.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:50:44]    INFO >> epoch 008:    794 / 5058 loss=0.151, ups=2.48, num_updates=36200, lr=2.8e-05, gnorm=0.926, clip=0, train_wall=38, gb_free=73.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:51:52]    INFO >> epoch 008:    894 / 5058 loss=0.167, ups=1.53, num_updates=36300, lr=2.8e-05, gnorm=2.17, clip=0, train_wall=61, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:52:58]    INFO >> epoch 008:    994 / 5058 loss=0.106, ups=1.59, num_updates=36400, lr=2.8e-05, gnorm=0.453, clip=0, train_wall=58, gb_free=63.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:53:59]    INFO >> epoch 008:   1094 / 5058 loss=0.052, ups=1.7, num_updates=36500, lr=2.8e-05, gnorm=0.788, clip=0, train_wall=54, gb_free=65 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:55:05]    INFO >> epoch 008:   1194 / 5058 loss=0.032, ups=1.59, num_updates=36600, lr=2.7e-05, gnorm=0.147, clip=0, train_wall=58, gb_free=76.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:56:11]    INFO >> epoch 008:   1294 / 5058 loss=0.087, ups=1.57, num_updates=36700, lr=2.7e-05, gnorm=0.738, clip=0, train_wall=59, gb_free=63.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:57:16]    INFO >> epoch 008:   1394 / 5058 loss=0.054, ups=1.61, num_updates=36800, lr=2.7e-05, gnorm=0.77, clip=0, train_wall=57, gb_free=64 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:58:24]    INFO >> epoch 008:   1494 / 5058 loss=0.244, ups=1.52, num_updates=36900, lr=2.7e-05, gnorm=0.581, clip=0, train_wall=60, gb_free=68 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:59:30]    INFO >> epoch 008:   1594 / 5058 loss=0.075, ups=1.59, num_updates=37000, lr=2.7e-05, gnorm=1.135, clip=0, train_wall=56, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:00:41]    INFO >> epoch 008:   1694 / 5058 loss=0.117, ups=1.47, num_updates=37100, lr=2.6e-05, gnorm=0.833, clip=0, train_wall=61, gb_free=65.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:01:40]    INFO >> epoch 008:   1794 / 5058 loss=0.107, ups=1.76, num_updates=37200, lr=2.6e-05, gnorm=1.162, clip=0, train_wall=51, gb_free=72.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:02:23]    INFO >> epoch 008:   1894 / 5058 loss=0.132, ups=2.44, num_updates=37300, lr=2.6e-05, gnorm=0.648, clip=0, train_wall=38, gb_free=68.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:03:26]    INFO >> epoch 008:   1994 / 5058 loss=0.105, ups=1.65, num_updates=37400, lr=2.6e-05, gnorm=0.57, clip=0, train_wall=55, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:04:38]    INFO >> epoch 008:   2094 / 5058 loss=0.07, ups=1.45, num_updates=37500, lr=2.6e-05, gnorm=1.401, clip=0, train_wall=62, gb_free=63.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:05:48]    INFO >> epoch 008:   2194 / 5058 loss=0.262, ups=1.47, num_updates=37600, lr=2.5e-05, gnorm=2.045, clip=0, train_wall=61, gb_free=63.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:06:59]    INFO >> epoch 008:   2294 / 5058 loss=0.063, ups=1.47, num_updates=37700, lr=2.5e-05, gnorm=0.941, clip=0, train_wall=61, gb_free=65 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:08:09]    INFO >> epoch 008:   2394 / 5058 loss=0.078, ups=1.48, num_updates=37800, lr=2.5e-05, gnorm=0.323, clip=0, train_wall=61, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:09:23]    INFO >> epoch 008:   2494 / 5058 loss=0.115, ups=1.41, num_updates=37900, lr=2.5e-05, gnorm=1.368, clip=0, train_wall=63, gb_free=64 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:10:26]    INFO >> epoch 008:   2594 / 5058 loss=0.111, ups=1.66, num_updates=38000, lr=2.4e-05, gnorm=1.287, clip=0, train_wall=54, gb_free=64.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:11:30]    INFO >> epoch 008:   2694 / 5058 loss=0.088, ups=1.64, num_updates=38100, lr=2.4e-05, gnorm=0.718, clip=0, train_wall=55, gb_free=64 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:12:36]    INFO >> epoch 008:   2794 / 5058 loss=0.091, ups=1.57, num_updates=38200, lr=2.4e-05, gnorm=0.623, clip=0, train_wall=58, gb_free=63.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:13:18]    INFO >> epoch 008:   2894 / 5058 loss=0.308, ups=2.48, num_updates=38300, lr=2.4e-05, gnorm=0.771, clip=0, train_wall=37, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:14:27]    INFO >> epoch 008:   2994 / 5058 loss=0.147, ups=1.51, num_updates=38400, lr=2.4e-05, gnorm=1.46, clip=0, train_wall=60, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:15:26]    INFO >> epoch 008:   3094 / 5058 loss=0.056, ups=1.76, num_updates=38500, lr=2.3e-05, gnorm=0.145, clip=0, train_wall=52, gb_free=68 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:16:26]    INFO >> epoch 008:   3194 / 5058 loss=0.251, ups=1.74, num_updates=38600, lr=2.3e-05, gnorm=0.959, clip=0, train_wall=52, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:17:30]    INFO >> epoch 008:   3294 / 5058 loss=0.075, ups=1.64, num_updates=38700, lr=2.3e-05, gnorm=0.616, clip=0, train_wall=55, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:18:46]    INFO >> epoch 008:   3394 / 5058 loss=0.222, ups=1.37, num_updates=38800, lr=2.3e-05, gnorm=2.513, clip=0, train_wall=65, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:19:55]    INFO >> epoch 008:   3494 / 5058 loss=0.189, ups=1.5, num_updates=38900, lr=2.3e-05, gnorm=0.928, clip=0, train_wall=60, gb_free=66.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:21:05]    INFO >> epoch 008:   3594 / 5058 loss=0.075, ups=1.5, num_updates=39000, lr=2.2e-05, gnorm=1.275, clip=0, train_wall=60, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:22:10]    INFO >> epoch 008:   3694 / 5058 loss=0.134, ups=1.6, num_updates=39100, lr=2.2e-05, gnorm=1.049, clip=0, train_wall=57, gb_free=64 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:23:20]    INFO >> epoch 008:   3794 / 5058 loss=0.055, ups=1.48, num_updates=39200, lr=2.2e-05, gnorm=0.643, clip=0, train_wall=61, gb_free=65.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:24:14]    INFO >> epoch 008:   3894 / 5058 loss=0.124, ups=1.95, num_updates=39300, lr=2.2e-05, gnorm=1.009, clip=0, train_wall=47, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:25:20]    INFO >> epoch 008:   3994 / 5058 loss=0.124, ups=1.57, num_updates=39400, lr=2.2e-05, gnorm=0.867, clip=0, train_wall=57, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:26:27]    INFO >> epoch 008:   4094 / 5058 loss=0.48, ups=1.56, num_updates=39500, lr=2.1e-05, gnorm=0.513, clip=0, train_wall=58, gb_free=65.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:27:39]    INFO >> epoch 008:   4194 / 5058 loss=0.143, ups=1.45, num_updates=39600, lr=2.1e-05, gnorm=1.062, clip=0, train_wall=62, gb_free=64.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:28:43]    INFO >> epoch 008:   4294 / 5058 loss=0.106, ups=1.61, num_updates=39700, lr=2.1e-05, gnorm=1.135, clip=0, train_wall=56, gb_free=64.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:29:46]    INFO >> epoch 008:   4394 / 5058 loss=0.074, ups=1.67, num_updates=39800, lr=2.1e-05, gnorm=0.831, clip=0, train_wall=55, gb_free=72.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:30:53]    INFO >> epoch 008:   4494 / 5058 loss=0.07, ups=1.56, num_updates=39900, lr=2.1e-05, gnorm=0.874, clip=0, train_wall=59, gb_free=67.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:32:00]    INFO >> epoch 008:   4594 / 5058 loss=0.188, ups=1.55, num_updates=40000, lr=2e-05, gnorm=0.816, clip=0, train_wall=60, gb_free=63.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:33:12]    INFO >> epoch 008:   4694 / 5058 loss=0.233, ups=1.44, num_updates=40100, lr=2e-05, gnorm=2.36, clip=0, train_wall=64, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:34:17]    INFO >> epoch 008:   4794 / 5058 loss=0.053, ups=1.6, num_updates=40200, lr=2e-05, gnorm=0.861, clip=0, train_wall=57, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:35:00]    INFO >> epoch 008:   4894 / 5058 loss=0.057, ups=2.38, num_updates=40300, lr=2e-05, gnorm=0.434, clip=0, train_wall=39, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:36:09]    INFO >> epoch 008:   4994 / 5058 loss=0.083, ups=1.5, num_updates=40400, lr=2e-05, gnorm=1.362, clip=0, train_wall=61, gb_free=64.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:36:58]    INFO >> epoch 008 | loss 0.088 | ups 1.46 | num_updates 40464 | lr 1.9e-05 | gnorm 1.001 | clip 0 | train_wall 2808 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-21 06:36:58] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 06:43:57]    INFO >> epoch 008 | valid on 'valid' subset | loss 0.114 | num_updates 40464 | best_loss 0.114 (progress_bar.py:267, print())[0m
[32m[2025-11-21 06:43:59]    INFO >> saved checkpoint /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints/checkpoint8.pt (epoch 8 @ 40464 updates, score 0.114) (writing took 1.993949 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 06:44:00] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 06:44:22]    INFO >> epoch 009:     36 / 5058 loss=0.221, ups=0.21, num_updates=40500, lr=1.9e-05, gnorm=2.114, clip=0, train_wall=61, gb_free=68.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:45:05]    INFO >> epoch 009:    136 / 5058 loss=0.13, ups=2.4, num_updates=40600, lr=1.9e-05, gnorm=0.458, clip=0, train_wall=38, gb_free=64.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:46:04]    INFO >> epoch 009:    236 / 5058 loss=0.233, ups=1.79, num_updates=40700, lr=1.9e-05, gnorm=0.454, clip=0, train_wall=51, gb_free=67.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:47:14]    INFO >> epoch 009:    336 / 5058 loss=0.132, ups=1.49, num_updates=40800, lr=1.9e-05, gnorm=0.654, clip=0, train_wall=61, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:48:27]    INFO >> epoch 009:    436 / 5058 loss=0.228, ups=1.41, num_updates=40900, lr=1.9e-05, gnorm=2.009, clip=0, train_wall=64, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:49:32]    INFO >> epoch 009:    536 / 5058 loss=0.087, ups=1.61, num_updates=41000, lr=1.8e-05, gnorm=1.018, clip=0, train_wall=57, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:50:32]    INFO >> epoch 009:    636 / 5058 loss=0.078, ups=1.74, num_updates=41100, lr=1.8e-05, gnorm=1.121, clip=0, train_wall=52, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:51:14]    INFO >> epoch 009:    736 / 5058 loss=0.214, ups=2.45, num_updates=41200, lr=1.8e-05, gnorm=1.1, clip=0, train_wall=37, gb_free=74.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:52:11]    INFO >> epoch 009:    836 / 5058 loss=0.137, ups=1.85, num_updates=41300, lr=1.8e-05, gnorm=0.891, clip=0, train_wall=49, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:53:23]    INFO >> epoch 009:    936 / 5058 loss=0.148, ups=1.44, num_updates=41400, lr=1.8e-05, gnorm=1.709, clip=0, train_wall=63, gb_free=65.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:54:27]    INFO >> epoch 009:   1036 / 5058 loss=0.103, ups=1.64, num_updates=41500, lr=1.7e-05, gnorm=0.697, clip=0, train_wall=56, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:55:32]    INFO >> epoch 009:   1136 / 5058 loss=0.038, ups=1.6, num_updates=41600, lr=1.7e-05, gnorm=0.328, clip=0, train_wall=57, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:56:45]    INFO >> epoch 009:   1236 / 5058 loss=0.041, ups=1.43, num_updates=41700, lr=1.7e-05, gnorm=0.357, clip=0, train_wall=63, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:57:54]    INFO >> epoch 009:   1336 / 5058 loss=0.058, ups=1.49, num_updates=41800, lr=1.7e-05, gnorm=0.578, clip=0, train_wall=60, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:59:02]    INFO >> epoch 009:   1436 / 5058 loss=0.095, ups=1.53, num_updates=41900, lr=1.7e-05, gnorm=0.907, clip=0, train_wall=59, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:00:10]    INFO >> epoch 009:   1536 / 5058 loss=0.074, ups=1.54, num_updates=42000, lr=1.6e-05, gnorm=0.975, clip=0, train_wall=59, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:01:18]    INFO >> epoch 009:   1636 / 5058 loss=0.125, ups=1.54, num_updates=42100, lr=1.6e-05, gnorm=1.007, clip=0, train_wall=59, gb_free=63.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:02:27]    INFO >> epoch 009:   1736 / 5058 loss=0.126, ups=1.51, num_updates=42200, lr=1.6e-05, gnorm=1.048, clip=0, train_wall=60, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:03:16]    INFO >> epoch 009:   1836 / 5058 loss=0.074, ups=2.16, num_updates=42300, lr=1.6e-05, gnorm=0.522, clip=0, train_wall=42, gb_free=75.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:04:09]    INFO >> epoch 009:   1936 / 5058 loss=0.114, ups=1.92, num_updates=42400, lr=1.6e-05, gnorm=1.112, clip=0, train_wall=48, gb_free=65.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:05:16]    INFO >> epoch 009:   2036 / 5058 loss=0.08, ups=1.56, num_updates=42500, lr=1.5e-05, gnorm=0.334, clip=0, train_wall=58, gb_free=66.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:06:30]    INFO >> epoch 009:   2136 / 5058 loss=0.088, ups=1.41, num_updates=42600, lr=1.5e-05, gnorm=1.963, clip=0, train_wall=64, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:07:41]    INFO >> epoch 009:   2236 / 5058 loss=0.085, ups=1.46, num_updates=42700, lr=1.5e-05, gnorm=1.268, clip=0, train_wall=62, gb_free=68.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:08:53]    INFO >> epoch 009:   2336 / 5058 loss=0.072, ups=1.48, num_updates=42800, lr=1.5e-05, gnorm=0.703, clip=0, train_wall=61, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:10:03]    INFO >> epoch 009:   2436 / 5058 loss=0.078, ups=1.48, num_updates=42900, lr=1.4e-05, gnorm=0.638, clip=0, train_wall=61, gb_free=64.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:11:17]    INFO >> epoch 009:   2536 / 5058 loss=0.199, ups=1.4, num_updates=43000, lr=1.4e-05, gnorm=1.681, clip=0, train_wall=64, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:12:16]    INFO >> epoch 009:   2636 / 5058 loss=0.074, ups=1.77, num_updates=43100, lr=1.4e-05, gnorm=0.978, clip=0, train_wall=52, gb_free=69 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:13:26]    INFO >> epoch 009:   2736 / 5058 loss=0.102, ups=1.5, num_updates=43200, lr=1.4e-05, gnorm=0.482, clip=0, train_wall=59, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:14:17]    INFO >> epoch 009:   2836 / 5058 loss=0.089, ups=1.99, num_updates=43300, lr=1.4e-05, gnorm=0.455, clip=0, train_wall=45, gb_free=63.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:15:15]    INFO >> epoch 009:   2936 / 5058 loss=0.188, ups=1.81, num_updates=43400, lr=1.3e-05, gnorm=1.402, clip=0, train_wall=50, gb_free=64.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:16:26]    INFO >> epoch 009:   3036 / 5058 loss=0.089, ups=1.47, num_updates=43500, lr=1.3e-05, gnorm=0.876, clip=0, train_wall=61, gb_free=69 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:17:28]    INFO >> epoch 009:   3136 / 5058 loss=0.063, ups=1.69, num_updates=43600, lr=1.3e-05, gnorm=0.476, clip=0, train_wall=53, gb_free=64.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:18:25]    INFO >> epoch 009:   3236 / 5058 loss=0.329, ups=1.84, num_updates=43700, lr=1.3e-05, gnorm=0.558, clip=0, train_wall=49, gb_free=67.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:19:34]    INFO >> epoch 009:   3336 / 5058 loss=0.101, ups=1.51, num_updates=43800, lr=1.3e-05, gnorm=1.583, clip=0, train_wall=60, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:20:47]    INFO >> epoch 009:   3436 / 5058 loss=0.222, ups=1.42, num_updates=43900, lr=1.2e-05, gnorm=2.405, clip=0, train_wall=64, gb_free=63.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:21:55]    INFO >> epoch 009:   3536 / 5058 loss=0.088, ups=1.55, num_updates=44000, lr=1.2e-05, gnorm=0.539, clip=0, train_wall=59, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:23:01]    INFO >> epoch 009:   3636 / 5058 loss=0.07, ups=1.56, num_updates=44100, lr=1.2e-05, gnorm=0.892, clip=0, train_wall=59, gb_free=69 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:24:05]    INFO >> epoch 009:   3736 / 5058 loss=0.071, ups=1.64, num_updates=44200, lr=1.2e-05, gnorm=1.376, clip=0, train_wall=56, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:25:11]    INFO >> epoch 009:   3836 / 5058 loss=0.068, ups=1.59, num_updates=44300, lr=1.2e-05, gnorm=0.431, clip=0, train_wall=58, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:25:59]    INFO >> epoch 009:   3936 / 5058 loss=0.126, ups=2.12, num_updates=44400, lr=1.1e-05, gnorm=1.051, clip=0, train_wall=44, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:27:05]    INFO >> epoch 009:   4036 / 5058 loss=0.152, ups=1.59, num_updates=44500, lr=1.1e-05, gnorm=0.642, clip=0, train_wall=58, gb_free=68.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:28:13]    INFO >> epoch 009:   4136 / 5058 loss=0.124, ups=1.54, num_updates=44600, lr=1.1e-05, gnorm=0.765, clip=0, train_wall=60, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:29:18]    INFO >> epoch 009:   4236 / 5058 loss=0.15, ups=1.61, num_updates=44700, lr=1.1e-05, gnorm=1.713, clip=0, train_wall=57, gb_free=67.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:30:16]    INFO >> epoch 009:   4336 / 5058 loss=0.113, ups=1.81, num_updates=44800, lr=1.1e-05, gnorm=0.609, clip=0, train_wall=51, gb_free=65.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:31:25]    INFO >> epoch 009:   4436 / 5058 loss=0.071, ups=1.51, num_updates=44900, lr=1e-05, gnorm=0.921, clip=0, train_wall=61, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:32:28]    INFO >> epoch 009:   4536 / 5058 loss=0.064, ups=1.64, num_updates=45000, lr=1e-05, gnorm=0.556, clip=0, train_wall=56, gb_free=66.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:33:40]    INFO >> epoch 009:   4636 / 5058 loss=0.199, ups=1.45, num_updates=45100, lr=1e-05, gnorm=1.756, clip=0, train_wall=63, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:34:50]    INFO >> epoch 009:   4736 / 5058 loss=0.095, ups=1.48, num_updates=45200, lr=1e-05, gnorm=1.796, clip=0, train_wall=62, gb_free=67.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:35:45]    INFO >> epoch 009:   4836 / 5058 loss=0.049, ups=1.91, num_updates=45300, lr=1e-05, gnorm=0.77, clip=0, train_wall=48, gb_free=77.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:36:34]    INFO >> epoch 009:   4936 / 5058 loss=0.07, ups=2.1, num_updates=45400, lr=9e-06, gnorm=0.51, clip=0, train_wall=44, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:37:43]    INFO >> epoch 009:   5036 / 5058 loss=0.179, ups=1.49, num_updates=45500, lr=9e-06, gnorm=2.194, clip=0, train_wall=61, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:37:59]    INFO >> epoch 009 | loss 0.088 | ups 1.44 | num_updates 45522 | lr 9e-06 | gnorm 0.993 | clip 0 | train_wall 2828 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-21 07:37:59] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 07:44:52]    INFO >> epoch 009 | valid on 'valid' subset | loss 0.113 | num_updates 45522 | best_loss 0.113 (progress_bar.py:267, print())[0m
[32m[2025-11-21 07:44:54]    INFO >> saved checkpoint /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints/checkpoint9.pt (epoch 9 @ 45522 updates, score 0.113) (writing took 1.623332 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 07:44:54] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 07:45:32]    INFO >> epoch 010:     78 / 5058 loss=0.219, ups=0.22, num_updates=45600, lr=9e-06, gnorm=1.193, clip=0, train_wall=47, gb_free=65.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:46:13]    INFO >> epoch 010:    178 / 5058 loss=0.128, ups=2.47, num_updates=45700, lr=9e-06, gnorm=0.434, clip=0, train_wall=38, gb_free=67 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:47:20]    INFO >> epoch 010:    278 / 5058 loss=0.166, ups=1.58, num_updates=45800, lr=9e-06, gnorm=0.547, clip=0, train_wall=59, gb_free=67 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:48:25]    INFO >> epoch 010:    378 / 5058 loss=0.208, ups=1.59, num_updates=45900, lr=8e-06, gnorm=0.832, clip=0, train_wall=58, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:49:31]    INFO >> epoch 010:    478 / 5058 loss=0.224, ups=1.59, num_updates=46000, lr=8e-06, gnorm=1.982, clip=0, train_wall=58, gb_free=64.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:50:34]    INFO >> epoch 010:    578 / 5058 loss=0.064, ups=1.64, num_updates=46100, lr=8e-06, gnorm=1.399, clip=0, train_wall=56, gb_free=64.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:51:20]    INFO >> epoch 010:    678 / 5058 loss=0.141, ups=2.26, num_updates=46200, lr=8e-06, gnorm=0.528, clip=0, train_wall=41, gb_free=77.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:52:02]    INFO >> epoch 010:    778 / 5058 loss=0.139, ups=2.54, num_updates=46300, lr=8e-06, gnorm=0.934, clip=0, train_wall=37, gb_free=77.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:53:07]    INFO >> epoch 010:    878 / 5058 loss=0.16, ups=1.59, num_updates=46400, lr=7e-06, gnorm=1.922, clip=0, train_wall=58, gb_free=68.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:54:13]    INFO >> epoch 010:    978 / 5058 loss=0.111, ups=1.58, num_updates=46500, lr=7e-06, gnorm=0.63, clip=0, train_wall=58, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:55:15]    INFO >> epoch 010:   1078 / 5058 loss=0.054, ups=1.65, num_updates=46600, lr=7e-06, gnorm=0.68, clip=0, train_wall=56, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:56:20]    INFO >> epoch 010:   1178 / 5058 loss=0.034, ups=1.61, num_updates=46700, lr=7e-06, gnorm=0.208, clip=0, train_wall=57, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:57:25]    INFO >> epoch 010:   1278 / 5058 loss=0.081, ups=1.6, num_updates=46800, lr=7e-06, gnorm=0.748, clip=0, train_wall=58, gb_free=64.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:58:29]    INFO >> epoch 010:   1378 / 5058 loss=0.054, ups=1.63, num_updates=46900, lr=6e-06, gnorm=0.823, clip=0, train_wall=57, gb_free=65.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:59:36]    INFO >> epoch 010:   1478 / 5058 loss=0.242, ups=1.55, num_updates=47000, lr=6e-06, gnorm=0.607, clip=0, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:00:38]    INFO >> epoch 010:   1578 / 5058 loss=0.077, ups=1.69, num_updates=47100, lr=6e-06, gnorm=1.004, clip=0, train_wall=55, gb_free=64 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:01:46]    INFO >> epoch 010:   1678 / 5058 loss=0.143, ups=1.54, num_updates=47200, lr=6e-06, gnorm=1, clip=0, train_wall=60, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:02:49]    INFO >> epoch 010:   1778 / 5058 loss=0.084, ups=1.66, num_updates=47300, lr=6e-06, gnorm=1.118, clip=0, train_wall=56, gb_free=63.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:03:26]    INFO >> epoch 010:   1878 / 5058 loss=0.282, ups=2.76, num_updates=47400, lr=5e-06, gnorm=0.351, clip=0, train_wall=34, gb_free=69.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:04:23]    INFO >> epoch 010:   1978 / 5058 loss=0.105, ups=1.84, num_updates=47500, lr=5e-06, gnorm=0.884, clip=0, train_wall=51, gb_free=68.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:05:31]    INFO >> epoch 010:   2078 / 5058 loss=0.063, ups=1.54, num_updates=47600, lr=5e-06, gnorm=0.689, clip=0, train_wall=60, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:06:39]    INFO >> epoch 010:   2178 / 5058 loss=0.256, ups=1.53, num_updates=47700, lr=5e-06, gnorm=2.459, clip=0, train_wall=61, gb_free=64.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:07:47]    INFO >> epoch 010:   2278 / 5058 loss=0.068, ups=1.53, num_updates=47800, lr=4e-06, gnorm=0.977, clip=0, train_wall=61, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:08:54]    INFO >> epoch 010:   2378 / 5058 loss=0.064, ups=1.56, num_updates=47900, lr=4e-06, gnorm=0.356, clip=0, train_wall=60, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:10:04]    INFO >> epoch 010:   2478 / 5058 loss=0.114, ups=1.5, num_updates=48000, lr=4e-06, gnorm=1.21, clip=0, train_wall=61, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:11:08]    INFO >> epoch 010:   2578 / 5058 loss=0.108, ups=1.63, num_updates=48100, lr=4e-06, gnorm=1.312, clip=0, train_wall=57, gb_free=71.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:12:04]    INFO >> epoch 010:   2678 / 5058 loss=0.082, ups=1.82, num_updates=48200, lr=4e-06, gnorm=0.579, clip=0, train_wall=51, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:13:09]    INFO >> epoch 010:   2778 / 5058 loss=0.107, ups=1.59, num_updates=48300, lr=3e-06, gnorm=0.731, clip=0, train_wall=58, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:13:51]    INFO >> epoch 010:   2878 / 5058 loss=0.312, ups=2.56, num_updates=48400, lr=3e-06, gnorm=0.498, clip=0, train_wall=37, gb_free=67.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:14:57]    INFO >> epoch 010:   2978 / 5058 loss=0.163, ups=1.58, num_updates=48500, lr=3e-06, gnorm=1.528, clip=0, train_wall=59, gb_free=65.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:15:55]    INFO >> epoch 010:   3078 / 5058 loss=0.06, ups=1.77, num_updates=48600, lr=3e-06, gnorm=0.273, clip=0, train_wall=53, gb_free=68.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:16:56]    INFO >> epoch 010:   3178 / 5058 loss=0.102, ups=1.72, num_updates=48700, lr=3e-06, gnorm=0.814, clip=0, train_wall=54, gb_free=73.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:17:51]    INFO >> epoch 010:   3278 / 5058 loss=0.079, ups=1.91, num_updates=48800, lr=2e-06, gnorm=0.415, clip=0, train_wall=49, gb_free=69 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:19:02]    INFO >> epoch 010:   3378 / 5058 loss=0.184, ups=1.46, num_updates=48900, lr=2e-06, gnorm=2.349, clip=0, train_wall=63, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:20:11]    INFO >> epoch 010:   3478 / 5058 loss=0.209, ups=1.53, num_updates=49000, lr=2e-06, gnorm=1.463, clip=0, train_wall=61, gb_free=70.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:21:17]    INFO >> epoch 010:   3578 / 5058 loss=0.091, ups=1.56, num_updates=49100, lr=2e-06, gnorm=1.079, clip=0, train_wall=59, gb_free=65.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:22:20]    INFO >> epoch 010:   3678 / 5058 loss=0.073, ups=1.66, num_updates=49200, lr=2e-06, gnorm=1, clip=0, train_wall=56, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:23:28]    INFO >> epoch 010:   3778 / 5058 loss=0.055, ups=1.52, num_updates=49300, lr=1e-06, gnorm=0.676, clip=0, train_wall=61, gb_free=64.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:24:20]    INFO >> epoch 010:   3878 / 5058 loss=0.099, ups=1.97, num_updates=49400, lr=1e-06, gnorm=0.698, clip=0, train_wall=47, gb_free=76.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:25:21]    INFO >> epoch 010:   3978 / 5058 loss=0.126, ups=1.73, num_updates=49500, lr=1e-06, gnorm=1.109, clip=0, train_wall=54, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:26:26]    INFO >> epoch 010:   4078 / 5058 loss=0.255, ups=1.62, num_updates=49600, lr=1e-06, gnorm=0.436, clip=0, train_wall=57, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:27:34]    INFO >> epoch 010:   4178 / 5058 loss=0.144, ups=1.52, num_updates=49700, lr=1e-06, gnorm=1.236, clip=0, train_wall=61, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:28:36]    INFO >> epoch 010:   4278 / 5058 loss=0.104, ups=1.69, num_updates=49800, lr=0, gnorm=1.094, clip=0, train_wall=55, gb_free=64.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:29:38]    INFO >> epoch 010:   4378 / 5058 loss=0.084, ups=1.7, num_updates=49900, lr=0, gnorm=1.066, clip=0, train_wall=55, gb_free=66.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:30:43]    INFO >> epoch 010:   4478 / 5058 loss=0.07, ups=1.6, num_updates=50000, lr=0, gnorm=0.864, clip=0, train_wall=58, gb_free=68.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:31:48]    INFO >> epoch 010:   4578 / 5058 loss=0.192, ups=1.59, num_updates=50100, lr=0, gnorm=0.701, clip=0, train_wall=58, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:33:00]    INFO >> epoch 010:   4678 / 5058 loss=0.216, ups=1.45, num_updates=50200, lr=0, gnorm=2.371, clip=0, train_wall=64, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:34:03]    INFO >> epoch 010:   4778 / 5058 loss=0.079, ups=1.65, num_updates=50300, lr=0, gnorm=1.198, clip=0, train_wall=56, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:34:50]    INFO >> epoch 010:   4878 / 5058 loss=0.035, ups=2.2, num_updates=50400, lr=0, gnorm=0.455, clip=0, train_wall=42, gb_free=64.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:35:51]    INFO >> epoch 010:   4978 / 5058 loss=0.071, ups=1.73, num_updates=50500, lr=0, gnorm=1.028, clip=0, train_wall=54, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:36:46]    INFO >> epoch 010 | loss 0.088 | ups 1.49 | num_updates 50580 | lr 0 | gnorm 0.991 | clip 0 | train_wall 2772 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-21 08:36:46] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 08:43:37]    INFO >> epoch 010 | valid on 'valid' subset | loss 0.112 | num_updates 50580 | best_loss 0.112 (progress_bar.py:267, print())[0m
[32m[2025-11-21 08:43:39]    INFO >> saved checkpoint /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints/checkpoint10.pt (epoch 10 @ 50580 updates, score 0.112) (writing took 1.630356 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 08:43:39] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 08:43:52]    INFO >> epoch 011:     20 / 5058 loss=0.224, ups=0.22, num_updates=50600, lr=0, gnorm=2.387, clip=0, train_wall=62, gb_free=72 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:44:32]    INFO >> epoch 011:    120 / 5058 loss=0.411, ups=2.59, num_updates=50700, lr=0, gnorm=0.564, clip=0, train_wall=36, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:45:27]    INFO >> epoch 011:    220 / 5058 loss=0.132, ups=1.91, num_updates=50800, lr=0, gnorm=0.496, clip=0, train_wall=49, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:46:32]    INFO >> epoch 011:    320 / 5058 loss=0.119, ups=1.61, num_updates=50900, lr=0, gnorm=0.503, clip=0, train_wall=58, gb_free=64.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:47:40]    INFO >> epoch 011:    420 / 5058 loss=0.223, ups=1.54, num_updates=51000, lr=0, gnorm=1.697, clip=0, train_wall=60, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:48:42]    INFO >> epoch 011:    520 / 5058 loss=0.143, ups=1.68, num_updates=51100, lr=0, gnorm=1.491, clip=0, train_wall=55, gb_free=63.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:49:42]    INFO >> epoch 011:    620 / 5058 loss=0.065, ups=1.73, num_updates=51200, lr=0, gnorm=1.08, clip=0, train_wall=54, gb_free=69.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:50:23]    INFO >> epoch 011:    720 / 5058 loss=0.174, ups=2.53, num_updates=51300, lr=0, gnorm=1.041, clip=0, train_wall=37, gb_free=70.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:51:11]    INFO >> epoch 011:    820 / 5058 loss=0.108, ups=2.16, num_updates=51400, lr=0, gnorm=0.382, clip=0, train_wall=43, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:52:19]    INFO >> epoch 011:    920 / 5058 loss=0.158, ups=1.52, num_updates=51500, lr=0, gnorm=2.191, clip=0, train_wall=61, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:53:26]    INFO >> epoch 011:   1020 / 5058 loss=0.087, ups=1.57, num_updates=51600, lr=0, gnorm=0.195, clip=0, train_wall=59, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:54:24]    INFO >> epoch 011:   1120 / 5058 loss=0.043, ups=1.8, num_updates=51700, lr=0, gnorm=0.785, clip=0, train_wall=52, gb_free=65 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:55:31]    INFO >> epoch 011:   1220 / 5058 loss=0.035, ups=1.56, num_updates=51800, lr=0, gnorm=0.273, clip=0, train_wall=59, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:56:37]    INFO >> epoch 011:   1320 / 5058 loss=0.079, ups=1.58, num_updates=51900, lr=0, gnorm=0.644, clip=0, train_wall=59, gb_free=69.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:57:41]    INFO >> epoch 011:   1420 / 5058 loss=0.067, ups=1.62, num_updates=52000, lr=0, gnorm=0.977, clip=0, train_wall=57, gb_free=68.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:58:48]    INFO >> epoch 011:   1520 / 5058 loss=0.098, ups=1.56, num_updates=52100, lr=0, gnorm=0.976, clip=0, train_wall=59, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 08:59:50]    INFO >> epoch 011:   1620 / 5058 loss=0.1, ups=1.69, num_updates=52200, lr=0, gnorm=0.975, clip=0, train_wall=55, gb_free=64.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:00:55]    INFO >> epoch 011:   1720 / 5058 loss=0.126, ups=1.59, num_updates=52300, lr=0, gnorm=0.981, clip=0, train_wall=59, gb_free=65.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:01:45]    INFO >> epoch 011:   1820 / 5058 loss=0.081, ups=2.08, num_updates=52400, lr=0, gnorm=0.634, clip=0, train_wall=45, gb_free=69.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:02:30]    INFO >> epoch 011:   1920 / 5058 loss=0.117, ups=2.27, num_updates=52500, lr=0, gnorm=1.214, clip=0, train_wall=41, gb_free=65.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:03:33]    INFO >> epoch 011:   2020 / 5058 loss=0.094, ups=1.67, num_updates=52600, lr=0, gnorm=0.264, clip=0, train_wall=56, gb_free=65 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:04:42]    INFO >> epoch 011:   2120 / 5058 loss=0.077, ups=1.49, num_updates=52700, lr=0, gnorm=1.52, clip=0, train_wall=62, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:05:49]    INFO >> epoch 011:   2220 / 5058 loss=0.139, ups=1.56, num_updates=52800, lr=0, gnorm=1.645, clip=0, train_wall=60, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:06:57]    INFO >> epoch 011:   2320 / 5058 loss=0.062, ups=1.54, num_updates=52900, lr=0, gnorm=0.736, clip=0, train_wall=60, gb_free=63.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:08:03]    INFO >> epoch 011:   2420 / 5058 loss=0.074, ups=1.57, num_updates=53000, lr=0, gnorm=0.42, clip=0, train_wall=59, gb_free=65.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:09:14]    INFO >> epoch 011:   2520 / 5058 loss=0.168, ups=1.47, num_updates=53100, lr=0, gnorm=1.822, clip=0, train_wall=63, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:10:12]    INFO >> epoch 011:   2620 / 5058 loss=0.088, ups=1.8, num_updates=53200, lr=0, gnorm=0.957, clip=0, train_wall=52, gb_free=70 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:11:14]    INFO >> epoch 011:   2720 / 5058 loss=0.086, ups=1.67, num_updates=53300, lr=0, gnorm=0.478, clip=0, train_wall=55, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:12:11]    INFO >> epoch 011:   2820 / 5058 loss=0.095, ups=1.84, num_updates=53400, lr=0, gnorm=0.446, clip=0, train_wall=51, gb_free=75.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:12:57]    INFO >> epoch 011:   2920 / 5058 loss=0.203, ups=2.27, num_updates=53500, lr=0, gnorm=1.19, clip=0, train_wall=41, gb_free=63.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:14:05]    INFO >> epoch 011:   3020 / 5058 loss=0.131, ups=1.53, num_updates=53600, lr=0, gnorm=1.034, clip=0, train_wall=61, gb_free=66.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:15:01]    INFO >> epoch 011:   3120 / 5058 loss=0.061, ups=1.88, num_updates=53700, lr=0, gnorm=0.426, clip=0, train_wall=50, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:15:57]    INFO >> epoch 011:   3220 / 5058 loss=0.347, ups=1.84, num_updates=53800, lr=0, gnorm=0.622, clip=0, train_wall=51, gb_free=68.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:17:00]    INFO >> epoch 011:   3320 / 5058 loss=0.091, ups=1.64, num_updates=53900, lr=0, gnorm=1.117, clip=0, train_wall=57, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:18:11]    INFO >> epoch 011:   3420 / 5058 loss=0.22, ups=1.46, num_updates=54000, lr=0, gnorm=2.388, clip=0, train_wall=63, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:19:18]    INFO >> epoch 011:   3520 / 5058 loss=0.133, ups=1.56, num_updates=54100, lr=0, gnorm=0.763, clip=0, train_wall=60, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:20:25]    INFO >> epoch 011:   3620 / 5058 loss=0.067, ups=1.56, num_updates=54200, lr=0, gnorm=1.023, clip=0, train_wall=60, gb_free=64.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:21:27]    INFO >> epoch 011:   3720 / 5058 loss=0.121, ups=1.69, num_updates=54300, lr=0, gnorm=1.126, clip=0, train_wall=55, gb_free=64.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:22:35]    INFO >> epoch 011:   3820 / 5058 loss=0.053, ups=1.54, num_updates=54400, lr=0, gnorm=0.609, clip=0, train_wall=60, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:23:22]    INFO >> epoch 011:   3920 / 5058 loss=0.115, ups=2.24, num_updates=54500, lr=0, gnorm=0.973, clip=0, train_wall=42, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:24:28]    INFO >> epoch 011:   4020 / 5058 loss=0.161, ups=1.58, num_updates=54600, lr=0, gnorm=0.71, clip=0, train_wall=59, gb_free=65.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:25:34]    INFO >> epoch 011:   4120 / 5058 loss=0.256, ups=1.58, num_updates=54700, lr=0, gnorm=0.554, clip=0, train_wall=59, gb_free=64.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:26:39]    INFO >> epoch 011:   4220 / 5058 loss=0.146, ups=1.6, num_updates=54800, lr=0, gnorm=1.824, clip=0, train_wall=58, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:27:38]    INFO >> epoch 011:   4320 / 5058 loss=0.102, ups=1.75, num_updates=54900, lr=0, gnorm=0.641, clip=0, train_wall=53, gb_free=68.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:28:43]    INFO >> epoch 011:   4420 / 5058 loss=0.073, ups=1.6, num_updates=55000, lr=0, gnorm=0.994, clip=0, train_wall=58, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:29:47]    INFO >> epoch 011:   4520 / 5058 loss=0.067, ups=1.64, num_updates=55100, lr=0, gnorm=0.712, clip=0, train_wall=57, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:30:56]    INFO >> epoch 011:   4620 / 5058 loss=0.196, ups=1.5, num_updates=55200, lr=0, gnorm=1.506, clip=0, train_wall=62, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:32:07]    INFO >> epoch 011:   4720 / 5058 loss=0.146, ups=1.47, num_updates=55300, lr=0, gnorm=2.094, clip=0, train_wall=63, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:33:05]    INFO >> epoch 011:   4820 / 5058 loss=0.047, ups=1.81, num_updates=55400, lr=0, gnorm=0.63, clip=0, train_wall=51, gb_free=76.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:33:50]    INFO >> epoch 011:   4920 / 5058 loss=0.093, ups=2.27, num_updates=55500, lr=0, gnorm=0.654, clip=0, train_wall=41, gb_free=69.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:34:58]    INFO >> epoch 011:   5020 / 5058 loss=0.108, ups=1.55, num_updates=55600, lr=0, gnorm=1.792, clip=0, train_wall=60, gb_free=64.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:35:25]    INFO >> epoch 011 | loss 0.087 | ups 1.5 | num_updates 55638 | lr 0 | gnorm 0.99 | clip 0 | train_wall 2772 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-21 09:35:25] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 09:42:13]    INFO >> epoch 011 | valid on 'valid' subset | loss 0.112 | num_updates 55638 | best_loss 0.112 (progress_bar.py:267, print())[0m
[32m[2025-11-21 09:42:15]    INFO >> saved checkpoint /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints/checkpoint11.pt (epoch 11 @ 55638 updates, score 0.112) (writing took 1.775843 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 09:42:15] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 09:42:44]    INFO >> epoch 012:     62 / 5058 loss=0.214, ups=0.22, num_updates=55700, lr=0, gnorm=1.534, clip=0, train_wall=49, gb_free=65.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:43:30]    INFO >> epoch 012:    162 / 5058 loss=0.129, ups=2.3, num_updates=55800, lr=0, gnorm=0.609, clip=0, train_wall=41, gb_free=78.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:44:31]    INFO >> epoch 012:    262 / 5058 loss=0.125, ups=1.69, num_updates=55900, lr=0, gnorm=0.267, clip=0, train_wall=55, gb_free=64 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:45:35]    INFO >> epoch 012:    362 / 5058 loss=0.222, ups=1.61, num_updates=56000, lr=0, gnorm=0.789, clip=0, train_wall=58, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:46:42]    INFO >> epoch 012:    462 / 5058 loss=0.229, ups=1.56, num_updates=56100, lr=0, gnorm=2.273, clip=0, train_wall=59, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:47:46]    INFO >> epoch 012:    562 / 5058 loss=0.068, ups=1.64, num_updates=56200, lr=0, gnorm=1.121, clip=0, train_wall=57, gb_free=70 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:48:39]    INFO >> epoch 012:    662 / 5058 loss=0.08, ups=1.96, num_updates=56300, lr=0, gnorm=0.759, clip=0, train_wall=47, gb_free=78.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:49:18]    INFO >> epoch 012:    762 / 5058 loss=0.144, ups=2.67, num_updates=56400, lr=0, gnorm=0.993, clip=0, train_wall=35, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:50:18]    INFO >> epoch 012:    862 / 5058 loss=0.153, ups=1.76, num_updates=56500, lr=0, gnorm=1.554, clip=0, train_wall=52, gb_free=64.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:51:24]    INFO >> epoch 012:    962 / 5058 loss=0.116, ups=1.56, num_updates=56600, lr=0, gnorm=0.949, clip=0, train_wall=59, gb_free=75.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:52:28]    INFO >> epoch 012:   1062 / 5058 loss=0.094, ups=1.64, num_updates=56700, lr=0, gnorm=0.737, clip=0, train_wall=57, gb_free=64 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:53:30]    INFO >> epoch 012:   1162 / 5058 loss=0.033, ups=1.66, num_updates=56800, lr=0, gnorm=0.211, clip=0, train_wall=56, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:54:36]    INFO >> epoch 012:   1262 / 5058 loss=0.064, ups=1.58, num_updates=56900, lr=0, gnorm=0.655, clip=0, train_wall=59, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:55:39]    INFO >> epoch 012:   1362 / 5058 loss=0.059, ups=1.66, num_updates=57000, lr=0, gnorm=0.881, clip=0, train_wall=56, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:56:47]    INFO >> epoch 012:   1462 / 5058 loss=0.147, ups=1.52, num_updates=57100, lr=0, gnorm=0.669, clip=0, train_wall=61, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:57:50]    INFO >> epoch 012:   1562 / 5058 loss=0.076, ups=1.66, num_updates=57200, lr=0, gnorm=0.897, clip=0, train_wall=56, gb_free=69.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 09:58:56]    INFO >> epoch 012:   1662 / 5058 loss=0.144, ups=1.59, num_updates=57300, lr=0, gnorm=1.096, clip=0, train_wall=58, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:00:00]    INFO >> epoch 012:   1762 / 5058 loss=0.112, ups=1.62, num_updates=57400, lr=0, gnorm=1.155, clip=0, train_wall=57, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:00:40]    INFO >> epoch 012:   1862 / 5058 loss=0.072, ups=2.56, num_updates=57500, lr=0, gnorm=0.34, clip=0, train_wall=37, gb_free=66.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:01:35]    INFO >> epoch 012:   1962 / 5058 loss=0.107, ups=1.94, num_updates=57600, lr=0, gnorm=0.924, clip=0, train_wall=48, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:02:40]    INFO >> epoch 012:   2062 / 5058 loss=0.069, ups=1.59, num_updates=57700, lr=0, gnorm=0.516, clip=0, train_wall=58, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:03:51]    INFO >> epoch 012:   2162 / 5058 loss=0.132, ups=1.47, num_updates=57800, lr=0, gnorm=2.279, clip=0, train_wall=63, gb_free=68.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:04:57]    INFO >> epoch 012:   2262 / 5058 loss=0.07, ups=1.57, num_updates=57900, lr=0, gnorm=0.915, clip=0, train_wall=59, gb_free=63.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:06:04]    INFO >> epoch 012:   2362 / 5058 loss=0.07, ups=1.56, num_updates=58000, lr=0, gnorm=0.767, clip=0, train_wall=59, gb_free=63.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:07:13]    INFO >> epoch 012:   2462 / 5058 loss=0.103, ups=1.53, num_updates=58100, lr=0, gnorm=0.914, clip=0, train_wall=61, gb_free=63.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:08:19]    INFO >> epoch 012:   2562 / 5058 loss=0.137, ups=1.58, num_updates=58200, lr=0, gnorm=1.246, clip=0, train_wall=59, gb_free=64 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:09:15]    INFO >> epoch 012:   2662 / 5058 loss=0.074, ups=1.86, num_updates=58300, lr=0, gnorm=0.813, clip=0, train_wall=50, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:10:22]    INFO >> epoch 012:   2762 / 5058 loss=0.115, ups=1.56, num_updates=58400, lr=0, gnorm=0.836, clip=0, train_wall=59, gb_free=64 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:11:05]    INFO >> epoch 012:   2862 / 5058 loss=0.403, ups=2.39, num_updates=58500, lr=0, gnorm=0.299, clip=0, train_wall=39, gb_free=77.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:12:07]    INFO >> epoch 012:   2962 / 5058 loss=0.155, ups=1.68, num_updates=58600, lr=0, gnorm=1.52, clip=0, train_wall=55, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:13:08]    INFO >> epoch 012:   3062 / 5058 loss=0.07, ups=1.68, num_updates=58700, lr=0, gnorm=0.46, clip=0, train_wall=55, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:14:06]    INFO >> epoch 012:   3162 / 5058 loss=0.074, ups=1.8, num_updates=58800, lr=0, gnorm=0.702, clip=0, train_wall=52, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:15:02]    INFO >> epoch 012:   3262 / 5058 loss=0.158, ups=1.88, num_updates=58900, lr=0, gnorm=0.509, clip=0, train_wall=50, gb_free=66.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:16:12]    INFO >> epoch 012:   3362 / 5058 loss=0.112, ups=1.5, num_updates=59000, lr=0, gnorm=2.069, clip=0, train_wall=62, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:17:21]    INFO >> epoch 012:   3462 / 5058 loss=0.212, ups=1.51, num_updates=59100, lr=0, gnorm=1.767, clip=0, train_wall=61, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:18:27]    INFO >> epoch 012:   3562 / 5058 loss=0.086, ups=1.58, num_updates=59200, lr=0, gnorm=1.02, clip=0, train_wall=59, gb_free=67.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:19:30]    INFO >> epoch 012:   3662 / 5058 loss=0.072, ups=1.64, num_updates=59300, lr=0, gnorm=0.838, clip=0, train_wall=57, gb_free=64.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:20:38]    INFO >> epoch 012:   3762 / 5058 loss=0.059, ups=1.54, num_updates=59400, lr=0, gnorm=0.8, clip=0, train_wall=60, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:21:36]    INFO >> epoch 012:   3862 / 5058 loss=0.086, ups=1.79, num_updates=59500, lr=0, gnorm=0.729, clip=0, train_wall=52, gb_free=77.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:22:32]    INFO >> epoch 012:   3962 / 5058 loss=0.13, ups=1.85, num_updates=59600, lr=0, gnorm=1.015, clip=0, train_wall=50, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:23:35]    INFO >> epoch 012:   4062 / 5058 loss=0.166, ups=1.66, num_updates=59700, lr=0, gnorm=0.403, clip=0, train_wall=56, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:24:43]    INFO >> epoch 012:   4162 / 5058 loss=0.144, ups=1.52, num_updates=59800, lr=0, gnorm=1.053, clip=0, train_wall=61, gb_free=66.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:25:46]    INFO >> epoch 012:   4262 / 5058 loss=0.122, ups=1.67, num_updates=59900, lr=0, gnorm=1.402, clip=0, train_wall=56, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:26:46]    INFO >> epoch 012:   4362 / 5058 loss=0.077, ups=1.74, num_updates=60000, lr=0, gnorm=1.05, clip=0, train_wall=54, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:27:53]    INFO >> epoch 012:   4462 / 5058 loss=0.07, ups=1.57, num_updates=60100, lr=0, gnorm=0.774, clip=0, train_wall=59, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:28:56]    INFO >> epoch 012:   4562 / 5058 loss=0.093, ups=1.65, num_updates=60200, lr=0, gnorm=0.374, clip=0, train_wall=56, gb_free=63.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:30:07]    INFO >> epoch 012:   4662 / 5058 loss=0.212, ups=1.45, num_updates=60300, lr=0, gnorm=2.363, clip=0, train_wall=64, gb_free=63.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:31:14]    INFO >> epoch 012:   4762 / 5058 loss=0.074, ups=1.57, num_updates=60400, lr=0, gnorm=1.414, clip=0, train_wall=59, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:32:01]    INFO >> epoch 012:   4862 / 5058 loss=0.045, ups=2.17, num_updates=60500, lr=0, gnorm=0.625, clip=0, train_wall=43, gb_free=65.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:32:58]    INFO >> epoch 012:   4962 / 5058 loss=0.073, ups=1.85, num_updates=60600, lr=0, gnorm=0.751, clip=0, train_wall=50, gb_free=74.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:34:06]    INFO >> epoch 012 | loss 0.087 | ups 1.5 | num_updates 60696 | lr 0 | gnorm 0.99 | clip 0 | train_wall 2772 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-21 10:34:06] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 10:40:55]    INFO >> epoch 012 | valid on 'valid' subset | loss 0.112 | num_updates 60696 | best_loss 0.112 (progress_bar.py:267, print())[0m
[32m[2025-11-21 10:40:56]    INFO >> saved checkpoint /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints/checkpoint12.pt (epoch 12 @ 60696 updates, score 0.112) (writing took 1.522633 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 10:40:56] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 10:40:59]    INFO >> epoch 013:      4 / 5058 loss=0.173, ups=0.22, num_updates=60700, lr=0, gnorm=2.507, clip=0, train_wall=63, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:41:43]    INFO >> epoch 013:    104 / 5058 loss=0.335, ups=2.36, num_updates=60800, lr=0, gnorm=0.749, clip=0, train_wall=39, gb_free=76.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:42:34]    INFO >> epoch 013:    204 / 5058 loss=0.138, ups=2.08, num_updates=60900, lr=0, gnorm=0.554, clip=0, train_wall=45, gb_free=65.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:43:39]    INFO >> epoch 013:    304 / 5058 loss=0.15, ups=1.6, num_updates=61000, lr=0, gnorm=0.36, clip=0, train_wall=58, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:44:46]    INFO >> epoch 013:    404 / 5058 loss=0.219, ups=1.56, num_updates=61100, lr=0, gnorm=1.611, clip=0, train_wall=60, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:45:50]    INFO >> epoch 013:    504 / 5058 loss=0.157, ups=1.63, num_updates=61200, lr=0, gnorm=1.518, clip=0, train_wall=57, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:46:52]    INFO >> epoch 013:    604 / 5058 loss=0.065, ups=1.68, num_updates=61300, lr=0, gnorm=1.158, clip=0, train_wall=55, gb_free=64 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:47:35]    INFO >> epoch 013:    704 / 5058 loss=0.136, ups=2.43, num_updates=61400, lr=0, gnorm=0.791, clip=0, train_wall=39, gb_free=71.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:48:19]    INFO >> epoch 013:    804 / 5058 loss=0.115, ups=2.35, num_updates=61500, lr=0, gnorm=0.704, clip=0, train_wall=40, gb_free=65.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:49:27]    INFO >> epoch 013:    904 / 5058 loss=0.176, ups=1.54, num_updates=61600, lr=0, gnorm=2.118, clip=0, train_wall=60, gb_free=63.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:50:33]    INFO >> epoch 013:   1004 / 5058 loss=0.097, ups=1.57, num_updates=61700, lr=0, gnorm=0.314, clip=0, train_wall=59, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:51:33]    INFO >> epoch 013:   1104 / 5058 loss=0.049, ups=1.75, num_updates=61800, lr=0, gnorm=0.776, clip=0, train_wall=53, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:52:37]    INFO >> epoch 013:   1204 / 5058 loss=0.033, ups=1.59, num_updates=61900, lr=0, gnorm=0.268, clip=0, train_wall=58, gb_free=65.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:53:41]    INFO >> epoch 013:   1304 / 5058 loss=0.082, ups=1.57, num_updates=62000, lr=0, gnorm=0.609, clip=0, train_wall=59, gb_free=65.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:54:49]    INFO >> epoch 013:   1404 / 5058 loss=0.053, ups=1.61, num_updates=62100, lr=0, gnorm=0.787, clip=0, train_wall=58, gb_free=64.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:55:56]    INFO >> epoch 013:   1504 / 5058 loss=0.256, ups=1.57, num_updates=62200, lr=0, gnorm=0.756, clip=0, train_wall=59, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:56:57]    INFO >> epoch 013:   1604 / 5058 loss=0.08, ups=1.7, num_updates=62300, lr=0, gnorm=1.227, clip=0, train_wall=55, gb_free=66.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:58:04]    INFO >> epoch 013:   1704 / 5058 loss=0.125, ups=1.55, num_updates=62400, lr=0, gnorm=0.838, clip=0, train_wall=60, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:58:58]    INFO >> epoch 013:   1804 / 5058 loss=0.099, ups=1.93, num_updates=62500, lr=0, gnorm=0.973, clip=0, train_wall=48, gb_free=75 (progress_bar.py:258, log())[0m
[32m[2025-11-21 10:59:38]    INFO >> epoch 013:   1904 / 5058 loss=0.152, ups=2.56, num_updates=62600, lr=0, gnorm=0.914, clip=0, train_wall=37, gb_free=72.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:00:42]    INFO >> epoch 013:   2004 / 5058 loss=0.093, ups=1.62, num_updates=62700, lr=0, gnorm=0.546, clip=0, train_wall=57, gb_free=69.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:01:50]    INFO >> epoch 013:   2104 / 5058 loss=0.07, ups=1.53, num_updates=62800, lr=0, gnorm=1.288, clip=0, train_wall=60, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:02:57]    INFO >> epoch 013:   2204 / 5058 loss=0.246, ups=1.55, num_updates=62900, lr=0, gnorm=1.789, clip=0, train_wall=60, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:04:06]    INFO >> epoch 013:   2304 / 5058 loss=0.061, ups=1.52, num_updates=63000, lr=0, gnorm=0.859, clip=0, train_wall=61, gb_free=63.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:05:11]    INFO >> epoch 013:   2404 / 5058 loss=0.063, ups=1.59, num_updates=63100, lr=0, gnorm=0.32, clip=0, train_wall=58, gb_free=64.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:06:23]    INFO >> epoch 013:   2504 / 5058 loss=0.139, ups=1.45, num_updates=63200, lr=0, gnorm=1.597, clip=0, train_wall=64, gb_free=64.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:07:22]    INFO >> epoch 013:   2604 / 5058 loss=0.105, ups=1.78, num_updates=63300, lr=0, gnorm=1.058, clip=0, train_wall=53, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:08:23]    INFO >> epoch 013:   2704 / 5058 loss=0.088, ups=1.7, num_updates=63400, lr=0, gnorm=0.64, clip=0, train_wall=55, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:09:26]    INFO >> epoch 013:   2804 / 5058 loss=0.079, ups=1.67, num_updates=63500, lr=0, gnorm=0.509, clip=0, train_wall=56, gb_free=76.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:10:06]    INFO >> epoch 013:   2904 / 5058 loss=0.246, ups=2.58, num_updates=63600, lr=0, gnorm=0.796, clip=0, train_wall=36, gb_free=74.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:11:13]    INFO >> epoch 013:   3004 / 5058 loss=0.136, ups=1.54, num_updates=63700, lr=0, gnorm=1.38, clip=0, train_wall=60, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:12:09]    INFO >> epoch 013:   3104 / 5058 loss=0.058, ups=1.89, num_updates=63800, lr=0, gnorm=0.21, clip=0, train_wall=49, gb_free=65.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:13:07]    INFO >> epoch 013:   3204 / 5058 loss=0.276, ups=1.78, num_updates=63900, lr=0, gnorm=0.885, clip=0, train_wall=52, gb_free=64.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:14:08]    INFO >> epoch 013:   3304 / 5058 loss=0.083, ups=1.7, num_updates=64000, lr=0, gnorm=0.775, clip=0, train_wall=55, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:15:20]    INFO >> epoch 013:   3404 / 5058 loss=0.221, ups=1.43, num_updates=64100, lr=0, gnorm=2.581, clip=0, train_wall=65, gb_free=63.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:16:28]    INFO >> epoch 013:   3504 / 5058 loss=0.127, ups=1.57, num_updates=64200, lr=0, gnorm=0.753, clip=0, train_wall=59, gb_free=66.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:17:35]    INFO >> epoch 013:   3604 / 5058 loss=0.074, ups=1.55, num_updates=64300, lr=0, gnorm=1.17, clip=0, train_wall=60, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:18:37]    INFO >> epoch 013:   3704 / 5058 loss=0.114, ups=1.69, num_updates=64400, lr=0, gnorm=0.944, clip=0, train_wall=55, gb_free=64.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:19:45]    INFO >> epoch 013:   3804 / 5058 loss=0.057, ups=1.53, num_updates=64500, lr=0, gnorm=0.707, clip=0, train_wall=61, gb_free=64.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:20:36]    INFO >> epoch 013:   3904 / 5058 loss=0.104, ups=2.09, num_updates=64600, lr=0, gnorm=1.057, clip=0, train_wall=45, gb_free=77.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:21:39]    INFO >> epoch 013:   4004 / 5058 loss=0.162, ups=1.64, num_updates=64700, lr=0, gnorm=0.722, clip=0, train_wall=57, gb_free=64.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:22:45]    INFO >> epoch 013:   4104 / 5058 loss=0.59, ups=1.6, num_updates=64800, lr=0, gnorm=0.549, clip=0, train_wall=58, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:23:52]    INFO >> epoch 013:   4204 / 5058 loss=0.117, ups=1.53, num_updates=64900, lr=0, gnorm=1.191, clip=0, train_wall=60, gb_free=64.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:24:52]    INFO >> epoch 013:   4304 / 5058 loss=0.123, ups=1.72, num_updates=65000, lr=0, gnorm=1.079, clip=0, train_wall=54, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:25:54]    INFO >> epoch 013:   4404 / 5058 loss=0.062, ups=1.68, num_updates=65100, lr=0, gnorm=0.95, clip=0, train_wall=55, gb_free=64.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:26:58]    INFO >> epoch 013:   4504 / 5058 loss=0.076, ups=1.62, num_updates=65200, lr=0, gnorm=0.926, clip=0, train_wall=57, gb_free=63.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:28:06]    INFO >> epoch 013:   4604 / 5058 loss=0.191, ups=1.54, num_updates=65300, lr=0, gnorm=1.2, clip=0, train_wall=60, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:29:18]    INFO >> epoch 013:   4704 / 5058 loss=0.222, ups=1.44, num_updates=65400, lr=0, gnorm=2.359, clip=0, train_wall=64, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:30:20]    INFO >> epoch 013:   4804 / 5058 loss=0.046, ups=1.69, num_updates=65500, lr=0, gnorm=0.615, clip=0, train_wall=55, gb_free=65.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:31:02]    INFO >> epoch 013:   4904 / 5058 loss=0.117, ups=2.45, num_updates=65600, lr=0, gnorm=0.513, clip=0, train_wall=38, gb_free=64.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:32:08]    INFO >> epoch 013:   5004 / 5058 loss=0.088, ups=1.57, num_updates=65700, lr=0, gnorm=1.591, clip=0, train_wall=59, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:32:46]    INFO >> epoch 013 | loss 0.087 | ups 1.5 | num_updates 65754 | lr 0 | gnorm 0.99 | clip 0 | train_wall 2772 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-21 11:32:46] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 11:39:36]    INFO >> epoch 013 | valid on 'valid' subset | loss 0.112 | num_updates 65754 | best_loss 0.112 (progress_bar.py:267, print())[0m
[32m[2025-11-21 11:39:38]    INFO >> saved checkpoint /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints/checkpoint13.pt (epoch 13 @ 65754 updates, score 0.112) (writing took 1.560403 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 11:39:38] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 11:40:00]    INFO >> epoch 014:     46 / 5058 loss=0.217, ups=0.22, num_updates=65800, lr=0, gnorm=1.89, clip=0, train_wall=54, gb_free=68 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:40:45]    INFO >> epoch 014:    146 / 5058 loss=0.131, ups=2.38, num_updates=65900, lr=0, gnorm=0.464, clip=0, train_wall=39, gb_free=69.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:41:41]    INFO >> epoch 014:    246 / 5058 loss=0.224, ups=1.82, num_updates=66000, lr=0, gnorm=0.441, clip=0, train_wall=51, gb_free=63.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:42:46]    INFO >> epoch 014:    346 / 5058 loss=0.138, ups=1.62, num_updates=66100, lr=0, gnorm=0.762, clip=0, train_wall=58, gb_free=69.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:43:56]    INFO >> epoch 014:    446 / 5058 loss=0.223, ups=1.49, num_updates=66200, lr=0, gnorm=2.108, clip=0, train_wall=62, gb_free=64.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:44:58]    INFO >> epoch 014:    546 / 5058 loss=0.07, ups=1.69, num_updates=66300, lr=0, gnorm=0.998, clip=0, train_wall=55, gb_free=64.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:45:52]    INFO >> epoch 014:    646 / 5058 loss=0.086, ups=1.92, num_updates=66400, lr=0, gnorm=0.913, clip=0, train_wall=49, gb_free=74.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:46:33]    INFO >> epoch 014:    746 / 5058 loss=0.148, ups=2.57, num_updates=66500, lr=0, gnorm=1.174, clip=0, train_wall=37, gb_free=67.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:47:28]    INFO >> epoch 014:    846 / 5058 loss=0.155, ups=1.9, num_updates=66600, lr=0, gnorm=0.966, clip=0, train_wall=49, gb_free=64.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:48:35]    INFO >> epoch 014:    946 / 5058 loss=0.138, ups=1.52, num_updates=66700, lr=0, gnorm=1.483, clip=0, train_wall=61, gb_free=70.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:49:36]    INFO >> epoch 014:   1046 / 5058 loss=0.098, ups=1.7, num_updates=66800, lr=0, gnorm=0.803, clip=0, train_wall=55, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:50:38]    INFO >> epoch 014:   1146 / 5058 loss=0.037, ups=1.69, num_updates=66900, lr=0, gnorm=0.188, clip=0, train_wall=55, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:51:46]    INFO >> epoch 014:   1246 / 5058 loss=0.043, ups=1.54, num_updates=67000, lr=0, gnorm=0.556, clip=0, train_wall=60, gb_free=64.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:52:52]    INFO >> epoch 014:   1346 / 5058 loss=0.052, ups=1.57, num_updates=67100, lr=0, gnorm=0.387, clip=0, train_wall=59, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:53:56]    INFO >> epoch 014:   1446 / 5058 loss=0.174, ups=1.63, num_updates=67200, lr=0, gnorm=1.073, clip=0, train_wall=57, gb_free=63.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:55:00]    INFO >> epoch 014:   1546 / 5058 loss=0.08, ups=1.64, num_updates=67300, lr=0, gnorm=0.961, clip=0, train_wall=57, gb_free=74.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:56:04]    INFO >> epoch 014:   1646 / 5058 loss=0.129, ups=1.62, num_updates=67400, lr=0, gnorm=1.108, clip=0, train_wall=57, gb_free=65 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:57:10]    INFO >> epoch 014:   1746 / 5058 loss=0.109, ups=1.58, num_updates=67500, lr=0, gnorm=1.027, clip=0, train_wall=59, gb_free=67 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:57:52]    INFO >> epoch 014:   1846 / 5058 loss=0.08, ups=2.48, num_updates=67600, lr=0, gnorm=0.508, clip=0, train_wall=38, gb_free=67.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:58:46]    INFO >> epoch 014:   1946 / 5058 loss=0.111, ups=1.96, num_updates=67700, lr=0, gnorm=1.028, clip=0, train_wall=48, gb_free=74.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 11:59:50]    INFO >> epoch 014:   2046 / 5058 loss=0.067, ups=1.63, num_updates=67800, lr=0, gnorm=0.378, clip=0, train_wall=57, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:00:59]    INFO >> epoch 014:   2146 / 5058 loss=0.109, ups=1.49, num_updates=67900, lr=0, gnorm=2.207, clip=0, train_wall=62, gb_free=65.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:02:06]    INFO >> epoch 014:   2246 / 5058 loss=0.073, ups=1.57, num_updates=68000, lr=0, gnorm=1.097, clip=0, train_wall=59, gb_free=64.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:03:13]    INFO >> epoch 014:   2346 / 5058 loss=0.09, ups=1.56, num_updates=68100, lr=0, gnorm=0.782, clip=0, train_wall=60, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:04:19]    INFO >> epoch 014:   2446 / 5058 loss=0.074, ups=1.56, num_updates=68200, lr=0, gnorm=0.525, clip=0, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:05:28]    INFO >> epoch 014:   2546 / 5058 loss=0.194, ups=1.51, num_updates=68300, lr=0, gnorm=1.454, clip=0, train_wall=62, gb_free=66.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:06:23]    INFO >> epoch 014:   2646 / 5058 loss=0.072, ups=1.86, num_updates=68400, lr=0, gnorm=0.958, clip=0, train_wall=50, gb_free=70.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:07:28]    INFO >> epoch 014:   2746 / 5058 loss=0.105, ups=1.61, num_updates=68500, lr=0, gnorm=0.722, clip=0, train_wall=58, gb_free=78.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:08:15]    INFO >> epoch 014:   2846 / 5058 loss=0.114, ups=2.29, num_updates=68600, lr=0, gnorm=0.165, clip=0, train_wall=41, gb_free=77.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:09:12]    INFO >> epoch 014:   2946 / 5058 loss=0.201, ups=1.8, num_updates=68700, lr=0, gnorm=1.498, clip=0, train_wall=52, gb_free=67.1 (progress_bar.py:258, log())[0m
^[[A[32m[2025-11-21 12:10:16]    INFO >> epoch 014:   3046 / 5058 loss=0.077, ups=1.62, num_updates=68800, lr=0, gnorm=0.759, clip=0, train_wall=58, gb_free=64 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:11:14]    INFO >> epoch 014:   3146 / 5058 loss=0.067, ups=1.79, num_updates=68900, lr=0, gnorm=0.501, clip=0, train_wall=52, gb_free=64.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:12:09]    INFO >> epoch 014:   3246 / 5058 loss=0.369, ups=1.92, num_updates=69000, lr=0, gnorm=0.513, clip=0, train_wall=49, gb_free=63.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:13:17]    INFO >> epoch 014:   3346 / 5058 loss=0.102, ups=1.54, num_updates=69100, lr=0, gnorm=1.672, clip=0, train_wall=61, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:14:27]    INFO >> epoch 014:   3446 / 5058 loss=0.232, ups=1.48, num_updates=69200, lr=0, gnorm=2.384, clip=0, train_wall=63, gb_free=71.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:15:33]    INFO >> epoch 014:   3546 / 5058 loss=0.073, ups=1.58, num_updates=69300, lr=0, gnorm=0.487, clip=0, train_wall=59, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:16:38]    INFO >> epoch 014:   3646 / 5058 loss=0.083, ups=1.62, num_updates=69400, lr=0, gnorm=1.02, clip=0, train_wall=58, gb_free=71.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:17:43]    INFO >> epoch 014:   3746 / 5058 loss=0.055, ups=1.6, num_updates=69500, lr=0, gnorm=1.075, clip=0, train_wall=58, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:18:44]    INFO >> epoch 014:   3846 / 5058 loss=0.093, ups=1.68, num_updates=69600, lr=0, gnorm=0.623, clip=0, train_wall=56, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:19:37]    INFO >> epoch 014:   3946 / 5058 loss=0.124, ups=1.98, num_updates=69700, lr=0, gnorm=0.971, clip=0, train_wall=47, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:20:41]    INFO >> epoch 014:   4046 / 5058 loss=0.141, ups=1.63, num_updates=69800, lr=0, gnorm=0.478, clip=0, train_wall=57, gb_free=64.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:21:48]    INFO >> epoch 014:   4146 / 5058 loss=0.139, ups=1.54, num_updates=69900, lr=0, gnorm=0.953, clip=0, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:22:51]    INFO >> epoch 014:   4246 / 5058 loss=0.132, ups=1.66, num_updates=70000, lr=0, gnorm=1.645, clip=0, train_wall=56, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:23:49]    INFO >> epoch 014:   4346 / 5058 loss=0.069, ups=1.78, num_updates=70100, lr=0, gnorm=0.634, clip=0, train_wall=53, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:24:58]    INFO >> epoch 014:   4446 / 5058 loss=0.117, ups=1.53, num_updates=70200, lr=0, gnorm=0.933, clip=0, train_wall=61, gb_free=64.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:25:59]    INFO >> epoch 014:   4546 / 5058 loss=0.061, ups=1.68, num_updates=70300, lr=0, gnorm=0.559, clip=0, train_wall=56, gb_free=68.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:27:10]    INFO >> epoch 014:   4646 / 5058 loss=0.202, ups=1.46, num_updates=70400, lr=0, gnorm=2.07, clip=0, train_wall=64, gb_free=71.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:28:17]    INFO >> epoch 014:   4746 / 5058 loss=0.093, ups=1.56, num_updates=70500, lr=0, gnorm=1.628, clip=0, train_wall=60, gb_free=78.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:29:10]    INFO >> epoch 014:   4846 / 5058 loss=0.044, ups=1.96, num_updates=70600, lr=0, gnorm=0.725, clip=0, train_wall=48, gb_free=74.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:30:03]    INFO >> epoch 014:   4946 / 5058 loss=0.075, ups=2, num_updates=70700, lr=0, gnorm=0.81, clip=0, train_wall=47, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:31:13]    INFO >> epoch 014:   5046 / 5058 loss=0.183, ups=1.5, num_updates=70800, lr=0, gnorm=2.24, clip=0, train_wall=62, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:31:20]    INFO >> epoch 014 | loss 0.087 | ups 1.5 | num_updates 70812 | lr 0 | gnorm 0.99 | clip 0 | train_wall 2779 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-21 12:31:20] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 12:38:08]    INFO >> epoch 014 | valid on 'valid' subset | loss 0.112 | num_updates 70812 | best_loss 0.112 (progress_bar.py:267, print())[0m
[32m[2025-11-21 12:38:09]    INFO >> saved checkpoint /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints/checkpoint14.pt (epoch 14 @ 70812 updates, score 0.112) (writing took 1.451892 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 12:38:09] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 12:38:49]    INFO >> epoch 015:     88 / 5058 loss=0.209, ups=0.23, num_updates=70900, lr=0, gnorm=0.93, clip=0, train_wall=43, gb_free=71.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:39:36]    INFO >> epoch 015:    188 / 5058 loss=0.137, ups=2.24, num_updates=71000, lr=0, gnorm=0.567, clip=0, train_wall=42, gb_free=64.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:40:43]    INFO >> epoch 015:    288 / 5058 loss=0.151, ups=1.56, num_updates=71100, lr=0, gnorm=0.433, clip=0, train_wall=60, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:41:49]    INFO >> epoch 015:    388 / 5058 loss=0.193, ups=1.58, num_updates=71200, lr=0, gnorm=1.027, clip=0, train_wall=59, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:42:52]    INFO >> epoch 015:    488 / 5058 loss=0.238, ups=1.62, num_updates=71300, lr=0, gnorm=1.843, clip=0, train_wall=58, gb_free=69.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:43:54]    INFO >> epoch 015:    588 / 5058 loss=0.063, ups=1.64, num_updates=71400, lr=0, gnorm=1.372, clip=0, train_wall=57, gb_free=65.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:44:40]    INFO >> epoch 015:    688 / 5058 loss=0.14, ups=2.41, num_updates=71500, lr=0, gnorm=0.527, clip=0, train_wall=39, gb_free=65.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:45:22]    INFO >> epoch 015:    788 / 5058 loss=0.127, ups=2.46, num_updates=71600, lr=0, gnorm=0.943, clip=0, train_wall=38, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:46:29]    INFO >> epoch 015:    888 / 5058 loss=0.167, ups=1.55, num_updates=71700, lr=0, gnorm=2.014, clip=0, train_wall=60, gb_free=67.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:47:35]    INFO >> epoch 015:    988 / 5058 loss=0.104, ups=1.58, num_updates=71800, lr=0, gnorm=0.49, clip=0, train_wall=59, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:48:36]    INFO >> epoch 015:   1088 / 5058 loss=0.053, ups=1.7, num_updates=71900, lr=0, gnorm=0.755, clip=0, train_wall=55, gb_free=65.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:49:43]    INFO >> epoch 015:   1188 / 5058 loss=0.032, ups=1.58, num_updates=72000, lr=0, gnorm=0.115, clip=0, train_wall=59, gb_free=64.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:50:47]    INFO >> epoch 015:   1288 / 5058 loss=0.088, ups=1.62, num_updates=72100, lr=0, gnorm=0.742, clip=0, train_wall=58, gb_free=64.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:51:50]    INFO >> epoch 015:   1388 / 5058 loss=0.054, ups=1.62, num_updates=72200, lr=0, gnorm=0.829, clip=0, train_wall=57, gb_free=64.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:52:57]    INFO >> epoch 015:   1488 / 5058 loss=0.24, ups=1.55, num_updates=72300, lr=0, gnorm=0.617, clip=0, train_wall=60, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:53:59]    INFO >> epoch 015:   1588 / 5058 loss=0.076, ups=1.69, num_updates=72400, lr=0, gnorm=1.2, clip=0, train_wall=55, gb_free=64.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:55:06]    INFO >> epoch 015:   1688 / 5058 loss=0.124, ups=1.55, num_updates=72500, lr=0, gnorm=0.87, clip=0, train_wall=60, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:56:06]    INFO >> epoch 015:   1788 / 5058 loss=0.105, ups=1.77, num_updates=72600, lr=0, gnorm=1.107, clip=0, train_wall=53, gb_free=78 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:56:44]    INFO >> epoch 015:   1888 / 5058 loss=0.102, ups=2.68, num_updates=72700, lr=0, gnorm=0.38, clip=0, train_wall=35, gb_free=64.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:57:43]    INFO >> epoch 015:   1988 / 5058 loss=0.127, ups=1.77, num_updates=72800, lr=0, gnorm=0.854, clip=0, train_wall=53, gb_free=64.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:58:51]    INFO >> epoch 015:   2088 / 5058 loss=0.069, ups=1.53, num_updates=72900, lr=0, gnorm=1.173, clip=0, train_wall=60, gb_free=65.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 12:59:59]    INFO >> epoch 015:   2188 / 5058 loss=0.249, ups=1.54, num_updates=73000, lr=0, gnorm=2.093, clip=0, train_wall=60, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:01:07]    INFO >> epoch 015:   2288 / 5058 loss=0.06, ups=1.52, num_updates=73100, lr=0, gnorm=0.875, clip=0, train_wall=61, gb_free=66.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:02:14]    INFO >> epoch 015:   2388 / 5058 loss=0.084, ups=1.56, num_updates=73200, lr=0, gnorm=0.336, clip=0, train_wall=60, gb_free=68.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:03:24]    INFO >> epoch 015:   2488 / 5058 loss=0.112, ups=1.5, num_updates=73300, lr=0, gnorm=1.286, clip=0, train_wall=62, gb_free=63.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:04:25]    INFO >> epoch 015:   2588 / 5058 loss=0.112, ups=1.71, num_updates=73400, lr=0, gnorm=1.242, clip=0, train_wall=55, gb_free=70.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:05:25]    INFO >> epoch 015:   2688 / 5058 loss=0.084, ups=1.74, num_updates=73500, lr=0, gnorm=0.639, clip=0, train_wall=54, gb_free=63.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:06:29]    INFO >> epoch 015:   2788 / 5058 loss=0.099, ups=1.64, num_updates=73600, lr=0, gnorm=0.641, clip=0, train_wall=57, gb_free=64.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:07:08]    INFO >> epoch 015:   2888 / 5058 loss=0.321, ups=2.6, num_updates=73700, lr=0, gnorm=0.596, clip=0, train_wall=36, gb_free=71.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:08:16]    INFO >> epoch 015:   2988 / 5058 loss=0.151, ups=1.55, num_updates=73800, lr=0, gnorm=1.467, clip=0, train_wall=60, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:09:12]    INFO >> epoch 015:   3088 / 5058 loss=0.059, ups=1.82, num_updates=73900, lr=0, gnorm=0.252, clip=0, train_wall=51, gb_free=71.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:10:09]    INFO >> epoch 015:   3188 / 5058 loss=0.167, ups=1.84, num_updates=74000, lr=0, gnorm=0.939, clip=0, train_wall=51, gb_free=77.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:11:10]    INFO >> epoch 015:   3288 / 5058 loss=0.07, ups=1.73, num_updates=74100, lr=0, gnorm=0.417, clip=0, train_wall=54, gb_free=65 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:12:22]    INFO >> epoch 015:   3388 / 5058 loss=0.222, ups=1.44, num_updates=74200, lr=0, gnorm=2.508, clip=0, train_wall=64, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:13:29]    INFO >> epoch 015:   3488 / 5058 loss=0.212, ups=1.55, num_updates=74300, lr=0, gnorm=1.148, clip=0, train_wall=60, gb_free=63.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:14:36]    INFO >> epoch 015:   3588 / 5058 loss=0.07, ups=1.56, num_updates=74400, lr=0, gnorm=1.083, clip=0, train_wall=60, gb_free=64.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:15:39]    INFO >> epoch 015:   3688 / 5058 loss=0.12, ups=1.66, num_updates=74500, lr=0, gnorm=0.977, clip=0, train_wall=56, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:16:47]    INFO >> epoch 015:   3788 / 5058 loss=0.055, ups=1.54, num_updates=74600, lr=0, gnorm=0.671, clip=0, train_wall=60, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:17:37]    INFO >> epoch 015:   3888 / 5058 loss=0.102, ups=2.05, num_updates=74700, lr=0, gnorm=0.833, clip=0, train_wall=46, gb_free=64.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:18:40]    INFO >> epoch 015:   3988 / 5058 loss=0.124, ups=1.66, num_updates=74800, lr=0, gnorm=0.967, clip=0, train_wall=56, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:19:43]    INFO >> epoch 015:   4088 / 5058 loss=0.249, ups=1.63, num_updates=74900, lr=0, gnorm=0.478, clip=0, train_wall=57, gb_free=68.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:20:52]    INFO >> epoch 015:   4188 / 5058 loss=0.143, ups=1.53, num_updates=75000, lr=0, gnorm=1.201, clip=0, train_wall=61, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:21:54]    INFO >> epoch 015:   4288 / 5058 loss=0.103, ups=1.7, num_updates=75100, lr=0, gnorm=1.084, clip=0, train_wall=55, gb_free=65.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:22:56]    INFO >> epoch 015:   4388 / 5058 loss=0.084, ups=1.69, num_updates=75200, lr=0, gnorm=1.065, clip=0, train_wall=55, gb_free=64.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:24:00]    INFO >> epoch 015:   4488 / 5058 loss=0.07, ups=1.62, num_updates=75300, lr=0, gnorm=0.875, clip=0, train_wall=57, gb_free=64.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:25:06]    INFO >> epoch 015:   4588 / 5058 loss=0.202, ups=1.58, num_updates=75400, lr=0, gnorm=0.872, clip=0, train_wall=59, gb_free=66.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:26:18]    INFO >> epoch 015:   4688 / 5058 loss=0.214, ups=1.44, num_updates=75500, lr=0, gnorm=2.477, clip=0, train_wall=64, gb_free=66.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:27:21]    INFO >> epoch 015:   4788 / 5058 loss=0.062, ups=1.65, num_updates=75600, lr=0, gnorm=0.917, clip=0, train_wall=56, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:28:06]    INFO >> epoch 015:   4888 / 5058 loss=0.041, ups=2.33, num_updates=75700, lr=0, gnorm=0.456, clip=0, train_wall=40, gb_free=73.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:29:08]    INFO >> epoch 015:   4988 / 5058 loss=0.078, ups=1.67, num_updates=75800, lr=0, gnorm=1.203, clip=0, train_wall=56, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:29:57]    INFO >> epoch 015 | loss 0.087 | ups 1.5 | num_updates 75870 | lr 0 | gnorm 0.99 | clip 0 | train_wall 2780 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-21 13:29:57] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 13:36:48]    INFO >> epoch 015 | valid on 'valid' subset | loss 0.112 | num_updates 75870 | best_loss 0.112 (progress_bar.py:267, print())[0m
[32m[2025-11-21 13:36:49]    INFO >> saved checkpoint /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints/checkpoint15.pt (epoch 15 @ 75870 updates, score 0.112) (writing took 1.471414 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 13:36:49] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 13:37:07]    INFO >> epoch 016:     30 / 5058 loss=0.213, ups=0.22, num_updates=75900, lr=0, gnorm=2.244, clip=0, train_wall=61, gb_free=74.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:37:46]    INFO >> epoch 016:    130 / 5058 loss=0.571, ups=2.67, num_updates=76000, lr=0, gnorm=0.57, clip=0, train_wall=35, gb_free=78.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:38:43]    INFO >> epoch 016:    230 / 5058 loss=0.123, ups=1.84, num_updates=76100, lr=0, gnorm=0.451, clip=0, train_wall=51, gb_free=68.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:39:47]    INFO >> epoch 016:    330 / 5058 loss=0.128, ups=1.61, num_updates=76200, lr=0, gnorm=0.633, clip=0, train_wall=58, gb_free=71.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:40:56]    INFO >> epoch 016:    430 / 5058 loss=0.228, ups=1.52, num_updates=76300, lr=0, gnorm=1.876, clip=0, train_wall=62, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:41:58]    INFO >> epoch 016:    530 / 5058 loss=0.1, ups=1.7, num_updates=76400, lr=0, gnorm=1.197, clip=0, train_wall=55, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:42:56]    INFO >> epoch 016:    630 / 5058 loss=0.071, ups=1.79, num_updates=76500, lr=0, gnorm=1.076, clip=0, train_wall=52, gb_free=63.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:43:36]    INFO >> epoch 016:    730 / 5058 loss=0.209, ups=2.59, num_updates=76600, lr=0, gnorm=1.088, clip=0, train_wall=36, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:44:26]    INFO >> epoch 016:    830 / 5058 loss=0.121, ups=2.06, num_updates=76700, lr=0, gnorm=0.755, clip=0, train_wall=46, gb_free=67.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:45:34]    INFO >> epoch 016:    930 / 5058 loss=0.154, ups=1.52, num_updates=76800, lr=0, gnorm=1.76, clip=0, train_wall=61, gb_free=68.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:46:36]    INFO >> epoch 016:   1030 / 5058 loss=0.105, ups=1.69, num_updates=76900, lr=0, gnorm=0.537, clip=0, train_wall=55, gb_free=76.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:47:38]    INFO >> epoch 016:   1130 / 5058 loss=0.039, ups=1.69, num_updates=77000, lr=0, gnorm=0.454, clip=0, train_wall=55, gb_free=68 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:48:45]    INFO >> epoch 016:   1230 / 5058 loss=0.041, ups=1.55, num_updates=77100, lr=0, gnorm=0.359, clip=0, train_wall=60, gb_free=66.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:49:51]    INFO >> epoch 016:   1330 / 5058 loss=0.066, ups=1.59, num_updates=77200, lr=0, gnorm=0.569, clip=0, train_wall=59, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:50:54]    INFO >> epoch 016:   1430 / 5058 loss=0.072, ups=1.65, num_updates=77300, lr=0, gnorm=0.956, clip=0, train_wall=57, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:51:59]    INFO >> epoch 016:   1530 / 5058 loss=0.085, ups=1.61, num_updates=77400, lr=0, gnorm=1.028, clip=0, train_wall=58, gb_free=63.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:53:02]    INFO >> epoch 016:   1630 / 5058 loss=0.111, ups=1.65, num_updates=77500, lr=0, gnorm=1.018, clip=0, train_wall=57, gb_free=66.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:54:08]    INFO >> epoch 016:   1730 / 5058 loss=0.131, ups=1.58, num_updates=77600, lr=0, gnorm=1.035, clip=0, train_wall=59, gb_free=68.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:54:54]    INFO >> epoch 016:   1830 / 5058 loss=0.075, ups=2.23, num_updates=77700, lr=0, gnorm=0.551, clip=0, train_wall=42, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:55:42]    INFO >> epoch 016:   1930 / 5058 loss=0.116, ups=2.14, num_updates=77800, lr=0, gnorm=1.153, clip=0, train_wall=44, gb_free=66.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:56:45]    INFO >> epoch 016:   2030 / 5058 loss=0.091, ups=1.68, num_updates=77900, lr=0, gnorm=0.334, clip=0, train_wall=56, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:57:54]    INFO >> epoch 016:   2130 / 5058 loss=0.081, ups=1.49, num_updates=78000, lr=0, gnorm=1.673, clip=0, train_wall=62, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 13:59:02]    INFO >> epoch 016:   2230 / 5058 loss=0.098, ups=1.55, num_updates=78100, lr=0, gnorm=1.447, clip=0, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:00:07]    INFO >> epoch 016:   2330 / 5058 loss=0.069, ups=1.57, num_updates=78200, lr=0, gnorm=0.708, clip=0, train_wall=59, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:01:14]    INFO >> epoch 016:   2430 / 5058 loss=0.078, ups=1.57, num_updates=78300, lr=0, gnorm=0.509, clip=0, train_wall=59, gb_free=64 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:02:25]    INFO >> epoch 016:   2530 / 5058 loss=0.181, ups=1.47, num_updates=78400, lr=0, gnorm=1.771, clip=0, train_wall=64, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:03:22]    INFO >> epoch 016:   2630 / 5058 loss=0.075, ups=1.84, num_updates=78500, lr=0, gnorm=0.933, clip=0, train_wall=51, gb_free=64.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:04:25]    INFO >> epoch 016:   2730 / 5058 loss=0.096, ups=1.65, num_updates=78600, lr=0, gnorm=0.473, clip=0, train_wall=57, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:05:15]    INFO >> epoch 016:   2830 / 5058 loss=0.088, ups=2.05, num_updates=78700, lr=0, gnorm=0.439, clip=0, train_wall=46, gb_free=76.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:06:08]    INFO >> epoch 016:   2930 / 5058 loss=0.218, ups=2.01, num_updates=78800, lr=0, gnorm=1.343, clip=0, train_wall=47, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:07:14]    INFO >> epoch 016:   3030 / 5058 loss=0.098, ups=1.56, num_updates=78900, lr=0, gnorm=0.892, clip=0, train_wall=60, gb_free=69.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:08:12]    INFO >> epoch 016:   3130 / 5058 loss=0.062, ups=1.83, num_updates=79000, lr=0, gnorm=0.415, clip=0, train_wall=51, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:09:05]    INFO >> epoch 016:   3230 / 5058 loss=0.347, ups=1.92, num_updates=79100, lr=0, gnorm=0.622, clip=0, train_wall=49, gb_free=67 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:10:11]    INFO >> epoch 016:   3330 / 5058 loss=0.099, ups=1.57, num_updates=79200, lr=0, gnorm=1.447, clip=0, train_wall=59, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:11:22]    INFO >> epoch 016:   3430 / 5058 loss=0.216, ups=1.47, num_updates=79300, lr=0, gnorm=2.242, clip=0, train_wall=63, gb_free=64.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:12:29]    INFO >> epoch 016:   3530 / 5058 loss=0.095, ups=1.57, num_updates=79400, lr=0, gnorm=0.63, clip=0, train_wall=60, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:13:35]    INFO >> epoch 016:   3630 / 5058 loss=0.07, ups=1.58, num_updates=79500, lr=0, gnorm=0.998, clip=0, train_wall=59, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:14:37]    INFO >> epoch 016:   3730 / 5058 loss=0.089, ups=1.68, num_updates=79600, lr=0, gnorm=1.309, clip=0, train_wall=56, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:15:44]    INFO >> epoch 016:   3830 / 5058 loss=0.056, ups=1.56, num_updates=79700, lr=0, gnorm=0.422, clip=0, train_wall=60, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:16:32]    INFO >> epoch 016:   3930 / 5058 loss=0.121, ups=2.21, num_updates=79800, lr=0, gnorm=0.999, clip=0, train_wall=43, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:17:37]    INFO >> epoch 016:   4030 / 5058 loss=0.154, ups=1.58, num_updates=79900, lr=0, gnorm=0.663, clip=0, train_wall=59, gb_free=67.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:18:44]    INFO >> epoch 016:   4130 / 5058 loss=0.111, ups=1.55, num_updates=80000, lr=0, gnorm=0.61, clip=0, train_wall=60, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:19:47]    INFO >> epoch 016:   4230 / 5058 loss=0.15, ups=1.62, num_updates=80100, lr=0, gnorm=1.838, clip=0, train_wall=58, gb_free=68.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:20:46]    INFO >> epoch 016:   4330 / 5058 loss=0.124, ups=1.81, num_updates=80200, lr=0, gnorm=0.701, clip=0, train_wall=51, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:21:54]    INFO >> epoch 016:   4430 / 5058 loss=0.071, ups=1.53, num_updates=80300, lr=0, gnorm=0.864, clip=0, train_wall=60, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:22:58]    INFO >> epoch 016:   4530 / 5058 loss=0.067, ups=1.64, num_updates=80400, lr=0, gnorm=0.712, clip=0, train_wall=57, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:24:08]    INFO >> epoch 016:   4630 / 5058 loss=0.193, ups=1.48, num_updates=80500, lr=0, gnorm=1.759, clip=0, train_wall=62, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:25:19]    INFO >> epoch 016:   4730 / 5058 loss=0.105, ups=1.47, num_updates=80600, lr=0, gnorm=1.865, clip=0, train_wall=62, gb_free=63.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:26:14]    INFO >> epoch 016:   4830 / 5058 loss=0.049, ups=1.86, num_updates=80700, lr=0, gnorm=0.798, clip=0, train_wall=50, gb_free=70.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:27:02]    INFO >> epoch 016:   4930 / 5058 loss=0.081, ups=2.18, num_updates=80800, lr=0, gnorm=0.485, clip=0, train_wall=43, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:28:11]    INFO >> epoch 016:   5030 / 5058 loss=0.125, ups=1.51, num_updates=80900, lr=0, gnorm=1.921, clip=0, train_wall=61, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:28:31]    INFO >> epoch 016 | loss 0.087 | ups 1.5 | num_updates 80928 | lr 0 | gnorm 0.99 | clip 0 | train_wall 2780 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-21 14:28:31] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:35:21]    INFO >> epoch 016 | valid on 'valid' subset | loss 0.112 | num_updates 80928 | best_loss 0.112 (progress_bar.py:267, print())[0m
[32m[2025-11-21 14:35:23]    INFO >> saved checkpoint /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints/checkpoint16.pt (epoch 16 @ 80928 updates, score 0.112) (writing took 1.741945 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 14:35:23] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:35:58]    INFO >> epoch 017:     72 / 5058 loss=0.239, ups=0.22, num_updates=81000, lr=0, gnorm=1.563, clip=0, train_wall=48, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:36:39]    INFO >> epoch 017:    172 / 5058 loss=0.115, ups=2.43, num_updates=81100, lr=0, gnorm=0.427, clip=0, train_wall=38, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:37:46]    INFO >> epoch 017:    272 / 5058 loss=0.161, ups=1.59, num_updates=81200, lr=0, gnorm=0.536, clip=0, train_wall=58, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:38:50]    INFO >> epoch 017:    372 / 5058 loss=0.222, ups=1.59, num_updates=81300, lr=0, gnorm=0.78, clip=0, train_wall=58, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:39:57]    INFO >> epoch 017:    472 / 5058 loss=0.223, ups=1.57, num_updates=81400, lr=0, gnorm=2.122, clip=0, train_wall=59, gb_free=64.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:41:02]    INFO >> epoch 017:    572 / 5058 loss=0.067, ups=1.64, num_updates=81500, lr=0, gnorm=1.292, clip=0, train_wall=56, gb_free=63.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:41:51]    INFO >> epoch 017:    672 / 5058 loss=0.077, ups=2.17, num_updates=81600, lr=0, gnorm=0.477, clip=0, train_wall=43, gb_free=78.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:42:32]    INFO >> epoch 017:    772 / 5058 loss=0.147, ups=2.46, num_updates=81700, lr=0, gnorm=1.079, clip=0, train_wall=38, gb_free=77.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:43:36]    INFO >> epoch 017:    872 / 5058 loss=0.155, ups=1.64, num_updates=81800, lr=0, gnorm=1.735, clip=0, train_wall=56, gb_free=64.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:44:43]    INFO >> epoch 017:    972 / 5058 loss=0.115, ups=1.57, num_updates=81900, lr=0, gnorm=0.777, clip=0, train_wall=59, gb_free=63.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:45:47]    INFO >> epoch 017:   1072 / 5058 loss=0.056, ups=1.62, num_updates=82000, lr=0, gnorm=0.649, clip=0, train_wall=57, gb_free=68.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:46:51]    INFO >> epoch 017:   1172 / 5058 loss=0.034, ups=1.63, num_updates=82100, lr=0, gnorm=0.22, clip=0, train_wall=57, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:47:57]    INFO >> epoch 017:   1272 / 5058 loss=0.08, ups=1.58, num_updates=82200, lr=0, gnorm=0.743, clip=0, train_wall=58, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:49:00]    INFO >> epoch 017:   1372 / 5058 loss=0.052, ups=1.63, num_updates=82300, lr=0, gnorm=0.811, clip=0, train_wall=57, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:50:08]    INFO >> epoch 017:   1472 / 5058 loss=0.175, ups=1.54, num_updates=82400, lr=0, gnorm=0.635, clip=0, train_wall=60, gb_free=66.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:51:11]    INFO >> epoch 017:   1572 / 5058 loss=0.076, ups=1.65, num_updates=82500, lr=0, gnorm=0.897, clip=0, train_wall=56, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:52:18]    INFO >> epoch 017:   1672 / 5058 loss=0.147, ups=1.55, num_updates=82600, lr=0, gnorm=1.142, clip=0, train_wall=59, gb_free=67.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:53:22]    INFO >> epoch 017:   1772 / 5058 loss=0.098, ups=1.64, num_updates=82700, lr=0, gnorm=1.116, clip=0, train_wall=56, gb_free=77.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:54:01]    INFO >> epoch 017:   1872 / 5058 loss=0.082, ups=2.62, num_updates=82800, lr=0, gnorm=0.333, clip=0, train_wall=36, gb_free=73.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:54:57]    INFO >> epoch 017:   1972 / 5058 loss=0.107, ups=1.87, num_updates=82900, lr=0, gnorm=0.924, clip=0, train_wall=50, gb_free=68 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:56:05]    INFO >> epoch 017:   2072 / 5058 loss=0.06, ups=1.55, num_updates=83000, lr=0, gnorm=0.527, clip=0, train_wall=60, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:57:13]    INFO >> epoch 017:   2172 / 5058 loss=0.259, ups=1.49, num_updates=83100, lr=0, gnorm=2.529, clip=0, train_wall=62, gb_free=65.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:58:21]    INFO >> epoch 017:   2272 / 5058 loss=0.07, ups=1.55, num_updates=83200, lr=0, gnorm=0.942, clip=0, train_wall=60, gb_free=70.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:59:29]    INFO >> epoch 017:   2372 / 5058 loss=0.063, ups=1.55, num_updates=83300, lr=0, gnorm=0.478, clip=0, train_wall=59, gb_free=68.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:00:39]    INFO >> epoch 017:   2472 / 5058 loss=0.11, ups=1.49, num_updates=83400, lr=0, gnorm=1.061, clip=0, train_wall=61, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:01:45]    INFO >> epoch 017:   2572 / 5058 loss=0.106, ups=1.59, num_updates=83500, lr=0, gnorm=1.244, clip=0, train_wall=58, gb_free=69.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:02:42]    INFO >> epoch 017:   2672 / 5058 loss=0.081, ups=1.83, num_updates=83600, lr=0, gnorm=0.737, clip=0, train_wall=51, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:03:47]    INFO >> epoch 017:   2772 / 5058 loss=0.111, ups=1.58, num_updates=83700, lr=0, gnorm=0.766, clip=0, train_wall=58, gb_free=64.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:04:29]    INFO >> epoch 017:   2872 / 5058 loss=0.372, ups=2.49, num_updates=83800, lr=0, gnorm=0.326, clip=0, train_wall=37, gb_free=70.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:05:35]    INFO >> epoch 017:   2972 / 5058 loss=0.159, ups=1.59, num_updates=83900, lr=0, gnorm=1.539, clip=0, train_wall=58, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:06:36]    INFO >> epoch 017:   3072 / 5058 loss=0.064, ups=1.72, num_updates=84000, lr=0, gnorm=0.427, clip=0, train_wall=54, gb_free=69.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:07:37]    INFO >> epoch 017:   3172 / 5058 loss=0.087, ups=1.72, num_updates=84100, lr=0, gnorm=0.69, clip=0, train_wall=54, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:08:30]    INFO >> epoch 017:   3272 / 5058 loss=0.116, ups=1.88, num_updates=84200, lr=0, gnorm=0.536, clip=0, train_wall=49, gb_free=70.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:09:43]    INFO >> epoch 017:   3372 / 5058 loss=0.129, ups=1.45, num_updates=84300, lr=0, gnorm=2.287, clip=0, train_wall=63, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:10:53]    INFO >> epoch 017:   3472 / 5058 loss=0.207, ups=1.48, num_updates=84400, lr=0, gnorm=1.521, clip=0, train_wall=61, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:11:59]    INFO >> epoch 017:   3572 / 5058 loss=0.089, ups=1.56, num_updates=84500, lr=0, gnorm=1.059, clip=0, train_wall=59, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:13:04]    INFO >> epoch 017:   3672 / 5058 loss=0.072, ups=1.62, num_updates=84600, lr=0, gnorm=0.966, clip=0, train_wall=57, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:14:13]    INFO >> epoch 017:   3772 / 5058 loss=0.056, ups=1.5, num_updates=84700, lr=0, gnorm=0.703, clip=0, train_wall=61, gb_free=64.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:15:08]    INFO >> epoch 017:   3872 / 5058 loss=0.099, ups=1.86, num_updates=84800, lr=0, gnorm=0.698, clip=0, train_wall=49, gb_free=71.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:16:09]    INFO >> epoch 017:   3972 / 5058 loss=0.129, ups=1.77, num_updates=84900, lr=0, gnorm=1.095, clip=0, train_wall=52, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:17:14]    INFO >> epoch 017:   4072 / 5058 loss=0.207, ups=1.6, num_updates=85000, lr=0, gnorm=0.445, clip=0, train_wall=57, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:18:24]    INFO >> epoch 017:   4172 / 5058 loss=0.145, ups=1.49, num_updates=85100, lr=0, gnorm=1.222, clip=0, train_wall=61, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:19:27]    INFO >> epoch 017:   4272 / 5058 loss=0.104, ups=1.65, num_updates=85200, lr=0, gnorm=1.104, clip=0, train_wall=56, gb_free=77.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:20:29]    INFO >> epoch 017:   4372 / 5058 loss=0.084, ups=1.69, num_updates=85300, lr=0, gnorm=1.065, clip=0, train_wall=54, gb_free=64 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:21:36]    INFO >> epoch 017:   4472 / 5058 loss=0.07, ups=1.57, num_updates=85400, lr=0, gnorm=0.789, clip=0, train_wall=59, gb_free=74.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:22:40]    INFO >> epoch 017:   4572 / 5058 loss=0.12, ups=1.59, num_updates=85500, lr=0, gnorm=0.643, clip=0, train_wall=58, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:23:54]    INFO >> epoch 017:   4672 / 5058 loss=0.216, ups=1.43, num_updates=85600, lr=0, gnorm=2.392, clip=0, train_wall=64, gb_free=65.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:24:58]    INFO >> epoch 017:   4772 / 5058 loss=0.079, ups=1.63, num_updates=85700, lr=0, gnorm=1.17, clip=0, train_wall=56, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:25:47]    INFO >> epoch 017:   4872 / 5058 loss=0.035, ups=2.08, num_updates=85800, lr=0, gnorm=0.556, clip=0, train_wall=44, gb_free=77.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:26:47]    INFO >> epoch 017:   4972 / 5058 loss=0.075, ups=1.76, num_updates=85900, lr=0, gnorm=0.971, clip=0, train_wall=52, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:27:49]    INFO >> epoch 017 | loss 0.087 | ups 1.48 | num_updates 85986 | lr 0 | gnorm 0.99 | clip 0 | train_wall 2777 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-21 15:27:49] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:34:45]    INFO >> epoch 017 | valid on 'valid' subset | loss 0.112 | num_updates 85986 | best_loss 0.112 (progress_bar.py:267, print())[0m
[32m[2025-11-21 15:34:47]    INFO >> saved checkpoint /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints/checkpoint17.pt (epoch 17 @ 85986 updates, score 0.112) (writing took 1.958442 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 15:34:47] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:34:57]    INFO >> epoch 018:     14 / 5058 loss=0.175, ups=0.21, num_updates=86000, lr=0, gnorm=2.244, clip=0, train_wall=64, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:35:39]    INFO >> epoch 018:    114 / 5058 loss=0.402, ups=2.55, num_updates=86100, lr=0, gnorm=0.804, clip=0, train_wall=37, gb_free=65.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:36:32]    INFO >> epoch 018:    214 / 5058 loss=0.132, ups=1.95, num_updates=86200, lr=0, gnorm=0.496, clip=0, train_wall=47, gb_free=64.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:37:36]    INFO >> epoch 018:    314 / 5058 loss=0.108, ups=1.62, num_updates=86300, lr=0, gnorm=0.368, clip=0, train_wall=58, gb_free=65.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:38:44]    INFO >> epoch 018:    414 / 5058 loss=0.264, ups=1.54, num_updates=86400, lr=0, gnorm=1.792, clip=0, train_wall=60, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:39:48]    INFO >> epoch 018:    514 / 5058 loss=0.144, ups=1.64, num_updates=86500, lr=0, gnorm=1.529, clip=0, train_wall=56, gb_free=69.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:40:51]    INFO >> epoch 018:    614 / 5058 loss=0.064, ups=1.65, num_updates=86600, lr=0, gnorm=0.995, clip=0, train_wall=56, gb_free=64 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:41:32]    INFO >> epoch 018:    714 / 5058 loss=0.157, ups=2.51, num_updates=86700, lr=0, gnorm=1.02, clip=0, train_wall=37, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:42:20]    INFO >> epoch 018:    814 / 5058 loss=0.109, ups=2.21, num_updates=86800, lr=0, gnorm=0.446, clip=0, train_wall=42, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:43:30]    INFO >> epoch 018:    914 / 5058 loss=0.164, ups=1.49, num_updates=86900, lr=0, gnorm=2.214, clip=0, train_wall=61, gb_free=63.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:44:37]    INFO >> epoch 018:   1014 / 5058 loss=0.093, ups=1.56, num_updates=87000, lr=0, gnorm=0.211, clip=0, train_wall=59, gb_free=64.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:45:36]    INFO >> epoch 018:   1114 / 5058 loss=0.044, ups=1.74, num_updates=87100, lr=0, gnorm=0.785, clip=0, train_wall=53, gb_free=65.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:46:42]    INFO >> epoch 018:   1214 / 5058 loss=0.036, ups=1.55, num_updates=87200, lr=0, gnorm=0.274, clip=0, train_wall=59, gb_free=67 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:47:50]    INFO >> epoch 018:   1314 / 5058 loss=0.071, ups=1.54, num_updates=87300, lr=0, gnorm=0.602, clip=0, train_wall=59, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:48:56]    INFO >> epoch 018:   1414 / 5058 loss=0.058, ups=1.58, num_updates=87400, lr=0, gnorm=0.896, clip=0, train_wall=58, gb_free=64 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:50:04]    INFO >> epoch 018:   1514 / 5058 loss=0.225, ups=1.54, num_updates=87500, lr=0, gnorm=0.98, clip=0, train_wall=59, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:51:07]    INFO >> epoch 018:   1614 / 5058 loss=0.086, ups=1.65, num_updates=87600, lr=0, gnorm=1.06, clip=0, train_wall=55, gb_free=64.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:52:14]    INFO >> epoch 018:   1714 / 5058 loss=0.12, ups=1.56, num_updates=87700, lr=0, gnorm=0.843, clip=0, train_wall=59, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:53:08]    INFO >> epoch 018:   1814 / 5058 loss=0.089, ups=1.95, num_updates=87800, lr=0, gnorm=0.811, clip=0, train_wall=47, gb_free=68 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:53:52]    INFO >> epoch 018:   1914 / 5058 loss=0.162, ups=2.32, num_updates=87900, lr=0, gnorm=1.193, clip=0, train_wall=40, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:54:58]    INFO >> epoch 018:   2014 / 5058 loss=0.073, ups=1.58, num_updates=88000, lr=0, gnorm=0.285, clip=0, train_wall=58, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:56:07]    INFO >> epoch 018:   2114 / 5058 loss=0.074, ups=1.5, num_updates=88100, lr=0, gnorm=1.365, clip=0, train_wall=61, gb_free=63.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:57:15]    INFO >> epoch 018:   2214 / 5058 loss=0.188, ups=1.53, num_updates=88200, lr=0, gnorm=1.794, clip=0, train_wall=60, gb_free=64.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:58:25]    INFO >> epoch 018:   2314 / 5058 loss=0.059, ups=1.5, num_updates=88300, lr=0, gnorm=0.744, clip=0, train_wall=61, gb_free=65 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:59:32]    INFO >> epoch 018:   2414 / 5058 loss=0.07, ups=1.56, num_updates=88400, lr=0, gnorm=0.382, clip=0, train_wall=58, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:00:44]    INFO >> epoch 018:   2514 / 5058 loss=0.155, ups=1.42, num_updates=88500, lr=0, gnorm=1.784, clip=0, train_wall=64, gb_free=68.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:01:44]    INFO >> epoch 018:   2614 / 5058 loss=0.099, ups=1.77, num_updates=88600, lr=0, gnorm=1.024, clip=0, train_wall=52, gb_free=70 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:02:47]    INFO >> epoch 018:   2714 / 5058 loss=0.089, ups=1.65, num_updates=88700, lr=0, gnorm=0.462, clip=0, train_wall=55, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:03:47]    INFO >> epoch 018:   2814 / 5058 loss=0.067, ups=1.73, num_updates=88800, lr=0, gnorm=0.472, clip=0, train_wall=53, gb_free=68.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:04:31]    INFO >> epoch 018:   2914 / 5058 loss=0.199, ups=2.36, num_updates=88900, lr=0, gnorm=1.128, clip=0, train_wall=39, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:05:40]    INFO >> epoch 018:   3014 / 5058 loss=0.138, ups=1.51, num_updates=89000, lr=0, gnorm=1.048, clip=0, train_wall=61, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:06:36]    INFO >> epoch 018:   3114 / 5058 loss=0.062, ups=1.86, num_updates=89100, lr=0, gnorm=0.387, clip=0, train_wall=50, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:07:35]    INFO >> epoch 018:   3214 / 5058 loss=0.353, ups=1.79, num_updates=89200, lr=0, gnorm=0.708, clip=0, train_wall=51, gb_free=63.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:08:37]    INFO >> epoch 018:   3314 / 5058 loss=0.087, ups=1.64, num_updates=89300, lr=0, gnorm=0.998, clip=0, train_wall=56, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:09:50]    INFO >> epoch 018:   3414 / 5058 loss=0.231, ups=1.44, num_updates=89400, lr=0, gnorm=2.479, clip=0, train_wall=63, gb_free=77.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:10:58]    INFO >> epoch 018:   3514 / 5058 loss=0.123, ups=1.53, num_updates=89500, lr=0, gnorm=0.749, clip=0, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:12:06]    INFO >> epoch 018:   3614 / 5058 loss=0.072, ups=1.53, num_updates=89600, lr=0, gnorm=1.058, clip=0, train_wall=60, gb_free=65 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:13:08]    INFO >> epoch 018:   3714 / 5058 loss=0.113, ups=1.69, num_updates=89700, lr=0, gnorm=1.043, clip=0, train_wall=55, gb_free=63.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:14:17]    INFO >> epoch 018:   3814 / 5058 loss=0.055, ups=1.5, num_updates=89800, lr=0, gnorm=0.64, clip=0, train_wall=61, gb_free=63.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:15:03]    INFO >> epoch 018:   3914 / 5058 loss=0.11, ups=2.26, num_updates=89900, lr=0, gnorm=1.033, clip=0, train_wall=41, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:16:10]    INFO >> epoch 018:   4014 / 5058 loss=0.161, ups=1.55, num_updates=90000, lr=0, gnorm=0.71, clip=0, train_wall=59, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:17:17]    INFO >> epoch 018:   4114 / 5058 loss=0.308, ups=1.56, num_updates=90100, lr=0, gnorm=0.55, clip=0, train_wall=59, gb_free=67.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:18:24]    INFO >> epoch 018:   4214 / 5058 loss=0.138, ups=1.56, num_updates=90200, lr=0, gnorm=1.697, clip=0, train_wall=59, gb_free=75.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:19:26]    INFO >> epoch 018:   4314 / 5058 loss=0.11, ups=1.68, num_updates=90300, lr=0, gnorm=0.771, clip=0, train_wall=55, gb_free=76.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:20:31]    INFO >> epoch 018:   4414 / 5058 loss=0.07, ups=1.61, num_updates=90400, lr=0, gnorm=0.975, clip=0, train_wall=57, gb_free=67.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:21:35]    INFO >> epoch 018:   4514 / 5058 loss=0.07, ups=1.62, num_updates=90500, lr=0, gnorm=0.731, clip=0, train_wall=57, gb_free=67.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:22:45]    INFO >> epoch 018:   4614 / 5058 loss=0.195, ups=1.48, num_updates=90600, lr=0, gnorm=1.366, clip=0, train_wall=62, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:23:58]    INFO >> epoch 018:   4714 / 5058 loss=0.188, ups=1.44, num_updates=90700, lr=0, gnorm=2.179, clip=0, train_wall=63, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:24:59]    INFO >> epoch 018:   4814 / 5058 loss=0.046, ups=1.69, num_updates=90800, lr=0, gnorm=0.599, clip=0, train_wall=54, gb_free=63.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:25:43]    INFO >> epoch 018:   4914 / 5058 loss=0.107, ups=2.34, num_updates=90900, lr=0, gnorm=0.681, clip=0, train_wall=39, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:26:52]    INFO >> epoch 018:   5014 / 5058 loss=0.096, ups=1.52, num_updates=91000, lr=0, gnorm=1.723, clip=0, train_wall=60, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:27:24]    INFO >> epoch 018 | loss 0.087 | ups 1.47 | num_updates 91044 | lr 0 | gnorm 0.99 | clip 0 | train_wall 2779 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-21 16:27:24] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:34:14]    INFO >> epoch 018 | valid on 'valid' subset | loss 0.112 | num_updates 91044 | best_loss 0.112 (progress_bar.py:267, print())[0m
[32m[2025-11-21 16:34:15]    INFO >> saved checkpoint /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints/checkpoint18.pt (epoch 18 @ 91044 updates, score 0.112) (writing took 1.833469 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 16:34:16] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:34:43]    INFO >> epoch 019:     56 / 5058 loss=0.205, ups=0.22, num_updates=91100, lr=0, gnorm=1.59, clip=0, train_wall=51, gb_free=69.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:35:29]    INFO >> epoch 019:    156 / 5058 loss=0.141, ups=2.22, num_updates=91200, lr=0, gnorm=0.623, clip=0, train_wall=42, gb_free=72.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:36:29]    INFO >> epoch 019:    256 / 5058 loss=0.196, ups=1.74, num_updates=91300, lr=0, gnorm=0.291, clip=0, train_wall=53, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:37:34]    INFO >> epoch 019:    356 / 5058 loss=0.156, ups=1.6, num_updates=91400, lr=0, gnorm=0.796, clip=0, train_wall=57, gb_free=69.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:38:44]    INFO >> epoch 019:    456 / 5058 loss=0.232, ups=1.5, num_updates=91500, lr=0, gnorm=2.3, clip=0, train_wall=61, gb_free=71.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:39:48]    INFO >> epoch 019:    556 / 5058 loss=0.067, ups=1.63, num_updates=91600, lr=0, gnorm=1.09, clip=0, train_wall=56, gb_free=64.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:40:44]    INFO >> epoch 019:    656 / 5058 loss=0.08, ups=1.88, num_updates=91700, lr=0, gnorm=0.79, clip=0, train_wall=49, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:41:23]    INFO >> epoch 019:    756 / 5058 loss=0.144, ups=2.66, num_updates=91800, lr=0, gnorm=0.993, clip=0, train_wall=35, gb_free=64.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:42:21]    INFO >> epoch 019:    856 / 5058 loss=0.159, ups=1.79, num_updates=91900, lr=0, gnorm=1.531, clip=0, train_wall=51, gb_free=69.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:43:30]    INFO >> epoch 019:    956 / 5058 loss=0.119, ups=1.51, num_updates=92000, lr=0, gnorm=0.949, clip=0, train_wall=60, gb_free=70.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:44:34]    INFO >> epoch 019:   1056 / 5058 loss=0.108, ups=1.63, num_updates=92100, lr=0, gnorm=0.751, clip=0, train_wall=56, gb_free=63.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:45:36]    INFO >> epoch 019:   1156 / 5058 loss=0.035, ups=1.66, num_updates=92200, lr=0, gnorm=0.213, clip=0, train_wall=55, gb_free=63.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:46:43]    INFO >> epoch 019:   1256 / 5058 loss=0.052, ups=1.54, num_updates=92300, lr=0, gnorm=0.571, clip=0, train_wall=59, gb_free=66.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:47:48]    INFO >> epoch 019:   1356 / 5058 loss=0.056, ups=1.61, num_updates=92400, lr=0, gnorm=0.817, clip=0, train_wall=57, gb_free=67.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:48:57]    INFO >> epoch 019:   1456 / 5058 loss=0.161, ups=1.51, num_updates=92500, lr=0, gnorm=0.768, clip=0, train_wall=60, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:50:02]    INFO >> epoch 019:   1556 / 5058 loss=0.078, ups=1.61, num_updates=92600, lr=0, gnorm=0.951, clip=0, train_wall=57, gb_free=64.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:51:07]    INFO >> epoch 019:   1656 / 5058 loss=0.144, ups=1.6, num_updates=92700, lr=0, gnorm=1.096, clip=0, train_wall=57, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:52:14]    INFO >> epoch 019:   1756 / 5058 loss=0.104, ups=1.55, num_updates=92800, lr=0, gnorm=0.883, clip=0, train_wall=59, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:52:55]    INFO >> epoch 019:   1856 / 5058 loss=0.086, ups=2.56, num_updates=92900, lr=0, gnorm=0.612, clip=0, train_wall=36, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:53:49]    INFO >> epoch 019:   1956 / 5058 loss=0.107, ups=1.92, num_updates=93000, lr=0, gnorm=0.924, clip=0, train_wall=48, gb_free=70.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:54:56]    INFO >> epoch 019:   2056 / 5058 loss=0.066, ups=1.55, num_updates=93100, lr=0, gnorm=0.446, clip=0, train_wall=59, gb_free=69.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:56:08]    INFO >> epoch 019:   2156 / 5058 loss=0.126, ups=1.45, num_updates=93200, lr=0, gnorm=2.288, clip=0, train_wall=62, gb_free=66.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:57:16]    INFO >> epoch 019:   2256 / 5058 loss=0.071, ups=1.54, num_updates=93300, lr=0, gnorm=0.964, clip=0, train_wall=59, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:58:23]    INFO >> epoch 019:   2356 / 5058 loss=0.072, ups=1.53, num_updates=93400, lr=0, gnorm=0.779, clip=0, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:59:32]    INFO >> epoch 019:   2456 / 5058 loss=0.097, ups=1.52, num_updates=93500, lr=0, gnorm=0.849, clip=0, train_wall=60, gb_free=64.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:00:39]    INFO >> epoch 019:   2556 / 5058 loss=0.149, ups=1.52, num_updates=93600, lr=0, gnorm=1.221, clip=0, train_wall=60, gb_free=68.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:01:37]    INFO >> epoch 019:   2656 / 5058 loss=0.071, ups=1.85, num_updates=93700, lr=0, gnorm=0.869, clip=0, train_wall=50, gb_free=71.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:02:44]    INFO >> epoch 019:   2756 / 5058 loss=0.118, ups=1.56, num_updates=93800, lr=0, gnorm=0.869, clip=0, train_wall=59, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:03:28]    INFO >> epoch 019:   2856 / 5058 loss=1.83, ups=2.33, num_updates=93900, lr=0, gnorm=0.18, clip=0, train_wall=40, gb_free=75.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:04:30]    INFO >> epoch 019:   2956 / 5058 loss=0.173, ups=1.7, num_updates=94000, lr=0, gnorm=1.517, clip=0, train_wall=54, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:05:33]    INFO >> epoch 019:   3056 / 5058 loss=0.071, ups=1.64, num_updates=94100, lr=0, gnorm=0.572, clip=0, train_wall=56, gb_free=63.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:06:32]    INFO >> epoch 019:   3156 / 5058 loss=0.073, ups=1.77, num_updates=94200, lr=0, gnorm=0.713, clip=0, train_wall=52, gb_free=68.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:07:27]    INFO >> epoch 019:   3256 / 5058 loss=0.177, ups=1.88, num_updates=94300, lr=0, gnorm=0.457, clip=0, train_wall=49, gb_free=64.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:08:37]    INFO >> epoch 019:   3356 / 5058 loss=0.109, ups=1.5, num_updates=94400, lr=0, gnorm=2.007, clip=0, train_wall=61, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:09:49]    INFO >> epoch 019:   3456 / 5058 loss=0.218, ups=1.46, num_updates=94500, lr=0, gnorm=1.881, clip=0, train_wall=62, gb_free=64.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:10:56]    INFO >> epoch 019:   3556 / 5058 loss=0.083, ups=1.56, num_updates=94600, lr=0, gnorm=0.944, clip=0, train_wall=59, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:12:01]    INFO >> epoch 019:   3656 / 5058 loss=0.076, ups=1.6, num_updates=94700, lr=0, gnorm=0.714, clip=0, train_wall=57, gb_free=74.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:13:09]    INFO >> epoch 019:   3756 / 5058 loss=0.058, ups=1.53, num_updates=94800, lr=0, gnorm=0.978, clip=0, train_wall=60, gb_free=65.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:14:09]    INFO >> epoch 019:   3856 / 5058 loss=0.086, ups=1.7, num_updates=94900, lr=0, gnorm=0.75, clip=0, train_wall=54, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:15:05]    INFO >> epoch 019:   3956 / 5058 loss=0.128, ups=1.88, num_updates=95000, lr=0, gnorm=0.929, clip=0, train_wall=49, gb_free=65.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:16:09]    INFO >> epoch 019:   4056 / 5058 loss=0.171, ups=1.64, num_updates=95100, lr=0, gnorm=0.489, clip=0, train_wall=56, gb_free=64 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:17:19]    INFO >> epoch 019:   4156 / 5058 loss=0.144, ups=1.49, num_updates=95200, lr=0, gnorm=1.053, clip=0, train_wall=61, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:18:23]    INFO >> epoch 019:   4256 / 5058 loss=0.122, ups=1.63, num_updates=95300, lr=0, gnorm=1.402, clip=0, train_wall=56, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:19:23]    INFO >> epoch 019:   4356 / 5058 loss=0.076, ups=1.73, num_updates=95400, lr=0, gnorm=0.907, clip=0, train_wall=53, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:20:32]    INFO >> epoch 019:   4456 / 5058 loss=0.103, ups=1.52, num_updates=95500, lr=0, gnorm=0.835, clip=0, train_wall=60, gb_free=74 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:21:35]    INFO >> epoch 019:   4556 / 5058 loss=0.059, ups=1.66, num_updates=95600, lr=0, gnorm=0.418, clip=0, train_wall=56, gb_free=64.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:22:46]    INFO >> epoch 019:   4656 / 5058 loss=0.209, ups=1.45, num_updates=95700, lr=0, gnorm=2.337, clip=0, train_wall=64, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:23:52]    INFO >> epoch 019:   4756 / 5058 loss=0.073, ups=1.59, num_updates=95800, lr=0, gnorm=1.406, clip=0, train_wall=59, gb_free=65.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:24:41]    INFO >> epoch 019:   4856 / 5058 loss=0.048, ups=2.11, num_updates=95900, lr=0, gnorm=0.64, clip=0, train_wall=45, gb_free=75.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:25:36]    INFO >> epoch 019:   4956 / 5058 loss=0.075, ups=1.89, num_updates=96000, lr=0, gnorm=0.81, clip=0, train_wall=49, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:26:47]    INFO >> epoch 019:   5056 / 5058 loss=0.17, ups=1.47, num_updates=96100, lr=0, gnorm=2.343, clip=0, train_wall=63, gb_free=64.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:26:48]    INFO >> epoch 019 | loss 0.087 | ups 1.48 | num_updates 96102 | lr 0 | gnorm 0.99 | clip 0 | train_wall 2780 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-21 17:26:48] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 17:33:36]    INFO >> epoch 019 | valid on 'valid' subset | loss 0.112 | num_updates 96102 | best_loss 0.112 (progress_bar.py:267, print())[0m
[32m[2025-11-21 17:33:38]    INFO >> saved checkpoint /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints/checkpoint19.pt (epoch 19 @ 96102 updates, score 0.112) (writing took 1.708493 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 17:33:38] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 17:34:23]    INFO >> epoch 020:     98 / 5058 loss=0.347, ups=0.23, num_updates=96200, lr=0, gnorm=0.913, clip=0, train_wall=41, gb_free=78.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:35:13]    INFO >> epoch 020:    198 / 5058 loss=0.133, ups=2.11, num_updates=96300, lr=0, gnorm=0.481, clip=0, train_wall=44, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:36:20]    INFO >> epoch 020:    298 / 5058 loss=0.151, ups=1.56, num_updates=96400, lr=0, gnorm=0.433, clip=0, train_wall=59, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:37:27]    INFO >> epoch 020:    398 / 5058 loss=0.213, ups=1.54, num_updates=96500, lr=0, gnorm=1.413, clip=0, train_wall=60, gb_free=66.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:38:32]    INFO >> epoch 020:    498 / 5058 loss=0.158, ups=1.61, num_updates=96600, lr=0, gnorm=1.583, clip=0, train_wall=57, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:39:36]    INFO >> epoch 020:    598 / 5058 loss=0.066, ups=1.64, num_updates=96700, lr=0, gnorm=1.248, clip=0, train_wall=56, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:40:20]    INFO >> epoch 020:    698 / 5058 loss=0.142, ups=2.34, num_updates=96800, lr=0, gnorm=0.722, clip=0, train_wall=39, gb_free=64.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:41:03]    INFO >> epoch 020:    798 / 5058 loss=0.137, ups=2.37, num_updates=96900, lr=0, gnorm=0.795, clip=0, train_wall=39, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:42:13]    INFO >> epoch 020:    898 / 5058 loss=0.167, ups=1.49, num_updates=97000, lr=0, gnorm=2.064, clip=0, train_wall=61, gb_free=66.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:43:20]    INFO >> epoch 020:    998 / 5058 loss=0.099, ups=1.55, num_updates=97100, lr=0, gnorm=0.39, clip=0, train_wall=59, gb_free=64.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:44:23]    INFO >> epoch 020:   1098 / 5058 loss=0.051, ups=1.68, num_updates=97200, lr=0, gnorm=0.767, clip=0, train_wall=54, gb_free=70.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:45:30]    INFO >> epoch 020:   1198 / 5058 loss=0.032, ups=1.56, num_updates=97300, lr=0, gnorm=0.2, clip=0, train_wall=58, gb_free=66.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:46:37]    INFO >> epoch 020:   1298 / 5058 loss=0.086, ups=1.55, num_updates=97400, lr=0, gnorm=0.686, clip=0, train_wall=59, gb_free=65.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:47:43]    INFO >> epoch 020:   1398 / 5058 loss=0.053, ups=1.58, num_updates=97500, lr=0, gnorm=0.787, clip=0, train_wall=58, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:48:50]    INFO >> epoch 020:   1498 / 5058 loss=0.24, ups=1.54, num_updates=97600, lr=0, gnorm=0.617, clip=0, train_wall=59, gb_free=64.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:49:54]    INFO >> epoch 020:   1598 / 5058 loss=0.078, ups=1.66, num_updates=97700, lr=0, gnorm=1.308, clip=0, train_wall=55, gb_free=68.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:51:03]    INFO >> epoch 020:   1698 / 5058 loss=0.124, ups=1.51, num_updates=97800, lr=0, gnorm=0.81, clip=0, train_wall=60, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:52:00]    INFO >> epoch 020:   1798 / 5058 loss=0.1, ups=1.83, num_updates=97900, lr=0, gnorm=1.059, clip=0, train_wall=50, gb_free=67.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:52:41]    INFO >> epoch 020:   1898 / 5058 loss=0.14, ups=2.5, num_updates=98000, lr=0, gnorm=0.697, clip=0, train_wall=37, gb_free=76.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:53:45]    INFO >> epoch 020:   1998 / 5058 loss=0.091, ups=1.63, num_updates=98100, lr=0, gnorm=0.542, clip=0, train_wall=56, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:54:54]    INFO >> epoch 020:   2098 / 5058 loss=0.073, ups=1.5, num_updates=98200, lr=0, gnorm=1.473, clip=0, train_wall=60, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:56:03]    INFO >> epoch 020:   2198 / 5058 loss=0.256, ups=1.51, num_updates=98300, lr=0, gnorm=1.789, clip=0, train_wall=60, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:57:14]    INFO >> epoch 020:   2298 / 5058 loss=0.062, ups=1.48, num_updates=98400, lr=0, gnorm=0.896, clip=0, train_wall=61, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:58:22]    INFO >> epoch 020:   2398 / 5058 loss=0.064, ups=1.54, num_updates=98500, lr=0, gnorm=0.316, clip=0, train_wall=59, gb_free=70.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 17:59:34]    INFO >> epoch 020:   2498 / 5058 loss=0.127, ups=1.43, num_updates=98600, lr=0, gnorm=1.472, clip=0, train_wall=63, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:00:34]    INFO >> epoch 020:   2598 / 5058 loss=0.108, ups=1.74, num_updates=98700, lr=0, gnorm=1.128, clip=0, train_wall=53, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:01:38]    INFO >> epoch 020:   2698 / 5058 loss=0.087, ups=1.65, num_updates=98800, lr=0, gnorm=0.656, clip=0, train_wall=55, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:02:42]    INFO >> epoch 020:   2798 / 5058 loss=0.087, ups=1.61, num_updates=98900, lr=0, gnorm=0.552, clip=0, train_wall=57, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:03:24]    INFO >> epoch 020:   2898 / 5058 loss=0.264, ups=2.5, num_updates=99000, lr=0, gnorm=0.774, clip=0, train_wall=37, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:04:32]    INFO >> epoch 020:   2998 / 5058 loss=0.136, ups=1.51, num_updates=99100, lr=0, gnorm=1.402, clip=0, train_wall=60, gb_free=65 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:05:29]    INFO >> epoch 020:   3098 / 5058 loss=0.057, ups=1.85, num_updates=99200, lr=0, gnorm=0.147, clip=0, train_wall=50, gb_free=71.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:06:29]    INFO >> epoch 020:   3198 / 5058 loss=0.251, ups=1.74, num_updates=99300, lr=0, gnorm=0.949, clip=0, train_wall=52, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:07:32]    INFO >> epoch 020:   3298 / 5058 loss=0.078, ups=1.66, num_updates=99400, lr=0, gnorm=0.681, clip=0, train_wall=55, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:08:45]    INFO >> epoch 020:   3398 / 5058 loss=0.224, ups=1.4, num_updates=99500, lr=0, gnorm=2.545, clip=0, train_wall=64, gb_free=65.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:09:51]    INFO >> epoch 020:   3498 / 5058 loss=0.176, ups=1.53, num_updates=99600, lr=0, gnorm=0.83, clip=0, train_wall=60, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:11:03]    INFO >> epoch 020:   3598 / 5058 loss=0.072, ups=1.52, num_updates=99700, lr=0, gnorm=1.199, clip=0, train_wall=60, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:12:07]    INFO >> epoch 020:   3698 / 5058 loss=0.131, ups=1.63, num_updates=99800, lr=0, gnorm=0.936, clip=0, train_wall=56, gb_free=73 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:13:17]    INFO >> epoch 020:   3798 / 5058 loss=0.055, ups=1.5, num_updates=99900, lr=0, gnorm=0.632, clip=0, train_wall=60, gb_free=68.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:14:08]    INFO >> epoch 020:   3898 / 5058 loss=0.136, ups=2.01, num_updates=100000, lr=0, gnorm=0.997, clip=0, train_wall=46, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:15:11]    INFO >> epoch 020:   3998 / 5058 loss=0.115, ups=1.65, num_updates=100100, lr=0, gnorm=0.889, clip=0, train_wall=56, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:16:15]    INFO >> epoch 020:   4098 / 5058 loss=0.721, ups=1.63, num_updates=100200, lr=0, gnorm=0.459, clip=0, train_wall=57, gb_free=63.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:17:23]    INFO >> epoch 020:   4198 / 5058 loss=0.138, ups=1.53, num_updates=100300, lr=0, gnorm=1.216, clip=0, train_wall=60, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:18:23]    INFO >> epoch 020:   4298 / 5058 loss=0.109, ups=1.71, num_updates=100400, lr=0, gnorm=1.122, clip=0, train_wall=54, gb_free=65.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:19:26]    INFO >> epoch 020:   4398 / 5058 loss=0.074, ups=1.69, num_updates=100500, lr=0, gnorm=0.906, clip=0, train_wall=55, gb_free=64 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:20:31]    INFO >> epoch 020:   4498 / 5058 loss=0.071, ups=1.63, num_updates=100600, lr=0, gnorm=0.992, clip=0, train_wall=57, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:21:38]    INFO >> epoch 020:   4598 / 5058 loss=0.177, ups=1.55, num_updates=100700, lr=0, gnorm=0.94, clip=0, train_wall=60, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:22:50]    INFO >> epoch 020:   4698 / 5058 loss=0.223, ups=1.44, num_updates=100800, lr=0, gnorm=2.398, clip=0, train_wall=64, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:23:53]    INFO >> epoch 020:   4798 / 5058 loss=0.047, ups=1.66, num_updates=100900, lr=0, gnorm=0.815, clip=0, train_wall=56, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:24:34]    INFO >> epoch 020:   4898 / 5058 loss=0.121, ups=2.52, num_updates=101000, lr=0, gnorm=0.47, clip=0, train_wall=37, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:25:39]    INFO >> epoch 020:   4998 / 5058 loss=0.087, ups=1.59, num_updates=101100, lr=0, gnorm=1.535, clip=0, train_wall=59, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:26:21]    INFO >> epoch 020 | loss 0.087 | ups 1.47 | num_updates 101160 | lr 0 | gnorm 0.99 | clip 0 | train_wall 2779 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-21 18:26:21] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 18:33:10]    INFO >> epoch 020 | valid on 'valid' subset | loss 0.112 | num_updates 101160 | best_loss 0.112 (progress_bar.py:267, print())[0m
[32m[2025-11-21 18:33:11]    INFO >> saved checkpoint /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints/checkpoint20.pt (epoch 20 @ 101160 updates, score 0.112) (writing took 1.498556 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[32m[2025-11-21 18:33:11]    INFO >> early stop since valid performance hasn't improved for last 10 runs (train.py:182, should_stop_early())[0m
[32m[2025-11-21 18:33:11]    INFO >> early stop since valid performance hasn't improved for last 10 runs (train.py:267, single_main())[0m
[32m[2025-11-21 18:33:11]    INFO >> done training in 68305.6 seconds (train.py:279, single_main())[0m
CodeEncoderLSTM: Creating BiLSTM with 6 layers, 512 hidden and input size
/bin/sh: 1: ncc-eval: not found

============================================================
Setting up experiment: exp_baseline
============================================================
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/logs
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/results
âœ“ Saved config to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/config.yml
âœ“ Also saved to: run/type_prediction/transformer/config/exp_baseline.yml
âœ“ Saved experiment info to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/info.txt

============================================================
Starting training...
============================================================

Command: python run/type_prediction/transformer/train.py --language exp_baseline 2>&1 | tee /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/logs/train.log

Note: Config file will be loaded from run/type_prediction/transformer/config/exp_baseline.yml


âœ“ Training completed successfully!

============================================================
Evaluating model on test set...
============================================================

Command: ncc-eval --configs /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/config.yml 2>&1 | tee /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/logs/eval.log


âœ“ Evaluation completed!
âœ“ Results saved to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/results/test_results.txt

============================================================
âœ“ Experiment completed: exp_baseline
âœ“ Results in: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline
============================================================


âœ“ exp_baseline completed in 19.77 hours

============================================================
Running: exp_d_model_256
GPU: 3
Time: 2025-11-21T18:33:14.248655
============================================================
[32m[2025-11-21 18:33:15]    INFO >> Load arguments in run/type_prediction/transformer/config/exp_d_model_256.yml (train.py:300, cli_main())[0m
[32m[2025-11-21 18:33:15]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 256, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:302, cli_main())[0m
[32m[2025-11-21 18:33:15]    INFO >> single GPU training... (train.py:331, cli_main())[0m
[32m[2025-11-21 18:33:16]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 256, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:208, single_main())[0m
[32m[2025-11-21 18:33:19]    INFO >> TypeTransformer(
  (decoder): RobertaEncoder(
    (encoder): CodeEncoderLSTM(
      (embedding): Embedding(10006, 512)
      (pos_encoder): PositionalEncoding()
      (encoder): LSTM(512, 512, num_layers=6, bidirectional=True)
    )
    (output): Sequential(
      (0): Linear(in_features=1024, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=10006, bias=True)
    )
  )
  (classification_heads): ModuleDict()
) (train.py:220, single_main())[0m
[32m[2025-11-21 18:33:19]    INFO >> model typetransformer, criterion TypePredictionCrossEntropyCriterion (train.py:221, single_main())[0m
[32m[2025-11-21 18:33:19]    INFO >> num. model params: 46481686 (num. trained: 46481686) (train.py:222, single_main())[0m
[32m[2025-11-21 18:33:19]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 18:33:19]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 81024 MB ; used memory = 896 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 18:33:19]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 18:33:19]    INFO >> training on 1 GPUs (train.py:229, single_main())[0m
[32m[2025-11-21 18:33:19]    INFO >> max tokens per GPU = None and max sentences per GPU = 32 (train.py:230, single_main())[0m
[32m[2025-11-21 18:33:19]    INFO >> no existing checkpoint found /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-21 18:33:19]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[33m[2025-11-21 18:33:29] WARNING >> 32222 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[0, 1, 2, 4, 6, 7, 8, 10, 14, 19] (data_utils.py:301, filter_by_size())[0m
[32m[2025-11-21 18:33:29]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-21 18:33:29] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:398: UserWarning: ntokens not found in Criterion logging outputs, cannot log wpb or wps
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:407: UserWarning: nsentences not found in Criterion logging outputs, cannot log bsz
  warnings.warn(
[32m[2025-11-21 18:34:14]    INFO >> epoch 001:    100 / 5058 loss=1.903, ups=2.33, num_updates=100, lr=1e-05, gnorm=0.104, clip=0, train_wall=41, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:35:04]    INFO >> epoch 001:    200 / 5058 loss=0.786, ups=2.13, num_updates=200, lr=2e-05, gnorm=0.068, clip=0, train_wall=44, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:36:09]    INFO >> epoch 001:    300 / 5058 loss=0.654, ups=1.6, num_updates=300, lr=3e-05, gnorm=0.053, clip=0, train_wall=59, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:37:16]    INFO >> epoch 001:    400 / 5058 loss=0.623, ups=1.56, num_updates=400, lr=4e-05, gnorm=2.819, clip=2, train_wall=60, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:38:18]    INFO >> epoch 001:    500 / 5058 loss=0.208, ups=1.65, num_updates=500, lr=5e-05, gnorm=4.476, clip=2, train_wall=57, gb_free=65.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:39:20]    INFO >> epoch 001:    600 / 5058 loss=0.079, ups=1.68, num_updates=600, lr=6e-05, gnorm=4.788, clip=8, train_wall=56, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:40:05]    INFO >> epoch 001:    700 / 5058 loss=0.174, ups=2.39, num_updates=700, lr=7e-05, gnorm=2.641, clip=4, train_wall=39, gb_free=69.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:40:47]    INFO >> epoch 001:    800 / 5058 loss=0.166, ups=2.43, num_updates=800, lr=8e-05, gnorm=3.325, clip=8, train_wall=39, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:41:55]    INFO >> epoch 001:    900 / 5058 loss=0.216, ups=1.53, num_updates=900, lr=9e-05, gnorm=6.91, clip=8, train_wall=61, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:43:01]    INFO >> epoch 001:   1000 / 5058 loss=0.142, ups=1.59, num_updates=1000, lr=0.0001, gnorm=1.451, clip=0, train_wall=59, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:44:01]    INFO >> epoch 001:   1100 / 5058 loss=0.058, ups=1.73, num_updates=1100, lr=0.0001, gnorm=2.202, clip=2, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:45:06]    INFO >> epoch 001:   1200 / 5058 loss=0.035, ups=1.61, num_updates=1200, lr=0.0001, gnorm=0.871, clip=0, train_wall=58, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:46:11]    INFO >> epoch 001:   1300 / 5058 loss=0.087, ups=1.59, num_updates=1300, lr=9.9e-05, gnorm=1.877, clip=2, train_wall=59, gb_free=67.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:47:14]    INFO >> epoch 001:   1400 / 5058 loss=0.059, ups=1.62, num_updates=1400, lr=9.9e-05, gnorm=2.488, clip=4, train_wall=58, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:48:20]    INFO >> epoch 001:   1500 / 5058 loss=0.279, ups=1.59, num_updates=1500, lr=9.9e-05, gnorm=1.776, clip=2, train_wall=59, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:49:22]    INFO >> epoch 001:   1600 / 5058 loss=0.11, ups=1.7, num_updates=1600, lr=9.9e-05, gnorm=3.243, clip=4, train_wall=55, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:50:29]    INFO >> epoch 001:   1700 / 5058 loss=0.113, ups=1.56, num_updates=1700, lr=9.9e-05, gnorm=2.319, clip=2, train_wall=60, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:51:23]    INFO >> epoch 001:   1800 / 5058 loss=0.12, ups=1.88, num_updates=1800, lr=9.8e-05, gnorm=3.115, clip=5, train_wall=50, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:52:04]    INFO >> epoch 001:   1900 / 5058 loss=0.153, ups=2.63, num_updates=1900, lr=9.8e-05, gnorm=1.952, clip=4, train_wall=36, gb_free=67.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:53:06]    INFO >> epoch 001:   2000 / 5058 loss=0.089, ups=1.67, num_updates=2000, lr=9.8e-05, gnorm=1.422, clip=1, train_wall=56, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:54:12]    INFO >> epoch 001:   2100 / 5058 loss=0.074, ups=1.55, num_updates=2100, lr=9.8e-05, gnorm=3.142, clip=2, train_wall=60, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:55:21]    INFO >> epoch 001:   2200 / 5058 loss=0.279, ups=1.55, num_updates=2200, lr=9.8e-05, gnorm=5.078, clip=6, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:56:28]    INFO >> epoch 001:   2300 / 5058 loss=0.068, ups=1.53, num_updates=2300, lr=9.7e-05, gnorm=2.452, clip=5, train_wall=61, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:57:34]    INFO >> epoch 001:   2400 / 5058 loss=0.063, ups=1.59, num_updates=2400, lr=9.7e-05, gnorm=0.773, clip=1, train_wall=59, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:58:43]    INFO >> epoch 001:   2500 / 5058 loss=0.149, ups=1.47, num_updates=2500, lr=9.7e-05, gnorm=3.718, clip=1, train_wall=63, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 18:59:43]    INFO >> epoch 001:   2600 / 5058 loss=0.128, ups=1.79, num_updates=2600, lr=9.7e-05, gnorm=2.608, clip=1, train_wall=53, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:00:43]    INFO >> epoch 001:   2700 / 5058 loss=0.076, ups=1.71, num_updates=2700, lr=9.7e-05, gnorm=1.387, clip=1, train_wall=55, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:01:46]    INFO >> epoch 001:   2800 / 5058 loss=0.081, ups=1.66, num_updates=2800, lr=9.6e-05, gnorm=1.32, clip=3, train_wall=56, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:02:28]    INFO >> epoch 001:   2900 / 5058 loss=0.274, ups=2.57, num_updates=2900, lr=9.6e-05, gnorm=2.005, clip=3, train_wall=37, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:03:34]    INFO >> epoch 001:   3000 / 5058 loss=0.145, ups=1.57, num_updates=3000, lr=9.6e-05, gnorm=3.103, clip=5, train_wall=60, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:04:28]    INFO >> epoch 001:   3100 / 5058 loss=0.058, ups=1.91, num_updates=3100, lr=9.6e-05, gnorm=0.282, clip=0, train_wall=49, gb_free=73.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:05:27]    INFO >> epoch 001:   3200 / 5058 loss=0.282, ups=1.79, num_updates=3200, lr=9.6e-05, gnorm=2.208, clip=2, train_wall=52, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:06:28]    INFO >> epoch 001:   3300 / 5058 loss=0.086, ups=1.71, num_updates=3300, lr=9.5e-05, gnorm=1.673, clip=1, train_wall=55, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:07:40]    INFO >> epoch 001:   3400 / 5058 loss=0.225, ups=1.44, num_updates=3400, lr=9.5e-05, gnorm=5.264, clip=3, train_wall=64, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:08:46]    INFO >> epoch 001:   3500 / 5058 loss=0.131, ups=1.57, num_updates=3500, lr=9.5e-05, gnorm=1.53, clip=0, train_wall=59, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:09:53]    INFO >> epoch 001:   3600 / 5058 loss=0.095, ups=1.57, num_updates=3600, lr=9.5e-05, gnorm=2.575, clip=1, train_wall=59, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:10:54]    INFO >> epoch 001:   3700 / 5058 loss=0.137, ups=1.7, num_updates=3700, lr=9.4e-05, gnorm=1.854, clip=2, train_wall=55, gb_free=66.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:12:02]    INFO >> epoch 001:   3800 / 5058 loss=0.058, ups=1.54, num_updates=3800, lr=9.4e-05, gnorm=1.191, clip=1, train_wall=60, gb_free=68.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:12:52]    INFO >> epoch 001:   3900 / 5058 loss=0.127, ups=2.05, num_updates=3900, lr=9.4e-05, gnorm=1.978, clip=2, train_wall=46, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:13:55]    INFO >> epoch 001:   4000 / 5058 loss=0.131, ups=1.66, num_updates=4000, lr=9.4e-05, gnorm=1.731, clip=1, train_wall=56, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:14:59]    INFO >> epoch 001:   4100 / 5058 loss=0.92, ups=1.64, num_updates=4100, lr=9.4e-05, gnorm=0.902, clip=1, train_wall=57, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:16:06]    INFO >> epoch 001:   4200 / 5058 loss=0.149, ups=1.54, num_updates=4200, lr=9.3e-05, gnorm=1.845, clip=3, train_wall=61, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:17:07]    INFO >> epoch 001:   4300 / 5058 loss=0.117, ups=1.74, num_updates=4300, lr=9.3e-05, gnorm=1.721, clip=1, train_wall=54, gb_free=73.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:18:07]    INFO >> epoch 001:   4400 / 5058 loss=0.082, ups=1.69, num_updates=4400, lr=9.3e-05, gnorm=1.476, clip=0, train_wall=55, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:19:11]    INFO >> epoch 001:   4500 / 5058 loss=0.072, ups=1.63, num_updates=4500, lr=9.3e-05, gnorm=1.371, clip=0, train_wall=57, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:20:18]    INFO >> epoch 001:   4600 / 5058 loss=0.241, ups=1.55, num_updates=4600, lr=9.3e-05, gnorm=1.805, clip=0, train_wall=60, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:21:30]    INFO >> epoch 001:   4700 / 5058 loss=0.215, ups=1.45, num_updates=4700, lr=9.2e-05, gnorm=3.803, clip=2, train_wall=64, gb_free=64.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:22:32]    INFO >> epoch 001:   4800 / 5058 loss=0.056, ups=1.68, num_updates=4800, lr=9.2e-05, gnorm=1.411, clip=0, train_wall=55, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:23:13]    INFO >> epoch 001:   4900 / 5058 loss=0.142, ups=2.5, num_updates=4900, lr=9.2e-05, gnorm=0.824, clip=0, train_wall=38, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:24:20]    INFO >> epoch 001:   5000 / 5058 loss=0.09, ups=1.59, num_updates=5000, lr=9.2e-05, gnorm=2.488, clip=0, train_wall=59, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:25:00]    INFO >> epoch 001 | loss 0.105 | ups 1.71 | num_updates 5058 | lr 9.2e-05 | gnorm 2.248 | clip 2.1 | train_wall 2776 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-21 19:25:00] WARNING >> 4469 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[2, 4, 10, 12, 13, 14, 16, 18, 19, 23] (data_utils.py:301, filter_by_size())[0m
[33m[2025-11-21 19:25:00] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 19:31:45]    INFO >> epoch 001 | valid on 'valid' subset | loss 0.138 | num_updates 5058 (progress_bar.py:267, print())[0m
[32m[2025-11-21 19:31:46]    INFO >> saved checkpoint /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256/checkpoints/checkpoint1.pt (epoch 1 @ 5058 updates, score 0.138) (writing took 1.128643 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 19:31:46] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 19:32:08]    INFO >> epoch 002:     42 / 5058 loss=0.24, ups=0.22, num_updates=5100, lr=9.2e-05, gnorm=3.337, clip=2, train_wall=56, gb_free=77.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:32:51]    INFO >> epoch 002:    142 / 5058 loss=0.134, ups=2.49, num_updates=5200, lr=9.1e-05, gnorm=0.773, clip=0, train_wall=38, gb_free=78.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:33:46]    INFO >> epoch 002:    242 / 5058 loss=0.233, ups=1.85, num_updates=5300, lr=9.1e-05, gnorm=0.782, clip=0, train_wall=51, gb_free=70.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:34:51]    INFO >> epoch 002:    342 / 5058 loss=0.135, ups=1.61, num_updates=5400, lr=9.1e-05, gnorm=1.255, clip=0, train_wall=58, gb_free=64.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:35:59]    INFO >> epoch 002:    442 / 5058 loss=0.248, ups=1.52, num_updates=5500, lr=9.1e-05, gnorm=3.406, clip=1, train_wall=62, gb_free=68.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:37:01]    INFO >> epoch 002:    542 / 5058 loss=0.08, ups=1.71, num_updates=5600, lr=9.1e-05, gnorm=1.697, clip=1, train_wall=55, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:37:56]    INFO >> epoch 002:    642 / 5058 loss=0.088, ups=1.88, num_updates=5700, lr=9e-05, gnorm=1.912, clip=0, train_wall=50, gb_free=67.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:38:35]    INFO >> epoch 002:    742 / 5058 loss=0.239, ups=2.65, num_updates=5800, lr=9e-05, gnorm=2.029, clip=0, train_wall=36, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:39:30]    INFO >> epoch 002:    842 / 5058 loss=0.144, ups=1.93, num_updates=5900, lr=9e-05, gnorm=1.643, clip=0, train_wall=49, gb_free=63.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:40:38]    INFO >> epoch 002:    942 / 5058 loss=0.159, ups=1.53, num_updates=6000, lr=9e-05, gnorm=2.681, clip=0, train_wall=61, gb_free=66.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:41:37]    INFO >> epoch 002:   1042 / 5058 loss=0.107, ups=1.73, num_updates=6100, lr=9e-05, gnorm=1.282, clip=0, train_wall=54, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:42:38]    INFO >> epoch 002:   1142 / 5058 loss=0.036, ups=1.71, num_updates=6200, lr=8.9e-05, gnorm=0.293, clip=0, train_wall=55, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:43:46]    INFO >> epoch 002:   1242 / 5058 loss=0.042, ups=1.54, num_updates=6300, lr=8.9e-05, gnorm=0.551, clip=0, train_wall=61, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:44:50]    INFO >> epoch 002:   1342 / 5058 loss=0.054, ups=1.6, num_updates=6400, lr=8.9e-05, gnorm=0.868, clip=0, train_wall=59, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:45:55]    INFO >> epoch 002:   1442 / 5058 loss=0.131, ups=1.64, num_updates=6500, lr=8.9e-05, gnorm=1.541, clip=0, train_wall=57, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:46:58]    INFO >> epoch 002:   1542 / 5058 loss=0.08, ups=1.65, num_updates=6600, lr=8.9e-05, gnorm=1.148, clip=0, train_wall=57, gb_free=69.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:48:02]    INFO >> epoch 002:   1642 / 5058 loss=0.137, ups=1.65, num_updates=6700, lr=8.8e-05, gnorm=1.963, clip=1, train_wall=57, gb_free=63.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:49:08]    INFO >> epoch 002:   1742 / 5058 loss=0.11, ups=1.58, num_updates=6800, lr=8.8e-05, gnorm=1.674, clip=0, train_wall=59, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:49:50]    INFO >> epoch 002:   1842 / 5058 loss=0.075, ups=2.44, num_updates=6900, lr=8.8e-05, gnorm=0.746, clip=0, train_wall=39, gb_free=72 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:50:43]    INFO >> epoch 002:   1942 / 5058 loss=0.116, ups=1.98, num_updates=7000, lr=8.8e-05, gnorm=1.846, clip=0, train_wall=47, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:51:45]    INFO >> epoch 002:   2042 / 5058 loss=0.068, ups=1.66, num_updates=7100, lr=8.8e-05, gnorm=0.476, clip=0, train_wall=56, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:52:56]    INFO >> epoch 002:   2142 / 5058 loss=0.103, ups=1.49, num_updates=7200, lr=8.7e-05, gnorm=3.36, clip=0, train_wall=62, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:54:02]    INFO >> epoch 002:   2242 / 5058 loss=0.08, ups=1.58, num_updates=7300, lr=8.7e-05, gnorm=1.933, clip=0, train_wall=59, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:55:09]    INFO >> epoch 002:   2342 / 5058 loss=0.088, ups=1.55, num_updates=7400, lr=8.7e-05, gnorm=1.022, clip=0, train_wall=60, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:56:14]    INFO >> epoch 002:   2442 / 5058 loss=0.078, ups=1.57, num_updates=7500, lr=8.7e-05, gnorm=0.795, clip=0, train_wall=59, gb_free=64.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:57:24]    INFO >> epoch 002:   2542 / 5058 loss=0.225, ups=1.5, num_updates=7600, lr=8.7e-05, gnorm=2.32, clip=0, train_wall=62, gb_free=72.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:58:19]    INFO >> epoch 002:   2642 / 5058 loss=0.074, ups=1.87, num_updates=7700, lr=8.6e-05, gnorm=1.402, clip=0, train_wall=50, gb_free=69.7 (progress_bar.py:258, log())[0m
[32m[2025-11-21 19:59:25]    INFO >> epoch 002:   2742 / 5058 loss=0.09, ups=1.59, num_updates=7800, lr=8.6e-05, gnorm=0.648, clip=0, train_wall=59, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-21 20:00:11]    INFO >> epoch 002:   2842 / 5058 loss=0.161, ups=2.28, num_updates=7900, lr=8.6e-05, gnorm=0.632, clip=0, train_wall=41, gb_free=78.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 20:01:07]    INFO >> epoch 002:   2942 / 5058 loss=0.203, ups=1.85, num_updates=8000, lr=8.6e-05, gnorm=2.179, clip=0, train_wall=51, gb_free=64.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 20:02:11]    INFO >> epoch 002:   3042 / 5058 loss=0.081, ups=1.61, num_updates=8100, lr=8.6e-05, gnorm=1.026, clip=0, train_wall=58, gb_free=72 (progress_bar.py:258, log())[0m
[32m[2025-11-21 20:03:09]    INFO >> epoch 002:   3142 / 5058 loss=0.066, ups=1.81, num_updates=8200, lr=8.5e-05, gnorm=0.713, clip=0, train_wall=52, gb_free=71.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 20:04:03]    INFO >> epoch 002:   3242 / 5058 loss=0.379, ups=1.93, num_updates=8300, lr=8.5e-05, gnorm=0.754, clip=0, train_wall=49, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-21 20:05:10]    INFO >> epoch 002:   3342 / 5058 loss=0.104, ups=1.55, num_updates=8400, lr=8.5e-05, gnorm=2.319, clip=0, train_wall=60, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-21 20:06:22]    INFO >> epoch 002:   3442 / 5058 loss=0.244, ups=1.48, num_updates=8500, lr=8.5e-05, gnorm=3.47, clip=0, train_wall=63, gb_free=66.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 20:07:28]    INFO >> epoch 002:   3542 / 5058 loss=0.085, ups=1.56, num_updates=8600, lr=8.4e-05, gnorm=0.775, clip=1, train_wall=60, gb_free=64.9 (progress_bar.py:258, log())[0m
[32m[2025-11-21 20:08:31]    INFO >> epoch 002:   3642 / 5058 loss=0.089, ups=1.63, num_updates=8700, lr=8.4e-05, gnorm=1.298, clip=0, train_wall=58, gb_free=64 (progress_bar.py:258, log())[0m
[32m[2025-11-21 20:09:35]    INFO >> epoch 002:   3742 / 5058 loss=0.065, ups=1.62, num_updates=8800, lr=8.4e-05, gnorm=1.631, clip=0, train_wall=58, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 20:10:38]    INFO >> epoch 002:   3842 / 5058 loss=0.072, ups=1.66, num_updates=8900, lr=8.4e-05, gnorm=0.598, clip=0, train_wall=56, gb_free=72.4 (progress_bar.py:258, log())[0m
[32m[2025-11-21 20:11:30]    INFO >> epoch 002:   3942 / 5058 loss=0.154, ups=2.02, num_updates=9000, lr=8.4e-05, gnorm=1.231, clip=0, train_wall=46, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 20:12:35]    INFO >> epoch 002:   4042 / 5058 loss=0.154, ups=1.62, num_updates=9100, lr=8.3e-05, gnorm=0.968, clip=0, train_wall=58, gb_free=66.3 (progress_bar.py:258, log())[0m
[32m[2025-11-21 20:13:41]    INFO >> epoch 002:   4142 / 5058 loss=0.141, ups=1.55, num_updates=9200, lr=8.3e-05, gnorm=1.037, clip=0, train_wall=60, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-21 20:14:45]    INFO >> epoch 002:   4242 / 5058 loss=0.15, ups=1.65, num_updates=9300, lr=8.3e-05, gnorm=2.156, clip=0, train_wall=57, gb_free=75.2 (progress_bar.py:258, log())[0m
