(base) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ conda activate naturalc

EnvironmentNameNotFound: Could not find conda environment: naturalc
You can list all discoverable environments with `conda info --envs`.


(base) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ conda activate naturalcc
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ git pull
^[[A^[[Aremote: Enumerating objects: 9, done.[K
remote: Counting objects:  11% (1/9)[Kremote: Counting objects:  22% (2/9)[Kremote: Counting objects:  33% (3/9)[Kremote: Counting objects:  44% (4/9)[Kremote: Counting objects:  55% (5/9)[Kremote: Counting objects:  66% (6/9)[Kremote: Counting objects:  77% (7/9)[Kremote: Counting objects:  88% (8/9)[Kremote: Counting objects: 100% (9/9)[Kremote: Counting objects: 100% (9/9), done.[K
remote: Compressing objects:  14% (1/7)[Kremote: Compressing objects:  28% (2/7)[Kremote: Compressing objects:  42% (3/7)[Kremote: Compressing objects:  57% (4/7)[Kremote: Compressing objects:  71% (5/7)[Kremote: Compressing objects:  85% (6/7)[Kremote: Compressing objects: 100% (7/7)[Kremote: Compressing objects: 100% (7/7), done.[K
Unpacking objects:  12% (1/8)Unpacking objects:  25% (2/8)Unpacking objects:  37% (3/8)Unpacking objects:  50% (4/8)Unpacking objects:  62% (5/8)remote: Total 8 (delta 1), reused 8 (delta 1), pack-reused 0 (from 0)[K
Unpacking objects:  75% (6/8)Unpacking objects:  87% (7/8)Unpacking objects: 100% (8/8)Unpacking objects: 100% (8/8), 17.14 KiB | 373.00 KiB/s, done.
From https://github.com/Cicecoo/Type-Prediction
   c7b013f..73a76cf  master     -> cicecoo/master
Updating c7b013f..73a76cf
Fast-forward
 EXPERIMENT_GUIDE.md          | 348 [32m++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++[m
 QUICKSTART.md                | 155 [32m++++++++++++++++++++++++++++++[m
 analyze_results.py           | 379 [32m++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++[m
 experiment_suite.sh          | 136 [32m+++++++++++++++++++++++++++[m
 generate_experiment_suite.py | 419 [32m++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++[m
 run_batch_experiments.py     | 290 [32m+++++++++++++++++++++++++++++++++++++++++++++++++++++++++[m
 6 files changed, 1727 insertions(+)
 create mode 100644 EXPERIMENT_GUIDE.md
 create mode 100644 QUICKSTART.md
 create mode 100644 analyze_results.py
 create mode 100644 experiment_suite.sh
 create mode 100644 generate_experiment_suite.py
 create mode 100644 run_batch_experiments.py
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ git pullconda activate naturalcc[16Pgit pull[Kpython generate_experiment_suite.py
============================================================
Generating Transformer Type Prediction Experiment Suite
============================================================

1. Baseline
âœ“ Created experiment: exp_baseline

2. Model Size Experiments
âœ“ Created experiment: exp_d_model_256
âœ“ Created experiment: exp_d_model_512
âœ“ Created experiment: exp_d_model_1024

3. Layer Number Experiments
âœ“ Created experiment: exp_layers_4
âœ“ Created experiment: exp_layers_6
âœ“ Created experiment: exp_layers_8

4. Learning Rate Experiments
âœ“ Created experiment: exp_lr_1e-04
âœ“ Created experiment: exp_lr_5e-04
âœ“ Created experiment: exp_lr_1e-03

5. Dropout Experiments
âœ“ Created experiment: exp_dropout_0.0
âœ“ Created experiment: exp_dropout_0.1
âœ“ Created experiment: exp_dropout_0.2

6. Encoder Type Experiments
âœ“ Created experiment: exp_encoder_transformer
âœ“ Created experiment: exp_encoder_lstm

7. Batch Size Experiments
âœ“ Created experiment: exp_batch_16
âœ“ Created experiment: exp_batch_32
âœ“ Created experiment: exp_batch_64

============================================================
Total experiments created: 18
Experiments directory: experiments/transformer_series
============================================================
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py --group baseline --gpus 0

============================================================
Batch Experiment Runner
Mode: serial
Experiments: 1
GPUs: [0]
============================================================


============================================================
Running: exp_baseline
GPU: 0
Time: 2025-11-20T00:10:47.539636
============================================================
usage: run_transformer_experiment.py [-h] --exp-name EXP_NAME
                                     [--base-dir BASE_DIR]
                                     [--data-dir DATA_DIR]
                                     [--config-template CONFIG_TEMPLATE]
                                     [--encoder-type {lstm,transformer}]
                                     [--encoder-layers ENCODER_LAYERS]
                                     [--encoder-embed-dim ENCODER_EMBED_DIM]
                                     [--dropout DROPOUT] [--lr LR]
                                     [--batch-size BATCH_SIZE]
                                     [--max-epoch MAX_EPOCH]
                                     [--warmup-updates WARMUP_UPDATES]
                                     [--skip-train] [--skip-eval]
run_transformer_experiment.py: error: unrecognized arguments: --language python --d-model 512 --n-layers 6

âŒ exp_baseline failed with code 2

============================================================
Batch Run Summary
============================================================
Total: 1
Success: 0
Failed: 1
Success Rate: 0.0%

Failed experiments:
  - exp_baseline
============================================================

(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py --group baseline --gpus 0[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [C[C[C[C[C[C[C[3Pgenerate_experiment_suite.py
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cgit pull[Kconda activate naturalcc[16Pgit pullpython generate_experiment_suite.pyrun_batch_experiments.py --group baseline --gpus 0[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cpython run_batch_experiments.py --group baseline --gpus 3

============================================================
Batch Experiment Runner
Mode: serial
Experiments: 1
GPUs: [3]
============================================================


============================================================
Running: exp_baseline
GPU: 3
Time: 2025-11-20T00:11:14.767556
============================================================
usage: run_transformer_experiment.py [-h] --exp-name EXP_NAME
                                     [--base-dir BASE_DIR]
                                     [--data-dir DATA_DIR]
                                     [--config-template CONFIG_TEMPLATE]
                                     [--encoder-type {lstm,transformer}]
                                     [--encoder-layers ENCODER_LAYERS]
                                     [--encoder-embed-dim ENCODER_EMBED_DIM]
                                     [--dropout DROPOUT] [--lr LR]
                                     [--batch-size BATCH_SIZE]
                                     [--max-epoch MAX_EPOCH]
                                     [--warmup-updates WARMUP_UPDATES]
                                     [--skip-train] [--skip-eval]
run_transformer_experiment.py: error: unrecognized arguments: --language python --d-model 512 --n-layers 6

âŒ exp_baseline failed with code 2

============================================================
Batch Run Summary
============================================================
Total: 1
Success: 0
Failed: 1
Success Rate: 0.0%

Failed experiments:
  - exp_baseline
============================================================

(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py --group baseline --gpus 3[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py --group baseline --gpus 30[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [C[C[C[C[C[C[Cgenerate_experiment_suite.py[K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ git pull[K
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C
remote: Enumerating objects: 5, done.[K
remote: Counting objects:  20% (1/5)[Kremote: Counting objects:  40% (2/5)[Kremote: Counting objects:  60% (3/5)[Kremote: Counting objects:  80% (4/5)[Kremote: Counting objects: 100% (5/5)[Kremote: Counting objects: 100% (5/5), done.[K
remote: Compressing objects: 100% (1/1)[Kremote: Compressing objects: 100% (1/1), done.[K
remote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0 (from 0)[K
Unpacking objects:  33% (1/3)Unpacking objects:  66% (2/3)Unpacking objects: 100% (3/3)Unpacking objects: 100% (3/3), 956 bytes | 478.00 KiB/s, done.
From https://github.com/Cicecoo/Type-Prediction
   73a76cf..efbbb91  master     -> cicecoo/master
Updating 73a76cf..efbbb91
Fast-forward
 run_batch_experiments.py | 67 [32m+++++++++++++++++++++++++++++++++++++++++++++++++[m[31m------------------[m
 1 file changed, 49 insertions(+), 18 deletions(-)
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_transformer_experiment.py \
>   --exp-name baseline \
>   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer \
>   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer \
>   --max-epoch 5

============================================================
Setting up experiment: baseline
============================================================
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer/baseline
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer/baseline/checkpoints
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer/baseline/logs
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer/baseline/results
âœ“ Saved config to: /mnt/data1/zhaojunzhang/experiments/transformer/baseline/config.yml
âœ“ Also saved to: run/type_prediction/transformer/config/baseline.yml
âœ“ Saved experiment info to: /mnt/data1/zhaojunzhang/experiments/transformer/baseline/info.txt

============================================================
Starting training...
============================================================

Command: python run/type_prediction/transformer/train.py --language baseline 2>&1 | tee /mnt/data1/zhaojunzhang/experiments/transformer/baseline/logs/train.log

Note: Config file will be loaded from run/type_prediction/transformer/config/baseline.yml

[32m[2025-11-20 00:16:24]    INFO >> Load arguments in run/type_prediction/transformer/config/baseline.yml (train.py:300, cli_main())[0m
[32m[2025-11-20 00:16:24]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer/baseline/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 16, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 16, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 2, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 5, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer/baseline/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer/baseline/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:302, cli_main())[0m
[32m[2025-11-20 00:16:24]    INFO >> single GPU training... (train.py:331, cli_main())[0m
[32m[2025-11-20 00:16:24]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer/baseline/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 16, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 16, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 2, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 5, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer/baseline/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer/baseline/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:208, single_main())[0m
[32m[2025-11-20 00:16:26]    INFO >> TypeTransformer(
  (decoder): RobertaEncoder(
    (encoder): CodeEncoderLSTM(
      (embedding): Embedding(10006, 512)
      (pos_encoder): PositionalEncoding()
      (encoder): LSTM(512, 512, num_layers=6, bidirectional=True)
    )
    (output): Sequential(
      (0): Linear(in_features=1024, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=10006, bias=True)
    )
  )
  (classification_heads): ModuleDict()
) (train.py:220, single_main())[0m
[32m[2025-11-20 00:16:26]    INFO >> model typetransformer, criterion TypePredictionCrossEntropyCriterion (train.py:221, single_main())[0m
[32m[2025-11-20 00:16:26]    INFO >> num. model params: 46481686 (num. trained: 46481686) (train.py:222, single_main())[0m
[32m[2025-11-20 00:16:27]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 00:16:27]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 35002 MB ; used memory = 46917 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 00:16:27]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 00:16:27]    INFO >> training on 1 GPUs (train.py:229, single_main())[0m
[32m[2025-11-20 00:16:27]    INFO >> max tokens per GPU = None and max sentences per GPU = 16 (train.py:230, single_main())[0m
[32m[2025-11-20 00:16:27]    INFO >> no existing checkpoint found /mnt/data1/zhaojunzhang/experiments/transformer/baseline/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-20 00:16:27]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
^CTraceback (most recent call last):
  File "run_transformer_experiment.py", line 575, in <module>
    main()
  File "run_transformer_experiment.py", line 561, in main
    if not exp.train():
  File "run_transformer_experiment.py", line 361, in train
    result = subprocess.run(
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/subprocess.py", line 495, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/subprocess.py", line 1020, in communicate
    self.wait()
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/subprocess.py", line 1083, in wait
    return self._wait(timeout=timeout)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/subprocess.py", line 1822, in _wait
    (pid, sts) = self._try_wait(0)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/subprocess.py", line 1780, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
KeyboardInterrupt

(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ ^C
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python generate_experiment_suite.py \
>   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series \
>   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer
usage: generate_experiment_suite.py [-h] [--base-dir BASE_DIR]
                                    [--groups {baseline,model_size,layers,lr,dropout,encoder,batch_size,all} [{baseline,model_size,layers,lr,dropout,encoder,batch_size,all} ...]]
generate_experiment_suite.py: error: unrecognized arguments: --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ cat /mnt/data1/zhaojunzhang/experiments/transformer_series/experiment_index.json
cat: /mnt/data1/zhaojunzhang/experiments/transformer_series/experiment_index.json: No such file or directory
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python generate_experiment_suite.py \
>   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series \
>   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer
usage: generate_experiment_suite.py [-h] [--base-dir BASE_DIR]
                                    [--groups {baseline,model_size,layers,lr,dropout,encoder,batch_size,all} [{baseline,model_size,layers,lr,dropout,encoder,batch_size,all} ...]]
generate_experiment_suite.py: error: unrecognized arguments: --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ 
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python generate_experiment_suite.py   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer[A[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ cat /mnt/data1/zhaojunzhang/exper[65Piments/transformer_series/experiment_index.json
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python generate_experiment_suite.py   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer[A[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [C[C[C[C[C[C[Crun_transformer_experiment.py   --exp-name baseline   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer   --max-epoch 5[A[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ git pull[K
[K
[K[A[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C
remote: Enumerating objects: 5, done.[K
remote: Counting objects:  20% (1/5)[Kremote: Counting objects:  40% (2/5)[Kremote: Counting objects:  60% (3/5)[Kremote: Counting objects:  80% (4/5)[Kremote: Counting objects: 100% (5/5)[Kremote: Counting objects: 100% (5/5), done.[K
remote: Compressing objects: 100% (1/1)[Kremote: Compressing objects: 100% (1/1), done.[K
remote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0 (from 0)[K
Unpacking objects:  33% (1/3)Unpacking objects:  66% (2/3)Unpacking objects: 100% (3/3)Unpacking objects: 100% (3/3), 422 bytes | 422.00 KiB/s, done.
From https://github.com/Cicecoo/Type-Prediction
   efbbb91..999473b  master     -> cicecoo/master
Updating efbbb91..999473b
Fast-forward
 generate_experiment_suite.py | 3 [32m+++[m
 1 file changed, 3 insertions(+)
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python generate_experiment_suite.py \
>   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series \
>   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer
============================================================
Generating Transformer Type Prediction Experiment Suite
============================================================

1. Baseline
âœ“ Created experiment: exp_baseline

2. Model Size Experiments
âœ“ Created experiment: exp_d_model_256
âœ“ Created experiment: exp_d_model_512
âœ“ Created experiment: exp_d_model_1024

3. Layer Number Experiments
âœ“ Created experiment: exp_layers_4
âœ“ Created experiment: exp_layers_6
âœ“ Created experiment: exp_layers_8

4. Learning Rate Experiments
âœ“ Created experiment: exp_lr_1e-04
âœ“ Created experiment: exp_lr_5e-04
âœ“ Created experiment: exp_lr_1e-03

5. Dropout Experiments
âœ“ Created experiment: exp_dropout_0.0
âœ“ Created experiment: exp_dropout_0.1
âœ“ Created experiment: exp_dropout_0.2

6. Encoder Type Experiments
âœ“ Created experiment: exp_encoder_transformer
âœ“ Created experiment: exp_encoder_lstm

7. Batch Size Experiments
âœ“ Created experiment: exp_batch_16
âœ“ Created experiment: exp_batch_32
âœ“ Created experiment: exp_batch_64

============================================================
Total experiments created: 18
Experiments directory: /mnt/data1/zhaojunzhang/experiments/transformer_series
============================================================
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ cat /mnt/data1/zhaojunzhang/experiments/transformer_series/experiment_index.json
{
  "created": "2025-11-20T00:18:15.235011",
  "total_experiments": 18,
  "experiments": [
    {
      "name": "exp_baseline",
      "description": "Baseline configuration with default hyperparameters",
      "path": "exp_baseline"
    },
    {
      "name": "exp_d_model_256",
      "description": "Model dimension: 256",
      "path": "exp_d_model_256"
    },
    {
      "name": "exp_d_model_512",
      "description": "Model dimension: 512",
      "path": "exp_d_model_512"
    },
    {
      "name": "exp_d_model_1024",
      "description": "Model dimension: 1024",
      "path": "exp_d_model_1024"
    },
    {
      "name": "exp_layers_4",
      "description": "Number of encoder layers: 4",
      "path": "exp_layers_4"
    },
    {
      "name": "exp_layers_6",
      "description": "Number of encoder layers: 6",
      "path": "exp_layers_6"
    },
    {
      "name": "exp_layers_8",
      "description": "Number of encoder layers: 8",
      "path": "exp_layers_8"
    },
    {
      "name": "exp_lr_1e-04",
      "description": "Learning rate: 0.0001",
      "path": "exp_lr_1e-04"
    },
    {
      "name": "exp_lr_5e-04",
      "description": "Learning rate: 0.0005",
      "path": "exp_lr_5e-04"
    },
    {
      "name": "exp_lr_1e-03",
      "description": "Learning rate: 0.001",
      "path": "exp_lr_1e-03"
    },
    {
      "name": "exp_dropout_0.0",
      "description": "Dropout rate: 0.0",
      "path": "exp_dropout_0.0"
    },
    {
      "name": "exp_dropout_0.1",
      "description": "Dropout rate: 0.1",
      "path": "exp_dropout_0.1"
    },
    {
      "name": "exp_dropout_0.2",
      "description": "Dropout rate: 0.2",
      "path": "exp_dropout_0.2"
    },
    {
      "name": "exp_encoder_transformer",
      "description": "Encoder type: transformer",
      "path": "exp_encoder_transformer"
    },
    {
      "name": "exp_encoder_lstm",
      "description": "Encoder type: lstm",
      "path": "exp_encoder_lstm"
    },
    {
      "name": "exp_batch_16",
      "description": "Batch size: 16",
      "path": "exp_batch_16"
    },
    {
      "name": "exp_batch_32",
      "description": "Batch size: 32",
      "path": "exp_batch_32"
    },
    {
      "name": "exp_batch_64",
      "description": "Batch size: 64",
      "path": "exp_batch_64"
    }
  ]
}(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py \
>   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series \
>   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer \
>   --mode serial \
>   --gpu 0[K3
usage: run_batch_experiments.py [-h] [--exp-dir EXP_DIR]
                                [--group {baseline,model_size,layers,lr,dropout,encoder,batch_size}]
                                [--names NAMES [NAMES ...]] [--all] [--mode {serial,parallel}]
                                [--gpus GPUS [GPUS ...]] [--dry-run]
run_batch_experiments.py: error: unrecognized arguments: --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer   --mode serial   --gpu 3[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-da[1P[A[C[C[C[C[C[C-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-dat[1P[A[C[C[C[C[C-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data[1P[A[C[C[C[C-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data/[1P[A[C[C[Cexp-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-da[C[3@a/t[A[C[C

usage: run_batch_experiments.py [-h] [--exp-dir EXP_DIR]
                                [--group {baseline,model_size,layers,lr,dropout,encoder,batch_size}]
                                [--names NAMES [NAMES ...]] [--all] [--mode {serial,parallel}]
                                [--gpus GPUS [GPUS ...]] [--dry-run]
run_batch_experiments.py: error: unrecognized arguments: --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py   --exp-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer   --mode serial   --gpu 3[A[C[C[C[Cbase-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-d[1@a[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[A[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ cat /mnt/data1/zhaojunzhang/exper[65Piments/transformer_series/experiment_index.json
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python generate_experiment_suite.py   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer[A[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ git pull[K
[K
[K[A[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C
remote: Enumerating objects: 5, done.[K
remote: Counting objects:  20% (1/5)[Kremote: Counting objects:  40% (2/5)[Kremote: Counting objects:  60% (3/5)[Kremote: Counting objects:  80% (4/5)[Kremote: Counting objects: 100% (5/5)[Kremote: Counting objects: 100% (5/5), done.[K
remote: Compressing objects: 100% (1/1)[Kremote: Compressing objects: 100% (1/1), done.[K
remote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0 (from 0)[K
Unpacking objects:  33% (1/3)Unpacking objects:  66% (2/3)Unpacking objects: 100% (3/3)Unpacking objects: 100% (3/3), 520 bytes | 260.00 KiB/s, done.
From https://github.com/Cicecoo/Type-Prediction
   999473b..14faf10  master     -> cicecoo/master
Updating 999473b..14faf10
Fast-forward
 run_batch_experiments.py | 17 [32m+++++++++++++[m[31m----[m
 1 file changed, 13 insertions(+), 4 deletions(-)
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ python run_batch_experiments.py \
>   --base-dir /mnt/data1/zhaojunzhang/experiments/transformer_series \
>   --data-dir /mnt/data1/zhaojunzhang/typilus-data/transformer \
>   --mode serial \
>   --gpu 3 \
>   --all

============================================================
Batch Experiment Runner
Mode: serial
Experiments: 18
GPUs: [3]
============================================================


============================================================
Running: exp_baseline
GPU: 3
Time: 2025-11-20T00:20:41.766711
============================================================
[32m[2025-11-20 00:20:43]    INFO >> Load arguments in run/type_prediction/transformer/config/exp_baseline.yml (train.py:300, cli_main())[0m
[32m[2025-11-20 00:20:43]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:302, cli_main())[0m
[32m[2025-11-20 00:20:43]    INFO >> single GPU training... (train.py:331, cli_main())[0m
[32m[2025-11-20 00:20:43]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:208, single_main())[0m
[32m[2025-11-20 00:20:45]    INFO >> TypeTransformer(
  (decoder): RobertaEncoder(
    (encoder): CodeEncoderLSTM(
      (embedding): Embedding(10006, 512)
      (pos_encoder): PositionalEncoding()
      (encoder): LSTM(512, 512, num_layers=6, bidirectional=True)
    )
    (output): Sequential(
      (0): Linear(in_features=1024, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=10006, bias=True)
    )
  )
  (classification_heads): ModuleDict()
) (train.py:220, single_main())[0m
[32m[2025-11-20 00:20:45]    INFO >> model typetransformer, criterion TypePredictionCrossEntropyCriterion (train.py:221, single_main())[0m
[32m[2025-11-20 00:20:45]    INFO >> num. model params: 46481686 (num. trained: 46481686) (train.py:222, single_main())[0m
[32m[2025-11-20 00:20:45]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 00:20:45]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 35819 MB ; used memory = 46100 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 00:20:45]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 00:20:45]    INFO >> training on 1 GPUs (train.py:229, single_main())[0m
[32m[2025-11-20 00:20:45]    INFO >> max tokens per GPU = None and max sentences per GPU = 32 (train.py:230, single_main())[0m
[32m[2025-11-20 00:20:45]    INFO >> no existing checkpoint found /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-20 00:20:45]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[33m[2025-11-20 00:20:59] WARNING >> 32222 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[0, 1, 2, 4, 6, 7, 8, 10, 14, 19] (data_utils.py:301, filter_by_size())[0m
[32m[2025-11-20 00:20:59]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-20 00:20:59] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:398: UserWarning: ntokens not found in Criterion logging outputs, cannot log wpb or wps
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:407: UserWarning: nsentences not found in Criterion logging outputs, cannot log bsz
  warnings.warn(
[32m[2025-11-20 00:21:44]    INFO >> epoch 001:    100 / 5058 loss=1.903, ups=2.35, num_updates=100, lr=1e-05, gnorm=0.104, clip=0, train_wall=40, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:22:33]    INFO >> epoch 001:    200 / 5058 loss=0.786, ups=2.15, num_updates=200, lr=2e-05, gnorm=0.068, clip=0, train_wall=43, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:23:38]    INFO >> epoch 001:    300 / 5058 loss=0.654, ups=1.61, num_updates=300, lr=3e-05, gnorm=0.053, clip=0, train_wall=58, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:24:42]    INFO >> epoch 001:    400 / 5058 loss=0.623, ups=1.58, num_updates=400, lr=4e-05, gnorm=2.819, clip=2, train_wall=59, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:25:46]    INFO >> epoch 001:    500 / 5058 loss=0.208, ups=1.65, num_updates=500, lr=5e-05, gnorm=4.476, clip=2, train_wall=56, gb_free=65.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:26:47]    INFO >> epoch 001:    600 / 5058 loss=0.079, ups=1.7, num_updates=600, lr=6e-05, gnorm=4.788, clip=8, train_wall=55, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:27:30]    INFO >> epoch 001:    700 / 5058 loss=0.174, ups=2.4, num_updates=700, lr=7e-05, gnorm=2.641, clip=4, train_wall=39, gb_free=69.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:28:14]    INFO >> epoch 001:    800 / 5058 loss=0.166, ups=2.41, num_updates=800, lr=8e-05, gnorm=3.325, clip=8, train_wall=39, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:29:22]    INFO >> epoch 001:    900 / 5058 loss=0.216, ups=1.55, num_updates=900, lr=9e-05, gnorm=6.91, clip=8, train_wall=60, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:30:27]    INFO >> epoch 001:   1000 / 5058 loss=0.142, ups=1.61, num_updates=1000, lr=0.0001, gnorm=1.451, clip=0, train_wall=58, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:31:25]    INFO >> epoch 001:   1100 / 5058 loss=0.058, ups=1.75, num_updates=1100, lr=0.0001, gnorm=2.202, clip=2, train_wall=53, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:32:30]    INFO >> epoch 001:   1200 / 5058 loss=0.035, ups=1.62, num_updates=1200, lr=0.0001, gnorm=0.871, clip=0, train_wall=57, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:33:35]    INFO >> epoch 001:   1300 / 5058 loss=0.087, ups=1.59, num_updates=1300, lr=9.9e-05, gnorm=1.877, clip=2, train_wall=58, gb_free=67.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:34:39]    INFO >> epoch 001:   1400 / 5058 loss=0.059, ups=1.63, num_updates=1400, lr=9.9e-05, gnorm=2.488, clip=4, train_wall=57, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:35:44]    INFO >> epoch 001:   1500 / 5058 loss=0.279, ups=1.6, num_updates=1500, lr=9.9e-05, gnorm=1.776, clip=2, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:36:46]    INFO >> epoch 001:   1600 / 5058 loss=0.11, ups=1.71, num_updates=1600, lr=9.9e-05, gnorm=3.243, clip=4, train_wall=54, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:37:52]    INFO >> epoch 001:   1700 / 5058 loss=0.113, ups=1.56, num_updates=1700, lr=9.9e-05, gnorm=2.319, clip=2, train_wall=59, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:38:48]    INFO >> epoch 001:   1800 / 5058 loss=0.12, ups=1.89, num_updates=1800, lr=9.8e-05, gnorm=3.115, clip=5, train_wall=49, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:39:27]    INFO >> epoch 001:   1900 / 5058 loss=0.153, ups=2.64, num_updates=1900, lr=9.8e-05, gnorm=1.952, clip=4, train_wall=36, gb_free=67.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:40:29]    INFO >> epoch 001:   2000 / 5058 loss=0.089, ups=1.68, num_updates=2000, lr=9.8e-05, gnorm=1.422, clip=1, train_wall=56, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:41:36]    INFO >> epoch 001:   2100 / 5058 loss=0.074, ups=1.56, num_updates=2100, lr=9.8e-05, gnorm=3.142, clip=2, train_wall=59, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:42:43]    INFO >> epoch 001:   2200 / 5058 loss=0.279, ups=1.56, num_updates=2200, lr=9.8e-05, gnorm=5.078, clip=6, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:43:50]    INFO >> epoch 001:   2300 / 5058 loss=0.068, ups=1.54, num_updates=2300, lr=9.7e-05, gnorm=2.452, clip=5, train_wall=60, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:44:56]    INFO >> epoch 001:   2400 / 5058 loss=0.063, ups=1.6, num_updates=2400, lr=9.7e-05, gnorm=0.773, clip=1, train_wall=58, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:46:06]    INFO >> epoch 001:   2500 / 5058 loss=0.149, ups=1.49, num_updates=2500, lr=9.7e-05, gnorm=3.718, clip=1, train_wall=62, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:47:02]    INFO >> epoch 001:   2600 / 5058 loss=0.128, ups=1.8, num_updates=2600, lr=9.7e-05, gnorm=2.608, clip=1, train_wall=52, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:48:03]    INFO >> epoch 001:   2700 / 5058 loss=0.076, ups=1.72, num_updates=2700, lr=9.7e-05, gnorm=1.387, clip=1, train_wall=54, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:49:06]    INFO >> epoch 001:   2800 / 5058 loss=0.081, ups=1.67, num_updates=2800, lr=9.6e-05, gnorm=1.32, clip=3, train_wall=56, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:49:46]    INFO >> epoch 001:   2900 / 5058 loss=0.274, ups=2.59, num_updates=2900, lr=9.6e-05, gnorm=2.005, clip=3, train_wall=36, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:50:52]    INFO >> epoch 001:   3000 / 5058 loss=0.145, ups=1.58, num_updates=3000, lr=9.6e-05, gnorm=3.103, clip=5, train_wall=59, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:51:47]    INFO >> epoch 001:   3100 / 5058 loss=0.058, ups=1.92, num_updates=3100, lr=9.6e-05, gnorm=0.282, clip=0, train_wall=49, gb_free=73.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:52:43]    INFO >> epoch 001:   3200 / 5058 loss=0.282, ups=1.8, num_updates=3200, lr=9.6e-05, gnorm=2.208, clip=2, train_wall=52, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:53:44]    INFO >> epoch 001:   3300 / 5058 loss=0.086, ups=1.72, num_updates=3300, lr=9.5e-05, gnorm=1.673, clip=1, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:54:56]    INFO >> epoch 001:   3400 / 5058 loss=0.225, ups=1.46, num_updates=3400, lr=9.5e-05, gnorm=5.264, clip=3, train_wall=64, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:56:02]    INFO >> epoch 001:   3500 / 5058 loss=0.131, ups=1.58, num_updates=3500, lr=9.5e-05, gnorm=1.53, clip=0, train_wall=59, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:57:08]    INFO >> epoch 001:   3600 / 5058 loss=0.095, ups=1.58, num_updates=3600, lr=9.5e-05, gnorm=2.575, clip=1, train_wall=59, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:58:09]    INFO >> epoch 001:   3700 / 5058 loss=0.137, ups=1.7, num_updates=3700, lr=9.4e-05, gnorm=1.854, clip=2, train_wall=55, gb_free=66.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 00:59:16]    INFO >> epoch 001:   3800 / 5058 loss=0.058, ups=1.54, num_updates=3800, lr=9.4e-05, gnorm=1.191, clip=1, train_wall=60, gb_free=68.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:00:07]    INFO >> epoch 001:   3900 / 5058 loss=0.127, ups=2.07, num_updates=3900, lr=9.4e-05, gnorm=1.978, clip=2, train_wall=45, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:01:09]    INFO >> epoch 001:   4000 / 5058 loss=0.131, ups=1.67, num_updates=4000, lr=9.4e-05, gnorm=1.731, clip=1, train_wall=56, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:02:12]    INFO >> epoch 001:   4100 / 5058 loss=0.92, ups=1.65, num_updates=4100, lr=9.4e-05, gnorm=0.902, clip=1, train_wall=57, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:03:19]    INFO >> epoch 001:   4200 / 5058 loss=0.149, ups=1.55, num_updates=4200, lr=9.3e-05, gnorm=1.845, clip=3, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:04:19]    INFO >> epoch 001:   4300 / 5058 loss=0.117, ups=1.75, num_updates=4300, lr=9.3e-05, gnorm=1.721, clip=1, train_wall=53, gb_free=73.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:05:21]    INFO >> epoch 001:   4400 / 5058 loss=0.082, ups=1.71, num_updates=4400, lr=9.3e-05, gnorm=1.476, clip=0, train_wall=55, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:06:24]    INFO >> epoch 001:   4500 / 5058 loss=0.072, ups=1.65, num_updates=4500, lr=9.3e-05, gnorm=1.371, clip=0, train_wall=57, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:07:31]    INFO >> epoch 001:   4600 / 5058 loss=0.241, ups=1.56, num_updates=4600, lr=9.3e-05, gnorm=1.805, clip=0, train_wall=59, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:08:42]    INFO >> epoch 001:   4700 / 5058 loss=0.215, ups=1.47, num_updates=4700, lr=9.2e-05, gnorm=3.803, clip=2, train_wall=63, gb_free=64.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:09:44]    INFO >> epoch 001:   4800 / 5058 loss=0.056, ups=1.69, num_updates=4800, lr=9.2e-05, gnorm=1.411, clip=0, train_wall=55, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:10:25]    INFO >> epoch 001:   4900 / 5058 loss=0.142, ups=2.52, num_updates=4900, lr=9.2e-05, gnorm=0.824, clip=0, train_wall=37, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:11:30]    INFO >> epoch 001:   5000 / 5058 loss=0.09, ups=1.61, num_updates=5000, lr=9.2e-05, gnorm=2.488, clip=0, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:12:09]    INFO >> epoch 001 | loss 0.105 | ups 1.72 | num_updates 5058 | lr 9.2e-05 | gnorm 2.248 | clip 2.1 | train_wall 2745 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-20 01:12:09] WARNING >> 4469 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[2, 4, 10, 12, 13, 14, 16, 18, 19, 23] (data_utils.py:301, filter_by_size())[0m
[33m[2025-11-20 01:12:09] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
CodeEncoderLSTM: Creating BiLSTM with 6 layers, 512 hidden and input size
Traceback (most recent call last):
  File "run/type_prediction/transformer/train.py", line 339, in <module>
    cli_main()
  File "run/type_prediction/transformer/train.py", line 332, in cli_main
    single_main(args)
  File "run/type_prediction/transformer/train.py", line 254, in single_main
    valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)
  File "run/type_prediction/transformer/train.py", line 132, in validate
    trainer.valid_step(sample)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/trainers/ncc_trainers.py", line 565, in valid_step
    _loss, sample_size, logging_output = self.task.valid_step(
TypeError: cannot unpack non-iterable NoneType object

============================================================
Setting up experiment: exp_baseline
============================================================
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/checkpoints
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/logs
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/results
âœ“ Saved config to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/config.yml
âœ“ Also saved to: run/type_prediction/transformer/config/exp_baseline.yml
âœ“ Saved experiment info to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/info.txt

============================================================
Starting training...
============================================================

Command: python run/type_prediction/transformer/train.py --language exp_baseline 2>&1 | tee /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline/logs/train.log

Note: Config file will be loaded from run/type_prediction/transformer/config/exp_baseline.yml


âœ“ Training completed successfully!

============================================================
Evaluating model on test set...
============================================================

Warning: Best checkpoint not found, using last checkpoint
âœ— No checkpoint found for evaluation

============================================================
âœ“ Experiment completed: exp_baseline
âœ“ Results in: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_baseline
============================================================


âœ“ exp_baseline completed in 0.86 hours

============================================================
Running: exp_d_model_256
GPU: 3
Time: 2025-11-20T01:12:12.775627
============================================================
[32m[2025-11-20 01:12:14]    INFO >> Load arguments in run/type_prediction/transformer/config/exp_d_model_256.yml (train.py:300, cli_main())[0m
[32m[2025-11-20 01:12:14]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 256, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:302, cli_main())[0m
[32m[2025-11-20 01:12:14]    INFO >> single GPU training... (train.py:331, cli_main())[0m
[32m[2025-11-20 01:12:14]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 256, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:208, single_main())[0m
[32m[2025-11-20 01:12:16]    INFO >> TypeTransformer(
  (decoder): RobertaEncoder(
    (encoder): CodeEncoderLSTM(
      (embedding): Embedding(10006, 512)
      (pos_encoder): PositionalEncoding()
      (encoder): LSTM(512, 512, num_layers=6, bidirectional=True)
    )
    (output): Sequential(
      (0): Linear(in_features=1024, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=10006, bias=True)
    )
  )
  (classification_heads): ModuleDict()
) (train.py:220, single_main())[0m
[32m[2025-11-20 01:12:16]    INFO >> model typetransformer, criterion TypePredictionCrossEntropyCriterion (train.py:221, single_main())[0m
[32m[2025-11-20 01:12:16]    INFO >> num. model params: 46481686 (num. trained: 46481686) (train.py:222, single_main())[0m
[32m[2025-11-20 01:12:16]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 01:12:16]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 35815 MB ; used memory = 46104 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 01:12:16]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 01:12:16]    INFO >> training on 1 GPUs (train.py:229, single_main())[0m
[32m[2025-11-20 01:12:16]    INFO >> max tokens per GPU = None and max sentences per GPU = 32 (train.py:230, single_main())[0m
[32m[2025-11-20 01:12:16]    INFO >> no existing checkpoint found /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-20 01:12:16]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[33m[2025-11-20 01:12:28] WARNING >> 32222 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[0, 1, 2, 4, 6, 7, 8, 10, 14, 19] (data_utils.py:301, filter_by_size())[0m
[32m[2025-11-20 01:12:28]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-20 01:12:29] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:398: UserWarning: ntokens not found in Criterion logging outputs, cannot log wpb or wps
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:407: UserWarning: nsentences not found in Criterion logging outputs, cannot log bsz
  warnings.warn(
[32m[2025-11-20 01:13:13]    INFO >> epoch 001:    100 / 5058 loss=1.903, ups=2.34, num_updates=100, lr=1e-05, gnorm=0.104, clip=0, train_wall=41, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:14:02]    INFO >> epoch 001:    200 / 5058 loss=0.786, ups=2.14, num_updates=200, lr=2e-05, gnorm=0.068, clip=0, train_wall=44, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:15:06]    INFO >> epoch 001:    300 / 5058 loss=0.654, ups=1.61, num_updates=300, lr=3e-05, gnorm=0.053, clip=0, train_wall=58, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:16:13]    INFO >> epoch 001:    400 / 5058 loss=0.623, ups=1.57, num_updates=400, lr=4e-05, gnorm=2.819, clip=2, train_wall=60, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:17:16]    INFO >> epoch 001:    500 / 5058 loss=0.208, ups=1.65, num_updates=500, lr=5e-05, gnorm=4.476, clip=2, train_wall=57, gb_free=65.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:18:18]    INFO >> epoch 001:    600 / 5058 loss=0.079, ups=1.69, num_updates=600, lr=6e-05, gnorm=4.788, clip=8, train_wall=55, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:19:01]    INFO >> epoch 001:    700 / 5058 loss=0.174, ups=2.4, num_updates=700, lr=7e-05, gnorm=2.641, clip=4, train_wall=39, gb_free=69.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:19:44]    INFO >> epoch 001:    800 / 5058 loss=0.166, ups=2.43, num_updates=800, lr=8e-05, gnorm=3.325, clip=8, train_wall=39, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:20:52]    INFO >> epoch 001:    900 / 5058 loss=0.216, ups=1.54, num_updates=900, lr=9e-05, gnorm=6.91, clip=8, train_wall=61, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:21:57]    INFO >> epoch 001:   1000 / 5058 loss=0.142, ups=1.6, num_updates=1000, lr=0.0001, gnorm=1.451, clip=0, train_wall=59, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:22:56]    INFO >> epoch 001:   1100 / 5058 loss=0.058, ups=1.74, num_updates=1100, lr=0.0001, gnorm=2.202, clip=2, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:24:00]    INFO >> epoch 001:   1200 / 5058 loss=0.035, ups=1.62, num_updates=1200, lr=0.0001, gnorm=0.871, clip=0, train_wall=58, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:25:06]    INFO >> epoch 001:   1300 / 5058 loss=0.087, ups=1.59, num_updates=1300, lr=9.9e-05, gnorm=1.877, clip=2, train_wall=59, gb_free=67.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:26:10]    INFO >> epoch 001:   1400 / 5058 loss=0.059, ups=1.63, num_updates=1400, lr=9.9e-05, gnorm=2.488, clip=4, train_wall=58, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:27:15]    INFO >> epoch 001:   1500 / 5058 loss=0.279, ups=1.6, num_updates=1500, lr=9.9e-05, gnorm=1.776, clip=2, train_wall=59, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:28:17]    INFO >> epoch 001:   1600 / 5058 loss=0.11, ups=1.71, num_updates=1600, lr=9.9e-05, gnorm=3.243, clip=4, train_wall=55, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:29:23]    INFO >> epoch 001:   1700 / 5058 loss=0.113, ups=1.56, num_updates=1700, lr=9.9e-05, gnorm=2.319, clip=2, train_wall=60, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:30:19]    INFO >> epoch 001:   1800 / 5058 loss=0.12, ups=1.89, num_updates=1800, lr=9.8e-05, gnorm=3.115, clip=5, train_wall=50, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:30:58]    INFO >> epoch 001:   1900 / 5058 loss=0.153, ups=2.63, num_updates=1900, lr=9.8e-05, gnorm=1.952, clip=4, train_wall=36, gb_free=67.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:31:59]    INFO >> epoch 001:   2000 / 5058 loss=0.089, ups=1.67, num_updates=2000, lr=9.8e-05, gnorm=1.422, clip=1, train_wall=56, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:33:07]    INFO >> epoch 001:   2100 / 5058 loss=0.074, ups=1.56, num_updates=2100, lr=9.8e-05, gnorm=3.142, clip=2, train_wall=60, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:34:14]    INFO >> epoch 001:   2200 / 5058 loss=0.279, ups=1.56, num_updates=2200, lr=9.8e-05, gnorm=5.078, clip=6, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:35:22]    INFO >> epoch 001:   2300 / 5058 loss=0.068, ups=1.54, num_updates=2300, lr=9.7e-05, gnorm=2.452, clip=5, train_wall=61, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:36:26]    INFO >> epoch 001:   2400 / 5058 loss=0.063, ups=1.6, num_updates=2400, lr=9.7e-05, gnorm=0.773, clip=1, train_wall=59, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:37:37]    INFO >> epoch 001:   2500 / 5058 loss=0.149, ups=1.48, num_updates=2500, lr=9.7e-05, gnorm=3.718, clip=1, train_wall=63, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:38:35]    INFO >> epoch 001:   2600 / 5058 loss=0.128, ups=1.8, num_updates=2600, lr=9.7e-05, gnorm=2.608, clip=1, train_wall=52, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:39:35]    INFO >> epoch 001:   2700 / 5058 loss=0.076, ups=1.72, num_updates=2700, lr=9.7e-05, gnorm=1.387, clip=1, train_wall=55, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:40:38]    INFO >> epoch 001:   2800 / 5058 loss=0.081, ups=1.67, num_updates=2800, lr=9.6e-05, gnorm=1.32, clip=3, train_wall=56, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:41:18]    INFO >> epoch 001:   2900 / 5058 loss=0.274, ups=2.58, num_updates=2900, lr=9.6e-05, gnorm=2.005, clip=3, train_wall=37, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:42:24]    INFO >> epoch 001:   3000 / 5058 loss=0.145, ups=1.57, num_updates=3000, lr=9.6e-05, gnorm=3.103, clip=5, train_wall=59, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:43:19]    INFO >> epoch 001:   3100 / 5058 loss=0.058, ups=1.91, num_updates=3100, lr=9.6e-05, gnorm=0.282, clip=0, train_wall=49, gb_free=73.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:44:16]    INFO >> epoch 001:   3200 / 5058 loss=0.282, ups=1.8, num_updates=3200, lr=9.6e-05, gnorm=2.208, clip=2, train_wall=52, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:45:17]    INFO >> epoch 001:   3300 / 5058 loss=0.086, ups=1.71, num_updates=3300, lr=9.5e-05, gnorm=1.673, clip=1, train_wall=55, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:46:29]    INFO >> epoch 001:   3400 / 5058 loss=0.225, ups=1.45, num_updates=3400, lr=9.5e-05, gnorm=5.264, clip=3, train_wall=64, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:47:35]    INFO >> epoch 001:   3500 / 5058 loss=0.131, ups=1.57, num_updates=3500, lr=9.5e-05, gnorm=1.53, clip=0, train_wall=60, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:48:42]    INFO >> epoch 001:   3600 / 5058 loss=0.095, ups=1.57, num_updates=3600, lr=9.5e-05, gnorm=2.575, clip=1, train_wall=60, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:49:44]    INFO >> epoch 001:   3700 / 5058 loss=0.137, ups=1.69, num_updates=3700, lr=9.4e-05, gnorm=1.854, clip=2, train_wall=55, gb_free=66.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:50:51]    INFO >> epoch 001:   3800 / 5058 loss=0.058, ups=1.55, num_updates=3800, lr=9.4e-05, gnorm=1.191, clip=1, train_wall=60, gb_free=68.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:51:42]    INFO >> epoch 001:   3900 / 5058 loss=0.127, ups=2.05, num_updates=3900, lr=9.4e-05, gnorm=1.978, clip=2, train_wall=46, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:52:45]    INFO >> epoch 001:   4000 / 5058 loss=0.131, ups=1.66, num_updates=4000, lr=9.4e-05, gnorm=1.731, clip=1, train_wall=56, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:53:48]    INFO >> epoch 001:   4100 / 5058 loss=0.92, ups=1.64, num_updates=4100, lr=9.4e-05, gnorm=0.902, clip=1, train_wall=57, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:54:55]    INFO >> epoch 001:   4200 / 5058 loss=0.149, ups=1.54, num_updates=4200, lr=9.3e-05, gnorm=1.845, clip=3, train_wall=61, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:55:56]    INFO >> epoch 001:   4300 / 5058 loss=0.117, ups=1.73, num_updates=4300, lr=9.3e-05, gnorm=1.721, clip=1, train_wall=54, gb_free=73.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:56:58]    INFO >> epoch 001:   4400 / 5058 loss=0.082, ups=1.69, num_updates=4400, lr=9.3e-05, gnorm=1.476, clip=0, train_wall=55, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:58:01]    INFO >> epoch 001:   4500 / 5058 loss=0.072, ups=1.63, num_updates=4500, lr=9.3e-05, gnorm=1.371, clip=0, train_wall=57, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 01:59:08]    INFO >> epoch 001:   4600 / 5058 loss=0.241, ups=1.55, num_updates=4600, lr=9.3e-05, gnorm=1.805, clip=0, train_wall=60, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:00:20]    INFO >> epoch 001:   4700 / 5058 loss=0.215, ups=1.45, num_updates=4700, lr=9.2e-05, gnorm=3.803, clip=2, train_wall=64, gb_free=64.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:01:22]    INFO >> epoch 001:   4800 / 5058 loss=0.056, ups=1.68, num_updates=4800, lr=9.2e-05, gnorm=1.411, clip=0, train_wall=55, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:02:03]    INFO >> epoch 001:   4900 / 5058 loss=0.142, ups=2.5, num_updates=4900, lr=9.2e-05, gnorm=0.824, clip=0, train_wall=38, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:03:09]    INFO >> epoch 001:   5000 / 5058 loss=0.09, ups=1.59, num_updates=5000, lr=9.2e-05, gnorm=2.488, clip=0, train_wall=59, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:03:49]    INFO >> epoch 001 | loss 0.105 | ups 1.71 | num_updates 5058 | lr 9.2e-05 | gnorm 2.248 | clip 2.1 | train_wall 2771 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-20 02:03:49] WARNING >> 4469 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[2, 4, 10, 12, 13, 14, 16, 18, 19, 23] (data_utils.py:301, filter_by_size())[0m
[33m[2025-11-20 02:03:49] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
CodeEncoderLSTM: Creating BiLSTM with 6 layers, 512 hidden and input size
Traceback (most recent call last):
  File "run/type_prediction/transformer/train.py", line 339, in <module>
    cli_main()
  File "run/type_prediction/transformer/train.py", line 332, in cli_main
    single_main(args)
  File "run/type_prediction/transformer/train.py", line 254, in single_main
    valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)
  File "run/type_prediction/transformer/train.py", line 132, in validate
    trainer.valid_step(sample)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/trainers/ncc_trainers.py", line 565, in valid_step
    _loss, sample_size, logging_output = self.task.valid_step(
TypeError: cannot unpack non-iterable NoneType object

============================================================
Setting up experiment: exp_d_model_256
============================================================
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256/checkpoints
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256/logs
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256/results
âœ“ Saved config to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256/config.yml
âœ“ Also saved to: run/type_prediction/transformer/config/exp_d_model_256.yml
âœ“ Saved experiment info to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256/info.txt

============================================================
Starting training...
============================================================

Command: python run/type_prediction/transformer/train.py --language exp_d_model_256 2>&1 | tee /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256/logs/train.log

Note: Config file will be loaded from run/type_prediction/transformer/config/exp_d_model_256.yml


âœ“ Training completed successfully!

============================================================
Evaluating model on test set...
============================================================

Warning: Best checkpoint not found, using last checkpoint
âœ— No checkpoint found for evaluation

============================================================
âœ“ Experiment completed: exp_d_model_256
âœ“ Results in: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_256
============================================================


âœ“ exp_d_model_256 completed in 0.86 hours

============================================================
Running: exp_d_model_512
GPU: 3
Time: 2025-11-20T02:03:52.566098
============================================================
[32m[2025-11-20 02:03:54]    INFO >> Load arguments in run/type_prediction/transformer/config/exp_d_model_512.yml (train.py:300, cli_main())[0m
[32m[2025-11-20 02:03:54]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_512/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_512/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_512/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:302, cli_main())[0m
[32m[2025-11-20 02:03:54]    INFO >> single GPU training... (train.py:331, cli_main())[0m
[32m[2025-11-20 02:03:54]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_512/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_512/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_512/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:208, single_main())[0m
[32m[2025-11-20 02:03:57]    INFO >> TypeTransformer(
  (decoder): RobertaEncoder(
    (encoder): CodeEncoderLSTM(
      (embedding): Embedding(10006, 512)
      (pos_encoder): PositionalEncoding()
      (encoder): LSTM(512, 512, num_layers=6, bidirectional=True)
    )
    (output): Sequential(
      (0): Linear(in_features=1024, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=10006, bias=True)
    )
  )
  (classification_heads): ModuleDict()
) (train.py:220, single_main())[0m
[32m[2025-11-20 02:03:57]    INFO >> model typetransformer, criterion TypePredictionCrossEntropyCriterion (train.py:221, single_main())[0m
[32m[2025-11-20 02:03:57]    INFO >> num. model params: 46481686 (num. trained: 46481686) (train.py:222, single_main())[0m
[32m[2025-11-20 02:03:57]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 02:03:57]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 38699 MB ; used memory = 43220 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 02:03:57]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 02:03:57]    INFO >> training on 1 GPUs (train.py:229, single_main())[0m
[32m[2025-11-20 02:03:57]    INFO >> max tokens per GPU = None and max sentences per GPU = 32 (train.py:230, single_main())[0m
[32m[2025-11-20 02:03:57]    INFO >> no existing checkpoint found /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_512/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-20 02:03:57]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[33m[2025-11-20 02:04:07] WARNING >> 32222 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[0, 1, 2, 4, 6, 7, 8, 10, 14, 19] (data_utils.py:301, filter_by_size())[0m
[32m[2025-11-20 02:04:07]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-20 02:04:08] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:398: UserWarning: ntokens not found in Criterion logging outputs, cannot log wpb or wps
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:407: UserWarning: nsentences not found in Criterion logging outputs, cannot log bsz
  warnings.warn(
[32m[2025-11-20 02:04:52]    INFO >> epoch 001:    100 / 5058 loss=1.903, ups=2.37, num_updates=100, lr=1e-05, gnorm=0.104, clip=0, train_wall=40, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:05:41]    INFO >> epoch 001:    200 / 5058 loss=0.786, ups=2.16, num_updates=200, lr=2e-05, gnorm=0.068, clip=0, train_wall=44, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:06:45]    INFO >> epoch 001:    300 / 5058 loss=0.654, ups=1.63, num_updates=300, lr=3e-05, gnorm=0.053, clip=0, train_wall=58, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:07:48]    INFO >> epoch 001:    400 / 5058 loss=0.623, ups=1.59, num_updates=400, lr=4e-05, gnorm=2.819, clip=2, train_wall=59, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:08:51]    INFO >> epoch 001:    500 / 5058 loss=0.208, ups=1.67, num_updates=500, lr=5e-05, gnorm=4.476, clip=2, train_wall=56, gb_free=65.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:09:52]    INFO >> epoch 001:    600 / 5058 loss=0.079, ups=1.71, num_updates=600, lr=6e-05, gnorm=4.788, clip=8, train_wall=55, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:10:35]    INFO >> epoch 001:    700 / 5058 loss=0.174, ups=2.43, num_updates=700, lr=7e-05, gnorm=2.641, clip=4, train_wall=39, gb_free=69.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:11:18]    INFO >> epoch 001:    800 / 5058 loss=0.166, ups=2.46, num_updates=800, lr=8e-05, gnorm=3.325, clip=8, train_wall=39, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:12:25]    INFO >> epoch 001:    900 / 5058 loss=0.216, ups=1.56, num_updates=900, lr=9e-05, gnorm=6.91, clip=8, train_wall=60, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:13:29]    INFO >> epoch 001:   1000 / 5058 loss=0.142, ups=1.62, num_updates=1000, lr=0.0001, gnorm=1.451, clip=0, train_wall=58, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:14:28]    INFO >> epoch 001:   1100 / 5058 loss=0.058, ups=1.76, num_updates=1100, lr=0.0001, gnorm=2.202, clip=2, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:15:31]    INFO >> epoch 001:   1200 / 5058 loss=0.035, ups=1.64, num_updates=1200, lr=0.0001, gnorm=0.871, clip=0, train_wall=57, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:16:36]    INFO >> epoch 001:   1300 / 5058 loss=0.087, ups=1.61, num_updates=1300, lr=9.9e-05, gnorm=1.877, clip=2, train_wall=58, gb_free=67.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:17:39]    INFO >> epoch 001:   1400 / 5058 loss=0.059, ups=1.65, num_updates=1400, lr=9.9e-05, gnorm=2.488, clip=4, train_wall=57, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:18:44]    INFO >> epoch 001:   1500 / 5058 loss=0.279, ups=1.62, num_updates=1500, lr=9.9e-05, gnorm=1.776, clip=2, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:19:45]    INFO >> epoch 001:   1600 / 5058 loss=0.11, ups=1.73, num_updates=1600, lr=9.9e-05, gnorm=3.243, clip=4, train_wall=55, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:20:51]    INFO >> epoch 001:   1700 / 5058 loss=0.113, ups=1.58, num_updates=1700, lr=9.9e-05, gnorm=2.319, clip=2, train_wall=59, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:21:44]    INFO >> epoch 001:   1800 / 5058 loss=0.12, ups=1.91, num_updates=1800, lr=9.8e-05, gnorm=3.115, clip=5, train_wall=49, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:22:25]    INFO >> epoch 001:   1900 / 5058 loss=0.153, ups=2.66, num_updates=1900, lr=9.8e-05, gnorm=1.952, clip=4, train_wall=36, gb_free=67.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:23:25]    INFO >> epoch 001:   2000 / 5058 loss=0.089, ups=1.69, num_updates=2000, lr=9.8e-05, gnorm=1.422, clip=1, train_wall=56, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:24:31]    INFO >> epoch 001:   2100 / 5058 loss=0.074, ups=1.58, num_updates=2100, lr=9.8e-05, gnorm=3.142, clip=2, train_wall=60, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:25:37]    INFO >> epoch 001:   2200 / 5058 loss=0.279, ups=1.57, num_updates=2200, lr=9.8e-05, gnorm=5.078, clip=6, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:26:44]    INFO >> epoch 001:   2300 / 5058 loss=0.068, ups=1.56, num_updates=2300, lr=9.7e-05, gnorm=2.452, clip=5, train_wall=60, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:27:49]    INFO >> epoch 001:   2400 / 5058 loss=0.063, ups=1.61, num_updates=2400, lr=9.7e-05, gnorm=0.773, clip=1, train_wall=58, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:28:58]    INFO >> epoch 001:   2500 / 5058 loss=0.149, ups=1.5, num_updates=2500, lr=9.7e-05, gnorm=3.718, clip=1, train_wall=63, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:29:56]    INFO >> epoch 001:   2600 / 5058 loss=0.128, ups=1.82, num_updates=2600, lr=9.7e-05, gnorm=2.608, clip=1, train_wall=52, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:30:56]    INFO >> epoch 001:   2700 / 5058 loss=0.076, ups=1.74, num_updates=2700, lr=9.7e-05, gnorm=1.387, clip=1, train_wall=54, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:31:58]    INFO >> epoch 001:   2800 / 5058 loss=0.081, ups=1.69, num_updates=2800, lr=9.6e-05, gnorm=1.32, clip=3, train_wall=56, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:32:38]    INFO >> epoch 001:   2900 / 5058 loss=0.274, ups=2.61, num_updates=2900, lr=9.6e-05, gnorm=2.005, clip=3, train_wall=36, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:33:43]    INFO >> epoch 001:   3000 / 5058 loss=0.145, ups=1.59, num_updates=3000, lr=9.6e-05, gnorm=3.103, clip=5, train_wall=59, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:34:36]    INFO >> epoch 001:   3100 / 5058 loss=0.058, ups=1.94, num_updates=3100, lr=9.6e-05, gnorm=0.282, clip=0, train_wall=49, gb_free=73.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:35:34]    INFO >> epoch 001:   3200 / 5058 loss=0.282, ups=1.82, num_updates=3200, lr=9.6e-05, gnorm=2.208, clip=2, train_wall=52, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:36:34]    INFO >> epoch 001:   3300 / 5058 loss=0.086, ups=1.74, num_updates=3300, lr=9.5e-05, gnorm=1.673, clip=1, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:37:45]    INFO >> epoch 001:   3400 / 5058 loss=0.225, ups=1.47, num_updates=3400, lr=9.5e-05, gnorm=5.264, clip=3, train_wall=64, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:38:50]    INFO >> epoch 001:   3500 / 5058 loss=0.131, ups=1.59, num_updates=3500, lr=9.5e-05, gnorm=1.53, clip=0, train_wall=59, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:39:55]    INFO >> epoch 001:   3600 / 5058 loss=0.095, ups=1.59, num_updates=3600, lr=9.5e-05, gnorm=2.575, clip=1, train_wall=59, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:40:56]    INFO >> epoch 001:   3700 / 5058 loss=0.137, ups=1.72, num_updates=3700, lr=9.4e-05, gnorm=1.854, clip=2, train_wall=55, gb_free=66.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:42:03]    INFO >> epoch 001:   3800 / 5058 loss=0.058, ups=1.57, num_updates=3800, lr=9.4e-05, gnorm=1.191, clip=1, train_wall=60, gb_free=68.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:42:52]    INFO >> epoch 001:   3900 / 5058 loss=0.127, ups=2.08, num_updates=3900, lr=9.4e-05, gnorm=1.978, clip=2, train_wall=45, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:43:54]    INFO >> epoch 001:   4000 / 5058 loss=0.131, ups=1.69, num_updates=4000, lr=9.4e-05, gnorm=1.731, clip=1, train_wall=56, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:44:57]    INFO >> epoch 001:   4100 / 5058 loss=0.92, ups=1.66, num_updates=4100, lr=9.4e-05, gnorm=0.902, clip=1, train_wall=57, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:46:02]    INFO >> epoch 001:   4200 / 5058 loss=0.149, ups=1.57, num_updates=4200, lr=9.3e-05, gnorm=1.845, clip=3, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:47:01]    INFO >> epoch 001:   4300 / 5058 loss=0.117, ups=1.77, num_updates=4300, lr=9.3e-05, gnorm=1.721, clip=1, train_wall=54, gb_free=73.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:48:02]    INFO >> epoch 001:   4400 / 5058 loss=0.082, ups=1.72, num_updates=4400, lr=9.3e-05, gnorm=1.476, clip=0, train_wall=55, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:49:05]    INFO >> epoch 001:   4500 / 5058 loss=0.072, ups=1.66, num_updates=4500, lr=9.3e-05, gnorm=1.371, clip=0, train_wall=57, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:50:11]    INFO >> epoch 001:   4600 / 5058 loss=0.241, ups=1.58, num_updates=4600, lr=9.3e-05, gnorm=1.805, clip=0, train_wall=60, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:51:21]    INFO >> epoch 001:   4700 / 5058 loss=0.215, ups=1.48, num_updates=4700, lr=9.2e-05, gnorm=3.803, clip=2, train_wall=64, gb_free=64.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:52:22]    INFO >> epoch 001:   4800 / 5058 loss=0.056, ups=1.71, num_updates=4800, lr=9.2e-05, gnorm=1.411, clip=0, train_wall=55, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:53:03]    INFO >> epoch 001:   4900 / 5058 loss=0.142, ups=2.54, num_updates=4900, lr=9.2e-05, gnorm=0.824, clip=0, train_wall=37, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:54:07]    INFO >> epoch 001:   5000 / 5058 loss=0.09, ups=1.62, num_updates=5000, lr=9.2e-05, gnorm=2.488, clip=0, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:54:47]    INFO >> epoch 001 | loss 0.105 | ups 1.73 | num_updates 5058 | lr 9.2e-05 | gnorm 2.248 | clip 2.1 | train_wall 2753 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-20 02:54:47] WARNING >> 4469 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[2, 4, 10, 12, 13, 14, 16, 18, 19, 23] (data_utils.py:301, filter_by_size())[0m
[33m[2025-11-20 02:54:47] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
CodeEncoderLSTM: Creating BiLSTM with 6 layers, 512 hidden and input size
Traceback (most recent call last):
  File "run/type_prediction/transformer/train.py", line 339, in <module>
    cli_main()
  File "run/type_prediction/transformer/train.py", line 332, in cli_main
    single_main(args)
  File "run/type_prediction/transformer/train.py", line 254, in single_main
    valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)
  File "run/type_prediction/transformer/train.py", line 132, in validate
    trainer.valid_step(sample)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/trainers/ncc_trainers.py", line 565, in valid_step
    _loss, sample_size, logging_output = self.task.valid_step(
TypeError: cannot unpack non-iterable NoneType object

============================================================
Setting up experiment: exp_d_model_512
============================================================
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_512
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_512/checkpoints
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_512/logs
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_512/results
âœ“ Saved config to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_512/config.yml
âœ“ Also saved to: run/type_prediction/transformer/config/exp_d_model_512.yml
âœ“ Saved experiment info to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_512/info.txt

============================================================
Starting training...
============================================================

Command: python run/type_prediction/transformer/train.py --language exp_d_model_512 2>&1 | tee /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_512/logs/train.log

Note: Config file will be loaded from run/type_prediction/transformer/config/exp_d_model_512.yml


âœ“ Training completed successfully!

============================================================
Evaluating model on test set...
============================================================

Warning: Best checkpoint not found, using last checkpoint
âœ— No checkpoint found for evaluation

============================================================
âœ“ Experiment completed: exp_d_model_512
âœ“ Results in: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_512
============================================================


âœ“ exp_d_model_512 completed in 0.85 hours

============================================================
Running: exp_d_model_1024
GPU: 3
Time: 2025-11-20T02:54:50.448474
============================================================
[32m[2025-11-20 02:54:52]    INFO >> Load arguments in run/type_prediction/transformer/config/exp_d_model_1024.yml (train.py:300, cli_main())[0m
[32m[2025-11-20 02:54:52]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_1024/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_1024/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_1024/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:302, cli_main())[0m
[32m[2025-11-20 02:54:52]    INFO >> single GPU training... (train.py:331, cli_main())[0m
[32m[2025-11-20 02:54:53]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_1024/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_1024/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_1024/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:208, single_main())[0m
[32m[2025-11-20 02:54:55]    INFO >> TypeTransformer(
  (decoder): RobertaEncoder(
    (encoder): CodeEncoderLSTM(
      (embedding): Embedding(10006, 512)
      (pos_encoder): PositionalEncoding()
      (encoder): LSTM(512, 512, num_layers=6, bidirectional=True)
    )
    (output): Sequential(
      (0): Linear(in_features=1024, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=10006, bias=True)
    )
  )
  (classification_heads): ModuleDict()
) (train.py:220, single_main())[0m
[32m[2025-11-20 02:54:55]    INFO >> model typetransformer, criterion TypePredictionCrossEntropyCriterion (train.py:221, single_main())[0m
[32m[2025-11-20 02:54:55]    INFO >> num. model params: 46481686 (num. trained: 46481686) (train.py:222, single_main())[0m
[32m[2025-11-20 02:54:55]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 02:54:55]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 38699 MB ; used memory = 43220 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 02:54:55]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 02:54:55]    INFO >> training on 1 GPUs (train.py:229, single_main())[0m
[32m[2025-11-20 02:54:55]    INFO >> max tokens per GPU = None and max sentences per GPU = 32 (train.py:230, single_main())[0m
[32m[2025-11-20 02:54:55]    INFO >> no existing checkpoint found /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_1024/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-20 02:54:55]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[33m[2025-11-20 02:55:05] WARNING >> 32222 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[0, 1, 2, 4, 6, 7, 8, 10, 14, 19] (data_utils.py:301, filter_by_size())[0m
[32m[2025-11-20 02:55:05]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-20 02:55:05] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:398: UserWarning: ntokens not found in Criterion logging outputs, cannot log wpb or wps
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:407: UserWarning: nsentences not found in Criterion logging outputs, cannot log bsz
  warnings.warn(
[32m[2025-11-20 02:55:50]    INFO >> epoch 001:    100 / 5058 loss=1.903, ups=2.37, num_updates=100, lr=1e-05, gnorm=0.104, clip=0, train_wall=40, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:56:38]    INFO >> epoch 001:    200 / 5058 loss=0.786, ups=2.16, num_updates=200, lr=2e-05, gnorm=0.068, clip=0, train_wall=44, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:57:43]    INFO >> epoch 001:    300 / 5058 loss=0.654, ups=1.63, num_updates=300, lr=3e-05, gnorm=0.053, clip=0, train_wall=58, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:58:48]    INFO >> epoch 001:    400 / 5058 loss=0.623, ups=1.59, num_updates=400, lr=4e-05, gnorm=2.819, clip=2, train_wall=59, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 02:59:49]    INFO >> epoch 001:    500 / 5058 loss=0.208, ups=1.67, num_updates=500, lr=5e-05, gnorm=4.476, clip=2, train_wall=56, gb_free=65.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:00:50]    INFO >> epoch 001:    600 / 5058 loss=0.079, ups=1.71, num_updates=600, lr=6e-05, gnorm=4.788, clip=8, train_wall=55, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:01:33]    INFO >> epoch 001:    700 / 5058 loss=0.174, ups=2.43, num_updates=700, lr=7e-05, gnorm=2.641, clip=4, train_wall=39, gb_free=69.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:02:16]    INFO >> epoch 001:    800 / 5058 loss=0.166, ups=2.46, num_updates=800, lr=8e-05, gnorm=3.325, clip=8, train_wall=39, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:03:23]    INFO >> epoch 001:    900 / 5058 loss=0.216, ups=1.56, num_updates=900, lr=9e-05, gnorm=6.91, clip=8, train_wall=60, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:04:27]    INFO >> epoch 001:   1000 / 5058 loss=0.142, ups=1.62, num_updates=1000, lr=0.0001, gnorm=1.451, clip=0, train_wall=58, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:05:26]    INFO >> epoch 001:   1100 / 5058 loss=0.058, ups=1.76, num_updates=1100, lr=0.0001, gnorm=2.202, clip=2, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:06:29]    INFO >> epoch 001:   1200 / 5058 loss=0.035, ups=1.64, num_updates=1200, lr=0.0001, gnorm=0.871, clip=0, train_wall=57, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:07:34]    INFO >> epoch 001:   1300 / 5058 loss=0.087, ups=1.61, num_updates=1300, lr=9.9e-05, gnorm=1.877, clip=2, train_wall=58, gb_free=67.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:08:38]    INFO >> epoch 001:   1400 / 5058 loss=0.059, ups=1.65, num_updates=1400, lr=9.9e-05, gnorm=2.488, clip=4, train_wall=57, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:09:42]    INFO >> epoch 001:   1500 / 5058 loss=0.279, ups=1.61, num_updates=1500, lr=9.9e-05, gnorm=1.776, clip=2, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:10:43]    INFO >> epoch 001:   1600 / 5058 loss=0.11, ups=1.73, num_updates=1600, lr=9.9e-05, gnorm=3.243, clip=4, train_wall=55, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:11:49]    INFO >> epoch 001:   1700 / 5058 loss=0.113, ups=1.58, num_updates=1700, lr=9.9e-05, gnorm=2.319, clip=2, train_wall=59, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:12:42]    INFO >> epoch 001:   1800 / 5058 loss=0.12, ups=1.91, num_updates=1800, lr=9.8e-05, gnorm=3.115, clip=5, train_wall=49, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:13:21]    INFO >> epoch 001:   1900 / 5058 loss=0.153, ups=2.66, num_updates=1900, lr=9.8e-05, gnorm=1.952, clip=4, train_wall=36, gb_free=67.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:14:23]    INFO >> epoch 001:   2000 / 5058 loss=0.089, ups=1.69, num_updates=2000, lr=9.8e-05, gnorm=1.422, clip=1, train_wall=56, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:15:29]    INFO >> epoch 001:   2100 / 5058 loss=0.074, ups=1.58, num_updates=2100, lr=9.8e-05, gnorm=3.142, clip=2, train_wall=59, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:16:35]    INFO >> epoch 001:   2200 / 5058 loss=0.279, ups=1.58, num_updates=2200, lr=9.8e-05, gnorm=5.078, clip=6, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:17:42]    INFO >> epoch 001:   2300 / 5058 loss=0.068, ups=1.56, num_updates=2300, lr=9.7e-05, gnorm=2.452, clip=5, train_wall=60, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:18:47]    INFO >> epoch 001:   2400 / 5058 loss=0.063, ups=1.62, num_updates=2400, lr=9.7e-05, gnorm=0.773, clip=1, train_wall=58, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:19:56]    INFO >> epoch 001:   2500 / 5058 loss=0.149, ups=1.5, num_updates=2500, lr=9.7e-05, gnorm=3.718, clip=1, train_wall=62, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:20:54]    INFO >> epoch 001:   2600 / 5058 loss=0.128, ups=1.81, num_updates=2600, lr=9.7e-05, gnorm=2.608, clip=1, train_wall=52, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:21:54]    INFO >> epoch 001:   2700 / 5058 loss=0.076, ups=1.74, num_updates=2700, lr=9.7e-05, gnorm=1.387, clip=1, train_wall=54, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:22:56]    INFO >> epoch 001:   2800 / 5058 loss=0.081, ups=1.69, num_updates=2800, lr=9.6e-05, gnorm=1.32, clip=3, train_wall=56, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:23:36]    INFO >> epoch 001:   2900 / 5058 loss=0.274, ups=2.61, num_updates=2900, lr=9.6e-05, gnorm=2.005, clip=3, train_wall=36, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:24:41]    INFO >> epoch 001:   3000 / 5058 loss=0.145, ups=1.59, num_updates=3000, lr=9.6e-05, gnorm=3.103, clip=5, train_wall=59, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:25:34]    INFO >> epoch 001:   3100 / 5058 loss=0.058, ups=1.94, num_updates=3100, lr=9.6e-05, gnorm=0.282, clip=0, train_wall=49, gb_free=73.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:26:31]    INFO >> epoch 001:   3200 / 5058 loss=0.282, ups=1.82, num_updates=3200, lr=9.6e-05, gnorm=2.208, clip=2, train_wall=52, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:27:32]    INFO >> epoch 001:   3300 / 5058 loss=0.086, ups=1.74, num_updates=3300, lr=9.5e-05, gnorm=1.673, clip=1, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:28:42]    INFO >> epoch 001:   3400 / 5058 loss=0.225, ups=1.47, num_updates=3400, lr=9.5e-05, gnorm=5.264, clip=3, train_wall=64, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:29:48]    INFO >> epoch 001:   3500 / 5058 loss=0.131, ups=1.6, num_updates=3500, lr=9.5e-05, gnorm=1.53, clip=0, train_wall=59, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:30:53]    INFO >> epoch 001:   3600 / 5058 loss=0.095, ups=1.6, num_updates=3600, lr=9.5e-05, gnorm=2.575, clip=1, train_wall=59, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:31:54]    INFO >> epoch 001:   3700 / 5058 loss=0.137, ups=1.73, num_updates=3700, lr=9.4e-05, gnorm=1.854, clip=2, train_wall=55, gb_free=66.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:32:59]    INFO >> epoch 001:   3800 / 5058 loss=0.058, ups=1.57, num_updates=3800, lr=9.4e-05, gnorm=1.191, clip=1, train_wall=60, gb_free=68.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:33:50]    INFO >> epoch 001:   3900 / 5058 loss=0.127, ups=2.08, num_updates=3900, lr=9.4e-05, gnorm=1.978, clip=2, train_wall=45, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:34:51]    INFO >> epoch 001:   4000 / 5058 loss=0.131, ups=1.69, num_updates=4000, lr=9.4e-05, gnorm=1.731, clip=1, train_wall=56, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:35:54]    INFO >> epoch 001:   4100 / 5058 loss=0.92, ups=1.66, num_updates=4100, lr=9.4e-05, gnorm=0.902, clip=1, train_wall=57, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:37:01]    INFO >> epoch 001:   4200 / 5058 loss=0.149, ups=1.56, num_updates=4200, lr=9.3e-05, gnorm=1.845, clip=3, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:38:00]    INFO >> epoch 001:   4300 / 5058 loss=0.117, ups=1.74, num_updates=4300, lr=9.3e-05, gnorm=1.721, clip=1, train_wall=53, gb_free=73.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:39:01]    INFO >> epoch 001:   4400 / 5058 loss=0.082, ups=1.7, num_updates=4400, lr=9.3e-05, gnorm=1.476, clip=0, train_wall=55, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:40:05]    INFO >> epoch 001:   4500 / 5058 loss=0.072, ups=1.64, num_updates=4500, lr=9.3e-05, gnorm=1.371, clip=0, train_wall=57, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:41:11]    INFO >> epoch 001:   4600 / 5058 loss=0.241, ups=1.56, num_updates=4600, lr=9.3e-05, gnorm=1.805, clip=0, train_wall=59, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:42:23]    INFO >> epoch 001:   4700 / 5058 loss=0.215, ups=1.45, num_updates=4700, lr=9.2e-05, gnorm=3.803, clip=2, train_wall=63, gb_free=64.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:43:26]    INFO >> epoch 001:   4800 / 5058 loss=0.056, ups=1.67, num_updates=4800, lr=9.2e-05, gnorm=1.411, clip=0, train_wall=55, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:44:07]    INFO >> epoch 001:   4900 / 5058 loss=0.142, ups=2.48, num_updates=4900, lr=9.2e-05, gnorm=0.824, clip=0, train_wall=37, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:45:13]    INFO >> epoch 001:   5000 / 5058 loss=0.09, ups=1.58, num_updates=5000, lr=9.2e-05, gnorm=2.488, clip=0, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:45:55]    INFO >> epoch 001 | loss 0.105 | ups 1.73 | num_updates 5058 | lr 9.2e-05 | gnorm 2.248 | clip 2.1 | train_wall 2748 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-20 03:45:55] WARNING >> 4469 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[2, 4, 10, 12, 13, 14, 16, 18, 19, 23] (data_utils.py:301, filter_by_size())[0m
[33m[2025-11-20 03:45:55] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
CodeEncoderLSTM: Creating BiLSTM with 6 layers, 512 hidden and input size
Traceback (most recent call last):
  File "run/type_prediction/transformer/train.py", line 339, in <module>
    cli_main()
  File "run/type_prediction/transformer/train.py", line 332, in cli_main
    single_main(args)
  File "run/type_prediction/transformer/train.py", line 254, in single_main
    valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)
  File "run/type_prediction/transformer/train.py", line 132, in validate
    trainer.valid_step(sample)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/trainers/ncc_trainers.py", line 565, in valid_step
    _loss, sample_size, logging_output = self.task.valid_step(
TypeError: cannot unpack non-iterable NoneType object

============================================================
Setting up experiment: exp_d_model_1024
============================================================
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_1024
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_1024/checkpoints
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_1024/logs
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_1024/results
âœ“ Saved config to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_1024/config.yml
âœ“ Also saved to: run/type_prediction/transformer/config/exp_d_model_1024.yml
âœ“ Saved experiment info to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_1024/info.txt

============================================================
Starting training...
============================================================

Command: python run/type_prediction/transformer/train.py --language exp_d_model_1024 2>&1 | tee /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_1024/logs/train.log

Note: Config file will be loaded from run/type_prediction/transformer/config/exp_d_model_1024.yml


âœ“ Training completed successfully!

============================================================
Evaluating model on test set...
============================================================

Warning: Best checkpoint not found, using last checkpoint
âœ— No checkpoint found for evaluation

============================================================
âœ“ Experiment completed: exp_d_model_1024
âœ“ Results in: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_d_model_1024
============================================================


âœ“ exp_d_model_1024 completed in 0.85 hours

============================================================
Running: exp_layers_4
GPU: 3
Time: 2025-11-20T03:45:58.786184
============================================================
[32m[2025-11-20 03:46:00]    INFO >> Load arguments in run/type_prediction/transformer/config/exp_layers_4.yml (train.py:300, cli_main())[0m
[32m[2025-11-20 03:46:00]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_4/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 4, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_4/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_4/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:302, cli_main())[0m
[32m[2025-11-20 03:46:00]    INFO >> single GPU training... (train.py:331, cli_main())[0m
[32m[2025-11-20 03:46:00]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_4/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 4, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_4/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_4/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:208, single_main())[0m
[32m[2025-11-20 03:46:02]    INFO >> TypeTransformer(
  (decoder): RobertaEncoder(
    (encoder): CodeEncoderLSTM(
      (embedding): Embedding(10006, 512)
      (pos_encoder): PositionalEncoding()
      (encoder): LSTM(512, 512, num_layers=6, bidirectional=True)
    )
    (output): Sequential(
      (0): Linear(in_features=1024, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=10006, bias=True)
    )
  )
  (classification_heads): ModuleDict()
) (train.py:220, single_main())[0m
[32m[2025-11-20 03:46:02]    INFO >> model typetransformer, criterion TypePredictionCrossEntropyCriterion (train.py:221, single_main())[0m
[32m[2025-11-20 03:46:02]    INFO >> num. model params: 46481686 (num. trained: 46481686) (train.py:222, single_main())[0m
[32m[2025-11-20 03:46:02]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 03:46:02]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 38699 MB ; used memory = 43220 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 03:46:02]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 03:46:02]    INFO >> training on 1 GPUs (train.py:229, single_main())[0m
[32m[2025-11-20 03:46:02]    INFO >> max tokens per GPU = None and max sentences per GPU = 32 (train.py:230, single_main())[0m
[32m[2025-11-20 03:46:02]    INFO >> no existing checkpoint found /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_4/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-20 03:46:02]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[33m[2025-11-20 03:46:14] WARNING >> 32222 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[0, 1, 2, 4, 6, 7, 8, 10, 14, 19] (data_utils.py:301, filter_by_size())[0m
[32m[2025-11-20 03:46:14]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-20 03:46:15] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:398: UserWarning: ntokens not found in Criterion logging outputs, cannot log wpb or wps
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:407: UserWarning: nsentences not found in Criterion logging outputs, cannot log bsz
  warnings.warn(
[32m[2025-11-20 03:46:59]    INFO >> epoch 001:    100 / 5058 loss=1.903, ups=2.35, num_updates=100, lr=1e-05, gnorm=0.104, clip=0, train_wall=40, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:47:49]    INFO >> epoch 001:    200 / 5058 loss=0.786, ups=2.14, num_updates=200, lr=2e-05, gnorm=0.068, clip=0, train_wall=44, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:48:53]    INFO >> epoch 001:    300 / 5058 loss=0.654, ups=1.62, num_updates=300, lr=3e-05, gnorm=0.053, clip=0, train_wall=58, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:49:59]    INFO >> epoch 001:    400 / 5058 loss=0.623, ups=1.58, num_updates=400, lr=4e-05, gnorm=2.819, clip=2, train_wall=59, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:51:02]    INFO >> epoch 001:    500 / 5058 loss=0.208, ups=1.66, num_updates=500, lr=5e-05, gnorm=4.476, clip=2, train_wall=56, gb_free=65.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:52:04]    INFO >> epoch 001:    600 / 5058 loss=0.079, ups=1.69, num_updates=600, lr=6e-05, gnorm=4.788, clip=8, train_wall=55, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:52:47]    INFO >> epoch 001:    700 / 5058 loss=0.174, ups=2.4, num_updates=700, lr=7e-05, gnorm=2.641, clip=4, train_wall=39, gb_free=69.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:53:29]    INFO >> epoch 001:    800 / 5058 loss=0.166, ups=2.44, num_updates=800, lr=8e-05, gnorm=3.325, clip=8, train_wall=39, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:54:37]    INFO >> epoch 001:    900 / 5058 loss=0.216, ups=1.54, num_updates=900, lr=9e-05, gnorm=6.91, clip=8, train_wall=60, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:55:42]    INFO >> epoch 001:   1000 / 5058 loss=0.142, ups=1.6, num_updates=1000, lr=0.0001, gnorm=1.451, clip=0, train_wall=58, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:56:42]    INFO >> epoch 001:   1100 / 5058 loss=0.058, ups=1.74, num_updates=1100, lr=0.0001, gnorm=2.202, clip=2, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:57:46]    INFO >> epoch 001:   1200 / 5058 loss=0.035, ups=1.62, num_updates=1200, lr=0.0001, gnorm=0.871, clip=0, train_wall=58, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:58:52]    INFO >> epoch 001:   1300 / 5058 loss=0.087, ups=1.59, num_updates=1300, lr=9.9e-05, gnorm=1.877, clip=2, train_wall=59, gb_free=67.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 03:59:56]    INFO >> epoch 001:   1400 / 5058 loss=0.059, ups=1.62, num_updates=1400, lr=9.9e-05, gnorm=2.488, clip=4, train_wall=57, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:01:01]    INFO >> epoch 001:   1500 / 5058 loss=0.279, ups=1.6, num_updates=1500, lr=9.9e-05, gnorm=1.776, clip=2, train_wall=59, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:02:01]    INFO >> epoch 001:   1600 / 5058 loss=0.11, ups=1.71, num_updates=1600, lr=9.9e-05, gnorm=3.243, clip=4, train_wall=55, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:03:08]    INFO >> epoch 001:   1700 / 5058 loss=0.113, ups=1.57, num_updates=1700, lr=9.9e-05, gnorm=2.319, clip=2, train_wall=60, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:04:03]    INFO >> epoch 001:   1800 / 5058 loss=0.12, ups=1.89, num_updates=1800, lr=9.8e-05, gnorm=3.115, clip=5, train_wall=50, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:04:43]    INFO >> epoch 001:   1900 / 5058 loss=0.153, ups=2.63, num_updates=1900, lr=9.8e-05, gnorm=1.952, clip=4, train_wall=36, gb_free=67.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:05:45]    INFO >> epoch 001:   2000 / 5058 loss=0.089, ups=1.67, num_updates=2000, lr=9.8e-05, gnorm=1.422, clip=1, train_wall=56, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:06:52]    INFO >> epoch 001:   2100 / 5058 loss=0.074, ups=1.55, num_updates=2100, lr=9.8e-05, gnorm=3.142, clip=2, train_wall=60, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:08:00]    INFO >> epoch 001:   2200 / 5058 loss=0.279, ups=1.55, num_updates=2200, lr=9.8e-05, gnorm=5.078, clip=6, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:09:08]    INFO >> epoch 001:   2300 / 5058 loss=0.068, ups=1.53, num_updates=2300, lr=9.7e-05, gnorm=2.452, clip=5, train_wall=61, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:10:13]    INFO >> epoch 001:   2400 / 5058 loss=0.063, ups=1.59, num_updates=2400, lr=9.7e-05, gnorm=0.773, clip=1, train_wall=59, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:11:24]    INFO >> epoch 001:   2500 / 5058 loss=0.149, ups=1.48, num_updates=2500, lr=9.7e-05, gnorm=3.718, clip=1, train_wall=63, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:12:22]    INFO >> epoch 001:   2600 / 5058 loss=0.128, ups=1.79, num_updates=2600, lr=9.7e-05, gnorm=2.608, clip=1, train_wall=52, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:13:23]    INFO >> epoch 001:   2700 / 5058 loss=0.076, ups=1.71, num_updates=2700, lr=9.7e-05, gnorm=1.387, clip=1, train_wall=54, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:14:25]    INFO >> epoch 001:   2800 / 5058 loss=0.081, ups=1.66, num_updates=2800, lr=9.6e-05, gnorm=1.32, clip=3, train_wall=56, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:15:07]    INFO >> epoch 001:   2900 / 5058 loss=0.274, ups=2.57, num_updates=2900, lr=9.6e-05, gnorm=2.005, clip=3, train_wall=37, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:16:13]    INFO >> epoch 001:   3000 / 5058 loss=0.145, ups=1.57, num_updates=3000, lr=9.6e-05, gnorm=3.103, clip=5, train_wall=59, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:17:07]    INFO >> epoch 001:   3100 / 5058 loss=0.058, ups=1.91, num_updates=3100, lr=9.6e-05, gnorm=0.282, clip=0, train_wall=49, gb_free=73.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:18:05]    INFO >> epoch 001:   3200 / 5058 loss=0.282, ups=1.79, num_updates=3200, lr=9.6e-05, gnorm=2.208, clip=2, train_wall=52, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:19:06]    INFO >> epoch 001:   3300 / 5058 loss=0.086, ups=1.71, num_updates=3300, lr=9.5e-05, gnorm=1.673, clip=1, train_wall=55, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:20:18]    INFO >> epoch 001:   3400 / 5058 loss=0.225, ups=1.44, num_updates=3400, lr=9.5e-05, gnorm=5.264, clip=3, train_wall=64, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:21:25]    INFO >> epoch 001:   3500 / 5058 loss=0.131, ups=1.57, num_updates=3500, lr=9.5e-05, gnorm=1.53, clip=0, train_wall=59, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:22:31]    INFO >> epoch 001:   3600 / 5058 loss=0.095, ups=1.57, num_updates=3600, lr=9.5e-05, gnorm=2.575, clip=1, train_wall=59, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:23:33]    INFO >> epoch 001:   3700 / 5058 loss=0.137, ups=1.7, num_updates=3700, lr=9.4e-05, gnorm=1.854, clip=2, train_wall=55, gb_free=66.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:24:40]    INFO >> epoch 001:   3800 / 5058 loss=0.058, ups=1.54, num_updates=3800, lr=9.4e-05, gnorm=1.191, clip=1, train_wall=60, gb_free=68.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:25:30]    INFO >> epoch 001:   3900 / 5058 loss=0.127, ups=2.06, num_updates=3900, lr=9.4e-05, gnorm=1.978, clip=2, train_wall=46, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:26:33]    INFO >> epoch 001:   4000 / 5058 loss=0.131, ups=1.66, num_updates=4000, lr=9.4e-05, gnorm=1.731, clip=1, train_wall=56, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:27:37]    INFO >> epoch 001:   4100 / 5058 loss=0.92, ups=1.64, num_updates=4100, lr=9.4e-05, gnorm=0.902, clip=1, train_wall=57, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:28:45]    INFO >> epoch 001:   4200 / 5058 loss=0.149, ups=1.54, num_updates=4200, lr=9.3e-05, gnorm=1.845, clip=3, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:29:45]    INFO >> epoch 001:   4300 / 5058 loss=0.117, ups=1.74, num_updates=4300, lr=9.3e-05, gnorm=1.721, clip=1, train_wall=54, gb_free=73.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:30:47]    INFO >> epoch 001:   4400 / 5058 loss=0.082, ups=1.69, num_updates=4400, lr=9.3e-05, gnorm=1.476, clip=0, train_wall=55, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:31:51]    INFO >> epoch 001:   4500 / 5058 loss=0.072, ups=1.63, num_updates=4500, lr=9.3e-05, gnorm=1.371, clip=0, train_wall=57, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:32:56]    INFO >> epoch 001:   4600 / 5058 loss=0.241, ups=1.55, num_updates=4600, lr=9.3e-05, gnorm=1.805, clip=0, train_wall=60, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:34:09]    INFO >> epoch 001:   4700 / 5058 loss=0.215, ups=1.45, num_updates=4700, lr=9.2e-05, gnorm=3.803, clip=2, train_wall=64, gb_free=64.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:35:10]    INFO >> epoch 001:   4800 / 5058 loss=0.056, ups=1.68, num_updates=4800, lr=9.2e-05, gnorm=1.411, clip=0, train_wall=55, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:35:52]    INFO >> epoch 001:   4900 / 5058 loss=0.142, ups=2.51, num_updates=4900, lr=9.2e-05, gnorm=0.824, clip=0, train_wall=38, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:36:58]    INFO >> epoch 001:   5000 / 5058 loss=0.09, ups=1.59, num_updates=5000, lr=9.2e-05, gnorm=2.488, clip=0, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:37:38]    INFO >> epoch 001 | loss 0.105 | ups 1.71 | num_updates 5058 | lr 9.2e-05 | gnorm 2.248 | clip 2.1 | train_wall 2762 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-20 04:37:38] WARNING >> 4469 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[2, 4, 10, 12, 13, 14, 16, 18, 19, 23] (data_utils.py:301, filter_by_size())[0m
[33m[2025-11-20 04:37:38] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
CodeEncoderLSTM: Creating BiLSTM with 6 layers, 512 hidden and input size
Traceback (most recent call last):
  File "run/type_prediction/transformer/train.py", line 339, in <module>
    cli_main()
  File "run/type_prediction/transformer/train.py", line 332, in cli_main
    single_main(args)
  File "run/type_prediction/transformer/train.py", line 254, in single_main
    valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)
  File "run/type_prediction/transformer/train.py", line 132, in validate
    trainer.valid_step(sample)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/trainers/ncc_trainers.py", line 565, in valid_step
    _loss, sample_size, logging_output = self.task.valid_step(
TypeError: cannot unpack non-iterable NoneType object

============================================================
Setting up experiment: exp_layers_4
============================================================
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_4
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_4/checkpoints
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_4/logs
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_4/results
âœ“ Saved config to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_4/config.yml
âœ“ Also saved to: run/type_prediction/transformer/config/exp_layers_4.yml
âœ“ Saved experiment info to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_4/info.txt

============================================================
Starting training...
============================================================

Command: python run/type_prediction/transformer/train.py --language exp_layers_4 2>&1 | tee /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_4/logs/train.log

Note: Config file will be loaded from run/type_prediction/transformer/config/exp_layers_4.yml


âœ“ Training completed successfully!

============================================================
Evaluating model on test set...
============================================================

Warning: Best checkpoint not found, using last checkpoint
âœ— No checkpoint found for evaluation

============================================================
âœ“ Experiment completed: exp_layers_4
âœ“ Results in: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_4
============================================================


âœ“ exp_layers_4 completed in 0.86 hours

============================================================
Running: exp_layers_6
GPU: 3
Time: 2025-11-20T04:37:41.527633
============================================================
[32m[2025-11-20 04:37:43]    INFO >> Load arguments in run/type_prediction/transformer/config/exp_layers_6.yml (train.py:300, cli_main())[0m
[32m[2025-11-20 04:37:43]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_6/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_6/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_6/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:302, cli_main())[0m
[32m[2025-11-20 04:37:43]    INFO >> single GPU training... (train.py:331, cli_main())[0m
[32m[2025-11-20 04:37:43]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_6/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_6/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_6/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:208, single_main())[0m
[32m[2025-11-20 04:37:44]    INFO >> TypeTransformer(
  (decoder): RobertaEncoder(
    (encoder): CodeEncoderLSTM(
      (embedding): Embedding(10006, 512)
      (pos_encoder): PositionalEncoding()
      (encoder): LSTM(512, 512, num_layers=6, bidirectional=True)
    )
    (output): Sequential(
      (0): Linear(in_features=1024, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=10006, bias=True)
    )
  )
  (classification_heads): ModuleDict()
) (train.py:220, single_main())[0m
[32m[2025-11-20 04:37:44]    INFO >> model typetransformer, criterion TypePredictionCrossEntropyCriterion (train.py:221, single_main())[0m
[32m[2025-11-20 04:37:44]    INFO >> num. model params: 46481686 (num. trained: 46481686) (train.py:222, single_main())[0m
[32m[2025-11-20 04:37:45]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 04:37:45]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 81024 MB ; used memory = 896 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 04:37:45]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 04:37:45]    INFO >> training on 1 GPUs (train.py:229, single_main())[0m
[32m[2025-11-20 04:37:45]    INFO >> max tokens per GPU = None and max sentences per GPU = 32 (train.py:230, single_main())[0m
[32m[2025-11-20 04:37:45]    INFO >> no existing checkpoint found /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_6/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-20 04:37:45]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[33m[2025-11-20 04:37:55] WARNING >> 32222 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[0, 1, 2, 4, 6, 7, 8, 10, 14, 19] (data_utils.py:301, filter_by_size())[0m
[32m[2025-11-20 04:37:55]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-20 04:37:55] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:398: UserWarning: ntokens not found in Criterion logging outputs, cannot log wpb or wps
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:407: UserWarning: nsentences not found in Criterion logging outputs, cannot log bsz
  warnings.warn(
[32m[2025-11-20 04:38:41]    INFO >> epoch 001:    100 / 5058 loss=1.903, ups=2.35, num_updates=100, lr=1e-05, gnorm=0.104, clip=0, train_wall=41, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:39:29]    INFO >> epoch 001:    200 / 5058 loss=0.786, ups=2.14, num_updates=200, lr=2e-05, gnorm=0.068, clip=0, train_wall=44, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:40:34]    INFO >> epoch 001:    300 / 5058 loss=0.654, ups=1.61, num_updates=300, lr=3e-05, gnorm=0.053, clip=0, train_wall=58, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:41:40]    INFO >> epoch 001:    400 / 5058 loss=0.623, ups=1.58, num_updates=400, lr=4e-05, gnorm=2.819, clip=2, train_wall=60, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:42:43]    INFO >> epoch 001:    500 / 5058 loss=0.208, ups=1.66, num_updates=500, lr=5e-05, gnorm=4.476, clip=2, train_wall=57, gb_free=65.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:43:44]    INFO >> epoch 001:    600 / 5058 loss=0.079, ups=1.7, num_updates=600, lr=6e-05, gnorm=4.788, clip=8, train_wall=56, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:44:27]    INFO >> epoch 001:    700 / 5058 loss=0.174, ups=2.4, num_updates=700, lr=7e-05, gnorm=2.641, clip=4, train_wall=39, gb_free=69.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:45:10]    INFO >> epoch 001:    800 / 5058 loss=0.166, ups=2.44, num_updates=800, lr=8e-05, gnorm=3.325, clip=8, train_wall=39, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:46:17]    INFO >> epoch 001:    900 / 5058 loss=0.216, ups=1.55, num_updates=900, lr=9e-05, gnorm=6.91, clip=8, train_wall=61, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:47:22]    INFO >> epoch 001:   1000 / 5058 loss=0.142, ups=1.6, num_updates=1000, lr=0.0001, gnorm=1.451, clip=0, train_wall=59, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:48:22]    INFO >> epoch 001:   1100 / 5058 loss=0.058, ups=1.74, num_updates=1100, lr=0.0001, gnorm=2.202, clip=2, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:49:27]    INFO >> epoch 001:   1200 / 5058 loss=0.035, ups=1.62, num_updates=1200, lr=0.0001, gnorm=0.871, clip=0, train_wall=58, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:50:32]    INFO >> epoch 001:   1300 / 5058 loss=0.087, ups=1.6, num_updates=1300, lr=9.9e-05, gnorm=1.877, clip=2, train_wall=59, gb_free=67.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:51:36]    INFO >> epoch 001:   1400 / 5058 loss=0.059, ups=1.63, num_updates=1400, lr=9.9e-05, gnorm=2.488, clip=4, train_wall=58, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:52:41]    INFO >> epoch 001:   1500 / 5058 loss=0.279, ups=1.6, num_updates=1500, lr=9.9e-05, gnorm=1.776, clip=2, train_wall=59, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:53:42]    INFO >> epoch 001:   1600 / 5058 loss=0.11, ups=1.71, num_updates=1600, lr=9.9e-05, gnorm=3.243, clip=4, train_wall=55, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:54:47]    INFO >> epoch 001:   1700 / 5058 loss=0.113, ups=1.57, num_updates=1700, lr=9.9e-05, gnorm=2.319, clip=2, train_wall=60, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:55:43]    INFO >> epoch 001:   1800 / 5058 loss=0.12, ups=1.89, num_updates=1800, lr=9.8e-05, gnorm=3.115, clip=5, train_wall=50, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:56:22]    INFO >> epoch 001:   1900 / 5058 loss=0.153, ups=2.64, num_updates=1900, lr=9.8e-05, gnorm=1.952, clip=4, train_wall=36, gb_free=67.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:57:24]    INFO >> epoch 001:   2000 / 5058 loss=0.089, ups=1.68, num_updates=2000, lr=9.8e-05, gnorm=1.422, clip=1, train_wall=56, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:58:31]    INFO >> epoch 001:   2100 / 5058 loss=0.074, ups=1.56, num_updates=2100, lr=9.8e-05, gnorm=3.142, clip=2, train_wall=60, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 04:59:38]    INFO >> epoch 001:   2200 / 5058 loss=0.279, ups=1.56, num_updates=2200, lr=9.8e-05, gnorm=5.078, clip=6, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:00:45]    INFO >> epoch 001:   2300 / 5058 loss=0.068, ups=1.54, num_updates=2300, lr=9.7e-05, gnorm=2.452, clip=5, train_wall=61, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:01:51]    INFO >> epoch 001:   2400 / 5058 loss=0.063, ups=1.6, num_updates=2400, lr=9.7e-05, gnorm=0.773, clip=1, train_wall=59, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:03:01]    INFO >> epoch 001:   2500 / 5058 loss=0.149, ups=1.49, num_updates=2500, lr=9.7e-05, gnorm=3.718, clip=1, train_wall=63, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:03:59]    INFO >> epoch 001:   2600 / 5058 loss=0.128, ups=1.8, num_updates=2600, lr=9.7e-05, gnorm=2.608, clip=1, train_wall=53, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:05:00]    INFO >> epoch 001:   2700 / 5058 loss=0.076, ups=1.72, num_updates=2700, lr=9.7e-05, gnorm=1.387, clip=1, train_wall=55, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:06:02]    INFO >> epoch 001:   2800 / 5058 loss=0.081, ups=1.67, num_updates=2800, lr=9.6e-05, gnorm=1.32, clip=3, train_wall=56, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:06:42]    INFO >> epoch 001:   2900 / 5058 loss=0.274, ups=2.58, num_updates=2900, lr=9.6e-05, gnorm=2.005, clip=3, train_wall=37, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:07:48]    INFO >> epoch 001:   3000 / 5058 loss=0.145, ups=1.58, num_updates=3000, lr=9.6e-05, gnorm=3.103, clip=5, train_wall=60, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:08:42]    INFO >> epoch 001:   3100 / 5058 loss=0.058, ups=1.92, num_updates=3100, lr=9.6e-05, gnorm=0.282, clip=0, train_wall=49, gb_free=73.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:09:40]    INFO >> epoch 001:   3200 / 5058 loss=0.282, ups=1.8, num_updates=3200, lr=9.6e-05, gnorm=2.208, clip=2, train_wall=52, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:10:40]    INFO >> epoch 001:   3300 / 5058 loss=0.086, ups=1.72, num_updates=3300, lr=9.5e-05, gnorm=1.673, clip=1, train_wall=55, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:11:52]    INFO >> epoch 001:   3400 / 5058 loss=0.225, ups=1.46, num_updates=3400, lr=9.5e-05, gnorm=5.264, clip=3, train_wall=64, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:12:58]    INFO >> epoch 001:   3500 / 5058 loss=0.131, ups=1.58, num_updates=3500, lr=9.5e-05, gnorm=1.53, clip=0, train_wall=60, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:14:04]    INFO >> epoch 001:   3600 / 5058 loss=0.095, ups=1.58, num_updates=3600, lr=9.5e-05, gnorm=2.575, clip=1, train_wall=60, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:15:05]    INFO >> epoch 001:   3700 / 5058 loss=0.137, ups=1.71, num_updates=3700, lr=9.4e-05, gnorm=1.854, clip=2, train_wall=55, gb_free=66.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:16:12]    INFO >> epoch 001:   3800 / 5058 loss=0.058, ups=1.55, num_updates=3800, lr=9.4e-05, gnorm=1.191, clip=1, train_wall=60, gb_free=68.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:17:02]    INFO >> epoch 001:   3900 / 5058 loss=0.127, ups=2.06, num_updates=3900, lr=9.4e-05, gnorm=1.978, clip=2, train_wall=46, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:18:04]    INFO >> epoch 001:   4000 / 5058 loss=0.131, ups=1.67, num_updates=4000, lr=9.4e-05, gnorm=1.731, clip=1, train_wall=56, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:19:08]    INFO >> epoch 001:   4100 / 5058 loss=0.92, ups=1.65, num_updates=4100, lr=9.4e-05, gnorm=0.902, clip=1, train_wall=57, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:20:15]    INFO >> epoch 001:   4200 / 5058 loss=0.149, ups=1.55, num_updates=4200, lr=9.3e-05, gnorm=1.845, clip=3, train_wall=61, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:21:15]    INFO >> epoch 001:   4300 / 5058 loss=0.117, ups=1.75, num_updates=4300, lr=9.3e-05, gnorm=1.721, clip=1, train_wall=54, gb_free=73.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:22:16]    INFO >> epoch 001:   4400 / 5058 loss=0.082, ups=1.7, num_updates=4400, lr=9.3e-05, gnorm=1.476, clip=0, train_wall=55, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:23:18]    INFO >> epoch 001:   4500 / 5058 loss=0.072, ups=1.65, num_updates=4500, lr=9.3e-05, gnorm=1.371, clip=0, train_wall=57, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:24:25]    INFO >> epoch 001:   4600 / 5058 loss=0.241, ups=1.57, num_updates=4600, lr=9.3e-05, gnorm=1.805, clip=0, train_wall=60, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:25:37]    INFO >> epoch 001:   4700 / 5058 loss=0.215, ups=1.46, num_updates=4700, lr=9.2e-05, gnorm=3.803, clip=2, train_wall=64, gb_free=64.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:26:37]    INFO >> epoch 001:   4800 / 5058 loss=0.056, ups=1.7, num_updates=4800, lr=9.2e-05, gnorm=1.411, clip=0, train_wall=56, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:27:20]    INFO >> epoch 001:   4900 / 5058 loss=0.142, ups=2.52, num_updates=4900, lr=9.2e-05, gnorm=0.824, clip=0, train_wall=38, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:28:25]    INFO >> epoch 001:   5000 / 5058 loss=0.09, ups=1.6, num_updates=5000, lr=9.2e-05, gnorm=2.488, clip=0, train_wall=59, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:29:05]    INFO >> epoch 001 | loss 0.105 | ups 1.72 | num_updates 5058 | lr 9.2e-05 | gnorm 2.248 | clip 2.1 | train_wall 2777 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-20 05:29:05] WARNING >> 4469 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[2, 4, 10, 12, 13, 14, 16, 18, 19, 23] (data_utils.py:301, filter_by_size())[0m
[33m[2025-11-20 05:29:05] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
CodeEncoderLSTM: Creating BiLSTM with 6 layers, 512 hidden and input size
Traceback (most recent call last):
  File "run/type_prediction/transformer/train.py", line 339, in <module>
    cli_main()
  File "run/type_prediction/transformer/train.py", line 332, in cli_main
    single_main(args)
  File "run/type_prediction/transformer/train.py", line 254, in single_main
    valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)
  File "run/type_prediction/transformer/train.py", line 132, in validate
    trainer.valid_step(sample)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/trainers/ncc_trainers.py", line 565, in valid_step
    _loss, sample_size, logging_output = self.task.valid_step(
TypeError: cannot unpack non-iterable NoneType object

============================================================
Setting up experiment: exp_layers_6
============================================================
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_6
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_6/checkpoints
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_6/logs
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_6/results
âœ“ Saved config to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_6/config.yml
âœ“ Also saved to: run/type_prediction/transformer/config/exp_layers_6.yml
âœ“ Saved experiment info to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_6/info.txt

============================================================
Starting training...
============================================================

Command: python run/type_prediction/transformer/train.py --language exp_layers_6 2>&1 | tee /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_6/logs/train.log

Note: Config file will be loaded from run/type_prediction/transformer/config/exp_layers_6.yml


âœ“ Training completed successfully!

============================================================
Evaluating model on test set...
============================================================

Warning: Best checkpoint not found, using last checkpoint
âœ— No checkpoint found for evaluation

============================================================
âœ“ Experiment completed: exp_layers_6
âœ“ Results in: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_6
============================================================


âœ“ exp_layers_6 completed in 0.86 hours

============================================================
Running: exp_layers_8
GPU: 3
Time: 2025-11-20T05:29:08.560213
============================================================
[32m[2025-11-20 05:29:10]    INFO >> Load arguments in run/type_prediction/transformer/config/exp_layers_8.yml (train.py:300, cli_main())[0m
[32m[2025-11-20 05:29:10]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_8/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 8, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_8/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_8/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:302, cli_main())[0m
[32m[2025-11-20 05:29:10]    INFO >> single GPU training... (train.py:331, cli_main())[0m
[32m[2025-11-20 05:29:10]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_8/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 8, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_8/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_8/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:208, single_main())[0m
[32m[2025-11-20 05:29:11]    INFO >> TypeTransformer(
  (decoder): RobertaEncoder(
    (encoder): CodeEncoderLSTM(
      (embedding): Embedding(10006, 512)
      (pos_encoder): PositionalEncoding()
      (encoder): LSTM(512, 512, num_layers=6, bidirectional=True)
    )
    (output): Sequential(
      (0): Linear(in_features=1024, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=10006, bias=True)
    )
  )
  (classification_heads): ModuleDict()
) (train.py:220, single_main())[0m
[32m[2025-11-20 05:29:11]    INFO >> model typetransformer, criterion TypePredictionCrossEntropyCriterion (train.py:221, single_main())[0m
[32m[2025-11-20 05:29:11]    INFO >> num. model params: 46481686 (num. trained: 46481686) (train.py:222, single_main())[0m
[32m[2025-11-20 05:29:12]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 05:29:12]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 81024 MB ; used memory = 896 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 05:29:12]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 05:29:12]    INFO >> training on 1 GPUs (train.py:229, single_main())[0m
[32m[2025-11-20 05:29:12]    INFO >> max tokens per GPU = None and max sentences per GPU = 32 (train.py:230, single_main())[0m
[32m[2025-11-20 05:29:12]    INFO >> no existing checkpoint found /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_8/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-20 05:29:12]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[33m[2025-11-20 05:29:22] WARNING >> 32222 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[0, 1, 2, 4, 6, 7, 8, 10, 14, 19] (data_utils.py:301, filter_by_size())[0m
[32m[2025-11-20 05:29:22]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-20 05:29:22] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:398: UserWarning: ntokens not found in Criterion logging outputs, cannot log wpb or wps
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:407: UserWarning: nsentences not found in Criterion logging outputs, cannot log bsz
  warnings.warn(
[32m[2025-11-20 05:30:08]    INFO >> epoch 001:    100 / 5058 loss=1.903, ups=2.35, num_updates=100, lr=1e-05, gnorm=0.104, clip=0, train_wall=41, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:30:56]    INFO >> epoch 001:    200 / 5058 loss=0.786, ups=2.14, num_updates=200, lr=2e-05, gnorm=0.068, clip=0, train_wall=44, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:32:01]    INFO >> epoch 001:    300 / 5058 loss=0.654, ups=1.61, num_updates=300, lr=3e-05, gnorm=0.053, clip=0, train_wall=58, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:33:07]    INFO >> epoch 001:    400 / 5058 loss=0.623, ups=1.58, num_updates=400, lr=4e-05, gnorm=2.819, clip=2, train_wall=60, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:34:10]    INFO >> epoch 001:    500 / 5058 loss=0.208, ups=1.66, num_updates=500, lr=5e-05, gnorm=4.476, clip=2, train_wall=57, gb_free=65.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:35:11]    INFO >> epoch 001:    600 / 5058 loss=0.079, ups=1.7, num_updates=600, lr=6e-05, gnorm=4.788, clip=8, train_wall=56, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:35:54]    INFO >> epoch 001:    700 / 5058 loss=0.174, ups=2.4, num_updates=700, lr=7e-05, gnorm=2.641, clip=4, train_wall=39, gb_free=69.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:36:37]    INFO >> epoch 001:    800 / 5058 loss=0.166, ups=2.44, num_updates=800, lr=8e-05, gnorm=3.325, clip=8, train_wall=39, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:37:44]    INFO >> epoch 001:    900 / 5058 loss=0.216, ups=1.55, num_updates=900, lr=9e-05, gnorm=6.91, clip=8, train_wall=61, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:38:49]    INFO >> epoch 001:   1000 / 5058 loss=0.142, ups=1.61, num_updates=1000, lr=0.0001, gnorm=1.451, clip=0, train_wall=59, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:39:49]    INFO >> epoch 001:   1100 / 5058 loss=0.058, ups=1.74, num_updates=1100, lr=0.0001, gnorm=2.202, clip=2, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:40:53]    INFO >> epoch 001:   1200 / 5058 loss=0.035, ups=1.62, num_updates=1200, lr=0.0001, gnorm=0.871, clip=0, train_wall=58, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:41:58]    INFO >> epoch 001:   1300 / 5058 loss=0.087, ups=1.6, num_updates=1300, lr=9.9e-05, gnorm=1.877, clip=2, train_wall=59, gb_free=67.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:43:02]    INFO >> epoch 001:   1400 / 5058 loss=0.059, ups=1.63, num_updates=1400, lr=9.9e-05, gnorm=2.488, clip=4, train_wall=58, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:44:08]    INFO >> epoch 001:   1500 / 5058 loss=0.279, ups=1.6, num_updates=1500, lr=9.9e-05, gnorm=1.776, clip=2, train_wall=59, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:45:07]    INFO >> epoch 001:   1600 / 5058 loss=0.11, ups=1.71, num_updates=1600, lr=9.9e-05, gnorm=3.243, clip=4, train_wall=55, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:46:14]    INFO >> epoch 001:   1700 / 5058 loss=0.113, ups=1.57, num_updates=1700, lr=9.9e-05, gnorm=2.319, clip=2, train_wall=60, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:47:09]    INFO >> epoch 001:   1800 / 5058 loss=0.12, ups=1.9, num_updates=1800, lr=9.8e-05, gnorm=3.115, clip=5, train_wall=50, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:47:49]    INFO >> epoch 001:   1900 / 5058 loss=0.153, ups=2.64, num_updates=1900, lr=9.8e-05, gnorm=1.952, clip=4, train_wall=36, gb_free=67.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:48:51]    INFO >> epoch 001:   2000 / 5058 loss=0.089, ups=1.68, num_updates=2000, lr=9.8e-05, gnorm=1.422, clip=1, train_wall=56, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:49:57]    INFO >> epoch 001:   2100 / 5058 loss=0.074, ups=1.56, num_updates=2100, lr=9.8e-05, gnorm=3.142, clip=2, train_wall=60, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:51:04]    INFO >> epoch 001:   2200 / 5058 loss=0.279, ups=1.56, num_updates=2200, lr=9.8e-05, gnorm=5.078, clip=6, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:52:12]    INFO >> epoch 001:   2300 / 5058 loss=0.068, ups=1.54, num_updates=2300, lr=9.7e-05, gnorm=2.452, clip=5, train_wall=61, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:53:17]    INFO >> epoch 001:   2400 / 5058 loss=0.063, ups=1.6, num_updates=2400, lr=9.7e-05, gnorm=0.773, clip=1, train_wall=59, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:54:27]    INFO >> epoch 001:   2500 / 5058 loss=0.149, ups=1.49, num_updates=2500, lr=9.7e-05, gnorm=3.718, clip=1, train_wall=63, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:55:25]    INFO >> epoch 001:   2600 / 5058 loss=0.128, ups=1.8, num_updates=2600, lr=9.7e-05, gnorm=2.608, clip=1, train_wall=53, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:56:26]    INFO >> epoch 001:   2700 / 5058 loss=0.076, ups=1.72, num_updates=2700, lr=9.7e-05, gnorm=1.387, clip=1, train_wall=55, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:57:27]    INFO >> epoch 001:   2800 / 5058 loss=0.081, ups=1.67, num_updates=2800, lr=9.6e-05, gnorm=1.32, clip=3, train_wall=56, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:58:08]    INFO >> epoch 001:   2900 / 5058 loss=0.274, ups=2.58, num_updates=2900, lr=9.6e-05, gnorm=2.005, clip=3, train_wall=37, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 05:59:14]    INFO >> epoch 001:   3000 / 5058 loss=0.145, ups=1.58, num_updates=3000, lr=9.6e-05, gnorm=3.103, clip=5, train_wall=60, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:00:07]    INFO >> epoch 001:   3100 / 5058 loss=0.058, ups=1.92, num_updates=3100, lr=9.6e-05, gnorm=0.282, clip=0, train_wall=49, gb_free=73.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:01:06]    INFO >> epoch 001:   3200 / 5058 loss=0.282, ups=1.8, num_updates=3200, lr=9.6e-05, gnorm=2.208, clip=2, train_wall=52, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:02:06]    INFO >> epoch 001:   3300 / 5058 loss=0.086, ups=1.72, num_updates=3300, lr=9.5e-05, gnorm=1.673, clip=1, train_wall=55, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:03:18]    INFO >> epoch 001:   3400 / 5058 loss=0.225, ups=1.46, num_updates=3400, lr=9.5e-05, gnorm=5.264, clip=3, train_wall=64, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:04:24]    INFO >> epoch 001:   3500 / 5058 loss=0.131, ups=1.58, num_updates=3500, lr=9.5e-05, gnorm=1.53, clip=0, train_wall=59, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:05:30]    INFO >> epoch 001:   3600 / 5058 loss=0.095, ups=1.58, num_updates=3600, lr=9.5e-05, gnorm=2.575, clip=1, train_wall=59, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:06:31]    INFO >> epoch 001:   3700 / 5058 loss=0.137, ups=1.71, num_updates=3700, lr=9.4e-05, gnorm=1.854, clip=2, train_wall=55, gb_free=66.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:07:38]    INFO >> epoch 001:   3800 / 5058 loss=0.058, ups=1.55, num_updates=3800, lr=9.4e-05, gnorm=1.191, clip=1, train_wall=60, gb_free=68.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:08:28]    INFO >> epoch 001:   3900 / 5058 loss=0.127, ups=2.06, num_updates=3900, lr=9.4e-05, gnorm=1.978, clip=2, train_wall=46, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:09:30]    INFO >> epoch 001:   4000 / 5058 loss=0.131, ups=1.67, num_updates=4000, lr=9.4e-05, gnorm=1.731, clip=1, train_wall=56, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:10:34]    INFO >> epoch 001:   4100 / 5058 loss=0.92, ups=1.64, num_updates=4100, lr=9.4e-05, gnorm=0.902, clip=1, train_wall=57, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:11:41]    INFO >> epoch 001:   4200 / 5058 loss=0.149, ups=1.55, num_updates=4200, lr=9.3e-05, gnorm=1.845, clip=3, train_wall=61, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:12:41]    INFO >> epoch 001:   4300 / 5058 loss=0.117, ups=1.74, num_updates=4300, lr=9.3e-05, gnorm=1.721, clip=1, train_wall=54, gb_free=73.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:13:41]    INFO >> epoch 001:   4400 / 5058 loss=0.082, ups=1.7, num_updates=4400, lr=9.3e-05, gnorm=1.476, clip=0, train_wall=55, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:14:45]    INFO >> epoch 001:   4500 / 5058 loss=0.072, ups=1.64, num_updates=4500, lr=9.3e-05, gnorm=1.371, clip=0, train_wall=57, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:15:51]    INFO >> epoch 001:   4600 / 5058 loss=0.241, ups=1.56, num_updates=4600, lr=9.3e-05, gnorm=1.805, clip=0, train_wall=60, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:17:03]    INFO >> epoch 001:   4700 / 5058 loss=0.215, ups=1.46, num_updates=4700, lr=9.2e-05, gnorm=3.803, clip=2, train_wall=64, gb_free=64.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:18:04]    INFO >> epoch 001:   4800 / 5058 loss=0.056, ups=1.69, num_updates=4800, lr=9.2e-05, gnorm=1.411, clip=0, train_wall=55, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:18:45]    INFO >> epoch 001:   4900 / 5058 loss=0.142, ups=2.51, num_updates=4900, lr=9.2e-05, gnorm=0.824, clip=0, train_wall=38, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:19:51]    INFO >> epoch 001:   5000 / 5058 loss=0.09, ups=1.6, num_updates=5000, lr=9.2e-05, gnorm=2.488, clip=0, train_wall=59, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:20:32]    INFO >> epoch 001 | loss 0.105 | ups 1.72 | num_updates 5058 | lr 9.2e-05 | gnorm 2.248 | clip 2.1 | train_wall 2774 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-20 06:20:32] WARNING >> 4469 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[2, 4, 10, 12, 13, 14, 16, 18, 19, 23] (data_utils.py:301, filter_by_size())[0m
[33m[2025-11-20 06:20:32] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
CodeEncoderLSTM: Creating BiLSTM with 6 layers, 512 hidden and input size
Traceback (most recent call last):
  File "run/type_prediction/transformer/train.py", line 339, in <module>
    cli_main()
  File "run/type_prediction/transformer/train.py", line 332, in cli_main
    single_main(args)
  File "run/type_prediction/transformer/train.py", line 254, in single_main
    valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)
  File "run/type_prediction/transformer/train.py", line 132, in validate
    trainer.valid_step(sample)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/trainers/ncc_trainers.py", line 565, in valid_step
    _loss, sample_size, logging_output = self.task.valid_step(
TypeError: cannot unpack non-iterable NoneType object

============================================================
Setting up experiment: exp_layers_8
============================================================
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_8
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_8/checkpoints
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_8/logs
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_8/results
âœ“ Saved config to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_8/config.yml
âœ“ Also saved to: run/type_prediction/transformer/config/exp_layers_8.yml
âœ“ Saved experiment info to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_8/info.txt

============================================================
Starting training...
============================================================

Command: python run/type_prediction/transformer/train.py --language exp_layers_8 2>&1 | tee /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_8/logs/train.log

Note: Config file will be loaded from run/type_prediction/transformer/config/exp_layers_8.yml


âœ“ Training completed successfully!

============================================================
Evaluating model on test set...
============================================================

Warning: Best checkpoint not found, using last checkpoint
âœ— No checkpoint found for evaluation

============================================================
âœ“ Experiment completed: exp_layers_8
âœ“ Results in: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_layers_8
============================================================


âœ“ exp_layers_8 completed in 0.86 hours

============================================================
Running: exp_lr_1e-04
GPU: 3
Time: 2025-11-20T06:20:35.605291
============================================================
[32m[2025-11-20 06:20:37]    INFO >> Load arguments in run/type_prediction/transformer/config/exp_lr_1e-04.yml (train.py:300, cli_main())[0m
[32m[2025-11-20 06:20:37]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-04/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-04/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-04/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:302, cli_main())[0m
[32m[2025-11-20 06:20:37]    INFO >> single GPU training... (train.py:331, cli_main())[0m
[32m[2025-11-20 06:20:37]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-04/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-04/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-04/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:208, single_main())[0m
[32m[2025-11-20 06:20:39]    INFO >> TypeTransformer(
  (decoder): RobertaEncoder(
    (encoder): CodeEncoderLSTM(
      (embedding): Embedding(10006, 512)
      (pos_encoder): PositionalEncoding()
      (encoder): LSTM(512, 512, num_layers=6, bidirectional=True)
    )
    (output): Sequential(
      (0): Linear(in_features=1024, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=10006, bias=True)
    )
  )
  (classification_heads): ModuleDict()
) (train.py:220, single_main())[0m
[32m[2025-11-20 06:20:39]    INFO >> model typetransformer, criterion TypePredictionCrossEntropyCriterion (train.py:221, single_main())[0m
[32m[2025-11-20 06:20:39]    INFO >> num. model params: 46481686 (num. trained: 46481686) (train.py:222, single_main())[0m
[32m[2025-11-20 06:20:39]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 06:20:39]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 81024 MB ; used memory = 896 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 06:20:39]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 06:20:39]    INFO >> training on 1 GPUs (train.py:229, single_main())[0m
[32m[2025-11-20 06:20:39]    INFO >> max tokens per GPU = None and max sentences per GPU = 32 (train.py:230, single_main())[0m
[32m[2025-11-20 06:20:39]    INFO >> no existing checkpoint found /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-04/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-20 06:20:39]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[33m[2025-11-20 06:20:49] WARNING >> 32222 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[0, 1, 2, 4, 6, 7, 8, 10, 14, 19] (data_utils.py:301, filter_by_size())[0m
[32m[2025-11-20 06:20:49]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-20 06:20:49] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:398: UserWarning: ntokens not found in Criterion logging outputs, cannot log wpb or wps
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:407: UserWarning: nsentences not found in Criterion logging outputs, cannot log bsz
  warnings.warn(
[32m[2025-11-20 06:21:34]    INFO >> epoch 001:    100 / 5058 loss=1.903, ups=2.35, num_updates=100, lr=1e-05, gnorm=0.104, clip=0, train_wall=41, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:22:23]    INFO >> epoch 001:    200 / 5058 loss=0.786, ups=2.14, num_updates=200, lr=2e-05, gnorm=0.068, clip=0, train_wall=44, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:23:28]    INFO >> epoch 001:    300 / 5058 loss=0.654, ups=1.62, num_updates=300, lr=3e-05, gnorm=0.053, clip=0, train_wall=58, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:24:34]    INFO >> epoch 001:    400 / 5058 loss=0.623, ups=1.58, num_updates=400, lr=4e-05, gnorm=2.819, clip=2, train_wall=60, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:25:37]    INFO >> epoch 001:    500 / 5058 loss=0.208, ups=1.66, num_updates=500, lr=5e-05, gnorm=4.476, clip=2, train_wall=57, gb_free=65.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:26:37]    INFO >> epoch 001:    600 / 5058 loss=0.079, ups=1.7, num_updates=600, lr=6e-05, gnorm=4.788, clip=8, train_wall=56, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:27:21]    INFO >> epoch 001:    700 / 5058 loss=0.174, ups=2.4, num_updates=700, lr=7e-05, gnorm=2.641, clip=4, train_wall=39, gb_free=69.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:28:04]    INFO >> epoch 001:    800 / 5058 loss=0.166, ups=2.44, num_updates=800, lr=8e-05, gnorm=3.325, clip=8, train_wall=39, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:29:11]    INFO >> epoch 001:    900 / 5058 loss=0.216, ups=1.55, num_updates=900, lr=9e-05, gnorm=6.91, clip=8, train_wall=61, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:30:16]    INFO >> epoch 001:   1000 / 5058 loss=0.142, ups=1.6, num_updates=1000, lr=0.0001, gnorm=1.451, clip=0, train_wall=59, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:31:16]    INFO >> epoch 001:   1100 / 5058 loss=0.058, ups=1.74, num_updates=1100, lr=0.0001, gnorm=2.202, clip=2, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:32:20]    INFO >> epoch 001:   1200 / 5058 loss=0.035, ups=1.62, num_updates=1200, lr=0.0001, gnorm=0.871, clip=0, train_wall=58, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:33:25]    INFO >> epoch 001:   1300 / 5058 loss=0.087, ups=1.6, num_updates=1300, lr=9.9e-05, gnorm=1.877, clip=2, train_wall=59, gb_free=67.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:34:28]    INFO >> epoch 001:   1400 / 5058 loss=0.059, ups=1.63, num_updates=1400, lr=9.9e-05, gnorm=2.488, clip=4, train_wall=58, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:35:33]    INFO >> epoch 001:   1500 / 5058 loss=0.279, ups=1.6, num_updates=1500, lr=9.9e-05, gnorm=1.776, clip=2, train_wall=59, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:36:34]    INFO >> epoch 001:   1600 / 5058 loss=0.11, ups=1.71, num_updates=1600, lr=9.9e-05, gnorm=3.243, clip=4, train_wall=55, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:37:41]    INFO >> epoch 001:   1700 / 5058 loss=0.113, ups=1.57, num_updates=1700, lr=9.9e-05, gnorm=2.319, clip=2, train_wall=60, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:38:36]    INFO >> epoch 001:   1800 / 5058 loss=0.12, ups=1.89, num_updates=1800, lr=9.8e-05, gnorm=3.115, clip=5, train_wall=50, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:39:16]    INFO >> epoch 001:   1900 / 5058 loss=0.153, ups=2.64, num_updates=1900, lr=9.8e-05, gnorm=1.952, clip=4, train_wall=36, gb_free=67.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:40:18]    INFO >> epoch 001:   2000 / 5058 loss=0.089, ups=1.68, num_updates=2000, lr=9.8e-05, gnorm=1.422, clip=1, train_wall=56, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:41:25]    INFO >> epoch 001:   2100 / 5058 loss=0.074, ups=1.56, num_updates=2100, lr=9.8e-05, gnorm=3.142, clip=2, train_wall=60, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:42:31]    INFO >> epoch 001:   2200 / 5058 loss=0.279, ups=1.56, num_updates=2200, lr=9.8e-05, gnorm=5.078, clip=6, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:43:39]    INFO >> epoch 001:   2300 / 5058 loss=0.068, ups=1.54, num_updates=2300, lr=9.7e-05, gnorm=2.452, clip=5, train_wall=61, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:44:44]    INFO >> epoch 001:   2400 / 5058 loss=0.063, ups=1.6, num_updates=2400, lr=9.7e-05, gnorm=0.773, clip=1, train_wall=59, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:45:54]    INFO >> epoch 001:   2500 / 5058 loss=0.149, ups=1.49, num_updates=2500, lr=9.7e-05, gnorm=3.718, clip=1, train_wall=63, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:46:52]    INFO >> epoch 001:   2600 / 5058 loss=0.128, ups=1.8, num_updates=2600, lr=9.7e-05, gnorm=2.608, clip=1, train_wall=53, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:47:52]    INFO >> epoch 001:   2700 / 5058 loss=0.076, ups=1.72, num_updates=2700, lr=9.7e-05, gnorm=1.387, clip=1, train_wall=55, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:48:54]    INFO >> epoch 001:   2800 / 5058 loss=0.081, ups=1.68, num_updates=2800, lr=9.6e-05, gnorm=1.32, clip=3, train_wall=56, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:49:34]    INFO >> epoch 001:   2900 / 5058 loss=0.274, ups=2.59, num_updates=2900, lr=9.6e-05, gnorm=2.005, clip=3, train_wall=37, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:50:40]    INFO >> epoch 001:   3000 / 5058 loss=0.145, ups=1.58, num_updates=3000, lr=9.6e-05, gnorm=3.103, clip=5, train_wall=60, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:51:35]    INFO >> epoch 001:   3100 / 5058 loss=0.058, ups=1.92, num_updates=3100, lr=9.6e-05, gnorm=0.282, clip=0, train_wall=49, gb_free=73.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:52:33]    INFO >> epoch 001:   3200 / 5058 loss=0.282, ups=1.81, num_updates=3200, lr=9.6e-05, gnorm=2.208, clip=2, train_wall=52, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:53:32]    INFO >> epoch 001:   3300 / 5058 loss=0.086, ups=1.73, num_updates=3300, lr=9.5e-05, gnorm=1.673, clip=1, train_wall=55, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:54:45]    INFO >> epoch 001:   3400 / 5058 loss=0.225, ups=1.45, num_updates=3400, lr=9.5e-05, gnorm=5.264, clip=3, train_wall=64, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:55:51]    INFO >> epoch 001:   3500 / 5058 loss=0.131, ups=1.58, num_updates=3500, lr=9.5e-05, gnorm=1.53, clip=0, train_wall=59, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:56:57]    INFO >> epoch 001:   3600 / 5058 loss=0.095, ups=1.58, num_updates=3600, lr=9.5e-05, gnorm=2.575, clip=1, train_wall=59, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:57:56]    INFO >> epoch 001:   3700 / 5058 loss=0.137, ups=1.71, num_updates=3700, lr=9.4e-05, gnorm=1.854, clip=2, train_wall=55, gb_free=66.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:59:03]    INFO >> epoch 001:   3800 / 5058 loss=0.058, ups=1.56, num_updates=3800, lr=9.4e-05, gnorm=1.191, clip=1, train_wall=60, gb_free=68.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 06:59:54]    INFO >> epoch 001:   3900 / 5058 loss=0.127, ups=2.07, num_updates=3900, lr=9.4e-05, gnorm=1.978, clip=2, train_wall=46, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:00:57]    INFO >> epoch 001:   4000 / 5058 loss=0.131, ups=1.68, num_updates=4000, lr=9.4e-05, gnorm=1.731, clip=1, train_wall=56, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:02:00]    INFO >> epoch 001:   4100 / 5058 loss=0.92, ups=1.65, num_updates=4100, lr=9.4e-05, gnorm=0.902, clip=1, train_wall=57, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:03:07]    INFO >> epoch 001:   4200 / 5058 loss=0.149, ups=1.55, num_updates=4200, lr=9.3e-05, gnorm=1.845, clip=3, train_wall=61, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:04:06]    INFO >> epoch 001:   4300 / 5058 loss=0.117, ups=1.75, num_updates=4300, lr=9.3e-05, gnorm=1.721, clip=1, train_wall=54, gb_free=73.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:05:07]    INFO >> epoch 001:   4400 / 5058 loss=0.082, ups=1.71, num_updates=4400, lr=9.3e-05, gnorm=1.476, clip=0, train_wall=55, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:06:10]    INFO >> epoch 001:   4500 / 5058 loss=0.072, ups=1.65, num_updates=4500, lr=9.3e-05, gnorm=1.371, clip=0, train_wall=57, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:07:17]    INFO >> epoch 001:   4600 / 5058 loss=0.241, ups=1.57, num_updates=4600, lr=9.3e-05, gnorm=1.805, clip=0, train_wall=60, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:08:28]    INFO >> epoch 001:   4700 / 5058 loss=0.215, ups=1.47, num_updates=4700, lr=9.2e-05, gnorm=3.803, clip=2, train_wall=64, gb_free=64.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:09:29]    INFO >> epoch 001:   4800 / 5058 loss=0.056, ups=1.7, num_updates=4800, lr=9.2e-05, gnorm=1.411, clip=0, train_wall=55, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:10:10]    INFO >> epoch 001:   4900 / 5058 loss=0.142, ups=2.52, num_updates=4900, lr=9.2e-05, gnorm=0.824, clip=0, train_wall=38, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:11:15]    INFO >> epoch 001:   5000 / 5058 loss=0.09, ups=1.6, num_updates=5000, lr=9.2e-05, gnorm=2.488, clip=0, train_wall=59, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:11:55]    INFO >> epoch 001 | loss 0.105 | ups 1.72 | num_updates 5058 | lr 9.2e-05 | gnorm 2.248 | clip 2.1 | train_wall 2774 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-20 07:11:55] WARNING >> 4469 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[2, 4, 10, 12, 13, 14, 16, 18, 19, 23] (data_utils.py:301, filter_by_size())[0m
[33m[2025-11-20 07:11:55] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
CodeEncoderLSTM: Creating BiLSTM with 6 layers, 512 hidden and input size
Traceback (most recent call last):
  File "run/type_prediction/transformer/train.py", line 339, in <module>
    cli_main()
  File "run/type_prediction/transformer/train.py", line 332, in cli_main
    single_main(args)
  File "run/type_prediction/transformer/train.py", line 254, in single_main
    valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)
  File "run/type_prediction/transformer/train.py", line 132, in validate
    trainer.valid_step(sample)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/trainers/ncc_trainers.py", line 565, in valid_step
    _loss, sample_size, logging_output = self.task.valid_step(
TypeError: cannot unpack non-iterable NoneType object

============================================================
Setting up experiment: exp_lr_1e-04
============================================================
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-04
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-04/checkpoints
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-04/logs
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-04/results
âœ“ Saved config to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-04/config.yml
âœ“ Also saved to: run/type_prediction/transformer/config/exp_lr_1e-04.yml
âœ“ Saved experiment info to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-04/info.txt

============================================================
Starting training...
============================================================

Command: python run/type_prediction/transformer/train.py --language exp_lr_1e-04 2>&1 | tee /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-04/logs/train.log

Note: Config file will be loaded from run/type_prediction/transformer/config/exp_lr_1e-04.yml


âœ“ Training completed successfully!

============================================================
Evaluating model on test set...
============================================================

Warning: Best checkpoint not found, using last checkpoint
âœ— No checkpoint found for evaluation

============================================================
âœ“ Experiment completed: exp_lr_1e-04
âœ“ Results in: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-04
============================================================


âœ“ exp_lr_1e-04 completed in 0.86 hours

============================================================
Running: exp_lr_5e-04
GPU: 3
Time: 2025-11-20T07:11:58.762356
============================================================
[32m[2025-11-20 07:12:00]    INFO >> Load arguments in run/type_prediction/transformer/config/exp_lr_5e-04.yml (train.py:300, cli_main())[0m
[32m[2025-11-20 07:12:00]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_5e-04/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_5e-04/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_5e-04/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:302, cli_main())[0m
[32m[2025-11-20 07:12:00]    INFO >> single GPU training... (train.py:331, cli_main())[0m
[32m[2025-11-20 07:12:00]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_5e-04/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_5e-04/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_5e-04/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:208, single_main())[0m
[32m[2025-11-20 07:12:03]    INFO >> TypeTransformer(
  (decoder): RobertaEncoder(
    (encoder): CodeEncoderLSTM(
      (embedding): Embedding(10006, 512)
      (pos_encoder): PositionalEncoding()
      (encoder): LSTM(512, 512, num_layers=6, bidirectional=True)
    )
    (output): Sequential(
      (0): Linear(in_features=1024, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=10006, bias=True)
    )
  )
  (classification_heads): ModuleDict()
) (train.py:220, single_main())[0m
[32m[2025-11-20 07:12:03]    INFO >> model typetransformer, criterion TypePredictionCrossEntropyCriterion (train.py:221, single_main())[0m
[32m[2025-11-20 07:12:03]    INFO >> num. model params: 46481686 (num. trained: 46481686) (train.py:222, single_main())[0m
[32m[2025-11-20 07:12:03]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 07:12:03]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 81024 MB ; used memory = 896 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 07:12:03]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 07:12:03]    INFO >> training on 1 GPUs (train.py:229, single_main())[0m
[32m[2025-11-20 07:12:03]    INFO >> max tokens per GPU = None and max sentences per GPU = 32 (train.py:230, single_main())[0m
[32m[2025-11-20 07:12:03]    INFO >> no existing checkpoint found /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_5e-04/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-20 07:12:03]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[33m[2025-11-20 07:12:13] WARNING >> 32222 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[0, 1, 2, 4, 6, 7, 8, 10, 14, 19] (data_utils.py:301, filter_by_size())[0m
[32m[2025-11-20 07:12:13]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-20 07:12:14] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:398: UserWarning: ntokens not found in Criterion logging outputs, cannot log wpb or wps
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:407: UserWarning: nsentences not found in Criterion logging outputs, cannot log bsz
  warnings.warn(
[32m[2025-11-20 07:12:58]    INFO >> epoch 001:    100 / 5058 loss=1.903, ups=2.37, num_updates=100, lr=1e-05, gnorm=0.104, clip=0, train_wall=40, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:13:47]    INFO >> epoch 001:    200 / 5058 loss=0.786, ups=2.16, num_updates=200, lr=2e-05, gnorm=0.068, clip=0, train_wall=44, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:14:51]    INFO >> epoch 001:    300 / 5058 loss=0.654, ups=1.63, num_updates=300, lr=3e-05, gnorm=0.053, clip=0, train_wall=58, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:15:55]    INFO >> epoch 001:    400 / 5058 loss=0.623, ups=1.59, num_updates=400, lr=4e-05, gnorm=2.819, clip=2, train_wall=59, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:16:58]    INFO >> epoch 001:    500 / 5058 loss=0.208, ups=1.67, num_updates=500, lr=5e-05, gnorm=4.476, clip=2, train_wall=56, gb_free=65.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:17:59]    INFO >> epoch 001:    600 / 5058 loss=0.079, ups=1.71, num_updates=600, lr=6e-05, gnorm=4.788, clip=8, train_wall=55, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:18:41]    INFO >> epoch 001:    700 / 5058 loss=0.174, ups=2.42, num_updates=700, lr=7e-05, gnorm=2.641, clip=4, train_wall=39, gb_free=69.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:19:25]    INFO >> epoch 001:    800 / 5058 loss=0.166, ups=2.46, num_updates=800, lr=8e-05, gnorm=3.325, clip=8, train_wall=39, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:20:31]    INFO >> epoch 001:    900 / 5058 loss=0.216, ups=1.56, num_updates=900, lr=9e-05, gnorm=6.91, clip=8, train_wall=60, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:21:36]    INFO >> epoch 001:   1000 / 5058 loss=0.142, ups=1.62, num_updates=1000, lr=0.0001, gnorm=1.451, clip=0, train_wall=58, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:22:34]    INFO >> epoch 001:   1100 / 5058 loss=0.058, ups=1.76, num_updates=1100, lr=0.0001, gnorm=2.202, clip=2, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:23:38]    INFO >> epoch 001:   1200 / 5058 loss=0.035, ups=1.64, num_updates=1200, lr=0.0001, gnorm=0.871, clip=0, train_wall=57, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:24:43]    INFO >> epoch 001:   1300 / 5058 loss=0.087, ups=1.61, num_updates=1300, lr=9.9e-05, gnorm=1.877, clip=2, train_wall=58, gb_free=67.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:25:46]    INFO >> epoch 001:   1400 / 5058 loss=0.059, ups=1.64, num_updates=1400, lr=9.9e-05, gnorm=2.488, clip=4, train_wall=57, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:26:51]    INFO >> epoch 001:   1500 / 5058 loss=0.279, ups=1.61, num_updates=1500, lr=9.9e-05, gnorm=1.776, clip=2, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:27:51]    INFO >> epoch 001:   1600 / 5058 loss=0.11, ups=1.73, num_updates=1600, lr=9.9e-05, gnorm=3.243, clip=4, train_wall=55, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:28:57]    INFO >> epoch 001:   1700 / 5058 loss=0.113, ups=1.58, num_updates=1700, lr=9.9e-05, gnorm=2.319, clip=2, train_wall=59, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:29:51]    INFO >> epoch 001:   1800 / 5058 loss=0.12, ups=1.91, num_updates=1800, lr=9.8e-05, gnorm=3.115, clip=5, train_wall=49, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:30:30]    INFO >> epoch 001:   1900 / 5058 loss=0.153, ups=2.66, num_updates=1900, lr=9.8e-05, gnorm=1.952, clip=4, train_wall=36, gb_free=67.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:31:32]    INFO >> epoch 001:   2000 / 5058 loss=0.089, ups=1.69, num_updates=2000, lr=9.8e-05, gnorm=1.422, clip=1, train_wall=56, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:32:38]    INFO >> epoch 001:   2100 / 5058 loss=0.074, ups=1.58, num_updates=2100, lr=9.8e-05, gnorm=3.142, clip=2, train_wall=60, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:33:44]    INFO >> epoch 001:   2200 / 5058 loss=0.279, ups=1.57, num_updates=2200, lr=9.8e-05, gnorm=5.078, clip=6, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:34:51]    INFO >> epoch 001:   2300 / 5058 loss=0.068, ups=1.56, num_updates=2300, lr=9.7e-05, gnorm=2.452, clip=5, train_wall=60, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:35:56]    INFO >> epoch 001:   2400 / 5058 loss=0.063, ups=1.61, num_updates=2400, lr=9.7e-05, gnorm=0.773, clip=1, train_wall=58, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:37:05]    INFO >> epoch 001:   2500 / 5058 loss=0.149, ups=1.5, num_updates=2500, lr=9.7e-05, gnorm=3.718, clip=1, train_wall=63, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:38:03]    INFO >> epoch 001:   2600 / 5058 loss=0.128, ups=1.81, num_updates=2600, lr=9.7e-05, gnorm=2.608, clip=1, train_wall=52, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:39:03]    INFO >> epoch 001:   2700 / 5058 loss=0.076, ups=1.74, num_updates=2700, lr=9.7e-05, gnorm=1.387, clip=1, train_wall=54, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:40:05]    INFO >> epoch 001:   2800 / 5058 loss=0.081, ups=1.69, num_updates=2800, lr=9.6e-05, gnorm=1.32, clip=3, train_wall=56, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:40:45]    INFO >> epoch 001:   2900 / 5058 loss=0.274, ups=2.61, num_updates=2900, lr=9.6e-05, gnorm=2.005, clip=3, train_wall=36, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:41:50]    INFO >> epoch 001:   3000 / 5058 loss=0.145, ups=1.59, num_updates=3000, lr=9.6e-05, gnorm=3.103, clip=5, train_wall=59, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:42:43]    INFO >> epoch 001:   3100 / 5058 loss=0.058, ups=1.94, num_updates=3100, lr=9.6e-05, gnorm=0.282, clip=0, train_wall=49, gb_free=73.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:43:41]    INFO >> epoch 001:   3200 / 5058 loss=0.282, ups=1.82, num_updates=3200, lr=9.6e-05, gnorm=2.208, clip=2, train_wall=52, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:44:41]    INFO >> epoch 001:   3300 / 5058 loss=0.086, ups=1.74, num_updates=3300, lr=9.5e-05, gnorm=1.673, clip=1, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:45:52]    INFO >> epoch 001:   3400 / 5058 loss=0.225, ups=1.47, num_updates=3400, lr=9.5e-05, gnorm=5.264, clip=3, train_wall=64, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:46:57]    INFO >> epoch 001:   3500 / 5058 loss=0.131, ups=1.59, num_updates=3500, lr=9.5e-05, gnorm=1.53, clip=0, train_wall=59, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:48:03]    INFO >> epoch 001:   3600 / 5058 loss=0.095, ups=1.59, num_updates=3600, lr=9.5e-05, gnorm=2.575, clip=1, train_wall=59, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:49:03]    INFO >> epoch 001:   3700 / 5058 loss=0.137, ups=1.72, num_updates=3700, lr=9.4e-05, gnorm=1.854, clip=2, train_wall=55, gb_free=66.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:50:10]    INFO >> epoch 001:   3800 / 5058 loss=0.058, ups=1.57, num_updates=3800, lr=9.4e-05, gnorm=1.191, clip=1, train_wall=60, gb_free=68.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:50:59]    INFO >> epoch 001:   3900 / 5058 loss=0.127, ups=2.08, num_updates=3900, lr=9.4e-05, gnorm=1.978, clip=2, train_wall=45, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:52:01]    INFO >> epoch 001:   4000 / 5058 loss=0.131, ups=1.69, num_updates=4000, lr=9.4e-05, gnorm=1.731, clip=1, train_wall=56, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:53:04]    INFO >> epoch 001:   4100 / 5058 loss=0.92, ups=1.66, num_updates=4100, lr=9.4e-05, gnorm=0.902, clip=1, train_wall=57, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:54:11]    INFO >> epoch 001:   4200 / 5058 loss=0.149, ups=1.56, num_updates=4200, lr=9.3e-05, gnorm=1.845, clip=3, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:55:09]    INFO >> epoch 001:   4300 / 5058 loss=0.117, ups=1.76, num_updates=4300, lr=9.3e-05, gnorm=1.721, clip=1, train_wall=53, gb_free=73.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:56:09]    INFO >> epoch 001:   4400 / 5058 loss=0.082, ups=1.72, num_updates=4400, lr=9.3e-05, gnorm=1.476, clip=0, train_wall=55, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:57:12]    INFO >> epoch 001:   4500 / 5058 loss=0.072, ups=1.66, num_updates=4500, lr=9.3e-05, gnorm=1.371, clip=0, train_wall=57, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:58:18]    INFO >> epoch 001:   4600 / 5058 loss=0.241, ups=1.58, num_updates=4600, lr=9.3e-05, gnorm=1.805, clip=0, train_wall=60, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 07:59:29]    INFO >> epoch 001:   4700 / 5058 loss=0.215, ups=1.48, num_updates=4700, lr=9.2e-05, gnorm=3.803, clip=2, train_wall=63, gb_free=64.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:00:30]    INFO >> epoch 001:   4800 / 5058 loss=0.056, ups=1.71, num_updates=4800, lr=9.2e-05, gnorm=1.411, clip=0, train_wall=55, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:01:11]    INFO >> epoch 001:   4900 / 5058 loss=0.142, ups=2.54, num_updates=4900, lr=9.2e-05, gnorm=0.824, clip=0, train_wall=37, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:02:15]    INFO >> epoch 001:   5000 / 5058 loss=0.09, ups=1.61, num_updates=5000, lr=9.2e-05, gnorm=2.488, clip=0, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:02:55]    INFO >> epoch 001 | loss 0.105 | ups 1.73 | num_updates 5058 | lr 9.2e-05 | gnorm 2.248 | clip 2.1 | train_wall 2751 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-20 08:02:55] WARNING >> 4469 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[2, 4, 10, 12, 13, 14, 16, 18, 19, 23] (data_utils.py:301, filter_by_size())[0m
[33m[2025-11-20 08:02:55] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
CodeEncoderLSTM: Creating BiLSTM with 6 layers, 512 hidden and input size
Traceback (most recent call last):
  File "run/type_prediction/transformer/train.py", line 339, in <module>
    cli_main()
  File "run/type_prediction/transformer/train.py", line 332, in cli_main
    single_main(args)
  File "run/type_prediction/transformer/train.py", line 254, in single_main
    valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)
  File "run/type_prediction/transformer/train.py", line 132, in validate
    trainer.valid_step(sample)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/trainers/ncc_trainers.py", line 565, in valid_step
    _loss, sample_size, logging_output = self.task.valid_step(
TypeError: cannot unpack non-iterable NoneType object

============================================================
Setting up experiment: exp_lr_5e-04
============================================================
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_5e-04
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_5e-04/checkpoints
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_5e-04/logs
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_5e-04/results
âœ“ Saved config to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_5e-04/config.yml
âœ“ Also saved to: run/type_prediction/transformer/config/exp_lr_5e-04.yml
âœ“ Saved experiment info to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_5e-04/info.txt

============================================================
Starting training...
============================================================

Command: python run/type_prediction/transformer/train.py --language exp_lr_5e-04 2>&1 | tee /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_5e-04/logs/train.log

Note: Config file will be loaded from run/type_prediction/transformer/config/exp_lr_5e-04.yml


âœ“ Training completed successfully!

============================================================
Evaluating model on test set...
============================================================

Warning: Best checkpoint not found, using last checkpoint
âœ— No checkpoint found for evaluation

============================================================
âœ“ Experiment completed: exp_lr_5e-04
âœ“ Results in: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_5e-04
============================================================


âœ“ exp_lr_5e-04 completed in 0.85 hours

============================================================
Running: exp_lr_1e-03
GPU: 3
Time: 2025-11-20T08:02:58.312427
============================================================
[32m[2025-11-20 08:02:59]    INFO >> Load arguments in run/type_prediction/transformer/config/exp_lr_1e-03.yml (train.py:300, cli_main())[0m
[32m[2025-11-20 08:02:59]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-03/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-03/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-03/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:302, cli_main())[0m
[32m[2025-11-20 08:02:59]    INFO >> single GPU training... (train.py:331, cli_main())[0m
[32m[2025-11-20 08:03:00]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-03/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-03/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-03/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:208, single_main())[0m
[32m[2025-11-20 08:03:03]    INFO >> TypeTransformer(
  (decoder): RobertaEncoder(
    (encoder): CodeEncoderLSTM(
      (embedding): Embedding(10006, 512)
      (pos_encoder): PositionalEncoding()
      (encoder): LSTM(512, 512, num_layers=6, bidirectional=True)
    )
    (output): Sequential(
      (0): Linear(in_features=1024, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=10006, bias=True)
    )
  )
  (classification_heads): ModuleDict()
) (train.py:220, single_main())[0m
[32m[2025-11-20 08:03:03]    INFO >> model typetransformer, criterion TypePredictionCrossEntropyCriterion (train.py:221, single_main())[0m
[32m[2025-11-20 08:03:03]    INFO >> num. model params: 46481686 (num. trained: 46481686) (train.py:222, single_main())[0m
[32m[2025-11-20 08:03:03]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 08:03:03]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 81024 MB ; used memory = 896 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 08:03:03]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 08:03:03]    INFO >> training on 1 GPUs (train.py:229, single_main())[0m
[32m[2025-11-20 08:03:03]    INFO >> max tokens per GPU = None and max sentences per GPU = 32 (train.py:230, single_main())[0m
[32m[2025-11-20 08:03:03]    INFO >> no existing checkpoint found /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-03/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-20 08:03:03]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[33m[2025-11-20 08:03:13] WARNING >> 32222 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[0, 1, 2, 4, 6, 7, 8, 10, 14, 19] (data_utils.py:301, filter_by_size())[0m
[32m[2025-11-20 08:03:13]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-20 08:03:13] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:398: UserWarning: ntokens not found in Criterion logging outputs, cannot log wpb or wps
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:407: UserWarning: nsentences not found in Criterion logging outputs, cannot log bsz
  warnings.warn(
[32m[2025-11-20 08:03:57]    INFO >> epoch 001:    100 / 5058 loss=1.903, ups=2.36, num_updates=100, lr=1e-05, gnorm=0.104, clip=0, train_wall=40, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:04:46]    INFO >> epoch 001:    200 / 5058 loss=0.786, ups=2.16, num_updates=200, lr=2e-05, gnorm=0.068, clip=0, train_wall=44, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:05:51]    INFO >> epoch 001:    300 / 5058 loss=0.654, ups=1.63, num_updates=300, lr=3e-05, gnorm=0.053, clip=0, train_wall=58, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:06:55]    INFO >> epoch 001:    400 / 5058 loss=0.623, ups=1.59, num_updates=400, lr=4e-05, gnorm=2.819, clip=2, train_wall=59, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:07:57]    INFO >> epoch 001:    500 / 5058 loss=0.208, ups=1.67, num_updates=500, lr=5e-05, gnorm=4.476, clip=2, train_wall=56, gb_free=65.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:08:58]    INFO >> epoch 001:    600 / 5058 loss=0.079, ups=1.71, num_updates=600, lr=6e-05, gnorm=4.788, clip=8, train_wall=55, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:09:41]    INFO >> epoch 001:    700 / 5058 loss=0.174, ups=2.42, num_updates=700, lr=7e-05, gnorm=2.641, clip=4, train_wall=39, gb_free=69.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:10:24]    INFO >> epoch 001:    800 / 5058 loss=0.166, ups=2.46, num_updates=800, lr=8e-05, gnorm=3.325, clip=8, train_wall=39, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:11:31]    INFO >> epoch 001:    900 / 5058 loss=0.216, ups=1.56, num_updates=900, lr=9e-05, gnorm=6.91, clip=8, train_wall=60, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:12:36]    INFO >> epoch 001:   1000 / 5058 loss=0.142, ups=1.62, num_updates=1000, lr=0.0001, gnorm=1.451, clip=0, train_wall=58, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:13:34]    INFO >> epoch 001:   1100 / 5058 loss=0.058, ups=1.76, num_updates=1100, lr=0.0001, gnorm=2.202, clip=2, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:14:38]    INFO >> epoch 001:   1200 / 5058 loss=0.035, ups=1.64, num_updates=1200, lr=0.0001, gnorm=0.871, clip=0, train_wall=57, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:15:42]    INFO >> epoch 001:   1300 / 5058 loss=0.087, ups=1.61, num_updates=1300, lr=9.9e-05, gnorm=1.877, clip=2, train_wall=58, gb_free=67.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:16:46]    INFO >> epoch 001:   1400 / 5058 loss=0.059, ups=1.64, num_updates=1400, lr=9.9e-05, gnorm=2.488, clip=4, train_wall=57, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:17:51]    INFO >> epoch 001:   1500 / 5058 loss=0.279, ups=1.61, num_updates=1500, lr=9.9e-05, gnorm=1.776, clip=2, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:18:51]    INFO >> epoch 001:   1600 / 5058 loss=0.11, ups=1.73, num_updates=1600, lr=9.9e-05, gnorm=3.243, clip=4, train_wall=55, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:19:57]    INFO >> epoch 001:   1700 / 5058 loss=0.113, ups=1.58, num_updates=1700, lr=9.9e-05, gnorm=2.319, clip=2, train_wall=59, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:20:51]    INFO >> epoch 001:   1800 / 5058 loss=0.12, ups=1.91, num_updates=1800, lr=9.8e-05, gnorm=3.115, clip=5, train_wall=49, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:21:31]    INFO >> epoch 001:   1900 / 5058 loss=0.153, ups=2.66, num_updates=1900, lr=9.8e-05, gnorm=1.952, clip=4, train_wall=36, gb_free=67.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:22:32]    INFO >> epoch 001:   2000 / 5058 loss=0.089, ups=1.69, num_updates=2000, lr=9.8e-05, gnorm=1.422, clip=1, train_wall=56, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:23:38]    INFO >> epoch 001:   2100 / 5058 loss=0.074, ups=1.58, num_updates=2100, lr=9.8e-05, gnorm=3.142, clip=2, train_wall=60, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:24:44]    INFO >> epoch 001:   2200 / 5058 loss=0.279, ups=1.58, num_updates=2200, lr=9.8e-05, gnorm=5.078, clip=6, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:25:51]    INFO >> epoch 001:   2300 / 5058 loss=0.068, ups=1.56, num_updates=2300, lr=9.7e-05, gnorm=2.452, clip=5, train_wall=60, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:26:55]    INFO >> epoch 001:   2400 / 5058 loss=0.063, ups=1.61, num_updates=2400, lr=9.7e-05, gnorm=0.773, clip=1, train_wall=58, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:28:05]    INFO >> epoch 001:   2500 / 5058 loss=0.149, ups=1.5, num_updates=2500, lr=9.7e-05, gnorm=3.718, clip=1, train_wall=63, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:29:03]    INFO >> epoch 001:   2600 / 5058 loss=0.128, ups=1.81, num_updates=2600, lr=9.7e-05, gnorm=2.608, clip=1, train_wall=52, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:30:03]    INFO >> epoch 001:   2700 / 5058 loss=0.076, ups=1.74, num_updates=2700, lr=9.7e-05, gnorm=1.387, clip=1, train_wall=54, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:31:05]    INFO >> epoch 001:   2800 / 5058 loss=0.081, ups=1.69, num_updates=2800, lr=9.6e-05, gnorm=1.32, clip=3, train_wall=56, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:31:44]    INFO >> epoch 001:   2900 / 5058 loss=0.274, ups=2.61, num_updates=2900, lr=9.6e-05, gnorm=2.005, clip=3, train_wall=36, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:32:50]    INFO >> epoch 001:   3000 / 5058 loss=0.145, ups=1.59, num_updates=3000, lr=9.6e-05, gnorm=3.103, clip=5, train_wall=59, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:33:43]    INFO >> epoch 001:   3100 / 5058 loss=0.058, ups=1.94, num_updates=3100, lr=9.6e-05, gnorm=0.282, clip=0, train_wall=49, gb_free=73.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:34:41]    INFO >> epoch 001:   3200 / 5058 loss=0.282, ups=1.82, num_updates=3200, lr=9.6e-05, gnorm=2.208, clip=2, train_wall=52, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:35:41]    INFO >> epoch 001:   3300 / 5058 loss=0.086, ups=1.74, num_updates=3300, lr=9.5e-05, gnorm=1.673, clip=1, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:36:52]    INFO >> epoch 001:   3400 / 5058 loss=0.225, ups=1.47, num_updates=3400, lr=9.5e-05, gnorm=5.264, clip=3, train_wall=64, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:37:57]    INFO >> epoch 001:   3500 / 5058 loss=0.131, ups=1.59, num_updates=3500, lr=9.5e-05, gnorm=1.53, clip=0, train_wall=59, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:39:03]    INFO >> epoch 001:   3600 / 5058 loss=0.095, ups=1.59, num_updates=3600, lr=9.5e-05, gnorm=2.575, clip=1, train_wall=59, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:40:03]    INFO >> epoch 001:   3700 / 5058 loss=0.137, ups=1.72, num_updates=3700, lr=9.4e-05, gnorm=1.854, clip=2, train_wall=55, gb_free=66.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:41:10]    INFO >> epoch 001:   3800 / 5058 loss=0.058, ups=1.57, num_updates=3800, lr=9.4e-05, gnorm=1.191, clip=1, train_wall=60, gb_free=68.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:41:59]    INFO >> epoch 001:   3900 / 5058 loss=0.127, ups=2.08, num_updates=3900, lr=9.4e-05, gnorm=1.978, clip=2, train_wall=45, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:43:01]    INFO >> epoch 001:   4000 / 5058 loss=0.131, ups=1.69, num_updates=4000, lr=9.4e-05, gnorm=1.731, clip=1, train_wall=56, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:44:04]    INFO >> epoch 001:   4100 / 5058 loss=0.92, ups=1.66, num_updates=4100, lr=9.4e-05, gnorm=0.902, clip=1, train_wall=57, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:45:11]    INFO >> epoch 001:   4200 / 5058 loss=0.149, ups=1.56, num_updates=4200, lr=9.3e-05, gnorm=1.845, clip=3, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:46:10]    INFO >> epoch 001:   4300 / 5058 loss=0.117, ups=1.76, num_updates=4300, lr=9.3e-05, gnorm=1.721, clip=1, train_wall=53, gb_free=73.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:47:09]    INFO >> epoch 001:   4400 / 5058 loss=0.082, ups=1.72, num_updates=4400, lr=9.3e-05, gnorm=1.476, clip=0, train_wall=55, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:48:12]    INFO >> epoch 001:   4500 / 5058 loss=0.072, ups=1.66, num_updates=4500, lr=9.3e-05, gnorm=1.371, clip=0, train_wall=57, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:49:18]    INFO >> epoch 001:   4600 / 5058 loss=0.241, ups=1.58, num_updates=4600, lr=9.3e-05, gnorm=1.805, clip=0, train_wall=60, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:50:29]    INFO >> epoch 001:   4700 / 5058 loss=0.215, ups=1.48, num_updates=4700, lr=9.2e-05, gnorm=3.803, clip=2, train_wall=63, gb_free=64.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:51:30]    INFO >> epoch 001:   4800 / 5058 loss=0.056, ups=1.71, num_updates=4800, lr=9.2e-05, gnorm=1.411, clip=0, train_wall=55, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:52:10]    INFO >> epoch 001:   4900 / 5058 loss=0.142, ups=2.54, num_updates=4900, lr=9.2e-05, gnorm=0.824, clip=0, train_wall=37, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:53:15]    INFO >> epoch 001:   5000 / 5058 loss=0.09, ups=1.61, num_updates=5000, lr=9.2e-05, gnorm=2.488, clip=0, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:53:55]    INFO >> epoch 001 | loss 0.105 | ups 1.73 | num_updates 5058 | lr 9.2e-05 | gnorm 2.248 | clip 2.1 | train_wall 2750 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-20 08:53:55] WARNING >> 4469 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[2, 4, 10, 12, 13, 14, 16, 18, 19, 23] (data_utils.py:301, filter_by_size())[0m
[33m[2025-11-20 08:53:55] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
CodeEncoderLSTM: Creating BiLSTM with 6 layers, 512 hidden and input size
Traceback (most recent call last):
  File "run/type_prediction/transformer/train.py", line 339, in <module>
    cli_main()
  File "run/type_prediction/transformer/train.py", line 332, in cli_main
    single_main(args)
  File "run/type_prediction/transformer/train.py", line 254, in single_main
    valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)
  File "run/type_prediction/transformer/train.py", line 132, in validate
    trainer.valid_step(sample)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/trainers/ncc_trainers.py", line 565, in valid_step
    _loss, sample_size, logging_output = self.task.valid_step(
TypeError: cannot unpack non-iterable NoneType object

============================================================
Setting up experiment: exp_lr_1e-03
============================================================
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-03
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-03/checkpoints
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-03/logs
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-03/results
âœ“ Saved config to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-03/config.yml
âœ“ Also saved to: run/type_prediction/transformer/config/exp_lr_1e-03.yml
âœ“ Saved experiment info to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-03/info.txt

============================================================
Starting training...
============================================================

Command: python run/type_prediction/transformer/train.py --language exp_lr_1e-03 2>&1 | tee /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-03/logs/train.log

Note: Config file will be loaded from run/type_prediction/transformer/config/exp_lr_1e-03.yml


âœ“ Training completed successfully!

============================================================
Evaluating model on test set...
============================================================

Warning: Best checkpoint not found, using last checkpoint
âœ— No checkpoint found for evaluation

============================================================
âœ“ Experiment completed: exp_lr_1e-03
âœ“ Results in: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_lr_1e-03
============================================================


âœ“ exp_lr_1e-03 completed in 0.85 hours

============================================================
Running: exp_dropout_0.0
GPU: 3
Time: 2025-11-20T08:53:58.105704
============================================================
[32m[2025-11-20 08:54:01]    INFO >> Load arguments in run/type_prediction/transformer/config/exp_dropout_0.0.yml (train.py:300, cli_main())[0m
[32m[2025-11-20 08:54:01]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.0/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.0/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.0/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:302, cli_main())[0m
[32m[2025-11-20 08:54:01]    INFO >> single GPU training... (train.py:331, cli_main())[0m
[32m[2025-11-20 08:54:01]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.0/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.0/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.0/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:208, single_main())[0m
[32m[2025-11-20 08:54:02]    INFO >> TypeTransformer(
  (decoder): RobertaEncoder(
    (encoder): CodeEncoderLSTM(
      (embedding): Embedding(10006, 512)
      (pos_encoder): PositionalEncoding()
      (encoder): LSTM(512, 512, num_layers=6, bidirectional=True)
    )
    (output): Sequential(
      (0): Linear(in_features=1024, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=10006, bias=True)
    )
  )
  (classification_heads): ModuleDict()
) (train.py:220, single_main())[0m
[32m[2025-11-20 08:54:02]    INFO >> model typetransformer, criterion TypePredictionCrossEntropyCriterion (train.py:221, single_main())[0m
[32m[2025-11-20 08:54:02]    INFO >> num. model params: 46481686 (num. trained: 46481686) (train.py:222, single_main())[0m
[32m[2025-11-20 08:54:03]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 08:54:03]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 81024 MB ; used memory = 896 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 08:54:03]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 08:54:03]    INFO >> training on 1 GPUs (train.py:229, single_main())[0m
[32m[2025-11-20 08:54:03]    INFO >> max tokens per GPU = None and max sentences per GPU = 32 (train.py:230, single_main())[0m
[32m[2025-11-20 08:54:03]    INFO >> no existing checkpoint found /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.0/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-20 08:54:03]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[33m[2025-11-20 08:54:13] WARNING >> 32222 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[0, 1, 2, 4, 6, 7, 8, 10, 14, 19] (data_utils.py:301, filter_by_size())[0m
[32m[2025-11-20 08:54:13]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-20 08:54:13] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:398: UserWarning: ntokens not found in Criterion logging outputs, cannot log wpb or wps
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:407: UserWarning: nsentences not found in Criterion logging outputs, cannot log bsz
  warnings.warn(
[32m[2025-11-20 08:54:57]    INFO >> epoch 001:    100 / 5058 loss=1.903, ups=2.37, num_updates=100, lr=1e-05, gnorm=0.104, clip=0, train_wall=40, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:55:46]    INFO >> epoch 001:    200 / 5058 loss=0.786, ups=2.16, num_updates=200, lr=2e-05, gnorm=0.068, clip=0, train_wall=44, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:56:50]    INFO >> epoch 001:    300 / 5058 loss=0.654, ups=1.63, num_updates=300, lr=3e-05, gnorm=0.053, clip=0, train_wall=58, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:57:56]    INFO >> epoch 001:    400 / 5058 loss=0.623, ups=1.59, num_updates=400, lr=4e-05, gnorm=2.819, clip=2, train_wall=59, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:58:57]    INFO >> epoch 001:    500 / 5058 loss=0.208, ups=1.67, num_updates=500, lr=5e-05, gnorm=4.476, clip=2, train_wall=56, gb_free=65.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 08:59:58]    INFO >> epoch 001:    600 / 5058 loss=0.079, ups=1.71, num_updates=600, lr=6e-05, gnorm=4.788, clip=8, train_wall=55, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:00:41]    INFO >> epoch 001:    700 / 5058 loss=0.174, ups=2.43, num_updates=700, lr=7e-05, gnorm=2.641, clip=4, train_wall=39, gb_free=69.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:01:24]    INFO >> epoch 001:    800 / 5058 loss=0.166, ups=2.46, num_updates=800, lr=8e-05, gnorm=3.325, clip=8, train_wall=39, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:02:31]    INFO >> epoch 001:    900 / 5058 loss=0.216, ups=1.56, num_updates=900, lr=9e-05, gnorm=6.91, clip=8, train_wall=60, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:03:35]    INFO >> epoch 001:   1000 / 5058 loss=0.142, ups=1.62, num_updates=1000, lr=0.0001, gnorm=1.451, clip=0, train_wall=58, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:04:33]    INFO >> epoch 001:   1100 / 5058 loss=0.058, ups=1.76, num_updates=1100, lr=0.0001, gnorm=2.202, clip=2, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:05:37]    INFO >> epoch 001:   1200 / 5058 loss=0.035, ups=1.64, num_updates=1200, lr=0.0001, gnorm=0.871, clip=0, train_wall=57, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:06:42]    INFO >> epoch 001:   1300 / 5058 loss=0.087, ups=1.61, num_updates=1300, lr=9.9e-05, gnorm=1.877, clip=2, train_wall=59, gb_free=67.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:07:45]    INFO >> epoch 001:   1400 / 5058 loss=0.059, ups=1.65, num_updates=1400, lr=9.9e-05, gnorm=2.488, clip=4, train_wall=57, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:08:50]    INFO >> epoch 001:   1500 / 5058 loss=0.279, ups=1.61, num_updates=1500, lr=9.9e-05, gnorm=1.776, clip=2, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:09:51]    INFO >> epoch 001:   1600 / 5058 loss=0.11, ups=1.73, num_updates=1600, lr=9.9e-05, gnorm=3.243, clip=4, train_wall=55, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:10:57]    INFO >> epoch 001:   1700 / 5058 loss=0.113, ups=1.58, num_updates=1700, lr=9.9e-05, gnorm=2.319, clip=2, train_wall=59, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:11:50]    INFO >> epoch 001:   1800 / 5058 loss=0.12, ups=1.91, num_updates=1800, lr=9.8e-05, gnorm=3.115, clip=5, train_wall=49, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:12:31]    INFO >> epoch 001:   1900 / 5058 loss=0.153, ups=2.66, num_updates=1900, lr=9.8e-05, gnorm=1.952, clip=4, train_wall=36, gb_free=67.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:13:31]    INFO >> epoch 001:   2000 / 5058 loss=0.089, ups=1.69, num_updates=2000, lr=9.8e-05, gnorm=1.422, clip=1, train_wall=56, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:14:37]    INFO >> epoch 001:   2100 / 5058 loss=0.074, ups=1.58, num_updates=2100, lr=9.8e-05, gnorm=3.142, clip=2, train_wall=60, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:15:43]    INFO >> epoch 001:   2200 / 5058 loss=0.279, ups=1.57, num_updates=2200, lr=9.8e-05, gnorm=5.078, clip=6, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:16:50]    INFO >> epoch 001:   2300 / 5058 loss=0.068, ups=1.56, num_updates=2300, lr=9.7e-05, gnorm=2.452, clip=5, train_wall=60, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:17:55]    INFO >> epoch 001:   2400 / 5058 loss=0.063, ups=1.61, num_updates=2400, lr=9.7e-05, gnorm=0.773, clip=1, train_wall=58, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:19:04]    INFO >> epoch 001:   2500 / 5058 loss=0.149, ups=1.5, num_updates=2500, lr=9.7e-05, gnorm=3.718, clip=1, train_wall=63, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:20:02]    INFO >> epoch 001:   2600 / 5058 loss=0.128, ups=1.81, num_updates=2600, lr=9.7e-05, gnorm=2.608, clip=1, train_wall=52, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:21:02]    INFO >> epoch 001:   2700 / 5058 loss=0.076, ups=1.74, num_updates=2700, lr=9.7e-05, gnorm=1.387, clip=1, train_wall=54, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:22:04]    INFO >> epoch 001:   2800 / 5058 loss=0.081, ups=1.69, num_updates=2800, lr=9.6e-05, gnorm=1.32, clip=3, train_wall=56, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:22:44]    INFO >> epoch 001:   2900 / 5058 loss=0.274, ups=2.61, num_updates=2900, lr=9.6e-05, gnorm=2.005, clip=3, train_wall=36, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:23:49]    INFO >> epoch 001:   3000 / 5058 loss=0.145, ups=1.59, num_updates=3000, lr=9.6e-05, gnorm=3.103, clip=5, train_wall=59, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:24:42]    INFO >> epoch 001:   3100 / 5058 loss=0.058, ups=1.94, num_updates=3100, lr=9.6e-05, gnorm=0.282, clip=0, train_wall=49, gb_free=73.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:25:40]    INFO >> epoch 001:   3200 / 5058 loss=0.282, ups=1.82, num_updates=3200, lr=9.6e-05, gnorm=2.208, clip=2, train_wall=52, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:26:40]    INFO >> epoch 001:   3300 / 5058 loss=0.086, ups=1.74, num_updates=3300, lr=9.5e-05, gnorm=1.673, clip=1, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:27:51]    INFO >> epoch 001:   3400 / 5058 loss=0.225, ups=1.47, num_updates=3400, lr=9.5e-05, gnorm=5.264, clip=3, train_wall=64, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:28:56]    INFO >> epoch 001:   3500 / 5058 loss=0.131, ups=1.6, num_updates=3500, lr=9.5e-05, gnorm=1.53, clip=0, train_wall=59, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:30:02]    INFO >> epoch 001:   3600 / 5058 loss=0.095, ups=1.59, num_updates=3600, lr=9.5e-05, gnorm=2.575, clip=1, train_wall=59, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:31:02]    INFO >> epoch 001:   3700 / 5058 loss=0.137, ups=1.72, num_updates=3700, lr=9.4e-05, gnorm=1.854, clip=2, train_wall=55, gb_free=66.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:32:09]    INFO >> epoch 001:   3800 / 5058 loss=0.058, ups=1.57, num_updates=3800, lr=9.4e-05, gnorm=1.191, clip=1, train_wall=60, gb_free=68.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:32:58]    INFO >> epoch 001:   3900 / 5058 loss=0.127, ups=2.08, num_updates=3900, lr=9.4e-05, gnorm=1.978, clip=2, train_wall=45, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:34:00]    INFO >> epoch 001:   4000 / 5058 loss=0.131, ups=1.69, num_updates=4000, lr=9.4e-05, gnorm=1.731, clip=1, train_wall=56, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:35:03]    INFO >> epoch 001:   4100 / 5058 loss=0.92, ups=1.66, num_updates=4100, lr=9.4e-05, gnorm=0.902, clip=1, train_wall=57, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:36:10]    INFO >> epoch 001:   4200 / 5058 loss=0.149, ups=1.56, num_updates=4200, lr=9.3e-05, gnorm=1.845, clip=3, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:37:09]    INFO >> epoch 001:   4300 / 5058 loss=0.117, ups=1.76, num_updates=4300, lr=9.3e-05, gnorm=1.721, clip=1, train_wall=54, gb_free=73.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:38:08]    INFO >> epoch 001:   4400 / 5058 loss=0.082, ups=1.72, num_updates=4400, lr=9.3e-05, gnorm=1.476, clip=0, train_wall=55, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:39:11]    INFO >> epoch 001:   4500 / 5058 loss=0.072, ups=1.66, num_updates=4500, lr=9.3e-05, gnorm=1.371, clip=0, train_wall=57, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:40:17]    INFO >> epoch 001:   4600 / 5058 loss=0.241, ups=1.58, num_updates=4600, lr=9.3e-05, gnorm=1.805, clip=0, train_wall=60, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:41:28]    INFO >> epoch 001:   4700 / 5058 loss=0.215, ups=1.48, num_updates=4700, lr=9.2e-05, gnorm=3.803, clip=2, train_wall=64, gb_free=64.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:42:29]    INFO >> epoch 001:   4800 / 5058 loss=0.056, ups=1.71, num_updates=4800, lr=9.2e-05, gnorm=1.411, clip=0, train_wall=55, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:43:10]    INFO >> epoch 001:   4900 / 5058 loss=0.142, ups=2.54, num_updates=4900, lr=9.2e-05, gnorm=0.824, clip=0, train_wall=37, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:44:14]    INFO >> epoch 001:   5000 / 5058 loss=0.09, ups=1.62, num_updates=5000, lr=9.2e-05, gnorm=2.488, clip=0, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:44:54]    INFO >> epoch 001 | loss 0.105 | ups 1.73 | num_updates 5058 | lr 9.2e-05 | gnorm 2.248 | clip 2.1 | train_wall 2754 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-20 09:44:54] WARNING >> 4469 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[2, 4, 10, 12, 13, 14, 16, 18, 19, 23] (data_utils.py:301, filter_by_size())[0m
[33m[2025-11-20 09:44:54] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
CodeEncoderLSTM: Creating BiLSTM with 6 layers, 512 hidden and input size
Traceback (most recent call last):
  File "run/type_prediction/transformer/train.py", line 339, in <module>
    cli_main()
  File "run/type_prediction/transformer/train.py", line 332, in cli_main
    single_main(args)
  File "run/type_prediction/transformer/train.py", line 254, in single_main
    valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)
  File "run/type_prediction/transformer/train.py", line 132, in validate
    trainer.valid_step(sample)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/trainers/ncc_trainers.py", line 565, in valid_step
    _loss, sample_size, logging_output = self.task.valid_step(
TypeError: cannot unpack non-iterable NoneType object

============================================================
Setting up experiment: exp_dropout_0.0
============================================================
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.0
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.0/checkpoints
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.0/logs
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.0/results
âœ“ Saved config to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.0/config.yml
âœ“ Also saved to: run/type_prediction/transformer/config/exp_dropout_0.0.yml
âœ“ Saved experiment info to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.0/info.txt

============================================================
Starting training...
============================================================

Command: python run/type_prediction/transformer/train.py --language exp_dropout_0.0 2>&1 | tee /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.0/logs/train.log

Note: Config file will be loaded from run/type_prediction/transformer/config/exp_dropout_0.0.yml


âœ“ Training completed successfully!

============================================================
Evaluating model on test set...
============================================================

Warning: Best checkpoint not found, using last checkpoint
âœ— No checkpoint found for evaluation

============================================================
âœ“ Experiment completed: exp_dropout_0.0
âœ“ Results in: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.0
============================================================


âœ“ exp_dropout_0.0 completed in 0.85 hours

============================================================
Running: exp_dropout_0.1
GPU: 3
Time: 2025-11-20T09:44:57.142218
============================================================
[32m[2025-11-20 09:45:00]    INFO >> Load arguments in run/type_prediction/transformer/config/exp_dropout_0.1.yml (train.py:300, cli_main())[0m
[32m[2025-11-20 09:45:00]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.1/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.1/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.1/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:302, cli_main())[0m
[32m[2025-11-20 09:45:00]    INFO >> single GPU training... (train.py:331, cli_main())[0m
[32m[2025-11-20 09:45:00]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.1/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.1/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.1/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:208, single_main())[0m
[32m[2025-11-20 09:45:01]    INFO >> TypeTransformer(
  (decoder): RobertaEncoder(
    (encoder): CodeEncoderLSTM(
      (embedding): Embedding(10006, 512)
      (pos_encoder): PositionalEncoding()
      (encoder): LSTM(512, 512, num_layers=6, bidirectional=True)
    )
    (output): Sequential(
      (0): Linear(in_features=1024, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=10006, bias=True)
    )
  )
  (classification_heads): ModuleDict()
) (train.py:220, single_main())[0m
[32m[2025-11-20 09:45:01]    INFO >> model typetransformer, criterion TypePredictionCrossEntropyCriterion (train.py:221, single_main())[0m
[32m[2025-11-20 09:45:01]    INFO >> num. model params: 46481686 (num. trained: 46481686) (train.py:222, single_main())[0m
[32m[2025-11-20 09:45:02]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 09:45:02]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 81024 MB ; used memory = 896 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 09:45:02]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 09:45:02]    INFO >> training on 1 GPUs (train.py:229, single_main())[0m
[32m[2025-11-20 09:45:02]    INFO >> max tokens per GPU = None and max sentences per GPU = 32 (train.py:230, single_main())[0m
[32m[2025-11-20 09:45:02]    INFO >> no existing checkpoint found /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.1/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-20 09:45:02]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[33m[2025-11-20 09:45:12] WARNING >> 32222 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[0, 1, 2, 4, 6, 7, 8, 10, 14, 19] (data_utils.py:301, filter_by_size())[0m
[32m[2025-11-20 09:45:12]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-20 09:45:12] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:398: UserWarning: ntokens not found in Criterion logging outputs, cannot log wpb or wps
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:407: UserWarning: nsentences not found in Criterion logging outputs, cannot log bsz
  warnings.warn(
[32m[2025-11-20 09:45:56]    INFO >> epoch 001:    100 / 5058 loss=1.903, ups=2.37, num_updates=100, lr=1e-05, gnorm=0.104, clip=0, train_wall=40, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:46:45]    INFO >> epoch 001:    200 / 5058 loss=0.786, ups=2.16, num_updates=200, lr=2e-05, gnorm=0.068, clip=0, train_wall=44, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:47:49]    INFO >> epoch 001:    300 / 5058 loss=0.654, ups=1.63, num_updates=300, lr=3e-05, gnorm=0.053, clip=0, train_wall=58, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:48:55]    INFO >> epoch 001:    400 / 5058 loss=0.623, ups=1.59, num_updates=400, lr=4e-05, gnorm=2.819, clip=2, train_wall=59, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:49:56]    INFO >> epoch 001:    500 / 5058 loss=0.208, ups=1.67, num_updates=500, lr=5e-05, gnorm=4.476, clip=2, train_wall=56, gb_free=65.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:50:57]    INFO >> epoch 001:    600 / 5058 loss=0.079, ups=1.71, num_updates=600, lr=6e-05, gnorm=4.788, clip=8, train_wall=55, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:51:40]    INFO >> epoch 001:    700 / 5058 loss=0.174, ups=2.42, num_updates=700, lr=7e-05, gnorm=2.641, clip=4, train_wall=39, gb_free=69.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:52:23]    INFO >> epoch 001:    800 / 5058 loss=0.166, ups=2.46, num_updates=800, lr=8e-05, gnorm=3.325, clip=8, train_wall=39, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:53:30]    INFO >> epoch 001:    900 / 5058 loss=0.216, ups=1.56, num_updates=900, lr=9e-05, gnorm=6.91, clip=8, train_wall=60, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:54:33]    INFO >> epoch 001:   1000 / 5058 loss=0.142, ups=1.62, num_updates=1000, lr=0.0001, gnorm=1.451, clip=0, train_wall=58, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:55:32]    INFO >> epoch 001:   1100 / 5058 loss=0.058, ups=1.76, num_updates=1100, lr=0.0001, gnorm=2.202, clip=2, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:56:36]    INFO >> epoch 001:   1200 / 5058 loss=0.035, ups=1.64, num_updates=1200, lr=0.0001, gnorm=0.871, clip=0, train_wall=57, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:57:41]    INFO >> epoch 001:   1300 / 5058 loss=0.087, ups=1.61, num_updates=1300, lr=9.9e-05, gnorm=1.877, clip=2, train_wall=58, gb_free=67.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:58:44]    INFO >> epoch 001:   1400 / 5058 loss=0.059, ups=1.65, num_updates=1400, lr=9.9e-05, gnorm=2.488, clip=4, train_wall=57, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 09:59:49]    INFO >> epoch 001:   1500 / 5058 loss=0.279, ups=1.61, num_updates=1500, lr=9.9e-05, gnorm=1.776, clip=2, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:00:50]    INFO >> epoch 001:   1600 / 5058 loss=0.11, ups=1.73, num_updates=1600, lr=9.9e-05, gnorm=3.243, clip=4, train_wall=55, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:01:56]    INFO >> epoch 001:   1700 / 5058 loss=0.113, ups=1.58, num_updates=1700, lr=9.9e-05, gnorm=2.319, clip=2, train_wall=60, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:02:49]    INFO >> epoch 001:   1800 / 5058 loss=0.12, ups=1.91, num_updates=1800, lr=9.8e-05, gnorm=3.115, clip=5, train_wall=49, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:03:30]    INFO >> epoch 001:   1900 / 5058 loss=0.153, ups=2.66, num_updates=1900, lr=9.8e-05, gnorm=1.952, clip=4, train_wall=36, gb_free=67.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:04:30]    INFO >> epoch 001:   2000 / 5058 loss=0.089, ups=1.69, num_updates=2000, lr=9.8e-05, gnorm=1.422, clip=1, train_wall=56, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:05:36]    INFO >> epoch 001:   2100 / 5058 loss=0.074, ups=1.58, num_updates=2100, lr=9.8e-05, gnorm=3.142, clip=2, train_wall=60, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:06:42]    INFO >> epoch 001:   2200 / 5058 loss=0.279, ups=1.57, num_updates=2200, lr=9.8e-05, gnorm=5.078, clip=6, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:07:49]    INFO >> epoch 001:   2300 / 5058 loss=0.068, ups=1.56, num_updates=2300, lr=9.7e-05, gnorm=2.452, clip=5, train_wall=60, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:08:54]    INFO >> epoch 001:   2400 / 5058 loss=0.063, ups=1.61, num_updates=2400, lr=9.7e-05, gnorm=0.773, clip=1, train_wall=58, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:10:03]    INFO >> epoch 001:   2500 / 5058 loss=0.149, ups=1.5, num_updates=2500, lr=9.7e-05, gnorm=3.718, clip=1, train_wall=63, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:11:01]    INFO >> epoch 001:   2600 / 5058 loss=0.128, ups=1.82, num_updates=2600, lr=9.7e-05, gnorm=2.608, clip=1, train_wall=52, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:12:01]    INFO >> epoch 001:   2700 / 5058 loss=0.076, ups=1.74, num_updates=2700, lr=9.7e-05, gnorm=1.387, clip=1, train_wall=54, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:13:03]    INFO >> epoch 001:   2800 / 5058 loss=0.081, ups=1.69, num_updates=2800, lr=9.6e-05, gnorm=1.32, clip=3, train_wall=56, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:13:43]    INFO >> epoch 001:   2900 / 5058 loss=0.274, ups=2.61, num_updates=2900, lr=9.6e-05, gnorm=2.005, clip=3, train_wall=36, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:14:48]    INFO >> epoch 001:   3000 / 5058 loss=0.145, ups=1.59, num_updates=3000, lr=9.6e-05, gnorm=3.103, clip=5, train_wall=59, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:15:41]    INFO >> epoch 001:   3100 / 5058 loss=0.058, ups=1.94, num_updates=3100, lr=9.6e-05, gnorm=0.282, clip=0, train_wall=49, gb_free=73.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:16:39]    INFO >> epoch 001:   3200 / 5058 loss=0.282, ups=1.82, num_updates=3200, lr=9.6e-05, gnorm=2.208, clip=2, train_wall=52, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:17:39]    INFO >> epoch 001:   3300 / 5058 loss=0.086, ups=1.74, num_updates=3300, lr=9.5e-05, gnorm=1.673, clip=1, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:18:50]    INFO >> epoch 001:   3400 / 5058 loss=0.225, ups=1.47, num_updates=3400, lr=9.5e-05, gnorm=5.264, clip=3, train_wall=64, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:19:55]    INFO >> epoch 001:   3500 / 5058 loss=0.131, ups=1.6, num_updates=3500, lr=9.5e-05, gnorm=1.53, clip=0, train_wall=59, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:21:01]    INFO >> epoch 001:   3600 / 5058 loss=0.095, ups=1.6, num_updates=3600, lr=9.5e-05, gnorm=2.575, clip=1, train_wall=59, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:22:01]    INFO >> epoch 001:   3700 / 5058 loss=0.137, ups=1.72, num_updates=3700, lr=9.4e-05, gnorm=1.854, clip=2, train_wall=55, gb_free=66.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:23:08]    INFO >> epoch 001:   3800 / 5058 loss=0.058, ups=1.57, num_updates=3800, lr=9.4e-05, gnorm=1.191, clip=1, train_wall=60, gb_free=68.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:23:57]    INFO >> epoch 001:   3900 / 5058 loss=0.127, ups=2.09, num_updates=3900, lr=9.4e-05, gnorm=1.978, clip=2, train_wall=45, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:24:59]    INFO >> epoch 001:   4000 / 5058 loss=0.131, ups=1.69, num_updates=4000, lr=9.4e-05, gnorm=1.731, clip=1, train_wall=56, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:26:02]    INFO >> epoch 001:   4100 / 5058 loss=0.92, ups=1.66, num_updates=4100, lr=9.4e-05, gnorm=0.902, clip=1, train_wall=57, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:27:08]    INFO >> epoch 001:   4200 / 5058 loss=0.149, ups=1.56, num_updates=4200, lr=9.3e-05, gnorm=1.845, clip=3, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:28:08]    INFO >> epoch 001:   4300 / 5058 loss=0.117, ups=1.76, num_updates=4300, lr=9.3e-05, gnorm=1.721, clip=1, train_wall=54, gb_free=73.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:29:07]    INFO >> epoch 001:   4400 / 5058 loss=0.082, ups=1.72, num_updates=4400, lr=9.3e-05, gnorm=1.476, clip=0, train_wall=55, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:30:10]    INFO >> epoch 001:   4500 / 5058 loss=0.072, ups=1.66, num_updates=4500, lr=9.3e-05, gnorm=1.371, clip=0, train_wall=57, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:31:16]    INFO >> epoch 001:   4600 / 5058 loss=0.241, ups=1.58, num_updates=4600, lr=9.3e-05, gnorm=1.805, clip=0, train_wall=60, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:32:26]    INFO >> epoch 001:   4700 / 5058 loss=0.215, ups=1.48, num_updates=4700, lr=9.2e-05, gnorm=3.803, clip=2, train_wall=63, gb_free=64.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:33:28]    INFO >> epoch 001:   4800 / 5058 loss=0.056, ups=1.71, num_updates=4800, lr=9.2e-05, gnorm=1.411, clip=0, train_wall=55, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:34:08]    INFO >> epoch 001:   4900 / 5058 loss=0.142, ups=2.54, num_updates=4900, lr=9.2e-05, gnorm=0.824, clip=0, train_wall=37, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:35:13]    INFO >> epoch 001:   5000 / 5058 loss=0.09, ups=1.62, num_updates=5000, lr=9.2e-05, gnorm=2.488, clip=0, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:35:52]    INFO >> epoch 001 | loss 0.105 | ups 1.73 | num_updates 5058 | lr 9.2e-05 | gnorm 2.248 | clip 2.1 | train_wall 2751 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-20 10:35:52] WARNING >> 4469 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[2, 4, 10, 12, 13, 14, 16, 18, 19, 23] (data_utils.py:301, filter_by_size())[0m
[33m[2025-11-20 10:35:52] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
CodeEncoderLSTM: Creating BiLSTM with 6 layers, 512 hidden and input size
Traceback (most recent call last):
  File "run/type_prediction/transformer/train.py", line 339, in <module>
    cli_main()
  File "run/type_prediction/transformer/train.py", line 332, in cli_main
    single_main(args)
  File "run/type_prediction/transformer/train.py", line 254, in single_main
    valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)
  File "run/type_prediction/transformer/train.py", line 132, in validate
    trainer.valid_step(sample)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/trainers/ncc_trainers.py", line 565, in valid_step
    _loss, sample_size, logging_output = self.task.valid_step(
TypeError: cannot unpack non-iterable NoneType object

============================================================
Setting up experiment: exp_dropout_0.1
============================================================
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.1
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.1/checkpoints
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.1/logs
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.1/results
âœ“ Saved config to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.1/config.yml
âœ“ Also saved to: run/type_prediction/transformer/config/exp_dropout_0.1.yml
âœ“ Saved experiment info to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.1/info.txt

============================================================
Starting training...
============================================================

Command: python run/type_prediction/transformer/train.py --language exp_dropout_0.1 2>&1 | tee /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.1/logs/train.log

Note: Config file will be loaded from run/type_prediction/transformer/config/exp_dropout_0.1.yml


âœ“ Training completed successfully!

============================================================
Evaluating model on test set...
============================================================

Warning: Best checkpoint not found, using last checkpoint
âœ— No checkpoint found for evaluation

============================================================
âœ“ Experiment completed: exp_dropout_0.1
âœ“ Results in: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.1
============================================================


âœ“ exp_dropout_0.1 completed in 0.85 hours

============================================================
Running: exp_dropout_0.2
GPU: 3
Time: 2025-11-20T10:35:55.990885
============================================================
[32m[2025-11-20 10:35:57]    INFO >> Load arguments in run/type_prediction/transformer/config/exp_dropout_0.2.yml (train.py:300, cli_main())[0m
[32m[2025-11-20 10:35:57]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.2/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.2, 'attention_dropout': 0.2, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.2/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.2/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:302, cli_main())[0m
[32m[2025-11-20 10:35:57]    INFO >> single GPU training... (train.py:331, cli_main())[0m
[32m[2025-11-20 10:35:57]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.2/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.2, 'attention_dropout': 0.2, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'lstm', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.2/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.2/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:208, single_main())[0m
[32m[2025-11-20 10:36:00]    INFO >> TypeTransformer(
  (decoder): RobertaEncoder(
    (encoder): CodeEncoderLSTM(
      (embedding): Embedding(10006, 512)
      (pos_encoder): PositionalEncoding()
      (encoder): LSTM(512, 512, num_layers=6, bidirectional=True)
    )
    (output): Sequential(
      (0): Linear(in_features=1024, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=10006, bias=True)
    )
  )
  (classification_heads): ModuleDict()
) (train.py:220, single_main())[0m
[32m[2025-11-20 10:36:00]    INFO >> model typetransformer, criterion TypePredictionCrossEntropyCriterion (train.py:221, single_main())[0m
[32m[2025-11-20 10:36:00]    INFO >> num. model params: 46481686 (num. trained: 46481686) (train.py:222, single_main())[0m
[32m[2025-11-20 10:36:00]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 10:36:00]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 81024 MB ; used memory = 896 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 10:36:00]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 10:36:00]    INFO >> training on 1 GPUs (train.py:229, single_main())[0m
[32m[2025-11-20 10:36:00]    INFO >> max tokens per GPU = None and max sentences per GPU = 32 (train.py:230, single_main())[0m
[32m[2025-11-20 10:36:00]    INFO >> no existing checkpoint found /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.2/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-20 10:36:00]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[33m[2025-11-20 10:36:11] WARNING >> 32222 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[0, 1, 2, 4, 6, 7, 8, 10, 14, 19] (data_utils.py:301, filter_by_size())[0m
[32m[2025-11-20 10:36:11]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-20 10:36:11] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:398: UserWarning: ntokens not found in Criterion logging outputs, cannot log wpb or wps
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:407: UserWarning: nsentences not found in Criterion logging outputs, cannot log bsz
  warnings.warn(
[32m[2025-11-20 10:36:55]    INFO >> epoch 001:    100 / 5058 loss=1.903, ups=2.36, num_updates=100, lr=1e-05, gnorm=0.104, clip=0, train_wall=40, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:37:44]    INFO >> epoch 001:    200 / 5058 loss=0.786, ups=2.16, num_updates=200, lr=2e-05, gnorm=0.068, clip=0, train_wall=44, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:38:48]    INFO >> epoch 001:    300 / 5058 loss=0.654, ups=1.63, num_updates=300, lr=3e-05, gnorm=0.053, clip=0, train_wall=58, gb_free=65.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:39:51]    INFO >> epoch 001:    400 / 5058 loss=0.623, ups=1.59, num_updates=400, lr=4e-05, gnorm=2.819, clip=2, train_wall=59, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:40:55]    INFO >> epoch 001:    500 / 5058 loss=0.208, ups=1.67, num_updates=500, lr=5e-05, gnorm=4.476, clip=2, train_wall=56, gb_free=65.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:41:56]    INFO >> epoch 001:    600 / 5058 loss=0.079, ups=1.71, num_updates=600, lr=6e-05, gnorm=4.788, clip=8, train_wall=55, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:42:39]    INFO >> epoch 001:    700 / 5058 loss=0.174, ups=2.43, num_updates=700, lr=7e-05, gnorm=2.641, clip=4, train_wall=39, gb_free=69.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:43:22]    INFO >> epoch 001:    800 / 5058 loss=0.166, ups=2.46, num_updates=800, lr=8e-05, gnorm=3.325, clip=8, train_wall=39, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:44:29]    INFO >> epoch 001:    900 / 5058 loss=0.216, ups=1.56, num_updates=900, lr=9e-05, gnorm=6.91, clip=8, train_wall=60, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:45:33]    INFO >> epoch 001:   1000 / 5058 loss=0.142, ups=1.62, num_updates=1000, lr=0.0001, gnorm=1.451, clip=0, train_wall=58, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:46:31]    INFO >> epoch 001:   1100 / 5058 loss=0.058, ups=1.76, num_updates=1100, lr=0.0001, gnorm=2.202, clip=2, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:47:35]    INFO >> epoch 001:   1200 / 5058 loss=0.035, ups=1.64, num_updates=1200, lr=0.0001, gnorm=0.871, clip=0, train_wall=57, gb_free=64.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:48:40]    INFO >> epoch 001:   1300 / 5058 loss=0.087, ups=1.61, num_updates=1300, lr=9.9e-05, gnorm=1.877, clip=2, train_wall=58, gb_free=67.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:49:44]    INFO >> epoch 001:   1400 / 5058 loss=0.059, ups=1.64, num_updates=1400, lr=9.9e-05, gnorm=2.488, clip=4, train_wall=57, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:50:48]    INFO >> epoch 001:   1500 / 5058 loss=0.279, ups=1.61, num_updates=1500, lr=9.9e-05, gnorm=1.776, clip=2, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:51:49]    INFO >> epoch 001:   1600 / 5058 loss=0.11, ups=1.73, num_updates=1600, lr=9.9e-05, gnorm=3.243, clip=4, train_wall=55, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:52:55]    INFO >> epoch 001:   1700 / 5058 loss=0.113, ups=1.58, num_updates=1700, lr=9.9e-05, gnorm=2.319, clip=2, train_wall=59, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:53:48]    INFO >> epoch 001:   1800 / 5058 loss=0.12, ups=1.91, num_updates=1800, lr=9.8e-05, gnorm=3.115, clip=5, train_wall=49, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:54:27]    INFO >> epoch 001:   1900 / 5058 loss=0.153, ups=2.66, num_updates=1900, lr=9.8e-05, gnorm=1.952, clip=4, train_wall=36, gb_free=67.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:55:29]    INFO >> epoch 001:   2000 / 5058 loss=0.089, ups=1.69, num_updates=2000, lr=9.8e-05, gnorm=1.422, clip=1, train_wall=56, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:56:35]    INFO >> epoch 001:   2100 / 5058 loss=0.074, ups=1.58, num_updates=2100, lr=9.8e-05, gnorm=3.142, clip=2, train_wall=60, gb_free=63.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:57:41]    INFO >> epoch 001:   2200 / 5058 loss=0.279, ups=1.58, num_updates=2200, lr=9.8e-05, gnorm=5.078, clip=6, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:58:48]    INFO >> epoch 001:   2300 / 5058 loss=0.068, ups=1.55, num_updates=2300, lr=9.7e-05, gnorm=2.452, clip=5, train_wall=60, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 10:59:53]    INFO >> epoch 001:   2400 / 5058 loss=0.063, ups=1.61, num_updates=2400, lr=9.7e-05, gnorm=0.773, clip=1, train_wall=58, gb_free=67.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:01:02]    INFO >> epoch 001:   2500 / 5058 loss=0.149, ups=1.5, num_updates=2500, lr=9.7e-05, gnorm=3.718, clip=1, train_wall=63, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:02:00]    INFO >> epoch 001:   2600 / 5058 loss=0.128, ups=1.81, num_updates=2600, lr=9.7e-05, gnorm=2.608, clip=1, train_wall=52, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:03:00]    INFO >> epoch 001:   2700 / 5058 loss=0.076, ups=1.74, num_updates=2700, lr=9.7e-05, gnorm=1.387, clip=1, train_wall=54, gb_free=66 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:04:02]    INFO >> epoch 001:   2800 / 5058 loss=0.081, ups=1.69, num_updates=2800, lr=9.6e-05, gnorm=1.32, clip=3, train_wall=56, gb_free=67.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:04:42]    INFO >> epoch 001:   2900 / 5058 loss=0.274, ups=2.61, num_updates=2900, lr=9.6e-05, gnorm=2.005, clip=3, train_wall=36, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:05:48]    INFO >> epoch 001:   3000 / 5058 loss=0.145, ups=1.59, num_updates=3000, lr=9.6e-05, gnorm=3.103, clip=5, train_wall=59, gb_free=66.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:06:40]    INFO >> epoch 001:   3100 / 5058 loss=0.058, ups=1.94, num_updates=3100, lr=9.6e-05, gnorm=0.282, clip=0, train_wall=49, gb_free=73.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:07:38]    INFO >> epoch 001:   3200 / 5058 loss=0.282, ups=1.82, num_updates=3200, lr=9.6e-05, gnorm=2.208, clip=2, train_wall=52, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:08:38]    INFO >> epoch 001:   3300 / 5058 loss=0.086, ups=1.74, num_updates=3300, lr=9.5e-05, gnorm=1.673, clip=1, train_wall=54, gb_free=63.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:09:49]    INFO >> epoch 001:   3400 / 5058 loss=0.225, ups=1.47, num_updates=3400, lr=9.5e-05, gnorm=5.264, clip=3, train_wall=64, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:10:55]    INFO >> epoch 001:   3500 / 5058 loss=0.131, ups=1.59, num_updates=3500, lr=9.5e-05, gnorm=1.53, clip=0, train_wall=59, gb_free=63.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:12:00]    INFO >> epoch 001:   3600 / 5058 loss=0.095, ups=1.59, num_updates=3600, lr=9.5e-05, gnorm=2.575, clip=1, train_wall=59, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:13:01]    INFO >> epoch 001:   3700 / 5058 loss=0.137, ups=1.72, num_updates=3700, lr=9.4e-05, gnorm=1.854, clip=2, train_wall=55, gb_free=66.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:14:07]    INFO >> epoch 001:   3800 / 5058 loss=0.058, ups=1.57, num_updates=3800, lr=9.4e-05, gnorm=1.191, clip=1, train_wall=60, gb_free=68.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:14:57]    INFO >> epoch 001:   3900 / 5058 loss=0.127, ups=2.09, num_updates=3900, lr=9.4e-05, gnorm=1.978, clip=2, train_wall=45, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:15:58]    INFO >> epoch 001:   4000 / 5058 loss=0.131, ups=1.69, num_updates=4000, lr=9.4e-05, gnorm=1.731, clip=1, train_wall=56, gb_free=63 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:17:01]    INFO >> epoch 001:   4100 / 5058 loss=0.92, ups=1.66, num_updates=4100, lr=9.4e-05, gnorm=0.902, clip=1, train_wall=57, gb_free=63.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:18:08]    INFO >> epoch 001:   4200 / 5058 loss=0.149, ups=1.56, num_updates=4200, lr=9.3e-05, gnorm=1.845, clip=3, train_wall=60, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:19:06]    INFO >> epoch 001:   4300 / 5058 loss=0.117, ups=1.76, num_updates=4300, lr=9.3e-05, gnorm=1.721, clip=1, train_wall=54, gb_free=73.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:20:07]    INFO >> epoch 001:   4400 / 5058 loss=0.082, ups=1.72, num_updates=4400, lr=9.3e-05, gnorm=1.476, clip=0, train_wall=55, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:21:10]    INFO >> epoch 001:   4500 / 5058 loss=0.072, ups=1.66, num_updates=4500, lr=9.3e-05, gnorm=1.371, clip=0, train_wall=57, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:22:16]    INFO >> epoch 001:   4600 / 5058 loss=0.241, ups=1.58, num_updates=4600, lr=9.3e-05, gnorm=1.805, clip=0, train_wall=60, gb_free=62.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:23:26]    INFO >> epoch 001:   4700 / 5058 loss=0.215, ups=1.48, num_updates=4700, lr=9.2e-05, gnorm=3.803, clip=2, train_wall=63, gb_free=64.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:24:27]    INFO >> epoch 001:   4800 / 5058 loss=0.056, ups=1.71, num_updates=4800, lr=9.2e-05, gnorm=1.411, clip=0, train_wall=55, gb_free=67.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:25:08]    INFO >> epoch 001:   4900 / 5058 loss=0.142, ups=2.54, num_updates=4900, lr=9.2e-05, gnorm=0.824, clip=0, train_wall=37, gb_free=65.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:26:13]    INFO >> epoch 001:   5000 / 5058 loss=0.09, ups=1.61, num_updates=5000, lr=9.2e-05, gnorm=2.488, clip=0, train_wall=58, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:26:52]    INFO >> epoch 001 | loss 0.105 | ups 1.73 | num_updates 5058 | lr 9.2e-05 | gnorm 2.248 | clip 2.1 | train_wall 2751 | gb_free 73.9 (progress_bar.py:267, print())[0m
[33m[2025-11-20 11:26:52] WARNING >> 4469 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[2, 4, 10, 12, 13, 14, 16, 18, 19, 23] (data_utils.py:301, filter_by_size())[0m
[33m[2025-11-20 11:26:52] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
CodeEncoderLSTM: Creating BiLSTM with 6 layers, 512 hidden and input size
Traceback (most recent call last):
  File "run/type_prediction/transformer/train.py", line 339, in <module>
    cli_main()
  File "run/type_prediction/transformer/train.py", line 332, in cli_main
    single_main(args)
  File "run/type_prediction/transformer/train.py", line 254, in single_main
    valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)
  File "run/type_prediction/transformer/train.py", line 132, in validate
    trainer.valid_step(sample)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/trainers/ncc_trainers.py", line 565, in valid_step
    _loss, sample_size, logging_output = self.task.valid_step(
TypeError: cannot unpack non-iterable NoneType object

============================================================
Setting up experiment: exp_dropout_0.2
============================================================
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.2
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.2/checkpoints
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.2/logs
âœ“ Created directory: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.2/results
âœ“ Saved config to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.2/config.yml
âœ“ Also saved to: run/type_prediction/transformer/config/exp_dropout_0.2.yml
âœ“ Saved experiment info to: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.2/info.txt

============================================================
Starting training...
============================================================

Command: python run/type_prediction/transformer/train.py --language exp_dropout_0.2 2>&1 | tee /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.2/logs/train.log

Note: Config file will be loaded from run/type_prediction/transformer/config/exp_dropout_0.2.yml


âœ“ Training completed successfully!

============================================================
Evaluating model on test set...
============================================================

Warning: Best checkpoint not found, using last checkpoint
âœ— No checkpoint found for evaluation

============================================================
âœ“ Experiment completed: exp_dropout_0.2
âœ“ Results in: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.2
============================================================


âœ“ exp_dropout_0.2 completed in 0.85 hours

============================================================
Running: exp_encoder_transformer
GPU: 3
Time: 2025-11-20T11:26:55.809963
============================================================
[32m[2025-11-20 11:26:57]    INFO >> Load arguments in run/type_prediction/transformer/config/exp_encoder_transformer.yml (train.py:300, cli_main())[0m
[32m[2025-11-20 11:26:57]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_encoder_transformer/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'transformer', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_encoder_transformer/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_encoder_transformer/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:302, cli_main())[0m
[32m[2025-11-20 11:26:57]    INFO >> single GPU training... (train.py:331, cli_main())[0m
[32m[2025-11-20 11:26:57]    INFO >> {'criterion': 'type_predicition_cross_entropy', 'optimizer': 'fairseq_adam', 'lr_scheduler': 'polynomial_decay', 'tokenizer': None, 'bpe': None, 'common': {'no_progress_bar': 0, 'log_interval': 100, 'log_format': 'simple', 'tensorboard_logdir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_encoder_transformer/logs/tensorboard', 'seed': 1, 'cpu': 0, 'fp16': 0, 'bf16': 0, 'memory_efficient_fp16': 0, 'memory_efficient_bf16': 0, 'fp16_no_flatten_grads': 0, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'task': 'type_prediction'}, 'dataset': {'num_workers': 0, 'skip_invalid_size_inputs_valid_test': 1, 'max_tokens': None, 'max_sentences': 32, 'required_batch_size_multiple': 8, 'dataset_impl': 'raw', 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': 0, 'max_tokens_valid': None, 'max_sentences_valid': 32, 'curriculum': 100, 'test_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'joined_dictionary': 1, 'srcdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt', 'tgtdict': '/mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt'}, 'distributed_training': {'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': 0, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': None, 'find_unused_parameters': 0, 'fast_stat_sync': 0, 'broadcast_buffers': 0, 'global_sync_iter': 50, 'warmup_iterations': 500}, 'task': {'data': '/mnt/data1/zhaojunzhang/typilus-data/transformer', 'sample_break_mode': 'complete', 'tokens_per_sample': 1024, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': 0, 'mask_whole_words': 0, 'pooler_activation_fn': 'tanh', 'source_lang': 'code', 'target_lang': 'type', 'load_alignments': 0, 'left_pad_source': 1, 'left_pad_target': 0, 'max_source_positions': 2048, 'max_target_positions': 2048, 'upsample_primary': 1, 'truncate_source': 1, 'eval_accuracy': 1}, 'model': {'arch': 'typetransformer', 'pooler_dropout': 0.0, 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'encoder_type': 'transformer', 'encoder_positional_embeddings': 1, 'encoder_embed_path': 0, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_layers': 6, 'encoder_attention_heads': 8, 'encoder_normalize_before': 0, 'encoder_learned_pos': 0, 'decoder_embed_path': '', 'decoder_embed_dim': 0, 'decoder_ffn_embed_dim': 0, 'decoder_layers': 0, 'decoder_attention_heads': 0, 'decoder_learned_pos': 0, 'decoder_normalize_before': 0, 'share_decoder_input_output_embed': 0, 'share_all_embeddings': 0, 'no_token_positional_embeddings': 0, 'adaptive_softmax_cutoff': 0, 'adaptive_softmax_dropout': 0.0, 'no_cross_attention': 0, 'cross_self_attention': 0, 'layer_wise_attention': 0, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'decoder_layers_to_keep': None, 'layernorm_embedding': 0, 'no_scale_embedding': 0, 'encoder_max_relative_len': 0, 'max_source_positions': 2048, 'max_target_positions': 2048}, 'optimization': {'max_epoch': 50, 'max_update': 0, 'clip_norm': 25, 'sentence_avg': None, 'update_freq': [1], 'lr': [0.0001], 'lrs': [0.0001], 'min_lr': -1, 'use_bmuf': 0, 'force_anneal': None, 'warmup_updates': 1000, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 50000, 'adam': {'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': 1}, 'adagrad': {'weight_decay': 0.0}, 'binary_cross_entropy': {'infonce': 0, 'loss-weights': '', 'log-keys': ''}}, 'checkpoint': {'save_dir': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_encoder_transformer/checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': None, 'reset_lr_scheduler': None, 'reset_meters': None, 'reset_optimizer': None, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 0, 'keep_last_epochs': 5, 'keep_best_checkpoints': 3, 'no_save': 0, 'no_epoch_checkpoints': 0, 'no_last_checkpoints': 0, 'no_save_optimizer_state': None, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': 0, 'patience': 10}, 'eval': {'path': '/mnt/data1/zhaojunzhang/experiments/transformer_series/exp_encoder_transformer/checkpoints/checkpoint_best.pt', 'remove_bpe': None, 'quiet': 0, 'results_path': None, 'model_overrides': '{}'}} (train.py:208, single_main())[0m
/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
[32m[2025-11-20 11:27:00]    INFO >> TypeTransformer(
  (decoder): RobertaEncoder(
    (encoder): CodeEncoder(
      (embedding): Embedding(10006, 512)
      (pos_encoder): PositionalEncoding()
      (encoder): TransformerEncoder(
        (layers): ModuleList(
          (0-5): 6 x TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (linear1): Linear(in_features=512, out_features=2048, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=2048, out_features=512, bias=True)
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.0, inplace=False)
          )
        )
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output): Sequential(
      (0): Linear(in_features=512, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=10006, bias=True)
    )
  )
  (classification_heads): ModuleDict()
) (train.py:220, single_main())[0m
[32m[2025-11-20 11:27:00]    INFO >> model typetransformer, criterion TypePredictionCrossEntropyCriterion (train.py:221, single_main())[0m
[32m[2025-11-20 11:27:00]    INFO >> num. model params: 29434134 (num. trained: 29434134) (train.py:222, single_main())[0m
[32m[2025-11-20 11:27:00]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 11:27:00]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 81024 MB ; used memory = 896 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 11:27:00]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-20 11:27:00]    INFO >> training on 1 GPUs (train.py:229, single_main())[0m
[32m[2025-11-20 11:27:00]    INFO >> max tokens per GPU = None and max sentences per GPU = 32 (train.py:230, single_main())[0m
[32m[2025-11-20 11:27:00]    INFO >> no existing checkpoint found /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_encoder_transformer/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-20 11:27:00]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[33m[2025-11-20 11:27:10] WARNING >> 32222 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[0, 1, 2, 4, 6, 7, 8, 10, 14, 19] (data_utils.py:301, filter_by_size())[0m
[32m[2025-11-20 11:27:10]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-20 11:27:11] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:398: UserWarning: ntokens not found in Criterion logging outputs, cannot log wpb or wps
  warnings.warn(
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:407: UserWarning: nsentences not found in Criterion logging outputs, cannot log bsz
  warnings.warn(
[32m[2025-11-20 11:28:07]    INFO >> epoch 001:    100 / 5058 loss=1.891, ups=1.89, num_updates=100, lr=1e-05, gnorm=1.41, clip=0, train_wall=51, gb_free=65.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:29:06]    INFO >> epoch 001:    200 / 5058 loss=0.703, ups=1.71, num_updates=200, lr=2e-05, gnorm=1.933, clip=3, train_wall=55, gb_free=64.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:30:30]    INFO >> epoch 001:    300 / 5058 loss=0.524, ups=1.25, num_updates=300, lr=3e-05, gnorm=0.834, clip=1, train_wall=76, gb_free=64.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:31:55]    INFO >> epoch 001:    400 / 5058 loss=0.677, ups=1.22, num_updates=400, lr=4e-05, gnorm=3.253, clip=1, train_wall=78, gb_free=64.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:33:16]    INFO >> epoch 001:    500 / 5058 loss=0.267, ups=1.3, num_updates=500, lr=5e-05, gnorm=3.47, clip=2, train_wall=73, gb_free=64.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:34:34]    INFO >> epoch 001:    600 / 5058 loss=0.086, ups=1.34, num_updates=600, lr=6e-05, gnorm=3.56, clip=7, train_wall=71, gb_free=66.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:35:27]    INFO >> epoch 001:    700 / 5058 loss=0.184, ups=1.97, num_updates=700, lr=7e-05, gnorm=1.977, clip=3, train_wall=48, gb_free=69.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:36:18]    INFO >> epoch 001:    800 / 5058 loss=0.174, ups=2.02, num_updates=800, lr=8e-05, gnorm=2.539, clip=3, train_wall=47, gb_free=62.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:37:45]    INFO >> epoch 001:    900 / 5058 loss=0.216, ups=1.19, num_updates=900, lr=9e-05, gnorm=5.712, clip=6, train_wall=79, gb_free=64.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:39:08]    INFO >> epoch 001:   1000 / 5058 loss=0.14, ups=1.25, num_updates=1000, lr=0.0001, gnorm=1.203, clip=0, train_wall=76, gb_free=62.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:40:24]    INFO >> epoch 001:   1100 / 5058 loss=0.057, ups=1.37, num_updates=1100, lr=0.0001, gnorm=1.875, clip=2, train_wall=69, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:41:46]    INFO >> epoch 001:   1200 / 5058 loss=0.035, ups=1.26, num_updates=1200, lr=0.0001, gnorm=0.774, clip=0, train_wall=75, gb_free=64.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:43:10]    INFO >> epoch 001:   1300 / 5058 loss=0.086, ups=1.24, num_updates=1300, lr=9.9e-05, gnorm=1.662, clip=2, train_wall=76, gb_free=66.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:44:33]    INFO >> epoch 001:   1400 / 5058 loss=0.058, ups=1.27, num_updates=1400, lr=9.9e-05, gnorm=2.258, clip=4, train_wall=75, gb_free=62.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:45:57]    INFO >> epoch 001:   1500 / 5058 loss=0.285, ups=1.24, num_updates=1500, lr=9.9e-05, gnorm=1.602, clip=2, train_wall=76, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:47:15]    INFO >> epoch 001:   1600 / 5058 loss=0.11, ups=1.35, num_updates=1600, lr=9.9e-05, gnorm=2.998, clip=4, train_wall=70, gb_free=62.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:48:40]    INFO >> epoch 001:   1700 / 5058 loss=0.113, ups=1.22, num_updates=1700, lr=9.9e-05, gnorm=2.135, clip=2, train_wall=78, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:49:49]    INFO >> epoch 001:   1800 / 5058 loss=0.121, ups=1.5, num_updates=1800, lr=9.8e-05, gnorm=2.905, clip=5, train_wall=63, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:50:36]    INFO >> epoch 001:   1900 / 5058 loss=0.151, ups=2.23, num_updates=1900, lr=9.8e-05, gnorm=1.779, clip=2, train_wall=43, gb_free=67 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:51:55]    INFO >> epoch 001:   2000 / 5058 loss=0.088, ups=1.31, num_updates=2000, lr=9.8e-05, gnorm=1.352, clip=1, train_wall=72, gb_free=63.9 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:53:22]    INFO >> epoch 001:   2100 / 5058 loss=0.074, ups=1.2, num_updates=2100, lr=9.8e-05, gnorm=3.039, clip=1, train_wall=78, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:54:48]    INFO >> epoch 001:   2200 / 5058 loss=0.278, ups=1.21, num_updates=2200, lr=9.8e-05, gnorm=5.071, clip=6, train_wall=79, gb_free=62.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:56:16]    INFO >> epoch 001:   2300 / 5058 loss=0.068, ups=1.19, num_updates=2300, lr=9.7e-05, gnorm=2.436, clip=5, train_wall=79, gb_free=66.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:57:39]    INFO >> epoch 001:   2400 / 5058 loss=0.064, ups=1.25, num_updates=2400, lr=9.7e-05, gnorm=0.779, clip=1, train_wall=76, gb_free=67.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 11:59:11]    INFO >> epoch 001:   2500 / 5058 loss=0.149, ups=1.14, num_updates=2500, lr=9.7e-05, gnorm=3.688, clip=1, train_wall=83, gb_free=63.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 12:00:24]    INFO >> epoch 001:   2600 / 5058 loss=0.128, ups=1.43, num_updates=2600, lr=9.7e-05, gnorm=2.595, clip=1, train_wall=66, gb_free=62.5 (progress_bar.py:258, log())[0m
[32m[2025-11-20 12:01:38]    INFO >> epoch 001:   2700 / 5058 loss=0.076, ups=1.36, num_updates=2700, lr=9.7e-05, gnorm=1.387, clip=1, train_wall=69, gb_free=65.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 12:03:00]    INFO >> epoch 001:   2800 / 5058 loss=0.081, ups=1.31, num_updates=2800, lr=9.6e-05, gnorm=1.31, clip=3, train_wall=72, gb_free=67.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 12:03:48]    INFO >> epoch 001:   2900 / 5058 loss=0.276, ups=2.15, num_updates=2900, lr=9.6e-05, gnorm=1.993, clip=3, train_wall=44, gb_free=62.2 (progress_bar.py:258, log())[0m
[32m[2025-11-20 12:05:14]    INFO >> epoch 001:   3000 / 5058 loss=0.146, ups=1.22, num_updates=3000, lr=9.6e-05, gnorm=3.189, clip=5, train_wall=77, gb_free=66.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 12:06:20]    INFO >> epoch 001:   3100 / 5058 loss=0.057, ups=1.57, num_updates=3100, lr=9.6e-05, gnorm=0.282, clip=0, train_wall=61, gb_free=73.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 12:07:34]    INFO >> epoch 001:   3200 / 5058 loss=0.288, ups=1.41, num_updates=3200, lr=9.6e-05, gnorm=2.256, clip=1, train_wall=67, gb_free=67 (progress_bar.py:258, log())[0m
[32m[2025-11-20 12:08:51]    INFO >> epoch 001:   3300 / 5058 loss=0.086, ups=1.35, num_updates=3300, lr=9.5e-05, gnorm=1.74, clip=1, train_wall=70, gb_free=63.1 (progress_bar.py:258, log())[0m
[32m[2025-11-20 12:10:25]    INFO >> epoch 001:   3400 / 5058 loss=0.226, ups=1.1, num_updates=3400, lr=9.5e-05, gnorm=5.527, clip=3, train_wall=86, gb_free=62.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 12:11:51]    INFO >> epoch 001:   3500 / 5058 loss=0.132, ups=1.22, num_updates=3500, lr=9.5e-05, gnorm=1.586, clip=0, train_wall=78, gb_free=62.7 (progress_bar.py:258, log())[0m
[32m[2025-11-20 12:13:15]    INFO >> epoch 001:   3600 / 5058 loss=0.094, ups=1.23, num_updates=3600, lr=9.5e-05, gnorm=2.732, clip=2, train_wall=77, gb_free=62.8 (progress_bar.py:258, log())[0m
[32m[2025-11-20 12:14:31]    INFO >> epoch 001:   3700 / 5058 loss=0.142, ups=1.35, num_updates=3700, lr=9.4e-05, gnorm=2.216, clip=3, train_wall=71, gb_free=66.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 12:15:59]    INFO >> epoch 001:   3800 / 5058 loss=0.059, ups=1.21, num_updates=3800, lr=9.4e-05, gnorm=1.281, clip=1, train_wall=79, gb_free=68.3 (progress_bar.py:258, log())[0m
[32m[2025-11-20 12:17:02]    INFO >> epoch 001:   3900 / 5058 loss=0.129, ups=1.66, num_updates=3900, lr=9.4e-05, gnorm=2.12, clip=2, train_wall=57, gb_free=63.4 (progress_bar.py:258, log())[0m
[32m[2025-11-20 12:18:22]    INFO >> epoch 001:   4000 / 5058 loss=0.131, ups=1.3, num_updates=4000, lr=9.4e-05, gnorm=1.871, clip=3, train_wall=73, gb_free=62.6 (progress_bar.py:258, log())[0m
[32m[2025-11-20 12:19:43]    INFO >> epoch 001:   4100 / 5058 loss=0.938, ups=1.29, num_updates=4100, lr=9.4e-05, gnorm=0.983, clip=1, train_wall=74, gb_free=63.4 (progress_bar.py:258, log())[0m
^CTraceback (most recent call last):
  File "run_batch_experiments.py", line 330, in <module>
    main()
  File "run_batch_experiments.py", line 327, in main
    runner.run_batch(experiments, mode=args.mode, gpus=args.gpus, dry_run=args.dry_run)
  File "run_batch_experiments.py", line 233, in run_batch
    success = self.run_experiment(exp['name'], gpu_id, dry_run)
  File "run_batch_experiments.py", line 103, in run_experiment
    for line in process.stdout:
KeyboardInterrupt

(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ ^C
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ ^C
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ ^C
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ ^C
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ ^C
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ ^C
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ 