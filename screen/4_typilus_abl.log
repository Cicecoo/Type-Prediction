(base) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ conda activate naturalcc
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ cd /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python run_experiments.py 
============================================================
Typilus å‚æ•°è°ƒä¼˜å®žéªŒç³»ç»Ÿ
============================================================
é…ç½®æ–‡ä»¶: experiments_lr.yml

å…± 16 ä¸ªå®žéªŒ:

1. embed_32: åµŒå…¥ç»´åº¦32 (å‡åŠï¼Œæµ‹è¯•æ˜¯å¦è¿‡æ‹Ÿåˆ)
2. embed_96: åµŒå…¥ç»´åº¦96 (å¢žåŠ 50%ï¼Œæå‡å®¹é‡)
3. embed_128: åµŒå…¥ç»´åº¦128 (ç¿»å€ï¼Œæµ‹è¯•å®¹é‡ä¸Šé™)
4. layers_1: å•å±‚GGNN (å‡å°‘å±‚æ•°ï¼Œæµ‹è¯•æ˜¯å¦è¿‡æ‹Ÿåˆ)
5. layers_3: ä¸‰å±‚GGNN (å¢žåŠ æ·±åº¦)
6. layers_4: å››å±‚GGNN (æ›´æ·±ç½‘ç»œ)
7. dropout_0: æ— Dropout (æµ‹è¯•æ˜¯å¦éœ€è¦æ­£åˆ™åŒ–)
8. dropout_015: Dropout=0.15 (é€‚åº¦å¢žå¼º)
9. dropout_02: Dropout=0.2 (å¼ºæ­£åˆ™åŒ–)
10. dropout_03: Dropout=0.3 (æžå¼ºæ­£åˆ™åŒ–)
11. batch_16: batch=16 (å‡åŠï¼Œæ›´å¤šæ›´æ–°)
12. batch_48: batch=48 (å¢žåŠ 50%)
13. batch_64: batch=64 (ç¿»å€ï¼Œæ›´ç¨³å®šæ¢¯åº¦)
14. combo_large: å¤§æ¨¡åž‹é…ç½® (embed=96, layers=3, dropout=0.15)
15. combo_deep: æ·±å±‚ç½‘ç»œ (layers=4, dropout=0.2, batch=48)
16. combo_wide: å®½ç½‘ç»œ (embed=128, layers=2, dropout=0.15)

============================================================

æŒ‰ Enter å¼€å§‹å®žéªŒ...

è¿›åº¦: 1/16

============================================================
å®žéªŒ: embed_32 - åµŒå…¥ç»´åº¦32 (å‡åŠï¼Œæµ‹è¯•æ˜¯å¦è¿‡æ‹Ÿåˆ)
æ—¶é—´: 2025-11-21 00:07:59
============================================================

[32m[2025-11-21 00:08:01]    INFO >> åŠ è½½é…ç½®: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/config.yml (train_enhanced.py:666, cli_main())[0m
[32m[2025-11-21 00:08:01]    INFO >> å•GPUè®­ç»ƒ... (train_enhanced.py:694, cli_main())[0m
[32m[2025-11-21 00:08:02]    INFO >> è®­ç»ƒæ—¥å¿—å°†ä¿å­˜åˆ°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs (train_enhanced.py:561, single_main())[0m
[32m[2025-11-21 00:08:02]    INFO >> [nodes] dictionary: 9999 types (typilus.py:102, setup_task())[0m
[32m[2025-11-21 00:08:02]    INFO >> [edges] dictionary: 0 types (typilus.py:102, setup_task())[0m
[32m[2025-11-21 00:08:02]    INFO >> [supernodes.annotation] dictionary: 99 types (typilus.py:106, setup_task())[0m
[32m[2025-11-21 00:08:13]    INFO >> Typilus(
  (encoder): GGNNEncoder(
    (node_embedding): Embedding(9999, 32, padding_idx=0)
    (node_layer): Sequential(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=32, out_features=32, bias=False)
      (2): Dropout(p=0.1, inplace=False)
    )
    (ggnns): ModuleList(
      (0): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=32, out_features=32, bias=False)
          (OCCURRENCE_OF): Linear(in_features=32, out_features=32, bias=False)
          (NEXT): Linear(in_features=32, out_features=32, bias=False)
          (SUBTOKEN_OF): Linear(in_features=32, out_features=32, bias=False)
          (COMPUTED_FROM): Linear(in_features=32, out_features=32, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=32, out_features=32, bias=False)
          (NEXT_USE): Linear(in_features=32, out_features=32, bias=False)
          (RETURNS_TO): Linear(in_features=32, out_features=32, bias=False)
          (_CHILD): Linear(in_features=32, out_features=32, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=32, out_features=32, bias=False)
          (_NEXT): Linear(in_features=32, out_features=32, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=32, out_features=32, bias=False)
          (_COMPUTED_FROM): Linear(in_features=32, out_features=32, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=32, out_features=32, bias=False)
          (_NEXT_USE): Linear(in_features=32, out_features=32, bias=False)
          (_RETURNS_TO): Linear(in_features=32, out_features=32, bias=False)
        )
        (rnn_cell): GRUCell(32, 32)
      )
      (1): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=32, out_features=32, bias=False)
          (OCCURRENCE_OF): Linear(in_features=32, out_features=32, bias=False)
          (NEXT): Linear(in_features=32, out_features=32, bias=False)
          (SUBTOKEN_OF): Linear(in_features=32, out_features=32, bias=False)
          (COMPUTED_FROM): Linear(in_features=32, out_features=32, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=32, out_features=32, bias=False)
          (NEXT_USE): Linear(in_features=32, out_features=32, bias=False)
          (RETURNS_TO): Linear(in_features=32, out_features=32, bias=False)
          (_CHILD): Linear(in_features=32, out_features=32, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=32, out_features=32, bias=False)
          (_NEXT): Linear(in_features=32, out_features=32, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=32, out_features=32, bias=False)
          (_COMPUTED_FROM): Linear(in_features=32, out_features=32, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=32, out_features=32, bias=False)
          (_NEXT_USE): Linear(in_features=32, out_features=32, bias=False)
          (_RETURNS_TO): Linear(in_features=32, out_features=32, bias=False)
        )
        (rnn_cell): GRUCell(64, 32)
      )
    )
  )
  (decoder): DenseDecoder(
    (cls_layers): Sequential(
      (0): Linear(in_features=32, out_features=32, bias=False)
      (1): Dropout(p=0.1, inplace=False)
      (2): Linear(in_features=32, out_features=99, bias=True)
    )
  )
) (train_enhanced.py:568, single_main())[0m
[32m[2025-11-21 00:08:13]    INFO >> æ¨¡åž‹: typilus, æŸå¤±å‡½æ•°: TypilusCriterion (train_enhanced.py:569, single_main())[0m
[32m[2025-11-21 00:08:13]    INFO >> æ¨¡åž‹å‚æ•°: 373795 (å¯è®­ç»ƒ: 373795) (train_enhanced.py:570, single_main())[0m
[32m[2025-11-21 00:08:13]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 00:08:13]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 7449 MB ; used memory = 74470 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 00:08:13]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 00:08:13]    INFO >> ä½¿ç”¨ 1 ä¸ªGPUè®­ç»ƒ (train_enhanced.py:576, single_main())[0m
[32m[2025-11-21 00:08:13]    INFO >> no existing checkpoint found /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-21 00:08:13]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
^[[B^[[B^[[B^[[B^[[B^[[B^[[A^[[A^[[A^[[A^[[A^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B[32m[2025-11-21 00:09:28]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-21 00:09:28] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
^[[B/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[33m[2025-11-21 00:09:35] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 3 has a total capacity of 79.14 GiB of which 45.69 MiB is free. Process 2803952 has 71.41 GiB memory in use. Including non-PyTorch memory, this process has 7.66 GiB memory in use. Of the allocated memory 6.56 GiB is allocated by PyTorch, and 614.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:09:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 6         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   6300 MiB |   6768 MiB | 364600 MiB | 358299 MiB |
|       from large pool |   6274 MiB |   6742 MiB | 360725 MiB | 354450 MiB |
|       from small pool |     25 MiB |     27 MiB |   3875 MiB |   3849 MiB |
|---------------------------------------------------------------------------|
| Active memory         |   6300 MiB |   6768 MiB | 364600 MiB | 358299 MiB |
|       from large pool |   6274 MiB |   6742 MiB | 360725 MiB | 354450 MiB |
|       from small pool |     25 MiB |     27 MiB |   3875 MiB |   3849 MiB |
|---------------------------------------------------------------------------|
| Requested memory      |   6278 MiB |   6745 MiB | 361619 MiB | 355340 MiB |
|       from large pool |   6252 MiB |   6719 MiB | 357749 MiB | 351496 MiB |
|       from small pool |     25 MiB |     27 MiB |   3869 MiB |   3843 MiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   7332 MiB |   7332 MiB |  15868 MiB |   8536 MiB |
|       from large pool |   7302 MiB |   7302 MiB |  15762 MiB |   8460 MiB |
|       from small pool |     30 MiB |     46 MiB |    106 MiB |     76 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory | 707963 KiB |    781 MiB | 259460 MiB | 258769 MiB |
|       from large pool | 705799 KiB |    778 MiB | 254897 MiB | 254208 MiB |
|       from small pool |   2164 KiB |     18 MiB |   4562 MiB |   4560 MiB |
|---------------------------------------------------------------------------|
| Allocations           |     603    |     621    |   50385    |   49782    |
|       from large pool |     261    |     277    |   26244    |   25983    |
|       from small pool |     342    |     354    |   24141    |   23799    |
|---------------------------------------------------------------------------|
| Active allocs         |     603    |     621    |   50385    |   49782    |
|       from large pool |     261    |     277    |   26244    |   25983    |
|       from small pool |     342    |     354    |   24141    |   23799    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     117    |     127    |     451    |     334    |
|       from large pool |     102    |     104    |     398    |     296    |
|       from small pool |      15    |      23    |      53    |      38    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      88    |      89    |   27429    |   27341    |
|       from large pool |      59    |      59    |   18379    |   18320    |
|       from small pool |      29    |      50    |    9050    |    9021    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:35] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 00:09:36] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 3 has a total capacity of 79.14 GiB of which 19.69 MiB is free. Process 2803952 has 71.41 GiB memory in use. Including non-PyTorch memory, this process has 7.68 GiB memory in use. Of the allocated memory 6.58 GiB is allocated by PyTorch, and 616.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:09:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 8         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   6681 MiB |   6741 MiB | 412541 MiB | 405860 MiB |
|       from large pool |   6641 MiB |   6702 MiB | 408195 MiB | 401554 MiB |
|       from small pool |     39 MiB |     40 MiB |   4345 MiB |   4306 MiB |
|---------------------------------------------------------------------------|
| Active memory         |   6681 MiB |   6741 MiB | 412541 MiB | 405860 MiB |
|       from large pool |   6641 MiB |   6702 MiB | 408195 MiB | 401554 MiB |
|       from small pool |     39 MiB |     40 MiB |   4345 MiB |   4306 MiB |
|---------------------------------------------------------------------------|
| Requested memory      |   6656 MiB |   6717 MiB | 409398 MiB | 402741 MiB |
|       from large pool |   6617 MiB |   6678 MiB | 405058 MiB | 398441 MiB |
|       from small pool |     39 MiB |     40 MiB |   4339 MiB |   4300 MiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   7358 MiB |   7360 MiB |  16514 MiB |   9156 MiB |
|       from large pool |   7316 MiB |   7316 MiB |  16382 MiB |   9066 MiB |
|       from small pool |     42 MiB |     48 MiB |    132 MiB |     90 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory | 629588 KiB | 715231 KiB | 301223 MiB | 300609 MiB |
|       from large pool | 626897 KiB | 711884 KiB | 296098 MiB | 295485 MiB |
|       from small pool |   2690 KiB |  25365 KiB |   5125 MiB |   5123 MiB |
|---------------------------------------------------------------------------|
| Allocations           |     771    |     774    |   55700    |   54929    |
|       from large pool |     300    |     304    |   28868    |   28568    |
|       from small pool |     471    |     474    |   26832    |   26361    |
|---------------------------------------------------------------------------|
| Active allocs         |     771    |     774    |   55700    |   54929    |
|       from large pool |     300    |     304    |   28868    |   28568    |
|       from small pool |     471    |     474    |   26832    |   26361    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     122    |     127    |     474    |     352    |
|       from large pool |     101    |     103    |     408    |     307    |
|       from small pool |      21    |      24    |      66    |      45    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      97    |     101    |   30503    |   30406    |
|       from large pool |      67    |      68    |   20188    |   20121    |
|       from small pool |      30    |      42    |   10315    |   10285    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:36] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 00:09:36] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 3 has a total capacity of 79.14 GiB of which 35.69 MiB is free. Process 2803952 has 71.41 GiB memory in use. Including non-PyTorch memory, this process has 7.67 GiB memory in use. Of the allocated memory 7.04 GiB is allocated by PyTorch, and 129.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:09:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 3            |        cudaMalloc retries: 10        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   7170 MiB |   7240 MiB | 426715 MiB | 419544 MiB |
|       from large pool |   7149 MiB |   7218 MiB | 422312 MiB | 415163 MiB |
|       from small pool |     21 MiB |     40 MiB |   4402 MiB |   4380 MiB |
|---------------------------------------------------------------------------|
| Active memory         |   7170 MiB |   7240 MiB | 426715 MiB | 419544 MiB |
|       from large pool |   7149 MiB |   7218 MiB | 422312 MiB | 415163 MiB |
|       from small pool |     21 MiB |     40 MiB |   4402 MiB |   4380 MiB |
|---------------------------------------------------------------------------|
| Requested memory      |   7100 MiB |   7168 MiB | 423485 MiB | 416384 MiB |
|       from large pool |   7078 MiB |   7146 MiB | 419088 MiB | 412010 MiB |
|       from small pool |     21 MiB |     40 MiB |   4396 MiB |   4374 MiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   7342 MiB |   7360 MiB |  22698 MiB |  15356 MiB |
|       from large pool |   7316 MiB |   7316 MiB |  22556 MiB |  15240 MiB |
|       from small pool |     26 MiB |     48 MiB |    142 MiB |    116 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory | 175489 KiB |   2014 MiB | 308714 MiB | 308542 MiB |
|       from large pool | 170628 KiB |   2002 MiB | 303508 MiB | 303342 MiB |
|       from small pool |   4861 KiB |     24 MiB |   5205 MiB |   5200 MiB |
|---------------------------------------------------------------------------|
| Allocations           |     581    |     774    |   56426    |   55845    |
|       from large pool |     250    |     304    |   29381    |   29131    |
|       from small pool |     331    |     474    |   27045    |   26714    |
|---------------------------------------------------------------------------|
| Active allocs         |     581    |     774    |   56426    |   55845    |
|       from large pool |     250    |     304    |   29381    |   29131    |
|       from small pool |     331    |     474    |   27045    |   26714    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     130    |     130    |     569    |     439    |
|       from large pool |     117    |     117    |     498    |     381    |
|       from small pool |      13    |      24    |      71    |      58    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      73    |     127    |   30971    |   30898    |
|       from large pool |      45    |      83    |   20564    |   20519    |
|       from small pool |      28    |      51    |   10407    |   10379    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:36] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 00:09:38] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 228.00 MiB. GPU 3 has a total capacity of 79.14 GiB of which 13.69 MiB is free. Process 2803952 has 71.41 GiB memory in use. Including non-PyTorch memory, this process has 7.69 GiB memory in use. Of the allocated memory 6.95 GiB is allocated by PyTorch, and 243.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:09:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 4            |        cudaMalloc retries: 13        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   6352 MiB |   7211 MiB | 571069 MiB | 564716 MiB |
|       from large pool |   6336 MiB |   7195 MiB | 565360 MiB | 559023 MiB |
|       from small pool |     15 MiB |     19 MiB |   5709 MiB |   5693 MiB |
|---------------------------------------------------------------------------|
| Active memory         |   6352 MiB |   7211 MiB | 571069 MiB | 564716 MiB |
|       from large pool |   6336 MiB |   7195 MiB | 565360 MiB | 559023 MiB |
|       from small pool |     15 MiB |     19 MiB |   5709 MiB |   5693 MiB |
|---------------------------------------------------------------------------|
| Requested memory      |   6336 MiB |   7189 MiB | 566817 MiB | 560481 MiB |
|       from large pool |   6320 MiB |   7173 MiB | 561115 MiB | 554795 MiB |
|       from small pool |     15 MiB |     19 MiB |   5701 MiB |   5685 MiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   7364 MiB |   7366 MiB |  27894 MiB |  20530 MiB |
|       from large pool |   7346 MiB |   7346 MiB |  27724 MiB |  20378 MiB |
|       from small pool |     18 MiB |     50 MiB |    170 MiB |    152 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory | 269690 KiB | 738269 KiB | 428256 MiB | 427992 MiB |
|       from large pool | 267381 KiB | 735674 KiB | 421468 MiB | 421207 MiB |
|       from small pool |   2309 KiB |  23362 KiB |   6787 MiB |   6785 MiB |
|---------------------------------------------------------------------------|
| Allocations           |     486    |     504    |   74429    |   73943    |
|       from large pool |     175    |     192    |   39075    |   38900    |
|       from small pool |     311    |     354    |   35354    |   35043    |
|---------------------------------------------------------------------------|
| Active allocs         |     486    |     504    |   74429    |   73943    |
|       from large pool |     175    |     192    |   39075    |   38900    |
|       from small pool |     311    |     354    |   35354    |   35043    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      88    |     142    |     632    |     544    |
|       from large pool |      79    |     117    |     547    |     468    |
|       from small pool |       9    |      25    |      85    |      76    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      73    |      74    |   41159    |   41086    |
|       from large pool |      53    |      53    |   27547    |   27494    |
|       from small pool |      20    |      42    |   13612    |   13592    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:38] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:09:40]    INFO >> epoch 001:     54 / 1539 loss=5.634, wps=2526, ups=4.17, wpb=604.6, bsz=604.6, num_updates=50, lr=0.0004, gnorm=6.289, clip=0, train_wall=11, gb_free=74.1, wall=86 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:09:41] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 3 has a total capacity of 79.14 GiB of which 23.69 MiB is free. Process 2803952 has 71.41 GiB memory in use. Including non-PyTorch memory, this process has 7.68 GiB memory in use. Of the allocated memory 6.97 GiB is allocated by PyTorch, and 215.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:09:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 5            |        cudaMalloc retries: 16        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   6994 MiB |   7187 MiB | 750409 MiB | 743415 MiB |
|       from large pool |   6979 MiB |   7172 MiB | 743103 MiB | 736123 MiB |
|       from small pool |     14 MiB |     18 MiB |   7305 MiB |   7291 MiB |
|---------------------------------------------------------------------------|
| Active memory         |   6994 MiB |   7187 MiB | 750409 MiB | 743415 MiB |
|       from large pool |   6979 MiB |   7172 MiB | 743103 MiB | 736123 MiB |
|       from small pool |     14 MiB |     18 MiB |   7305 MiB |   7291 MiB |
|---------------------------------------------------------------------------|
| Requested memory      |   6976 MiB |   7168 MiB | 745253 MiB | 738276 MiB |
|       from large pool |   6962 MiB |   7154 MiB | 737957 MiB | 730995 MiB |
|       from small pool |     14 MiB |     18 MiB |   7295 MiB |   7281 MiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   7354 MiB |   7354 MiB |  35638 MiB |  28284 MiB |
|       from large pool |   7336 MiB |   7336 MiB |  35434 MiB |  28098 MiB |
|       from small pool |     18 MiB |     52 MiB |    204 MiB |    186 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory | 296956 KiB | 479899 KiB | 596327 MiB | 596038 MiB |
|       from large pool | 292930 KiB | 476206 KiB | 587662 MiB | 587376 MiB |
|       from small pool |   4026 KiB |  23725 KiB |   8665 MiB |   8661 MiB |
|---------------------------------------------------------------------------|
| Allocations           |     437    |     446    |   94860    |   94423    |
|       from large pool |     136    |     144    |   49692    |   49556    |
|       from small pool |     301    |     353    |   45168    |   44867    |
|---------------------------------------------------------------------------|
| Active allocs         |     437    |     446    |   94860    |   94423    |
|       from large pool |     136    |     144    |   49692    |   49556    |
|       from small pool |     301    |     353    |   45168    |   44867    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      69    |     103    |     711    |     642    |
|       from large pool |      60    |      77    |     609    |     549    |
|       from small pool |       9    |      26    |     102    |      93    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      65    |      66    |   52256    |   52191    |
|       from large pool |      45    |      46    |   34723    |   34678    |
|       from small pool |      20    |      49    |   17533    |   17513    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:41] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:09:53]    INFO >> epoch 001:    105 / 1539 loss=5.669, wps=3143.9, ups=5.03, wpb=625, bsz=625, num_updates=100, lr=0.0004, gnorm=6.534, clip=0, train_wall=9, gb_free=75.5, wall=96 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:09:55] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 302.00 MiB. GPU 3 has a total capacity of 79.14 GiB of which 169.69 MiB is free. Process 2803952 has 71.41 GiB memory in use. Including non-PyTorch memory, this process has 7.54 GiB memory in use. Of the allocated memory 6.61 GiB is allocated by PyTorch, and 439.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:09:55] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:55] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:55] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:55] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 6            |        cudaMalloc retries: 18        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   5600 MiB |   6844 MiB |   1507 GiB |   1501 GiB |
|       from large pool |   5582 MiB |   6822 MiB |   1491 GiB |   1486 GiB |
|       from small pool |     18 MiB |     22 MiB |     15 GiB |     15 GiB |
|---------------------------------------------------------------------------|
| Active memory         |   5600 MiB |   6844 MiB |   1507 GiB |   1501 GiB |
|       from large pool |   5582 MiB |   6822 MiB |   1491 GiB |   1486 GiB |
|       from small pool |     18 MiB |     22 MiB |     15 GiB |     15 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |   5586 MiB |   6821 MiB |   1498 GiB |   1492 GiB |
|       from large pool |   5567 MiB |   6799 MiB |   1482 GiB |   1477 GiB |
|       from small pool |     18 MiB |     22 MiB |     15 GiB |     15 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   7208 MiB |   7326 MiB |  36932 MiB |  29724 MiB |
|       from large pool |   7184 MiB |   7266 MiB |  36684 MiB |  29500 MiB |
|       from small pool |     24 MiB |     60 MiB |    248 MiB |    224 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1073 MiB |   1394 MiB |   1424 GiB |   1422 GiB |
|       from large pool |   1069 MiB |   1392 MiB |   1405 GiB |   1404 GiB |
|       from small pool |      3 MiB |     26 MiB |     18 GiB |     18 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     491    |     541    |  198859    |  198368    |
|       from large pool |     170    |     213    |  103089    |  102919    |
|       from small pool |     321    |     354    |   95770    |   95449    |
|---------------------------------------------------------------------------|
| Active allocs         |     491    |     541    |  198859    |  198368    |
|       from large pool |     170    |     213    |  103089    |  102919    |
|       from small pool |     321    |     354    |   95770    |   95449    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      65    |      89    |     745    |     680    |
|       from large pool |      53    |      59    |     621    |     568    |
|       from small pool |      12    |      30    |     124    |     112    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      59    |      63    |  108652    |  108593    |
|       from large pool |      37    |      39    |   70589    |   70552    |
|       from small pool |      22    |      46    |   38063    |   38041    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:55] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 00:09:57] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 194.00 MiB. GPU 3 has a total capacity of 79.14 GiB of which 135.69 MiB is free. Process 2803952 has 71.41 GiB memory in use. Including non-PyTorch memory, this process has 7.57 GiB memory in use. Of the allocated memory 6.90 GiB is allocated by PyTorch, and 172.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:09:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 7            |        cudaMalloc retries: 19        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   6300 MiB |   7093 MiB |   1622 GiB |   1615 GiB |
|       from large pool |   6266 MiB |   7056 MiB |   1605 GiB |   1599 GiB |
|       from small pool |     33 MiB |     37 MiB |     16 GiB |     16 GiB |
|---------------------------------------------------------------------------|
| Active memory         |   6300 MiB |   7093 MiB |   1622 GiB |   1615 GiB |
|       from large pool |   6266 MiB |   7056 MiB |   1605 GiB |   1599 GiB |
|       from small pool |     33 MiB |     37 MiB |     16 GiB |     16 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |   6273 MiB |   7062 MiB |   1612 GiB |   1606 GiB |
|       from large pool |   6240 MiB |   7026 MiB |   1596 GiB |   1590 GiB |
|       from small pool |     33 MiB |     37 MiB |     16 GiB |     16 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   7242 MiB |   7272 MiB |  37530 MiB |  30288 MiB |
|       from large pool |   7200 MiB |   7226 MiB |  37258 MiB |  30058 MiB |
|       from small pool |     42 MiB |     46 MiB |    272 MiB |    230 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory | 523961 KiB |    792 MiB |   1555 GiB |   1554 GiB |
|       from large pool | 519313 KiB |    787 MiB |   1536 GiB |   1535 GiB |
|       from small pool |   4648 KiB |     26 MiB |     19 GiB |     19 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     607    |     656    |  212344    |  211737    |
|       from large pool |     258    |     299    |  110215    |  109957    |
|       from small pool |     349    |     359    |  102129    |  101780    |
|---------------------------------------------------------------------------|
| Active allocs         |     607    |     656    |  212344    |  211737    |
|       from large pool |     258    |     299    |  110215    |  109957    |
|       from small pool |     349    |     359    |  102129    |  101780    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      80    |      83    |     771    |     691    |
|       from large pool |      59    |      60    |     635    |     576    |
|       from small pool |      21    |      23    |     136    |     115    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      83    |      99    |  115624    |  115541    |
|       from large pool |      51    |      63    |   75180    |   75129    |
|       from small pool |      32    |      42    |   40444    |   40412    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:57] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 00:09:58] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 78.00 MiB. GPU 3 has a total capacity of 79.14 GiB of which 73.69 MiB is free. Process 2803952 has 71.41 GiB memory in use. Including non-PyTorch memory, this process has 7.63 GiB memory in use. Of the allocated memory 6.91 GiB is allocated by PyTorch, and 225.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:09:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 8            |        cudaMalloc retries: 21        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   6996 MiB |   7113 MiB |   1700 GiB |   1693 GiB |
|       from large pool |   6980 MiB |   7095 MiB |   1683 GiB |   1676 GiB |
|       from small pool |     16 MiB |     19 MiB |     17 GiB |     17 GiB |
|---------------------------------------------------------------------------|
| Active memory         |   6996 MiB |   7113 MiB |   1700 GiB |   1693 GiB |
|       from large pool |   6980 MiB |   7095 MiB |   1683 GiB |   1676 GiB |
|       from small pool |     16 MiB |     19 MiB |     17 GiB |     17 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |   6974 MiB |   7090 MiB |   1690 GiB |   1683 GiB |
|       from large pool |   6958 MiB |   7073 MiB |   1673 GiB |   1666 GiB |
|       from small pool |     16 MiB |     19 MiB |     17 GiB |     17 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   7304 MiB |   7334 MiB |  38208 MiB |  30904 MiB |
|       from large pool |   7284 MiB |   7284 MiB |  37920 MiB |  30636 MiB |
|       from small pool |     20 MiB |     54 MiB |    288 MiB |    268 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory | 312749 KiB |   1477 MiB |   1643 GiB |   1643 GiB |
|       from large pool | 310872 KiB |   1474 MiB |   1623 GiB |   1622 GiB |
|       from small pool |   1877 KiB |     22 MiB |     20 GiB |     20 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     527    |     536    |  222187    |  221660    |
|       from large pool |     214    |     221    |  115292    |  115078    |
|       from small pool |     313    |     354    |  106895    |  106582    |
|---------------------------------------------------------------------------|
| Active allocs         |     527    |     536    |  222187    |  221660    |
|       from large pool |     214    |     221    |  115292    |  115078    |
|       from small pool |     313    |     354    |  106895    |  106582    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      62    |      79    |     786    |     724    |
|       from large pool |      52    |      52    |     642    |     590    |
|       from small pool |      10    |      27    |     144    |     134    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      48    |      56    |  120893    |  120845    |
|       from large pool |      26    |      34    |   78520    |   78494    |
|       from small pool |      22    |      45    |   42373    |   42351    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:58] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 00:09:58] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 298.00 MiB. GPU 3 has a total capacity of 79.14 GiB of which 73.69 MiB is free. Process 2803952 has 71.41 GiB memory in use. Including non-PyTorch memory, this process has 7.63 GiB memory in use. Of the allocated memory 6.57 GiB is allocated by PyTorch, and 573.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:09:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 9            |        cudaMalloc retries: 22        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   5564 MiB |   6805 MiB |   1725 GiB |   1720 GiB |
|       from large pool |   5551 MiB |   6788 MiB |   1708 GiB |   1702 GiB |
|       from small pool |     13 MiB |     17 MiB |     17 GiB |     17 GiB |
|---------------------------------------------------------------------------|
| Active memory         |   5564 MiB |   6805 MiB |   1725 GiB |   1720 GiB |
|       from large pool |   5551 MiB |   6788 MiB |   1708 GiB |   1702 GiB |
|       from small pool |     13 MiB |     17 MiB |     17 GiB |     17 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |   5555 MiB |   6792 MiB |   1715 GiB |   1710 GiB |
|       from large pool |   5542 MiB |   6776 MiB |   1698 GiB |   1692 GiB |
|       from small pool |     13 MiB |     17 MiB |     17 GiB |     17 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   7304 MiB |   7326 MiB |  38232 MiB |  30928 MiB |
|       from large pool |   7284 MiB |   7284 MiB |  37920 MiB |  30636 MiB |
|       from small pool |     20 MiB |     42 MiB |    312 MiB |    292 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1737 MiB |   1737 MiB |   1672 GiB |   1670 GiB |
|       from large pool |   1732 MiB |   1732 MiB |   1651 GiB |   1650 GiB |
|       from small pool |      4 MiB |     28 MiB |     20 GiB |     20 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     491    |     541    |  224505    |  224014    |
|       from large pool |     176    |     219    |  116666    |  116490    |
|       from small pool |     315    |     354    |  107839    |  107524    |
|---------------------------------------------------------------------------|
| Active allocs         |     491    |     541    |  224505    |  224014    |
|       from large pool |     176    |     219    |  116666    |  116490    |
|       from small pool |     315    |     354    |  107839    |  107524    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      62    |      73    |     798    |     736    |
|       from large pool |      52    |      52    |     642    |     590    |
|       from small pool |      10    |      21    |     156    |     146    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      72    |      77    |  122112    |  122040    |
|       from large pool |      51    |      55    |   79429    |   79378    |
|       from small pool |      21    |      51    |   42683    |   42662    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:58] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 00:09:58] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 202.00 MiB. GPU 3 has a total capacity of 79.14 GiB of which 61.69 MiB is free. Process 2803952 has 71.41 GiB memory in use. Including non-PyTorch memory, this process has 7.64 GiB memory in use. Of the allocated memory 7.04 GiB is allocated by PyTorch, and 107.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:09:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 10           |        cudaMalloc retries: 23        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   6434 MiB |   7234 MiB |   1739 GiB |   1733 GiB |
|       from large pool |   6405 MiB |   7202 MiB |   1722 GiB |   1716 GiB |
|       from small pool |     28 MiB |     31 MiB |     17 GiB |     17 GiB |
|---------------------------------------------------------------------------|
| Active memory         |   6434 MiB |   7234 MiB |   1739 GiB |   1733 GiB |
|       from large pool |   6405 MiB |   7202 MiB |   1722 GiB |   1716 GiB |
|       from small pool |     28 MiB |     31 MiB |     17 GiB |     17 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |   6418 MiB |   7214 MiB |   1729 GiB |   1723 GiB |
|       from large pool |   6389 MiB |   7182 MiB |   1712 GiB |   1706 GiB |
|       from small pool |     28 MiB |     31 MiB |     17 GiB |     17 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   7316 MiB |   7326 MiB |  38246 MiB |  30930 MiB |
|       from large pool |   7284 MiB |   7284 MiB |  37920 MiB |  30636 MiB |
|       from small pool |     32 MiB |     42 MiB |    326 MiB |    294 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    879 MiB |   4343 MiB |   1688 GiB |   1687 GiB |
|       from large pool |    878 MiB |   4335 MiB |   1668 GiB |   1667 GiB |
|       from small pool |      1 MiB |     28 MiB |     20 GiB |     20 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     607    |     656    |  225407    |  224800    |
|       from large pool |     251    |     291    |  117270    |  117019    |
|       from small pool |     356    |     367    |  108137    |  107781    |
|---------------------------------------------------------------------------|
| Active allocs         |     607    |     656    |  225407    |  224800    |
|       from large pool |     251    |     291    |  117270    |  117019    |
|       from small pool |     356    |     367    |  108137    |  107781    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      68    |      73    |     805    |     737    |
|       from large pool |      52    |      52    |     642    |     590    |
|       from small pool |      16    |      21    |     163    |     147    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      61    |      98    |  122527    |  122466    |
|       from large pool |      37    |      74    |   79779    |   79742    |
|       from small pool |      24    |      51    |   42748    |   42724    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:09:58] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
^CTraceback (most recent call last):
  File "run_experiments.py", line 390, in <module>
    main()
  File "run_experiments.py", line 363, in main
    success = run_single_experiment(exp, str(train_script))
  File "run_experiments.py", line 115, in run_single_experiment
    for line in process.stdout:
KeyboardInterrupt

(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python run_experiments.py [A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ cd /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [Conda activate naturalcc[K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ sconda[1@ [C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ conda [1P[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [Cd /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python run_experiments.py [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cpython run_experiments.py --cong[Kfig experiments_model.yml
============================================================
Typilus å‚æ•°è°ƒä¼˜å®žéªŒç³»ç»Ÿ
============================================================
é…ç½®æ–‡ä»¶: experiments_model.yml

å…± 16 ä¸ªå®žéªŒ:

1. embed_32: åµŒå…¥ç»´åº¦32 (å‡åŠï¼Œæµ‹è¯•æ˜¯å¦è¿‡æ‹Ÿåˆ)
2. embed_96: åµŒå…¥ç»´åº¦96 (å¢žåŠ 50%ï¼Œæå‡å®¹é‡)
3. embed_128: åµŒå…¥ç»´åº¦128 (ç¿»å€ï¼Œæµ‹è¯•å®¹é‡ä¸Šé™)
4. layers_1: å•å±‚GGNN (å‡å°‘å±‚æ•°ï¼Œæµ‹è¯•æ˜¯å¦è¿‡æ‹Ÿåˆ)
5. layers_3: ä¸‰å±‚GGNN (å¢žåŠ æ·±åº¦)
6. layers_4: å››å±‚GGNN (æ›´æ·±ç½‘ç»œ)
7. dropout_0: æ— Dropout (æµ‹è¯•æ˜¯å¦éœ€è¦æ­£åˆ™åŒ–)
8. dropout_015: Dropout=0.15 (é€‚åº¦å¢žå¼º)
9. dropout_02: Dropout=0.2 (å¼ºæ­£åˆ™åŒ–)
10. dropout_03: Dropout=0.3 (æžå¼ºæ­£åˆ™åŒ–)
11. batch_16: batch=16 (å‡åŠï¼Œæ›´å¤šæ›´æ–°)
12. batch_48: batch=48 (å¢žåŠ 50%)
13. batch_64: batch=64 (ç¿»å€ï¼Œæ›´ç¨³å®šæ¢¯åº¦)
14. combo_large: å¤§æ¨¡åž‹é…ç½® (embed=96, layers=3, dropout=0.15)
15. combo_deep: æ·±å±‚ç½‘ç»œ (layers=4, dropout=0.2, batch=48)
16. combo_wide: å®½ç½‘ç»œ (embed=128, layers=2, dropout=0.15)

============================================================

æŒ‰ Enter å¼€å§‹å®žéªŒ...

è¿›åº¦: 1/16

============================================================
å®žéªŒ: embed_32 - åµŒå…¥ç»´åº¦32 (å‡åŠï¼Œæµ‹è¯•æ˜¯å¦è¿‡æ‹Ÿåˆ)
æ—¶é—´: 2025-11-21 00:12:46
============================================================

[32m[2025-11-21 00:12:48]    INFO >> åŠ è½½é…ç½®: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/config.yml (train_enhanced.py:666, cli_main())[0m
[32m[2025-11-21 00:12:48]    INFO >> å•GPUè®­ç»ƒ... (train_enhanced.py:694, cli_main())[0m
[32m[2025-11-21 00:12:48]    INFO >> è®­ç»ƒæ—¥å¿—å°†ä¿å­˜åˆ°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs (train_enhanced.py:561, single_main())[0m
[32m[2025-11-21 00:12:48]    INFO >> [nodes] dictionary: 9999 types (typilus.py:102, setup_task())[0m
[32m[2025-11-21 00:12:48]    INFO >> [edges] dictionary: 0 types (typilus.py:102, setup_task())[0m
[32m[2025-11-21 00:12:48]    INFO >> [supernodes.annotation] dictionary: 99 types (typilus.py:106, setup_task())[0m
[32m[2025-11-21 00:12:59]    INFO >> Typilus(
  (encoder): GGNNEncoder(
    (node_embedding): Embedding(9999, 32, padding_idx=0)
    (node_layer): Sequential(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=32, out_features=32, bias=False)
      (2): Dropout(p=0.1, inplace=False)
    )
    (ggnns): ModuleList(
      (0): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=32, out_features=32, bias=False)
          (OCCURRENCE_OF): Linear(in_features=32, out_features=32, bias=False)
          (NEXT): Linear(in_features=32, out_features=32, bias=False)
          (SUBTOKEN_OF): Linear(in_features=32, out_features=32, bias=False)
          (COMPUTED_FROM): Linear(in_features=32, out_features=32, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=32, out_features=32, bias=False)
          (NEXT_USE): Linear(in_features=32, out_features=32, bias=False)
          (RETURNS_TO): Linear(in_features=32, out_features=32, bias=False)
          (_CHILD): Linear(in_features=32, out_features=32, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=32, out_features=32, bias=False)
          (_NEXT): Linear(in_features=32, out_features=32, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=32, out_features=32, bias=False)
          (_COMPUTED_FROM): Linear(in_features=32, out_features=32, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=32, out_features=32, bias=False)
          (_NEXT_USE): Linear(in_features=32, out_features=32, bias=False)
          (_RETURNS_TO): Linear(in_features=32, out_features=32, bias=False)
        )
        (rnn_cell): GRUCell(32, 32)
      )
      (1): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=32, out_features=32, bias=False)
          (OCCURRENCE_OF): Linear(in_features=32, out_features=32, bias=False)
          (NEXT): Linear(in_features=32, out_features=32, bias=False)
          (SUBTOKEN_OF): Linear(in_features=32, out_features=32, bias=False)
          (COMPUTED_FROM): Linear(in_features=32, out_features=32, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=32, out_features=32, bias=False)
          (NEXT_USE): Linear(in_features=32, out_features=32, bias=False)
          (RETURNS_TO): Linear(in_features=32, out_features=32, bias=False)
          (_CHILD): Linear(in_features=32, out_features=32, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=32, out_features=32, bias=False)
          (_NEXT): Linear(in_features=32, out_features=32, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=32, out_features=32, bias=False)
          (_COMPUTED_FROM): Linear(in_features=32, out_features=32, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=32, out_features=32, bias=False)
          (_NEXT_USE): Linear(in_features=32, out_features=32, bias=False)
          (_RETURNS_TO): Linear(in_features=32, out_features=32, bias=False)
        )
        (rnn_cell): GRUCell(64, 32)
      )
    )
  )
  (decoder): DenseDecoder(
    (cls_layers): Sequential(
      (0): Linear(in_features=32, out_features=32, bias=False)
      (1): Dropout(p=0.1, inplace=False)
      (2): Linear(in_features=32, out_features=99, bias=True)
    )
  )
) (train_enhanced.py:568, single_main())[0m
[32m[2025-11-21 00:12:59]    INFO >> æ¨¡åž‹: typilus, æŸå¤±å‡½æ•°: TypilusCriterion (train_enhanced.py:569, single_main())[0m
[32m[2025-11-21 00:12:59]    INFO >> æ¨¡åž‹å‚æ•°: 373795 (å¯è®­ç»ƒ: 373795) (train_enhanced.py:570, single_main())[0m
[32m[2025-11-21 00:12:59]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 00:12:59]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 80579 MB ; used memory = 1340 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 00:12:59]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 00:12:59]    INFO >> ä½¿ç”¨ 1 ä¸ªGPUè®­ç»ƒ (train_enhanced.py:576, single_main())[0m
[32m[2025-11-21 00:12:59]    INFO >> no existing checkpoint found /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-21 00:12:59]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[32m[2025-11-21 00:14:20]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-21 00:14:20] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-21 00:14:27]    INFO >> epoch 001:     50 / 1539 loss=5.561, wps=5506.2, ups=7.62, wpb=720, bsz=720, num_updates=50, lr=0.0004, gnorm=6.392, clip=0, train_wall=6, gb_free=76.9, wall=83 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:14:33]    INFO >> epoch 001:    100 / 1539 loss=5.666, wps=6594.7, ups=8.26, wpb=798.5, bsz=798.5, num_updates=100, lr=0.0004, gnorm=6.722, clip=0, train_wall=5, gb_free=75.6, wall=89 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:14:39]    INFO >> epoch 001:    150 / 1539 loss=5.666, wps=7148.1, ups=8.73, wpb=818.6, bsz=818.6, num_updates=150, lr=0.0004, gnorm=7.876, clip=0, train_wall=5, gb_free=76.4, wall=95 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:14:44]    INFO >> epoch 001:    200 / 1539 loss=5.656, wps=6097.5, ups=9.59, wpb=635.8, bsz=635.8, num_updates=200, lr=0.0004, gnorm=9.186, clip=0, train_wall=4, gb_free=76.4, wall=100 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:14:51]    INFO >> epoch 001:    250 / 1539 loss=5.804, wps=5556.8, ups=8.71, wpb=638, bsz=638, num_updates=250, lr=0.0004, gnorm=9.324, clip=0, train_wall=5, gb_free=76.9, wall=106 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:14:57]    INFO >> epoch 001:    300 / 1539 loss=5.725, wps=6157.5, ups=7.81, wpb=788.8, bsz=788.8, num_updates=300, lr=0.0004, gnorm=9.754, clip=0, train_wall=6, gb_free=74.3, wall=113 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:15:03]    INFO >> epoch 001:    350 / 1539 loss=5.876, wps=5886.4, ups=8.85, wpb=665.5, bsz=665.5, num_updates=350, lr=0.0004, gnorm=10.477, clip=2, train_wall=5, gb_free=75, wall=118 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:15:09]    INFO >> epoch 001:    400 / 1539 loss=5.73, wps=7083.4, ups=8.24, wpb=859.7, bsz=859.7, num_updates=400, lr=0.0004, gnorm=11.428, clip=0, train_wall=5, gb_free=75, wall=124 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:15:13] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 51.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 77.10 GiB is allocated by PyTorch, and 1.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:15:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:15:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:15:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 2         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78893 MiB |  78953 MiB |   6726 GiB |   6649 GiB |
|       from large pool |  78445 MiB |  78505 MiB |   6646 GiB |   6569 GiB |
|       from small pool |    447 MiB |    448 MiB |     79 GiB |     79 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78893 MiB |  78953 MiB |   6726 GiB |   6649 GiB |
|       from large pool |  78445 MiB |  78505 MiB |   6646 GiB |   6569 GiB |
|       from small pool |    447 MiB |    448 MiB |     79 GiB |     79 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78537 MiB |  78597 MiB |   6678 GiB |   6601 GiB |
|       from large pool |  78091 MiB |  78151 MiB |   6598 GiB |   6522 GiB |
|       from small pool |    445 MiB |    446 MiB |     79 GiB |     79 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80456 MiB |  80456 MiB | 133742 MiB |  53286 MiB |
|       from large pool |  79998 MiB |  79998 MiB | 132854 MiB |  52856 MiB |
|       from small pool |    458 MiB |    458 MiB |    888 MiB |    430 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1502 MiB |   4359 MiB |   3213 GiB |   3211 GiB |
|       from large pool |   1492 MiB |   4356 MiB |   3117 GiB |   3116 GiB |
|       from small pool |     10 MiB |     26 MiB |     95 GiB |     95 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    8394    |    8397    |     917 K  |     908 K  |
|       from large pool |    1039    |    1040    |     414 K  |     413 K  |
|       from small pool |    7355    |    7358    |     502 K  |     495 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    8394    |    8397    |     917 K  |     908 K  |
|       from large pool |    1039    |    1040    |     414 K  |     413 K  |
|       from small pool |    7355    |    7358    |     502 K  |     495 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |    1067    |    1158    |    2190    |    1123    |
|       from large pool |     838    |     941    |    1746    |     908    |
|       from small pool |     229    |     229    |     444    |     215    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     332    |     332    |  559856    |  559524    |
|       from large pool |     140    |     140    |  316163    |  316023    |
|       from small pool |     192    |     192    |  243693    |  243501    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:15:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:15:13] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:15:22]    INFO >> epoch 001:    451 / 1539 loss=5.821, wps=2562, ups=4.17, wpb=614.5, bsz=614.5, num_updates=450, lr=0.0004, gnorm=10.841, clip=2, train_wall=5, gb_free=77.2, wall=136 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:15:30]    INFO >> epoch 001:    501 / 1539 loss=5.777, wps=4962.9, ups=6.51, wpb=762.8, bsz=762.8, num_updates=500, lr=0.0004, gnorm=10.374, clip=2, train_wall=7, gb_free=76.1, wall=144 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:15:36]    INFO >> epoch 001:    551 / 1539 loss=5.87, wps=5961.4, ups=9.13, wpb=653.1, bsz=653.1, num_updates=550, lr=0.0004, gnorm=10.254, clip=0, train_wall=5, gb_free=72.7, wall=149 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:15:41]    INFO >> epoch 001:    601 / 1539 loss=5.874, wps=5909.2, ups=8.83, wpb=668.9, bsz=668.9, num_updates=600, lr=0.0004, gnorm=10.29, clip=2, train_wall=5, gb_free=77.5, wall=155 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:15:47]    INFO >> epoch 001:    651 / 1539 loss=5.849, wps=6189.6, ups=8.71, wpb=710.3, bsz=710.3, num_updates=650, lr=0.0004, gnorm=9.807, clip=0, train_wall=5, gb_free=75.3, wall=161 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:15:55]    INFO >> epoch 001:    701 / 1539 loss=5.721, wps=5302.9, ups=7.87, wpb=673.4, bsz=673.4, num_updates=700, lr=0.0004, gnorm=9.862, clip=0, train_wall=5, gb_free=75.4, wall=167 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:16:01]    INFO >> epoch 001:    751 / 1539 loss=5.608, wps=6433.5, ups=8.46, wpb=760.6, bsz=760.6, num_updates=750, lr=0.0004, gnorm=10.257, clip=2, train_wall=5, gb_free=64.3, wall=173 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:16:06]    INFO >> epoch 001:    801 / 1539 loss=5.803, wps=6272.3, ups=8.74, wpb=717.9, bsz=717.9, num_updates=800, lr=0.0004, gnorm=9.836, clip=2, train_wall=5, gb_free=76.1, wall=179 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:16:12]    INFO >> epoch 001:    851 / 1539 loss=5.635, wps=5614.8, ups=8.74, wpb=642.6, bsz=642.6, num_updates=850, lr=0.0004, gnorm=10.342, clip=0, train_wall=5, gb_free=77.3, wall=185 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:16:18]    INFO >> epoch 001:    901 / 1539 loss=5.641, wps=5926.2, ups=8.97, wpb=660.3, bsz=660.3, num_updates=900, lr=0.0004, gnorm=10.222, clip=0, train_wall=5, gb_free=77.5, wall=190 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:16:23]    INFO >> epoch 001:    951 / 1539 loss=5.637, wps=6266.9, ups=8.77, wpb=714.2, bsz=714.2, num_updates=950, lr=0.0004, gnorm=10.706, clip=2, train_wall=5, gb_free=75.3, wall=196 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:16:31]    INFO >> epoch 001:   1001 / 1539 loss=5.585, wps=5555.2, ups=7.64, wpb=727.3, bsz=727.3, num_updates=1000, lr=0.0004, gnorm=10.374, clip=2, train_wall=6, gb_free=73.6, wall=202 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:16:37]    INFO >> epoch 001:   1051 / 1539 loss=5.541, wps=6676.7, ups=8.26, wpb=808.7, bsz=808.7, num_updates=1050, lr=0.0004, gnorm=9.971, clip=0, train_wall=5, gb_free=76.7, wall=208 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:16:44] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 43.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 77.15 GiB is allocated by PyTorch, and 1.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:16:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:16:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:16:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 4         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78945 MiB |  79005 MiB |  17406 GiB |  17329 GiB |
|       from large pool |  78585 MiB |  78645 MiB |  17216 GiB |  17139 GiB |
|       from small pool |    359 MiB |    360 MiB |    189 GiB |    189 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78945 MiB |  79005 MiB |  17406 GiB |  17329 GiB |
|       from large pool |  78585 MiB |  78645 MiB |  17216 GiB |  17139 GiB |
|       from small pool |    359 MiB |    360 MiB |    189 GiB |    189 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78581 MiB |  78640 MiB |  17303 GiB |  17226 GiB |
|       from large pool |  78224 MiB |  78283 MiB |  17114 GiB |  17037 GiB |
|       from small pool |    357 MiB |    358 MiB |    189 GiB |    189 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80464 MiB |  80482 MiB | 211170 MiB | 130706 MiB |
|       from large pool |  80066 MiB |  80084 MiB | 209898 MiB | 129832 MiB |
|       from small pool |    398 MiB |    398 MiB |   1272 MiB |    874 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1458 MiB |   6178 MiB |  13499 GiB |  13497 GiB |
|       from large pool |   1420 MiB |   6174 MiB |  13272 GiB |  13271 GiB |
|       from small pool |     38 MiB |     39 MiB |    226 GiB |    226 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6832    |    6835    |    2249 K  |    2242 K  |
|       from large pool |     903    |     904    |    1059 K  |    1059 K  |
|       from small pool |    5929    |    5932    |    1189 K  |    1183 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    6832    |    6835    |    2249 K  |    2242 K  |
|       from large pool |     903    |     904    |    1059 K  |    1059 K  |
|       from small pool |    5929    |    5932    |    1189 K  |    1183 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     774    |     775    |    2914    |    2140    |
|       from large pool |     575    |     576    |    2278    |    1703    |
|       from small pool |     199    |     199    |     636    |     437    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     471    |     472    |    1320 K  |    1320 K  |
|       from large pool |     110    |     111    |     768 K  |     768 K  |
|       from small pool |     361    |     362    |     552 K  |     551 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:16:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:16:44] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:16:44]    INFO >> epoch 001:   1102 / 1539 loss=5.486, wps=6224, ups=7, wpb=888.7, bsz=888.7, num_updates=1100, lr=0.0004, gnorm=10.71, clip=4, train_wall=6, gb_free=76.1, wall=216 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:16:50]    INFO >> epoch 001:   1152 / 1539 loss=5.553, wps=6094.8, ups=8.26, wpb=737.9, bsz=737.9, num_updates=1150, lr=0.0004, gnorm=9.845, clip=0, train_wall=5, gb_free=76.9, wall=222 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:16:56]    INFO >> epoch 001:   1202 / 1539 loss=5.492, wps=5762.2, ups=8.56, wpb=672.9, bsz=672.9, num_updates=1200, lr=0.0004, gnorm=10.257, clip=0, train_wall=5, gb_free=76.1, wall=227 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:17:04]    INFO >> epoch 001:   1252 / 1539 loss=5.501, wps=6154.8, ups=8.43, wpb=730.1, bsz=730.1, num_updates=1250, lr=0.0004, gnorm=10.966, clip=0, train_wall=5, gb_free=75.4, wall=233 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:17:10]    INFO >> epoch 001:   1302 / 1539 loss=5.421, wps=6092.4, ups=8.16, wpb=746.9, bsz=746.9, num_updates=1300, lr=0.0004, gnorm=9.267, clip=0, train_wall=5, gb_free=73.8, wall=240 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:17:16]    INFO >> epoch 001:   1352 / 1539 loss=5.537, wps=5307.1, ups=8.52, wpb=622.9, bsz=622.9, num_updates=1350, lr=0.0004, gnorm=9.089, clip=0, train_wall=5, gb_free=77.9, wall=245 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:17:22]    INFO >> epoch 001:   1402 / 1539 loss=5.473, wps=6189.7, ups=8.35, wpb=741.7, bsz=741.7, num_updates=1400, lr=0.0004, gnorm=9.363, clip=0, train_wall=5, gb_free=75.5, wall=251 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:17:27]    INFO >> epoch 001:   1452 / 1539 loss=5.486, wps=6032.3, ups=8.56, wpb=704.9, bsz=704.9, num_updates=1450, lr=0.0004, gnorm=9.287, clip=0, train_wall=5, gb_free=74.9, wall=257 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:17:33]    INFO >> epoch 001:   1502 / 1539 loss=5.335, wps=6050.5, ups=8.62, wpb=702.3, bsz=702.3, num_updates=1500, lr=0.0004, gnorm=9.971, clip=0, train_wall=5, gb_free=76.6, wall=263 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:17:39]    INFO >> epoch 001 | loss 5.633 | wps 5821.4 | ups 8.08 | wpb 720.8 | bsz 720.8 | num_updates 1537 | lr 0.0004 | gnorm 9.752 | clip 0.7 | train_wall 158 | gb_free 78.2 | wall 267 (progress_bar.py:267, print())[0m
[33m[2025-11-21 00:17:39] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:17:48]    INFO >> epoch 001 | valid on 'valid' subset | loss 5.506 | wps 16347.4 | wpb 5412.5 | bsz 5412.5 | num_updates 1537 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
[32m[2025-11-21 00:17:49]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 00:17:49]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/checkpoints/checkpoint_best.pt (epoch 1 @ 1537 updates, score 5.506) (writing took 0.013126 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 00:17:49] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-21 00:17:50]    INFO >> epoch 002:     13 / 1539 loss=5.361, wps=2298.6, ups=3.22, wpb=714.8, bsz=714.8, num_updates=1550, lr=0.0004, gnorm=8.534, clip=0, train_wall=5, gb_free=76.9, wall=279 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:17:56]    INFO >> epoch 002:     63 / 1539 loss=5.342, wps=6090.2, ups=9.05, wpb=672.6, bsz=672.6, num_updates=1600, lr=0.0004, gnorm=8.732, clip=0, train_wall=5, gb_free=76.5, wall=284 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:18:01]    INFO >> epoch 002:    113 / 1539 loss=5.28, wps=6323.8, ups=8.82, wpb=717.1, bsz=717.1, num_updates=1650, lr=0.0004, gnorm=8.934, clip=0, train_wall=5, gb_free=76.1, wall=290 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:18:07]    INFO >> epoch 002:    163 / 1539 loss=5.147, wps=6245.6, ups=8.65, wpb=722.3, bsz=722.3, num_updates=1700, lr=0.0004, gnorm=8.915, clip=0, train_wall=5, gb_free=75.8, wall=296 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:18:14]    INFO >> epoch 002:    213 / 1539 loss=5.285, wps=5969.9, ups=8.37, wpb=713.4, bsz=713.4, num_updates=1750, lr=0.0004, gnorm=10.408, clip=0, train_wall=5, gb_free=76.6, wall=302 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:18:21]    INFO >> epoch 002:    263 / 1539 loss=5.031, wps=6882.8, ups=7.99, wpb=861.4, bsz=861.4, num_updates=1800, lr=0.0004, gnorm=9.109, clip=2, train_wall=6, gb_free=77, wall=308 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:18:27]    INFO >> epoch 002:    313 / 1539 loss=5.238, wps=5540.8, ups=8.45, wpb=655.5, bsz=655.5, num_updates=1850, lr=0.0004, gnorm=8.997, clip=0, train_wall=5, gb_free=76.2, wall=314 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:18:32]    INFO >> epoch 002:    363 / 1539 loss=5.148, wps=5878, ups=8.86, wpb=663.3, bsz=663.3, num_updates=1900, lr=0.0004, gnorm=9.089, clip=0, train_wall=5, gb_free=72.4, wall=319 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:18:38]    INFO >> epoch 002:    413 / 1539 loss=5.165, wps=5970.9, ups=8.4, wpb=710.4, bsz=710.4, num_updates=1950, lr=0.0004, gnorm=8.357, clip=0, train_wall=5, gb_free=70.8, wall=325 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:18:45]    INFO >> epoch 002:    463 / 1539 loss=5.093, wps=6061.8, ups=7.69, wpb=788.1, bsz=788.1, num_updates=2000, lr=0.0004, gnorm=10.154, clip=2, train_wall=6, gb_free=76.2, wall=332 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:18:50]    INFO >> epoch 002:    513 / 1539 loss=5.144, wps=6349.2, ups=8.88, wpb=714.8, bsz=714.8, num_updates=2050, lr=0.0004, gnorm=9.822, clip=0, train_wall=5, gb_free=74.2, wall=337 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:18:56]    INFO >> epoch 002:    563 / 1539 loss=5.147, wps=5665.9, ups=9, wpb=629.4, bsz=629.4, num_updates=2100, lr=0.0004, gnorm=9.192, clip=0, train_wall=5, gb_free=76.3, wall=343 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:19:02]    INFO >> epoch 002:    613 / 1539 loss=5.08, wps=6786.5, ups=7.88, wpb=860.8, bsz=860.8, num_updates=2150, lr=0.0004, gnorm=9.688, clip=2, train_wall=6, gb_free=77.6, wall=349 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:19:08]    INFO >> epoch 002:    663 / 1539 loss=4.996, wps=6129.3, ups=8.09, wpb=758.1, bsz=758.1, num_updates=2200, lr=0.0004, gnorm=9.871, clip=0, train_wall=6, gb_free=76, wall=356 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:19:17]    INFO >> epoch 002:    713 / 1539 loss=5.082, wps=5779.6, ups=8.34, wpb=693, bsz=693, num_updates=2250, lr=0.0004, gnorm=9.263, clip=2, train_wall=5, gb_free=77.5, wall=362 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:19:23]    INFO >> epoch 002:    763 / 1539 loss=4.994, wps=5782.3, ups=8.38, wpb=689.7, bsz=689.7, num_updates=2300, lr=0.0004, gnorm=8.574, clip=2, train_wall=5, gb_free=73.8, wall=367 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:19:29]    INFO >> epoch 002:    813 / 1539 loss=5.068, wps=5505.4, ups=8.6, wpb=640.4, bsz=640.4, num_updates=2350, lr=0.0004, gnorm=7.969, clip=0, train_wall=5, gb_free=72.7, wall=373 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:19:35]    INFO >> epoch 002:    863 / 1539 loss=4.917, wps=5894.3, ups=7.89, wpb=747.2, bsz=747.2, num_updates=2400, lr=0.0004, gnorm=9.114, clip=4, train_wall=6, gb_free=75.4, wall=380 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:19:41]    INFO >> epoch 002:    913 / 1539 loss=5.022, wps=5821.3, ups=8.65, wpb=673.2, bsz=673.2, num_updates=2450, lr=0.0004, gnorm=7.555, clip=0, train_wall=5, gb_free=75.4, wall=385 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:19:47]    INFO >> epoch 002:    963 / 1539 loss=4.963, wps=5520, ups=8.71, wpb=633.4, bsz=633.4, num_updates=2500, lr=0.0004, gnorm=8.41, clip=2, train_wall=5, gb_free=77.1, wall=391 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:19:54] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 45.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 76.87 GiB is allocated by PyTorch, and 1.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:19:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:19:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:19:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 3            |        cudaMalloc retries: 5         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78656 MiB |  78715 MiB |  41925 GiB |  41848 GiB |
|       from large pool |  78209 MiB |  78269 MiB |  41461 GiB |  41385 GiB |
|       from small pool |    446 MiB |    447 MiB |    463 GiB |    462 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78656 MiB |  78715 MiB |  41925 GiB |  41848 GiB |
|       from large pool |  78209 MiB |  78269 MiB |  41461 GiB |  41385 GiB |
|       from small pool |    446 MiB |    447 MiB |    463 GiB |    462 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78417 MiB |  78476 MiB |  41700 GiB |  41623 GiB |
|       from large pool |  77972 MiB |  78032 MiB |  41237 GiB |  41161 GiB |
|       from small pool |    444 MiB |    445 MiB |    462 GiB |    462 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80462 MiB |  80464 MiB | 211230 MiB | 130768 MiB |
|       from large pool |  80006 MiB |  80006 MiB | 209898 MiB | 129892 MiB |
|       from small pool |    456 MiB |    458 MiB |   1332 MiB |    876 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1805 MiB |   4507 MiB |  32047 GiB |  32045 GiB |
|       from large pool |   1796 MiB |   4505 MiB |  31504 GiB |  31502 GiB |
|       from small pool |      9 MiB |     26 MiB |    542 GiB |    542 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    8374    |    8377    |    5379 K  |    5371 K  |
|       from large pool |    1037    |    1038    |    2478 K  |    2477 K  |
|       from small pool |    7337    |    7340    |    2901 K  |    2894 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    8374    |    8377    |    5379 K  |    5371 K  |
|       from large pool |    1037    |    1038    |    2478 K  |    2477 K  |
|       from small pool |    7337    |    7340    |    2901 K  |    2894 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     802    |     803    |    2944    |    2142    |
|       from large pool |     574    |     574    |    2278    |    1704    |
|       from small pool |     228    |     229    |     666    |     438    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     324    |     326    |    3163 K  |    3163 K  |
|       from large pool |     134    |     134    |    1793 K  |    1793 K  |
|       from small pool |     190    |     192    |    1369 K  |    1369 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:19:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:19:54] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:19:55]    INFO >> epoch 002:   1014 / 1539 loss=4.848, wps=6195.2, ups=7.09, wpb=874.4, bsz=874.4, num_updates=2550, lr=0.0004, gnorm=8.141, clip=0, train_wall=6, gb_free=75.9, wall=398 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:20:01]    INFO >> epoch 002:   1064 / 1539 loss=4.84, wps=6259.6, ups=8.14, wpb=769.2, bsz=769.2, num_updates=2600, lr=0.0004, gnorm=8.119, clip=0, train_wall=6, gb_free=71, wall=404 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:20:08]    INFO >> epoch 002:   1114 / 1539 loss=4.604, wps=5936.1, ups=7.73, wpb=767.6, bsz=767.6, num_updates=2650, lr=0.0004, gnorm=8.716, clip=0, train_wall=6, gb_free=77.1, wall=411 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:20:14]    INFO >> epoch 002:   1164 / 1539 loss=4.715, wps=6587.6, ups=8.28, wpb=795.4, bsz=795.4, num_updates=2700, lr=0.0004, gnorm=10.402, clip=4, train_wall=6, gb_free=76.1, wall=417 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:20:20]    INFO >> epoch 002:   1214 / 1539 loss=4.899, wps=5736.3, ups=8.19, wpb=700.1, bsz=700.1, num_updates=2750, lr=0.0004, gnorm=7.383, clip=0, train_wall=6, gb_free=77, wall=423 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:20:24] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 5.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 75.22 GiB is allocated by PyTorch, and 3.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:20:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:20:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:20:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 4            |        cudaMalloc retries: 7         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76966 MiB |  77026 MiB |  45794 GiB |  45719 GiB |
|       from large pool |  76626 MiB |  76686 MiB |  45288 GiB |  45213 GiB |
|       from small pool |    340 MiB |    341 MiB |    506 GiB |    506 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76966 MiB |  77026 MiB |  45794 GiB |  45719 GiB |
|       from large pool |  76626 MiB |  76686 MiB |  45288 GiB |  45213 GiB |
|       from small pool |    340 MiB |    341 MiB |    506 GiB |    506 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76666 MiB |  76725 MiB |  45550 GiB |  45475 GiB |
|       from large pool |  76328 MiB |  76387 MiB |  45044 GiB |  44970 GiB |
|       from small pool |    338 MiB |    339 MiB |    505 GiB |    505 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80502 MiB |  80504 MiB | 211352 MiB | 130850 MiB |
|       from large pool |  80126 MiB |  80126 MiB | 210018 MiB | 129892 MiB |
|       from small pool |    376 MiB |    456 MiB |   1334 MiB |    958 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3475 MiB |   7773 MiB |  35130 GiB |  35127 GiB |
|       from large pool |   3439 MiB |   7742 MiB |  34535 GiB |  34532 GiB |
|       from small pool |     35 MiB |     37 MiB |    594 GiB |    594 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6482    |    6485    |    5882 K  |    5875 K  |
|       from large pool |     871    |     872    |    2707 K  |    2707 K  |
|       from small pool |    5611    |    5614    |    3174 K  |    3168 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    6482    |    6485    |    5882 K  |    5875 K  |
|       from large pool |     871    |     872    |    2707 K  |    2707 K  |
|       from small pool |    5611    |    5614    |    3174 K  |    3168 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     764    |     802    |    2947    |    2183    |
|       from large pool |     576    |     576    |    2280    |    1704    |
|       from small pool |     188    |     228    |     667    |     479    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     529    |     530    |    3461 K  |    3460 K  |
|       from large pool |     185    |     185    |    1959 K  |    1959 K  |
|       from small pool |     344    |     345    |    1501 K  |    1501 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:20:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:20:24] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:20:28]    INFO >> epoch 002:   1265 / 1539 loss=4.729, wps=5719.3, ups=7.46, wpb=767.1, bsz=767.1, num_updates=2800, lr=0.0004, gnorm=7.471, clip=0, train_wall=6, gb_free=75.1, wall=430 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:20:34]    INFO >> epoch 002:   1315 / 1539 loss=4.628, wps=5810.8, ups=8.74, wpb=664.7, bsz=664.7, num_updates=2850, lr=0.0004, gnorm=7.828, clip=0, train_wall=5, gb_free=75.9, wall=435 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:20:40]    INFO >> epoch 002:   1365 / 1539 loss=4.636, wps=5870.1, ups=8.3, wpb=707, bsz=707, num_updates=2900, lr=0.0004, gnorm=6.967, clip=0, train_wall=5, gb_free=74.3, wall=441 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:20:46]    INFO >> epoch 002:   1415 / 1539 loss=4.687, wps=5726.8, ups=8.5, wpb=673.7, bsz=673.7, num_updates=2950, lr=0.0004, gnorm=7.784, clip=0, train_wall=5, gb_free=75.5, wall=447 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:20:52]    INFO >> epoch 002:   1465 / 1539 loss=4.654, wps=5262, ups=7.58, wpb=693.9, bsz=693.9, num_updates=3000, lr=0.0004, gnorm=7.691, clip=0, train_wall=6, gb_free=74.6, wall=454 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:20:58]    INFO >> epoch 002:   1515 / 1539 loss=4.636, wps=5386.1, ups=7.93, wpb=679.1, bsz=679.1, num_updates=3050, lr=0.0004, gnorm=7.667, clip=2, train_wall=6, gb_free=77.3, wall=460 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:21:02]    INFO >> epoch 002 | loss 4.959 | wps 5650 | ups 7.84 | wpb 720.8 | bsz 720.8 | num_updates 3074 | lr 0.0004 | gnorm 8.671 | clip 0.7 | train_wall 169 | gb_free 76 | wall 463 (progress_bar.py:267, print())[0m
[33m[2025-11-21 00:21:02] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:21:10]    INFO >> epoch 002 | valid on 'valid' subset | loss 4.75 | wps 18543.1 | wpb 5412.5 | bsz 5412.5 | num_updates 3074 | best_loss 5.506 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 00:21:10]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 00:21:10]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/checkpoints/checkpoint_last.pt (epoch 2 @ 3074 updates, score 4.75) (writing took 0.011590 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 00:21:10] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:21:13]    INFO >> epoch 003:     26 / 1539 loss=4.561, wps=2301.7, ups=3.4, wpb=677.9, bsz=677.9, num_updates=3100, lr=0.000392, gnorm=8.329, clip=0, train_wall=5, gb_free=77.3, wall=475 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:21:19]    INFO >> epoch 003:     76 / 1539 loss=4.647, wps=6543, ups=8.43, wpb=776.1, bsz=776.1, num_updates=3150, lr=0.000392, gnorm=7.845, clip=0, train_wall=5, gb_free=72.9, wall=481 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:21:26]    INFO >> epoch 003:    126 / 1539 loss=4.555, wps=6498, ups=7.51, wpb=865.1, bsz=865.1, num_updates=3200, lr=0.000392, gnorm=8.939, clip=0, train_wall=6, gb_free=75.3, wall=488 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:21:34]    INFO >> epoch 003:    176 / 1539 loss=4.702, wps=5729.9, ups=8.6, wpb=666.3, bsz=666.3, num_updates=3250, lr=0.000392, gnorm=7.524, clip=0, train_wall=5, gb_free=74.5, wall=493 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:21:39] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 5.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 75.22 GiB is allocated by PyTorch, and 3.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:21:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:21:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:21:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 5            |        cudaMalloc retries: 8         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76966 MiB |  77026 MiB |  56388 GiB |  56313 GiB |
|       from large pool |  76626 MiB |  76686 MiB |  55760 GiB |  55685 GiB |
|       from small pool |    340 MiB |    341 MiB |    628 GiB |    628 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76966 MiB |  77026 MiB |  56388 GiB |  56313 GiB |
|       from large pool |  76626 MiB |  76686 MiB |  55760 GiB |  55685 GiB |
|       from small pool |    340 MiB |    341 MiB |    628 GiB |    628 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76666 MiB |  76725 MiB |  56099 GiB |  56024 GiB |
|       from large pool |  76328 MiB |  76387 MiB |  55471 GiB |  55396 GiB |
|       from small pool |    338 MiB |    339 MiB |    627 GiB |    627 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80502 MiB |  80504 MiB | 211414 MiB | 130912 MiB |
|       from large pool |  80126 MiB |  80126 MiB | 210078 MiB | 129952 MiB |
|       from small pool |    376 MiB |    378 MiB |   1336 MiB |    960 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3475 MiB |   7773 MiB |  42880 GiB |  42877 GiB |
|       from large pool |   3439 MiB |   7742 MiB |  42148 GiB |  42145 GiB |
|       from small pool |     35 MiB |     37 MiB |    732 GiB |    732 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6482    |    6485    |    7182 K  |    7176 K  |
|       from large pool |     871    |     872    |    3236 K  |    3235 K  |
|       from small pool |    5611    |    5614    |    3945 K  |    3940 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    6482    |    6485    |    7182 K  |    7176 K  |
|       from large pool |     871    |     872    |    3236 K  |    3235 K  |
|       from small pool |    5611    |    5614    |    3945 K  |    3940 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     764    |     765    |    2949    |    2185    |
|       from large pool |     576    |     576    |    2281    |    1705    |
|       from small pool |     188    |     189    |     668    |     480    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     530    |     531    |    4244 K  |    4244 K  |
|       from large pool |     185    |     185    |    2343 K  |    2343 K  |
|       from small pool |     345    |     346    |    1901 K  |    1901 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:21:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:21:39] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:21:41]    INFO >> epoch 003:    227 / 1539 loss=4.672, wps=5662.3, ups=7.17, wpb=789.3, bsz=789.3, num_updates=3300, lr=0.000392, gnorm=6.724, clip=0, train_wall=6, gb_free=76.4, wall=500 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:21:47]    INFO >> epoch 003:    277 / 1539 loss=4.408, wps=6011.2, ups=8.17, wpb=735.6, bsz=735.6, num_updates=3350, lr=0.000392, gnorm=7.908, clip=0, train_wall=6, gb_free=76.1, wall=506 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:21:54]    INFO >> epoch 003:    327 / 1539 loss=4.449, wps=6518.8, ups=7.88, wpb=827.1, bsz=827.1, num_updates=3400, lr=0.000392, gnorm=7.157, clip=0, train_wall=6, gb_free=75.6, wall=513 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:22:00]    INFO >> epoch 003:    377 / 1539 loss=4.491, wps=5450.7, ups=8.2, wpb=664.4, bsz=664.4, num_updates=3450, lr=0.000392, gnorm=7.262, clip=0, train_wall=6, gb_free=74.1, wall=519 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:22:07]    INFO >> epoch 003:    427 / 1539 loss=4.61, wps=5470.9, ups=8.47, wpb=646.2, bsz=646.2, num_updates=3500, lr=0.000392, gnorm=6.669, clip=0, train_wall=5, gb_free=75.9, wall=525 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:22:13]    INFO >> epoch 003:    477 / 1539 loss=4.402, wps=6352.1, ups=8.19, wpb=775.9, bsz=775.9, num_updates=3550, lr=0.000392, gnorm=8.016, clip=0, train_wall=6, gb_free=73.2, wall=531 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:22:19]    INFO >> epoch 003:    527 / 1539 loss=4.334, wps=5721.6, ups=8.05, wpb=710.7, bsz=710.7, num_updates=3600, lr=0.000392, gnorm=7.833, clip=2, train_wall=6, gb_free=76.6, wall=537 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:22:26]    INFO >> epoch 003:    577 / 1539 loss=4.362, wps=6151.6, ups=7.88, wpb=780.7, bsz=780.7, num_updates=3650, lr=0.000392, gnorm=7.353, clip=0, train_wall=6, gb_free=74.4, wall=543 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:22:31]    INFO >> epoch 003:    627 / 1539 loss=4.455, wps=5875, ups=8.89, wpb=660.9, bsz=660.9, num_updates=3700, lr=0.000392, gnorm=7.52, clip=0, train_wall=5, gb_free=74.9, wall=549 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:22:39]    INFO >> epoch 003:    677 / 1539 loss=4.452, wps=6356.9, ups=8.21, wpb=774.2, bsz=774.2, num_updates=3750, lr=0.000392, gnorm=6.575, clip=0, train_wall=6, gb_free=71.9, wall=555 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:22:45]    INFO >> epoch 003:    727 / 1539 loss=4.224, wps=6059.9, ups=7.7, wpb=787.5, bsz=787.5, num_updates=3800, lr=0.000392, gnorm=6.947, clip=0, train_wall=6, gb_free=76.4, wall=562 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:22:52]    INFO >> epoch 003:    777 / 1539 loss=4.113, wps=6236.3, ups=7.78, wpb=801.7, bsz=801.7, num_updates=3850, lr=0.000392, gnorm=7.334, clip=2, train_wall=6, gb_free=46.7, wall=568 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:22:53] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 29.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.94 GiB is allocated by PyTorch, and 1.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:22:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:22:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:22:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 6            |        cudaMalloc retries: 10        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78727 MiB |  78787 MiB |  65588 GiB |  65511 GiB |
|       from large pool |  78280 MiB |  78340 MiB |  64859 GiB |  64782 GiB |
|       from small pool |    447 MiB |    448 MiB |    728 GiB |    728 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78727 MiB |  78787 MiB |  65588 GiB |  65511 GiB |
|       from large pool |  78280 MiB |  78340 MiB |  64859 GiB |  64782 GiB |
|       from small pool |    447 MiB |    448 MiB |    728 GiB |    728 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78477 MiB |  78537 MiB |  65250 GiB |  65173 GiB |
|       from large pool |  78032 MiB |  78091 MiB |  64522 GiB |  64446 GiB |
|       from small pool |    445 MiB |    446 MiB |    727 GiB |    727 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80478 MiB |  80506 MiB | 212816 MiB | 132338 MiB |
|       from large pool |  80022 MiB |  80066 MiB | 211398 MiB | 131376 MiB |
|       from small pool |    456 MiB |    458 MiB |   1418 MiB |    962 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1690 MiB |   4509 MiB |  50098 GiB |  50096 GiB |
|       from large pool |   1681 MiB |   4505 MiB |  49246 GiB |  49245 GiB |
|       from small pool |      8 MiB |     18 MiB |    851 GiB |    851 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    8385    |    8388    |    8370 K  |    8362 K  |
|       from large pool |    1038    |    1039    |    3797 K  |    3796 K  |
|       from small pool |    7347    |    7350    |    4573 K  |    4565 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    8385    |    8388    |    8370 K  |    8362 K  |
|       from large pool |    1038    |    1039    |    3797 K  |    3796 K  |
|       from small pool |    7347    |    7350    |    4573 K  |    4565 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     824    |     825    |    3012    |    2188    |
|       from large pool |     596    |     596    |    2303    |    1707    |
|       from small pool |     228    |     229    |     709    |     481    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     323    |     326    |    4941 K  |    4940 K  |
|       from large pool |     133    |     133    |    2747 K  |    2746 K  |
|       from small pool |     190    |     193    |    2194 K  |    2194 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:22:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:22:53] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:22:58]    INFO >> epoch 003:    828 / 1539 loss=4.276, wps=5573.7, ups=7.51, wpb=742.6, bsz=742.6, num_updates=3900, lr=0.000392, gnorm=8.318, clip=0, train_wall=5, gb_free=76.4, wall=575 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:23:04]    INFO >> epoch 003:    878 / 1539 loss=4.34, wps=6122.9, ups=8.32, wpb=735.9, bsz=735.9, num_updates=3950, lr=0.000392, gnorm=6.808, clip=2, train_wall=6, gb_free=76.1, wall=581 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:23:10]    INFO >> epoch 003:    928 / 1539 loss=4.359, wps=5608.4, ups=8.23, wpb=681.4, bsz=681.4, num_updates=4000, lr=0.000392, gnorm=6.127, clip=0, train_wall=6, gb_free=75.4, wall=587 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:23:17]    INFO >> epoch 003:    978 / 1539 loss=4.342, wps=5174.3, ups=7.34, wpb=705, bsz=705, num_updates=4050, lr=0.000392, gnorm=6.47, clip=0, train_wall=6, gb_free=76.1, wall=594 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:23:23]    INFO >> epoch 003:   1028 / 1539 loss=4.159, wps=5305.4, ups=8.2, wpb=646.9, bsz=646.9, num_updates=4100, lr=0.000392, gnorm=6.641, clip=0, train_wall=6, gb_free=77, wall=600 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:23:29]    INFO >> epoch 003:   1078 / 1539 loss=4.222, wps=5677.8, ups=8.41, wpb=675.4, bsz=675.4, num_updates=4150, lr=0.000392, gnorm=6.136, clip=0, train_wall=5, gb_free=75.3, wall=606 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:23:35]    INFO >> epoch 003:   1128 / 1539 loss=4.16, wps=5974, ups=8.6, wpb=695, bsz=695, num_updates=4200, lr=0.000392, gnorm=6.248, clip=0, train_wall=5, gb_free=74.6, wall=612 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:23:42]    INFO >> epoch 003:   1178 / 1539 loss=3.858, wps=6207.1, ups=7.43, wpb=835.2, bsz=835.2, num_updates=4250, lr=0.000392, gnorm=6.032, clip=0, train_wall=6, gb_free=75.8, wall=618 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:23:51]    INFO >> epoch 003:   1228 / 1539 loss=4.093, wps=5945.9, ups=8.03, wpb=740.1, bsz=740.1, num_updates=4300, lr=0.000392, gnorm=7.032, clip=0, train_wall=6, gb_free=75.4, wall=624 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:23:57]    INFO >> epoch 003:   1278 / 1539 loss=4.155, wps=5273.2, ups=8.45, wpb=624.4, bsz=624.4, num_updates=4350, lr=0.000392, gnorm=5.748, clip=0, train_wall=5, gb_free=77.3, wall=630 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:24:03]    INFO >> epoch 003:   1328 / 1539 loss=4.165, wps=5815.3, ups=8.55, wpb=679.9, bsz=679.9, num_updates=4400, lr=0.000392, gnorm=5.755, clip=0, train_wall=5, gb_free=76.1, wall=636 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:24:08]    INFO >> epoch 003:   1378 / 1539 loss=4.085, wps=5858.4, ups=8.46, wpb=692.2, bsz=692.2, num_updates=4450, lr=0.000392, gnorm=6.486, clip=0, train_wall=5, gb_free=75.5, wall=642 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:24:14]    INFO >> epoch 003:   1428 / 1539 loss=4.103, wps=5874.2, ups=8.64, wpb=680.1, bsz=680.1, num_updates=4500, lr=0.000392, gnorm=6.285, clip=0, train_wall=5, gb_free=73.5, wall=648 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:24:22]    INFO >> epoch 003:   1478 / 1539 loss=4.195, wps=5157.4, ups=8.32, wpb=619.7, bsz=619.7, num_updates=4550, lr=0.000392, gnorm=5.669, clip=0, train_wall=5, gb_free=77.1, wall=654 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:24:28]    INFO >> epoch 003:   1528 / 1539 loss=4.152, wps=5657.7, ups=8.29, wpb=682.4, bsz=682.4, num_updates=4600, lr=0.000392, gnorm=6.969, clip=0, train_wall=6, gb_free=76, wall=660 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:24:29]    INFO >> epoch 003 | loss 4.323 | wps 5596.6 | ups 7.76 | wpb 720.8 | bsz 720.8 | num_updates 4611 | lr 0.000392 | gnorm 6.949 | clip 0.2 | train_wall 172 | gb_free 77 | wall 661 (progress_bar.py:267, print())[0m
[33m[2025-11-21 00:24:29] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:24:37]    INFO >> epoch 003 | valid on 'valid' subset | loss 4.132 | wps 18267.6 | wpb 5412.5 | bsz 5412.5 | num_updates 4611 | best_loss 5.506 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 00:24:38]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 00:24:38]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/checkpoints/checkpoint_last.pt (epoch 3 @ 4611 updates, score 4.132) (writing took 0.015244 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 00:24:38] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:24:42]    INFO >> epoch 004:     39 / 1539 loss=4.093, wps=2646.4, ups=3.41, wpb=776.8, bsz=776.8, num_updates=4650, lr=0.000376, gnorm=5.976, clip=2, train_wall=5, gb_free=76.1, wall=675 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:24:48]    INFO >> epoch 004:     89 / 1539 loss=4.115, wps=5649.5, ups=8.38, wpb=674.1, bsz=674.1, num_updates=4700, lr=0.000376, gnorm=5.666, clip=0, train_wall=5, gb_free=75.5, wall=681 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:24:57]    INFO >> epoch 004:    139 / 1539 loss=3.642, wps=7460.3, ups=7.03, wpb=1061, bsz=1061, num_updates=4750, lr=0.000376, gnorm=7.379, clip=2, train_wall=7, gb_free=64.1, wall=688 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:25:03]    INFO >> epoch 004:    189 / 1539 loss=3.969, wps=6554.1, ups=8.61, wpb=761.2, bsz=761.2, num_updates=4800, lr=0.000376, gnorm=6.217, clip=0, train_wall=5, gb_free=77.2, wall=694 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:25:09]    INFO >> epoch 004:    239 / 1539 loss=4.107, wps=5970.2, ups=8.08, wpb=739.3, bsz=739.3, num_updates=4850, lr=0.000376, gnorm=6.848, clip=0, train_wall=6, gb_free=73.4, wall=700 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:25:15]    INFO >> epoch 004:    289 / 1539 loss=4.145, wps=5392, ups=8.41, wpb=641.5, bsz=641.5, num_updates=4900, lr=0.000376, gnorm=5.483, clip=0, train_wall=5, gb_free=76.4, wall=706 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:25:21]    INFO >> epoch 004:    339 / 1539 loss=4.036, wps=5862.3, ups=8.51, wpb=689.2, bsz=689.2, num_updates=4950, lr=0.000376, gnorm=5.951, clip=0, train_wall=5, gb_free=74.6, wall=712 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:25:28]    INFO >> epoch 004:    389 / 1539 loss=3.851, wps=6188.4, ups=8.07, wpb=766.4, bsz=766.4, num_updates=5000, lr=0.000376, gnorm=6.252, clip=0, train_wall=6, gb_free=76.1, wall=718 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:25:34]    INFO >> epoch 004:    439 / 1539 loss=4.121, wps=4992, ups=8.1, wpb=616.4, bsz=616.4, num_updates=5050, lr=0.000376, gnorm=5.583, clip=0, train_wall=6, gb_free=77.6, wall=724 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:25:40]    INFO >> epoch 004:    489 / 1539 loss=4.116, wps=5909, ups=8.16, wpb=724.4, bsz=724.4, num_updates=5100, lr=0.000376, gnorm=5.369, clip=0, train_wall=6, gb_free=74.1, wall=730 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:25:46]    INFO >> epoch 004:    539 / 1539 loss=4.019, wps=5711.5, ups=8.24, wpb=693.3, bsz=693.3, num_updates=5150, lr=0.000376, gnorm=6.082, clip=0, train_wall=6, gb_free=77.1, wall=736 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:25:52]    INFO >> epoch 004:    589 / 1539 loss=4.019, wps=5183.1, ups=8.35, wpb=621, bsz=621, num_updates=5200, lr=0.000376, gnorm=5.232, clip=0, train_wall=5, gb_free=76, wall=742 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:25:54] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 29.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.94 GiB is allocated by PyTorch, and 1.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:25:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:25:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:25:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 7            |        cudaMalloc retries: 11        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78727 MiB |  78787 MiB |  88799 GiB |  88722 GiB |
|       from large pool |  78280 MiB |  78340 MiB |  87813 GiB |  87736 GiB |
|       from small pool |    447 MiB |    448 MiB |    985 GiB |    985 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78727 MiB |  78787 MiB |  88799 GiB |  88722 GiB |
|       from large pool |  78280 MiB |  78340 MiB |  87813 GiB |  87736 GiB |
|       from small pool |    447 MiB |    448 MiB |    985 GiB |    985 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78477 MiB |  78537 MiB |  88346 GiB |  88269 GiB |
|       from large pool |  78032 MiB |  78091 MiB |  87362 GiB |  87285 GiB |
|       from small pool |    445 MiB |    446 MiB |    984 GiB |    984 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80478 MiB |  80480 MiB | 212878 MiB | 132400 MiB |
|       from large pool |  80022 MiB |  80022 MiB | 211458 MiB | 131436 MiB |
|       from small pool |    456 MiB |    458 MiB |   1420 MiB |    964 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1690 MiB |   4509 MiB |  67725 GiB |  67723 GiB |
|       from large pool |   1681 MiB |   4505 MiB |  66577 GiB |  66575 GiB |
|       from small pool |      8 MiB |     20 MiB |   1148 GiB |   1148 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    8385    |    8388    |   11315 K  |   11307 K  |
|       from large pool |    1038    |    1039    |    5128 K  |    5127 K  |
|       from small pool |    7347    |    7350    |    6186 K  |    6179 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    8385    |    8388    |   11315 K  |   11307 K  |
|       from large pool |    1038    |    1039    |    5128 K  |    5127 K  |
|       from small pool |    7347    |    7350    |    6186 K  |    6179 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     824    |     825    |    3014    |    2190    |
|       from large pool |     596    |     596    |    2304    |    1708    |
|       from small pool |     228    |     229    |     710    |     482    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     323    |     324    |    6670 K  |    6669 K  |
|       from large pool |     133    |     133    |    3709 K  |    3709 K  |
|       from small pool |     190    |     191    |    2960 K  |    2960 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:25:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:25:54] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:25:59]    INFO >> epoch 004:    640 / 1539 loss=3.92, wps=5087.4, ups=7.37, wpb=689.9, bsz=689.9, num_updates=5250, lr=0.000376, gnorm=5.908, clip=0, train_wall=6, gb_free=75.9, wall=749 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:26:05]    INFO >> epoch 004:    690 / 1539 loss=3.921, wps=5192.5, ups=8.26, wpb=628.6, bsz=628.6, num_updates=5300, lr=0.000376, gnorm=5.703, clip=0, train_wall=6, gb_free=75.7, wall=755 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:26:13]    INFO >> epoch 004:    740 / 1539 loss=4.074, wps=4996.5, ups=6.55, wpb=762.3, bsz=762.3, num_updates=5350, lr=0.000376, gnorm=5.785, clip=0, train_wall=7, gb_free=76.1, wall=763 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:26:19]    INFO >> epoch 004:    790 / 1539 loss=4.005, wps=5667.3, ups=8.3, wpb=683.1, bsz=683.1, num_updates=5400, lr=0.000376, gnorm=5.036, clip=0, train_wall=6, gb_free=76.2, wall=769 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:26:25]    INFO >> epoch 004:    840 / 1539 loss=3.947, wps=5830, ups=8.05, wpb=724.6, bsz=724.6, num_updates=5450, lr=0.000376, gnorm=5.475, clip=0, train_wall=6, gb_free=75.7, wall=775 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:26:34]    INFO >> epoch 004:    890 / 1539 loss=3.914, wps=6503, ups=7.95, wpb=817.5, bsz=817.5, num_updates=5500, lr=0.000376, gnorm=6.161, clip=0, train_wall=6, gb_free=75.4, wall=781 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:26:40]    INFO >> epoch 004:    940 / 1539 loss=3.923, wps=5681.6, ups=8.74, wpb=650.1, bsz=650.1, num_updates=5550, lr=0.000376, gnorm=4.789, clip=0, train_wall=5, gb_free=77.9, wall=787 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:26:46]    INFO >> epoch 004:    990 / 1539 loss=3.858, wps=6081.5, ups=8, wpb=759.9, bsz=759.9, num_updates=5600, lr=0.000376, gnorm=5.862, clip=0, train_wall=6, gb_free=76.8, wall=793 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:26:52]    INFO >> epoch 004:   1040 / 1539 loss=4.082, wps=5290.9, ups=8.74, wpb=605.5, bsz=605.5, num_updates=5650, lr=0.000376, gnorm=5.048, clip=0, train_wall=5, gb_free=76.2, wall=799 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:26:58]    INFO >> epoch 004:   1090 / 1539 loss=4.012, wps=6147, ups=8.04, wpb=764.8, bsz=764.8, num_updates=5700, lr=0.000376, gnorm=5.765, clip=0, train_wall=6, gb_free=76.4, wall=805 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:27:04]    INFO >> epoch 004:   1140 / 1539 loss=3.958, wps=5351.8, ups=8.56, wpb=625.5, bsz=625.5, num_updates=5750, lr=0.000376, gnorm=4.897, clip=0, train_wall=5, gb_free=78, wall=811 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:27:12]    INFO >> epoch 004:   1190 / 1539 loss=3.982, wps=6034.8, ups=7.83, wpb=771, bsz=771, num_updates=5800, lr=0.000376, gnorm=5.713, clip=0, train_wall=6, gb_free=75.2, wall=817 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:27:17]    INFO >> epoch 004:   1240 / 1539 loss=3.852, wps=5756, ups=8.85, wpb=650.4, bsz=650.4, num_updates=5850, lr=0.000376, gnorm=5.039, clip=0, train_wall=5, gb_free=76.5, wall=823 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:27:23]    INFO >> epoch 004:   1290 / 1539 loss=3.926, wps=5270.9, ups=8.21, wpb=641.7, bsz=641.7, num_updates=5900, lr=0.000376, gnorm=4.992, clip=0, train_wall=6, gb_free=77.6, wall=829 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:27:29]    INFO >> epoch 004:   1340 / 1539 loss=3.892, wps=6250.9, ups=8.37, wpb=746.5, bsz=746.5, num_updates=5950, lr=0.000376, gnorm=5.204, clip=0, train_wall=5, gb_free=76.2, wall=835 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:27:36]    INFO >> epoch 004:   1390 / 1539 loss=3.78, wps=6439.8, ups=7.9, wpb=815.3, bsz=815.3, num_updates=6000, lr=0.000376, gnorm=5.765, clip=0, train_wall=6, gb_free=76.4, wall=841 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:27:37] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 57.25 MiB is free. Including non-PyTorch memory, this process has 79.06 GiB memory in use. Of the allocated memory 76.61 GiB is allocated by PyTorch, and 1.95 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:27:37] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:27:37] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:27:37] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 8            |        cudaMalloc retries: 13        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78390 MiB |  78450 MiB | 101000 GiB | 100923 GiB |
|       from large pool |  78035 MiB |  78095 MiB |  99886 GiB |  99810 GiB |
|       from small pool |    354 MiB |    355 MiB |   1113 GiB |   1113 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78390 MiB |  78450 MiB | 101000 GiB | 100923 GiB |
|       from large pool |  78035 MiB |  78095 MiB |  99886 GiB |  99810 GiB |
|       from small pool |    354 MiB |    355 MiB |   1113 GiB |   1113 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78043 MiB |  78102 MiB | 100482 GiB | 100406 GiB |
|       from large pool |  77690 MiB |  77750 MiB |  99370 GiB |  99294 GiB |
|       from small pool |    352 MiB |    353 MiB |   1111 GiB |   1111 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80450 MiB |  80468 MiB | 253090 MiB | 172640 MiB |
|       from large pool |  80058 MiB |  80076 MiB | 251300 MiB | 171242 MiB |
|       from small pool |    392 MiB |    392 MiB |   1790 MiB |   1398 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1999 MiB |   6220 MiB |  77422 GiB |  77420 GiB |
|       from large pool |   1962 MiB |   6216 MiB |  76123 GiB |  76121 GiB |
|       from small pool |     37 MiB |     38 MiB |   1298 GiB |   1298 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6735    |    6738    |   12870 K  |   12863 K  |
|       from large pool |     894    |     895    |    5886 K  |    5885 K  |
|       from small pool |    5841    |    5844    |    6983 K  |    6977 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    6735    |    6738    |   12870 K  |   12863 K  |
|       from large pool |     894    |     895    |    5886 K  |    5885 K  |
|       from small pool |    5841    |    5844    |    6983 K  |    6977 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     811    |     812    |    3710    |    2899    |
|       from large pool |     615    |     616    |    2815    |    2200    |
|       from small pool |     196    |     196    |     895    |     699    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     517    |     517    |    7574 K  |    7574 K  |
|       from large pool |     162    |     162    |    4260 K  |    4260 K  |
|       from small pool |     355    |     355    |    3314 K  |    3313 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:27:37] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:27:37] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:27:44]    INFO >> epoch 004:   1441 / 1539 loss=3.917, wps=5476.5, ups=7.32, wpb=748.2, bsz=748.2, num_updates=6050, lr=0.000376, gnorm=5.335, clip=0, train_wall=6, gb_free=75.9, wall=848 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:27:50]    INFO >> epoch 004:   1491 / 1539 loss=3.964, wps=5682.9, ups=8.18, wpb=694.8, bsz=694.8, num_updates=6100, lr=0.000376, gnorm=5.134, clip=0, train_wall=6, gb_free=75.9, wall=854 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:27:56]    INFO >> epoch 004 | loss 3.948 | wps 5555.1 | ups 7.71 | wpb 720.8 | bsz 720.8 | num_updates 6148 | lr 0.000376 | gnorm 5.635 | clip 0.1 | train_wall 173 | gb_free 74.9 | wall 861 (progress_bar.py:267, print())[0m
[33m[2025-11-21 00:27:56] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:28:05]    INFO >> epoch 004 | valid on 'valid' subset | loss 3.953 | wps 17068.4 | wpb 5412.5 | bsz 5412.5 | num_updates 6148 | best_loss 5.506 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 00:28:06]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 00:28:06]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/checkpoints/checkpoint_last.pt (epoch 4 @ 6148 updates, score 3.953) (writing took 0.009196 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 00:28:06] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:28:06]    INFO >> epoch 005:      2 / 1539 loss=3.536, wps=2507.7, ups=3.12, wpb=803.1, bsz=803.1, num_updates=6150, lr=0.000354, gnorm=5.309, clip=0, train_wall=6, gb_free=75.6, wall=870 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:28:13]    INFO >> epoch 005:     52 / 1539 loss=3.971, wps=5410.4, ups=7.68, wpb=704.7, bsz=704.7, num_updates=6200, lr=0.000354, gnorm=4.936, clip=0, train_wall=6, gb_free=75.2, wall=877 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:28:20]    INFO >> epoch 005:    102 / 1539 loss=3.937, wps=5837.1, ups=8.48, wpb=688.6, bsz=688.6, num_updates=6250, lr=0.000354, gnorm=5.285, clip=0, train_wall=5, gb_free=74.5, wall=883 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:28:26]    INFO >> epoch 005:    152 / 1539 loss=3.936, wps=5527.6, ups=8.29, wpb=667.1, bsz=667.1, num_updates=6300, lr=0.000354, gnorm=4.901, clip=0, train_wall=6, gb_free=78.3, wall=889 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:28:32]    INFO >> epoch 005:    202 / 1539 loss=3.866, wps=5727.3, ups=8.45, wpb=677.9, bsz=677.9, num_updates=6350, lr=0.000354, gnorm=5.979, clip=0, train_wall=5, gb_free=76.1, wall=895 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:28:38]    INFO >> epoch 005:    252 / 1539 loss=3.77, wps=5921.1, ups=7.48, wpb=791.4, bsz=791.4, num_updates=6400, lr=0.000354, gnorm=5.53, clip=0, train_wall=6, gb_free=77.8, wall=901 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:28:45]    INFO >> epoch 005:    302 / 1539 loss=3.891, wps=6163.1, ups=7.77, wpb=793.5, bsz=793.5, num_updates=6450, lr=0.000354, gnorm=5.547, clip=0, train_wall=6, gb_free=76.5, wall=908 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:28:53]    INFO >> epoch 005:    352 / 1539 loss=3.543, wps=6086.3, ups=7.92, wpb=768.4, bsz=768.4, num_updates=6500, lr=0.000354, gnorm=5.055, clip=0, train_wall=6, gb_free=77.5, wall=914 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:28:59]    INFO >> epoch 005:    402 / 1539 loss=3.499, wps=6398.2, ups=7.38, wpb=867.3, bsz=867.3, num_updates=6550, lr=0.000354, gnorm=5.627, clip=2, train_wall=6, gb_free=76.7, wall=921 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:29:05]    INFO >> epoch 005:    452 / 1539 loss=3.903, wps=5038, ups=8.6, wpb=585.7, bsz=585.7, num_updates=6600, lr=0.000354, gnorm=4.996, clip=0, train_wall=5, gb_free=75.8, wall=927 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:29:11]    INFO >> epoch 005:    502 / 1539 loss=3.865, wps=5901.8, ups=8.18, wpb=721.4, bsz=721.4, num_updates=6650, lr=0.000354, gnorm=4.773, clip=0, train_wall=6, gb_free=78.5, wall=933 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:29:23]    INFO >> epoch 005:    552 / 1539 loss=3.862, wps=3117.7, ups=4.65, wpb=670, bsz=670, num_updates=6700, lr=0.000354, gnorm=4.941, clip=0, train_wall=10, gb_free=74.8, wall=944 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:29:28] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 51.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 77.12 GiB is allocated by PyTorch, and 1.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:29:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:29:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:29:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 9            |        cudaMalloc retries: 14        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78908 MiB |  78967 MiB | 114959 GiB | 114882 GiB |
|       from large pool |  78459 MiB |  78518 MiB | 113676 GiB | 113600 GiB |
|       from small pool |    448 MiB |    450 MiB |   1282 GiB |   1282 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78908 MiB |  78967 MiB | 114959 GiB | 114882 GiB |
|       from large pool |  78459 MiB |  78518 MiB | 113676 GiB | 113600 GiB |
|       from small pool |    448 MiB |    450 MiB |   1282 GiB |   1282 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78657 MiB |  78717 MiB | 114375 GiB | 114298 GiB |
|       from large pool |  78210 MiB |  78270 MiB | 113094 GiB | 113017 GiB |
|       from small pool |    446 MiB |    448 MiB |   1280 GiB |   1280 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80456 MiB |  80458 MiB | 253158 MiB | 172702 MiB |
|       from large pool |  79998 MiB |  79998 MiB | 251300 MiB | 171302 MiB |
|       from small pool |    458 MiB |    460 MiB |   1858 MiB |   1400 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1547 MiB |   4073 MiB |  87226 GiB |  87225 GiB |
|       from large pool |   1538 MiB |   4069 MiB |  85733 GiB |  85732 GiB |
|       from small pool |      9 MiB |     24 MiB |   1492 GiB |   1492 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    8418    |    8421    |   14686 K  |   14677 K  |
|       from large pool |    1041    |    1042    |    6637 K  |    6636 K  |
|       from small pool |    7377    |    7380    |    8048 K  |    8041 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    8418    |    8421    |   14686 K  |   14677 K  |
|       from large pool |    1041    |    1042    |    6637 K  |    6636 K  |
|       from small pool |    7377    |    7380    |    8048 K  |    8041 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     843    |     844    |    3744    |    2901    |
|       from large pool |     614    |     614    |    2815    |    2201    |
|       from small pool |     229    |     230    |     929    |     700    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     350    |     351    |    8669 K  |    8668 K  |
|       from large pool |     159    |     159    |    4804 K  |    4803 K  |
|       from small pool |     191    |     192    |    3865 K  |    3864 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:29:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:29:28] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:29:30]    INFO >> epoch 005:    603 / 1539 loss=3.87, wps=5196.9, ups=7.25, wpb=716.7, bsz=716.7, num_updates=6750, lr=0.000354, gnorm=5.036, clip=0, train_wall=6, gb_free=76.5, wall=950 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:29:37]    INFO >> epoch 005:    653 / 1539 loss=3.859, wps=6124.3, ups=7.93, wpb=772.1, bsz=772.1, num_updates=6800, lr=0.000354, gnorm=5.171, clip=0, train_wall=6, gb_free=77, wall=957 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:29:43]    INFO >> epoch 005:    703 / 1539 loss=3.795, wps=5734.4, ups=8.02, wpb=715.3, bsz=715.3, num_updates=6850, lr=0.000354, gnorm=5.858, clip=0, train_wall=6, gb_free=73.7, wall=963 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:29:49]    INFO >> epoch 005:    753 / 1539 loss=3.972, wps=6030.1, ups=8.33, wpb=723.6, bsz=723.6, num_updates=6900, lr=0.000354, gnorm=5.444, clip=0, train_wall=5, gb_free=76, wall=969 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:29:57]    INFO >> epoch 005:    803 / 1539 loss=3.887, wps=5323.8, ups=7.75, wpb=686.6, bsz=686.6, num_updates=6950, lr=0.000354, gnorm=5.353, clip=0, train_wall=6, gb_free=74.9, wall=975 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:30:03]    INFO >> epoch 005:    853 / 1539 loss=3.916, wps=5171.7, ups=8.14, wpb=635.5, bsz=635.5, num_updates=7000, lr=0.000354, gnorm=5.273, clip=0, train_wall=6, gb_free=74.9, wall=982 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:30:09]    INFO >> epoch 005:    903 / 1539 loss=3.755, wps=5430.8, ups=7.71, wpb=704.8, bsz=704.8, num_updates=7050, lr=0.000354, gnorm=5.278, clip=0, train_wall=6, gb_free=72.8, wall=988 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:30:16]    INFO >> epoch 005:    953 / 1539 loss=3.995, wps=5446.2, ups=7.42, wpb=734.4, bsz=734.4, num_updates=7100, lr=0.000354, gnorm=5.05, clip=0, train_wall=6, gb_free=75.2, wall=995 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:30:23]    INFO >> epoch 005:   1003 / 1539 loss=3.554, wps=6834.2, ups=7.25, wpb=943.3, bsz=943.3, num_updates=7150, lr=0.000354, gnorm=5.505, clip=0, train_wall=6, gb_free=76.3, wall=1002 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:30:31]    INFO >> epoch 005:   1053 / 1539 loss=3.822, wps=5750.7, ups=7.44, wpb=772.8, bsz=772.8, num_updates=7200, lr=0.000354, gnorm=5.078, clip=0, train_wall=6, gb_free=74.9, wall=1008 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:30:37]    INFO >> epoch 005:   1103 / 1539 loss=3.784, wps=5358.6, ups=8.02, wpb=667.9, bsz=667.9, num_updates=7250, lr=0.000354, gnorm=5.35, clip=0, train_wall=6, gb_free=74.1, wall=1015 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:30:40] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 9.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 75.53 GiB is allocated by PyTorch, and 3.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:30:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:30:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:30:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 10           |        cudaMalloc retries: 16        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77283 MiB |  77343 MiB | 123520 GiB | 123445 GiB |
|       from large pool |  76939 MiB |  76999 MiB | 122148 GiB | 122073 GiB |
|       from small pool |    343 MiB |    344 MiB |   1372 GiB |   1371 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77283 MiB |  77343 MiB | 123520 GiB | 123445 GiB |
|       from large pool |  76939 MiB |  76999 MiB | 122148 GiB | 122073 GiB |
|       from small pool |    343 MiB |    344 MiB |   1372 GiB |   1371 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76965 MiB |  77024 MiB | 122893 GiB | 122817 GiB |
|       from large pool |  76624 MiB |  76683 MiB | 121522 GiB | 121447 GiB |
|       from small pool |    341 MiB |    342 MiB |   1370 GiB |   1369 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80498 MiB |  80498 MiB | 253280 MiB | 172782 MiB |
|       from large pool |  80118 MiB |  80118 MiB | 251420 MiB | 171302 MiB |
|       from small pool |    380 MiB |    458 MiB |   1860 MiB |   1480 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3154 MiB |   6329 MiB |  93921 GiB |  93918 GiB |
|       from large pool |   3118 MiB |   6296 MiB |  92322 GiB |  92319 GiB |
|       from small pool |     36 MiB |     37 MiB |   1599 GiB |   1599 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6537    |    6540    |   15765 K  |   15759 K  |
|       from large pool |     876    |     877    |    7154 K  |    7153 K  |
|       from small pool |    5661    |    5664    |    8611 K  |    8605 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    6537    |    6540    |   15765 K  |   15759 K  |
|       from large pool |     876    |     877    |    7154 K  |    7153 K  |
|       from small pool |    5661    |    5664    |    8611 K  |    8605 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     806    |     843    |    3747    |    2941    |
|       from large pool |     616    |     616    |    2817    |    2201    |
|       from small pool |     190    |     229    |     930    |     740    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     547    |     548    |    9300 K  |    9300 K  |
|       from large pool |     203    |     203    |    5176 K  |    5176 K  |
|       from small pool |     344    |     345    |    4124 K  |    4124 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:30:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:30:40] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:30:44]    INFO >> epoch 005:   1154 / 1539 loss=3.885, wps=5492.8, ups=7.2, wpb=762.8, bsz=762.8, num_updates=7300, lr=0.000354, gnorm=4.963, clip=0, train_wall=6, gb_free=75, wall=1022 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:30:50]    INFO >> epoch 005:   1204 / 1539 loss=3.877, wps=5023.4, ups=8.25, wpb=608.9, bsz=608.9, num_updates=7350, lr=0.000354, gnorm=5.252, clip=2, train_wall=6, gb_free=76.3, wall=1028 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:30:56]    INFO >> epoch 005:   1254 / 1539 loss=3.791, wps=5839.4, ups=8.44, wpb=691.6, bsz=691.6, num_updates=7400, lr=0.000354, gnorm=4.874, clip=0, train_wall=5, gb_free=77.3, wall=1034 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:31:03]    INFO >> epoch 005:   1304 / 1539 loss=3.844, wps=5391.4, ups=8.26, wpb=652.5, bsz=652.5, num_updates=7450, lr=0.000354, gnorm=5.081, clip=0, train_wall=6, gb_free=73, wall=1040 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:31:10]    INFO >> epoch 005:   1354 / 1539 loss=3.799, wps=5767.7, ups=8.14, wpb=708.9, bsz=708.9, num_updates=7500, lr=0.000354, gnorm=5.03, clip=0, train_wall=6, gb_free=75.8, wall=1046 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:31:16]    INFO >> epoch 005:   1404 / 1539 loss=3.895, wps=5438, ups=8.32, wpb=653.7, bsz=653.7, num_updates=7550, lr=0.000354, gnorm=4.768, clip=0, train_wall=6, gb_free=75, wall=1052 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:31:22]    INFO >> epoch 005:   1454 / 1539 loss=3.885, wps=5451.5, ups=8.04, wpb=677.9, bsz=677.9, num_updates=7600, lr=0.000354, gnorm=5.143, clip=0, train_wall=6, gb_free=76.6, wall=1058 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:31:28]    INFO >> epoch 005:   1504 / 1539 loss=3.696, wps=6102.9, ups=7.57, wpb=805.9, bsz=805.9, num_updates=7650, lr=0.000354, gnorm=5.257, clip=0, train_wall=6, gb_free=76.9, wall=1065 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:31:33]    INFO >> epoch 005 | loss 3.822 | wps 5316.5 | ups 7.38 | wpb 720.8 | bsz 720.8 | num_updates 7685 | lr 0.000354 | gnorm 5.211 | clip 0.1 | train_wall 181 | gb_free 71 | wall 1069 (progress_bar.py:267, print())[0m
[33m[2025-11-21 00:31:33] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:31:44]    INFO >> epoch 005 | valid on 'valid' subset | loss 3.89 | wps 16025.8 | wpb 5412.5 | bsz 5412.5 | num_updates 7685 | best_loss 5.506 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 00:31:44]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 00:31:44]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/checkpoints/checkpoint_last.pt (epoch 5 @ 7685 updates, score 3.89) (writing took 0.011951 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 00:31:44] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:31:46]    INFO >> epoch 006:     15 / 1539 loss=3.778, wps=2385.5, ups=3.08, wpb=773.7, bsz=773.7, num_updates=7700, lr=0.000327, gnorm=5.052, clip=0, train_wall=6, gb_free=74.8, wall=1081 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:31:52]    INFO >> epoch 006:     65 / 1539 loss=3.725, wps=6309.7, ups=7.82, wpb=806.6, bsz=806.6, num_updates=7750, lr=0.000327, gnorm=5.134, clip=0, train_wall=6, gb_free=75, wall=1087 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:31:59]    INFO >> epoch 006:    115 / 1539 loss=3.792, wps=5886.5, ups=8, wpb=736.2, bsz=736.2, num_updates=7800, lr=0.000327, gnorm=5.321, clip=0, train_wall=6, gb_free=78, wall=1093 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:32:05]    INFO >> epoch 006:    165 / 1539 loss=3.83, wps=5505.1, ups=7.9, wpb=696.7, bsz=696.7, num_updates=7850, lr=0.000327, gnorm=4.512, clip=0, train_wall=6, gb_free=76.6, wall=1100 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:32:08] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 9.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 75.53 GiB is allocated by PyTorch, and 3.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:32:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:32:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:32:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 11           |        cudaMalloc retries: 17        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77283 MiB |  77343 MiB | 135202 GiB | 135127 GiB |
|       from large pool |  76939 MiB |  76999 MiB | 133696 GiB | 133621 GiB |
|       from small pool |    343 MiB |    344 MiB |   1506 GiB |   1506 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77283 MiB |  77343 MiB | 135202 GiB | 135127 GiB |
|       from large pool |  76939 MiB |  76999 MiB | 133696 GiB | 133621 GiB |
|       from small pool |    343 MiB |    344 MiB |   1506 GiB |   1506 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76965 MiB |  77024 MiB | 134521 GiB | 134446 GiB |
|       from large pool |  76624 MiB |  76683 MiB | 133017 GiB | 132942 GiB |
|       from small pool |    341 MiB |    342 MiB |   1504 GiB |   1503 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80498 MiB |  80500 MiB | 253342 MiB | 172844 MiB |
|       from large pool |  80118 MiB |  80118 MiB | 251480 MiB | 171362 MiB |
|       from small pool |    380 MiB |    382 MiB |   1862 MiB |   1482 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3154 MiB |   6330 MiB | 102074 GiB | 102071 GiB |
|       from large pool |   3118 MiB |   6296 MiB | 100323 GiB | 100320 GiB |
|       from small pool |     36 MiB |     38 MiB |   1750 GiB |   1750 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6537    |    6540    |   17220 K  |   17213 K  |
|       from large pool |     876    |     877    |    7766 K  |    7765 K  |
|       from small pool |    5661    |    5664    |    9453 K  |    9448 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    6537    |    6540    |   17220 K  |   17213 K  |
|       from large pool |     876    |     877    |    7766 K  |    7765 K  |
|       from small pool |    5661    |    5664    |    9453 K  |    9448 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     806    |     807    |    3749    |    2943    |
|       from large pool |     616    |     616    |    2818    |    2202    |
|       from small pool |     190    |     191    |     931    |     741    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     553    |     553    |   10166 K  |   10165 K  |
|       from large pool |     203    |     203    |    5621 K  |    5621 K  |
|       from small pool |     350    |     350    |    4544 K  |    4544 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:32:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:32:08] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:32:13]    INFO >> epoch 006:    216 / 1539 loss=3.762, wps=5185.9, ups=7.35, wpb=705.9, bsz=705.9, num_updates=7900, lr=0.000327, gnorm=4.871, clip=0, train_wall=6, gb_free=73.7, wall=1107 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:32:14] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 1.25 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 76.06 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:32:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:32:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:32:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 12           |        cudaMalloc retries: 18        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77886 MiB |  77946 MiB | 135885 GiB | 135809 GiB |
|       from large pool |  77448 MiB |  77507 MiB | 134369 GiB | 134293 GiB |
|       from small pool |    438 MiB |    439 MiB |   1516 GiB |   1515 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77886 MiB |  77946 MiB | 135885 GiB | 135809 GiB |
|       from large pool |  77448 MiB |  77507 MiB | 134369 GiB | 134293 GiB |
|       from small pool |    438 MiB |    439 MiB |   1516 GiB |   1515 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77635 MiB |  77695 MiB | 135200 GiB | 135124 GiB |
|       from large pool |  77199 MiB |  77258 MiB | 133686 GiB | 133611 GiB |
|       from small pool |    436 MiB |    437 MiB |   1513 GiB |   1513 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80506 MiB |  80506 MiB | 253410 MiB | 172904 MiB |
|       from large pool |  80058 MiB |  80058 MiB | 251480 MiB | 171422 MiB |
|       from small pool |    448 MiB |    448 MiB |   1930 MiB |   1482 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2619 MiB |   4071 MiB | 102597 GiB | 102594 GiB |
|       from large pool |   2609 MiB |   4069 MiB | 100834 GiB | 100832 GiB |
|       from small pool |      9 MiB |     26 MiB |   1762 GiB |   1762 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    8227    |    8228    |   17316 K  |   17308 K  |
|       from large pool |    1024    |    1025    |    7801 K  |    7800 K  |
|       from small pool |    7203    |    7204    |    9515 K  |    9508 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    8227    |    8228    |   17316 K  |   17308 K  |
|       from large pool |    1024    |    1025    |    7801 K  |    7800 K  |
|       from small pool |    7203    |    7204    |    9515 K  |    9508 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     839    |     839    |    3783    |    2944    |
|       from large pool |     615    |     615    |    2818    |    2203    |
|       from small pool |     224    |     224    |     965    |     741    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     341    |     344    |   10226 K  |   10226 K  |
|       from large pool |     159    |     159    |    5646 K  |    5645 K  |
|       from small pool |     182    |     185    |    4580 K  |    4580 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:32:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:32:14] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:32:20]    INFO >> epoch 006:    267 / 1539 loss=3.887, wps=4829.6, ups=7.31, wpb=660.9, bsz=660.9, num_updates=7950, lr=0.000327, gnorm=4.477, clip=0, train_wall=6, gb_free=76.9, wall=1113 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:32:26]    INFO >> epoch 006:    317 / 1539 loss=3.705, wps=5842.6, ups=8.43, wpb=693.3, bsz=693.3, num_updates=8000, lr=0.000327, gnorm=5.464, clip=0, train_wall=5, gb_free=75.3, wall=1119 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:32:32]    INFO >> epoch 006:    367 / 1539 loss=3.537, wps=6036, ups=7.74, wpb=779.8, bsz=779.8, num_updates=8050, lr=0.000327, gnorm=5.288, clip=2, train_wall=6, gb_free=76.1, wall=1126 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:32:38]    INFO >> epoch 006:    417 / 1539 loss=3.722, wps=5814.1, ups=8.42, wpb=690.6, bsz=690.6, num_updates=8100, lr=0.000327, gnorm=4.871, clip=0, train_wall=5, gb_free=76.1, wall=1132 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:32:47]    INFO >> epoch 006:    467 / 1539 loss=3.787, wps=5834.5, ups=6.99, wpb=835, bsz=835, num_updates=8150, lr=0.000327, gnorm=5.28, clip=0, train_wall=6, gb_free=78.1, wall=1139 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:32:53]    INFO >> epoch 006:    517 / 1539 loss=3.743, wps=5726.7, ups=7.98, wpb=717.3, bsz=717.3, num_updates=8200, lr=0.000327, gnorm=4.619, clip=0, train_wall=6, gb_free=74.1, wall=1145 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:32:59]    INFO >> epoch 006:    567 / 1539 loss=3.837, wps=6207.5, ups=7.86, wpb=790.1, bsz=790.1, num_updates=8250, lr=0.000327, gnorm=4.846, clip=0, train_wall=6, gb_free=75.1, wall=1152 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:33:05]    INFO >> epoch 006:    617 / 1539 loss=3.787, wps=5352.1, ups=8.31, wpb=644.4, bsz=644.4, num_updates=8300, lr=0.000327, gnorm=4.173, clip=0, train_wall=5, gb_free=76.4, wall=1158 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:33:12]    INFO >> epoch 006:    667 / 1539 loss=3.796, wps=5847.9, ups=7.9, wpb=740.3, bsz=740.3, num_updates=8350, lr=0.000327, gnorm=4.916, clip=0, train_wall=6, gb_free=75.6, wall=1164 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:33:19]    INFO >> epoch 006:    717 / 1539 loss=3.803, wps=5482.4, ups=8.74, wpb=627.3, bsz=627.3, num_updates=8400, lr=0.000327, gnorm=4.312, clip=0, train_wall=5, gb_free=77, wall=1170 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:33:26]    INFO >> epoch 006:    767 / 1539 loss=3.461, wps=6044.5, ups=7.26, wpb=832.6, bsz=832.6, num_updates=8450, lr=0.000327, gnorm=4.723, clip=0, train_wall=6, gb_free=74.5, wall=1177 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:33:32]    INFO >> epoch 006:    817 / 1539 loss=3.853, wps=5036.4, ups=8.38, wpb=601.3, bsz=601.3, num_updates=8500, lr=0.000327, gnorm=4.331, clip=0, train_wall=5, gb_free=76, wall=1182 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:33:38]    INFO >> epoch 006:    867 / 1539 loss=3.68, wps=6012.5, ups=8.21, wpb=732.2, bsz=732.2, num_updates=8550, lr=0.000327, gnorm=4.979, clip=0, train_wall=6, gb_free=76.8, wall=1189 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:33:44]    INFO >> epoch 006:    917 / 1539 loss=3.792, wps=5597.1, ups=7.77, wpb=720.8, bsz=720.8, num_updates=8600, lr=0.000327, gnorm=4.875, clip=0, train_wall=6, gb_free=70.3, wall=1195 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:33:53]    INFO >> epoch 006:    967 / 1539 loss=3.792, wps=6070.7, ups=7.2, wpb=843.1, bsz=843.1, num_updates=8650, lr=0.000327, gnorm=4.821, clip=0, train_wall=6, gb_free=74.9, wall=1202 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:33:59]    INFO >> epoch 006:   1017 / 1539 loss=3.762, wps=5877.7, ups=8.16, wpb=719.9, bsz=719.9, num_updates=8700, lr=0.000327, gnorm=4.378, clip=0, train_wall=6, gb_free=70.4, wall=1208 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:34:06]    INFO >> epoch 006:   1067 / 1539 loss=3.911, wps=5224.5, ups=6.84, wpb=763.3, bsz=763.3, num_updates=8750, lr=0.000327, gnorm=5.045, clip=0, train_wall=7, gb_free=73.6, wall=1215 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:34:13]    INFO >> epoch 006:   1117 / 1539 loss=3.723, wps=5488.3, ups=7.6, wpb=721.9, bsz=721.9, num_updates=8800, lr=0.000327, gnorm=4.844, clip=0, train_wall=6, gb_free=75.8, wall=1222 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:34:19]    INFO >> epoch 006:   1167 / 1539 loss=3.787, wps=5939.9, ups=7.74, wpb=767.6, bsz=767.6, num_updates=8850, lr=0.000327, gnorm=4.837, clip=0, train_wall=6, gb_free=76, wall=1228 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:34:27]    INFO >> epoch 006:   1217 / 1539 loss=3.801, wps=5370, ups=7.83, wpb=685.6, bsz=685.6, num_updates=8900, lr=0.000327, gnorm=5.23, clip=0, train_wall=6, gb_free=73.6, wall=1235 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:34:33]    INFO >> epoch 006:   1267 / 1539 loss=3.784, wps=5044, ups=7.83, wpb=643.8, bsz=643.8, num_updates=8950, lr=0.000327, gnorm=4.718, clip=0, train_wall=6, gb_free=76.6, wall=1241 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:34:39]    INFO >> epoch 006:   1317 / 1539 loss=3.707, wps=5872.2, ups=7.92, wpb=741.6, bsz=741.6, num_updates=9000, lr=0.000327, gnorm=4.632, clip=0, train_wall=6, gb_free=76, wall=1248 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:34:45]    INFO >> epoch 006:   1367 / 1539 loss=3.807, wps=5172.4, ups=8.53, wpb=606.1, bsz=606.1, num_updates=9050, lr=0.000327, gnorm=4.402, clip=0, train_wall=5, gb_free=76, wall=1253 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:34:51]    INFO >> epoch 006:   1417 / 1539 loss=3.763, wps=5956, ups=8.16, wpb=729.8, bsz=729.8, num_updates=9100, lr=0.000327, gnorm=4.856, clip=0, train_wall=6, gb_free=76.2, wall=1260 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:34:59]    INFO >> epoch 006:   1467 / 1539 loss=3.604, wps=5771.6, ups=8.02, wpb=719.8, bsz=719.8, num_updates=9150, lr=0.000327, gnorm=4.465, clip=2, train_wall=6, gb_free=74.6, wall=1266 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:35:05]    INFO >> epoch 006:   1517 / 1539 loss=3.767, wps=5701.3, ups=8.24, wpb=692, bsz=692, num_updates=9200, lr=0.000327, gnorm=4.58, clip=0, train_wall=6, gb_free=76.2, wall=1272 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:35:08]    INFO >> epoch 006 | loss 3.755 | wps 5394 | ups 7.48 | wpb 720.8 | bsz 720.8 | num_updates 9222 | lr 0.000327 | gnorm 4.802 | clip 0.1 | train_wall 177 | gb_free 76 | wall 1274 (progress_bar.py:267, print())[0m
[33m[2025-11-21 00:35:08] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:35:17]    INFO >> epoch 006 | valid on 'valid' subset | loss 3.879 | wps 16975 | wpb 5412.5 | bsz 5412.5 | num_updates 9222 | best_loss 5.506 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 00:35:17]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 00:35:17]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/checkpoints/checkpoint_last.pt (epoch 6 @ 9222 updates, score 3.879) (writing took 0.009101 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 00:35:17] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:35:21]    INFO >> epoch 007:     28 / 1539 loss=3.901, wps=2129.2, ups=3.12, wpb=682, bsz=682, num_updates=9250, lr=0.000295, gnorm=5.279, clip=0, train_wall=6, gb_free=75.8, wall=1288 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:35:28]    INFO >> epoch 007:     78 / 1539 loss=3.774, wps=5559.2, ups=7.47, wpb=744.2, bsz=744.2, num_updates=9300, lr=0.000295, gnorm=4.511, clip=0, train_wall=6, gb_free=77.5, wall=1295 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:35:35]    INFO >> epoch 007:    128 / 1539 loss=3.766, wps=6000.9, ups=8.4, wpb=714.6, bsz=714.6, num_updates=9350, lr=0.000295, gnorm=4.777, clip=0, train_wall=5, gb_free=76.2, wall=1300 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:35:42]    INFO >> epoch 007:    178 / 1539 loss=3.497, wps=6202.9, ups=7.63, wpb=813.1, bsz=813.1, num_updates=9400, lr=0.000295, gnorm=4.583, clip=0, train_wall=6, gb_free=75.8, wall=1307 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:35:48]    INFO >> epoch 007:    228 / 1539 loss=3.754, wps=6513.7, ups=7.81, wpb=833.7, bsz=833.7, num_updates=9450, lr=0.000295, gnorm=5.121, clip=0, train_wall=6, gb_free=76.5, wall=1313 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:35:54]    INFO >> epoch 007:    278 / 1539 loss=3.714, wps=5782.7, ups=7.84, wpb=738, bsz=738, num_updates=9500, lr=0.000295, gnorm=5.057, clip=0, train_wall=6, gb_free=73.9, wall=1320 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:36:01] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 1.25 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 76.06 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:36:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:36:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:36:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 13           |        cudaMalloc retries: 19        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77886 MiB |  77946 MiB | 164008 GiB | 163932 GiB |
|       from large pool |  77448 MiB |  77507 MiB | 162182 GiB | 162106 GiB |
|       from small pool |    438 MiB |    439 MiB |   1826 GiB |   1825 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77886 MiB |  77946 MiB | 164008 GiB | 163932 GiB |
|       from large pool |  77448 MiB |  77507 MiB | 162182 GiB | 162106 GiB |
|       from small pool |    438 MiB |    439 MiB |   1826 GiB |   1825 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77635 MiB |  77695 MiB | 163185 GiB | 163110 GiB |
|       from large pool |  77199 MiB |  77258 MiB | 161362 GiB | 161286 GiB |
|       from small pool |    436 MiB |    437 MiB |   1823 GiB |   1823 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80506 MiB |  80506 MiB | 253410 MiB | 172904 MiB |
|       from large pool |  80058 MiB |  80058 MiB | 251480 MiB | 171422 MiB |
|       from small pool |    448 MiB |    448 MiB |   1930 MiB |   1482 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2619 MiB |   4073 MiB | 123549 GiB | 123546 GiB |
|       from large pool |   2609 MiB |   4069 MiB | 121425 GiB | 121422 GiB |
|       from small pool |      9 MiB |     20 MiB |   2124 GiB |   2124 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    8222    |    8223    |   20876 K  |   20868 K  |
|       from large pool |    1024    |    1025    |    9416 K  |    9415 K  |
|       from small pool |    7198    |    7200    |   11459 K  |   11452 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    8222    |    8223    |   20876 K  |   20868 K  |
|       from large pool |    1024    |    1025    |    9416 K  |    9415 K  |
|       from small pool |    7198    |    7200    |   11459 K  |   11452 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     839    |     839    |    3783    |    2944    |
|       from large pool |     615    |     615    |    2818    |    2203    |
|       from small pool |     224    |     224    |     965    |     741    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     347    |     348    |   12327 K  |   12326 K  |
|       from large pool |     159    |     159    |    6814 K  |    6813 K  |
|       from small pool |     188    |     189    |    5513 K  |    5512 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:36:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:36:01] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:36:01]    INFO >> epoch 007:    329 / 1539 loss=3.749, wps=4635.1, ups=7.33, wpb=632, bsz=632, num_updates=9550, lr=0.000295, gnorm=4.209, clip=0, train_wall=6, gb_free=76.9, wall=1327 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:36:09]    INFO >> epoch 007:    379 / 1539 loss=3.878, wps=4794.3, ups=7.71, wpb=622.1, bsz=622.1, num_updates=9600, lr=0.000295, gnorm=4.535, clip=0, train_wall=6, gb_free=78.5, wall=1333 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:36:15]    INFO >> epoch 007:    429 / 1539 loss=3.735, wps=4960.6, ups=7.9, wpb=628.2, bsz=628.2, num_updates=9650, lr=0.000295, gnorm=4.392, clip=0, train_wall=6, gb_free=76.6, wall=1339 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:36:21] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 9.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 75.53 GiB is allocated by PyTorch, and 3.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:36:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:36:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:36:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 14           |        cudaMalloc retries: 21        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77283 MiB |  77343 MiB | 166352 GiB | 166276 GiB |
|       from large pool |  76939 MiB |  76999 MiB | 164501 GiB | 164426 GiB |
|       from small pool |    343 MiB |    344 MiB |   1850 GiB |   1850 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77283 MiB |  77343 MiB | 166352 GiB | 166276 GiB |
|       from large pool |  76939 MiB |  76999 MiB | 164501 GiB | 164426 GiB |
|       from small pool |    343 MiB |    344 MiB |   1850 GiB |   1850 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76965 MiB |  77024 MiB | 165517 GiB | 165442 GiB |
|       from large pool |  76624 MiB |  76683 MiB | 163669 GiB | 163594 GiB |
|       from small pool |    341 MiB |    342 MiB |   1848 GiB |   1847 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80498 MiB |  80506 MiB | 253472 MiB | 172974 MiB |
|       from large pool |  80118 MiB |  80118 MiB | 251540 MiB | 171422 MiB |
|       from small pool |    380 MiB |    448 MiB |   1932 MiB |   1552 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3154 MiB |   6330 MiB | 125378 GiB | 125375 GiB |
|       from large pool |   3118 MiB |   6296 MiB | 123224 GiB | 123221 GiB |
|       from small pool |     36 MiB |     38 MiB |   2153 GiB |   2153 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6537    |    6540    |   21171 K  |   21165 K  |
|       from large pool |     876    |     877    |    9556 K  |    9555 K  |
|       from small pool |    5661    |    5664    |   11615 K  |   11609 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    6537    |    6540    |   21171 K  |   21165 K  |
|       from large pool |     876    |     877    |    9556 K  |    9555 K  |
|       from small pool |    5661    |    5664    |   11615 K  |   11609 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     806    |     839    |    3785    |    2979    |
|       from large pool |     616    |     616    |    2819    |    2203    |
|       from small pool |     190    |     224    |     966    |     776    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     552    |     552    |   12500 K  |   12500 K  |
|       from large pool |     203    |     203    |    6915 K  |    6914 K  |
|       from small pool |     349    |     349    |    5585 K  |    5585 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:36:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:36:21] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:36:22]    INFO >> epoch 007:    480 / 1539 loss=3.674, wps=5371.8, ups=7.1, wpb=757, bsz=757, num_updates=9700, lr=0.000295, gnorm=4.524, clip=0, train_wall=6, gb_free=71.9, wall=1346 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:36:29]    INFO >> epoch 007:    530 / 1539 loss=3.671, wps=5685.7, ups=7.93, wpb=717.2, bsz=717.2, num_updates=9750, lr=0.000295, gnorm=4.961, clip=0, train_wall=6, gb_free=75.5, wall=1353 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:36:35]    INFO >> epoch 007:    580 / 1539 loss=3.767, wps=5108.7, ups=7.97, wpb=640.7, bsz=640.7, num_updates=9800, lr=0.000295, gnorm=4.477, clip=0, train_wall=6, gb_free=76.1, wall=1359 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:36:43]    INFO >> epoch 007:    630 / 1539 loss=3.755, wps=5378.2, ups=7.8, wpb=689.7, bsz=689.7, num_updates=9850, lr=0.000295, gnorm=5.128, clip=0, train_wall=6, gb_free=77.2, wall=1365 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:36:49]    INFO >> epoch 007:    680 / 1539 loss=3.527, wps=6393.8, ups=7.58, wpb=844, bsz=844, num_updates=9900, lr=0.000295, gnorm=5.172, clip=0, train_wall=6, gb_free=74.9, wall=1372 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:36:56]    INFO >> epoch 007:    730 / 1539 loss=3.782, wps=5338.1, ups=7.42, wpb=719.2, bsz=719.2, num_updates=9950, lr=0.000295, gnorm=4.654, clip=0, train_wall=6, gb_free=76.4, wall=1379 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:37:03]    INFO >> epoch 007:    780 / 1539 loss=3.871, wps=4836.2, ups=7.72, wpb=626.6, bsz=626.6, num_updates=10000, lr=0.000295, gnorm=4.383, clip=0, train_wall=6, gb_free=76.1, wall=1385 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:37:09]    INFO >> epoch 007:    830 / 1539 loss=3.768, wps=5987.6, ups=8.04, wpb=745.1, bsz=745.1, num_updates=10050, lr=0.000295, gnorm=4.33, clip=0, train_wall=6, gb_free=75.1, wall=1392 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:37:16]    INFO >> epoch 007:    880 / 1539 loss=3.748, wps=5411.3, ups=8, wpb=676.3, bsz=676.3, num_updates=10100, lr=0.000295, gnorm=4.793, clip=0, train_wall=6, gb_free=75.3, wall=1398 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:37:22]    INFO >> epoch 007:    930 / 1539 loss=3.801, wps=5495.9, ups=8.47, wpb=649.1, bsz=649.1, num_updates=10150, lr=0.000295, gnorm=4.257, clip=0, train_wall=5, gb_free=77.4, wall=1404 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:37:29]    INFO >> epoch 007:    980 / 1539 loss=3.531, wps=6055.5, ups=7.76, wpb=780, bsz=780, num_updates=10200, lr=0.000295, gnorm=5.232, clip=0, train_wall=6, gb_free=76.5, wall=1410 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:37:35]    INFO >> epoch 007:   1030 / 1539 loss=3.75, wps=5342.6, ups=8.14, wpb=656.4, bsz=656.4, num_updates=10250, lr=0.000295, gnorm=4.522, clip=0, train_wall=6, gb_free=76.5, wall=1416 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:37:42]    INFO >> epoch 007:   1080 / 1539 loss=3.719, wps=5616.6, ups=7.45, wpb=753.7, bsz=753.7, num_updates=10300, lr=0.000295, gnorm=4.653, clip=0, train_wall=6, gb_free=74.1, wall=1423 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:37:49]    INFO >> epoch 007:   1130 / 1539 loss=3.611, wps=6214.7, ups=7.89, wpb=788.1, bsz=788.1, num_updates=10350, lr=0.000295, gnorm=4.82, clip=0, train_wall=6, gb_free=77, wall=1429 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:37:56]    INFO >> epoch 007:   1180 / 1539 loss=3.642, wps=6133.9, ups=7.54, wpb=814, bsz=814, num_updates=10400, lr=0.000295, gnorm=5.017, clip=0, train_wall=6, gb_free=76.7, wall=1436 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:38:02]    INFO >> epoch 007:   1230 / 1539 loss=3.763, wps=5390.6, ups=8.07, wpb=667.6, bsz=667.6, num_updates=10450, lr=0.000295, gnorm=4.331, clip=0, train_wall=6, gb_free=75.7, wall=1442 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:38:08]    INFO >> epoch 007:   1280 / 1539 loss=3.641, wps=5513.2, ups=8.51, wpb=648, bsz=648, num_updates=10500, lr=0.000295, gnorm=4.441, clip=0, train_wall=5, gb_free=75, wall=1448 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:38:14]    INFO >> epoch 007:   1330 / 1539 loss=3.772, wps=5560.1, ups=8.36, wpb=665.2, bsz=665.2, num_updates=10550, lr=0.000295, gnorm=4.664, clip=0, train_wall=6, gb_free=76.5, wall=1454 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:38:22]    INFO >> epoch 007:   1380 / 1539 loss=3.645, wps=5483.5, ups=7.99, wpb=686.3, bsz=686.3, num_updates=10600, lr=0.000295, gnorm=4.836, clip=0, train_wall=6, gb_free=73.6, wall=1460 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:38:29]    INFO >> epoch 007:   1430 / 1539 loss=3.408, wps=6558.5, ups=7.03, wpb=932.5, bsz=932.5, num_updates=10650, lr=0.000295, gnorm=4.842, clip=0, train_wall=7, gb_free=73.9, wall=1467 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:38:35]    INFO >> epoch 007:   1480 / 1539 loss=3.763, wps=6006.8, ups=8.22, wpb=731, bsz=731, num_updates=10700, lr=0.000295, gnorm=4.526, clip=0, train_wall=6, gb_free=75.4, wall=1473 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:38:41]    INFO >> epoch 007:   1530 / 1539 loss=3.739, wps=5748.8, ups=7.62, wpb=754.5, bsz=754.5, num_updates=10750, lr=0.000295, gnorm=3.898, clip=0, train_wall=6, gb_free=77, wall=1480 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:38:42]    INFO >> epoch 007 | loss 3.703 | wps 5361.9 | ups 7.44 | wpb 720.8 | bsz 720.8 | num_updates 10759 | lr 0.000295 | gnorm 4.669 | clip 0 | train_wall 179 | gb_free 75 | wall 1481 (progress_bar.py:267, print())[0m
[33m[2025-11-21 00:38:42] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:38:51]    INFO >> epoch 007 | valid on 'valid' subset | loss 3.859 | wps 16593.6 | wpb 5412.5 | bsz 5412.5 | num_updates 10759 | best_loss 5.506 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 00:38:52]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 00:38:52]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/checkpoints/checkpoint_last.pt (epoch 7 @ 10759 updates, score 3.859) (writing took 0.012800 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 00:38:52] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:38:59]    INFO >> epoch 008:     41 / 1539 loss=3.547, wps=2287.9, ups=3.16, wpb=723.7, bsz=723.7, num_updates=10800, lr=0.000262, gnorm=4.625, clip=0, train_wall=6, gb_free=76.3, wall=1496 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:39:05]    INFO >> epoch 008:     91 / 1539 loss=3.764, wps=5802.2, ups=7.68, wpb=755.2, bsz=755.2, num_updates=10850, lr=0.000262, gnorm=4.436, clip=0, train_wall=6, gb_free=75.4, wall=1502 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:39:11]    INFO >> epoch 008:    141 / 1539 loss=3.743, wps=5025.2, ups=8.23, wpb=610.4, bsz=610.4, num_updates=10900, lr=0.000262, gnorm=4.258, clip=0, train_wall=6, gb_free=75.8, wall=1508 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:39:18]    INFO >> epoch 008:    191 / 1539 loss=3.685, wps=6143.1, ups=7.84, wpb=783.3, bsz=783.3, num_updates=10950, lr=0.000262, gnorm=4.411, clip=0, train_wall=6, gb_free=76.1, wall=1515 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:39:24]    INFO >> epoch 008:    241 / 1539 loss=3.46, wps=6199, ups=7.68, wpb=807.2, bsz=807.2, num_updates=11000, lr=0.000262, gnorm=5.327, clip=0, train_wall=6, gb_free=76.3, wall=1521 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:39:32]    INFO >> epoch 008:    291 / 1539 loss=3.813, wps=5306.2, ups=7.88, wpb=673.5, bsz=673.5, num_updates=11050, lr=0.000262, gnorm=4.772, clip=0, train_wall=6, gb_free=75.7, wall=1528 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:39:38]    INFO >> epoch 008:    341 / 1539 loss=3.797, wps=5190.6, ups=7.61, wpb=682.2, bsz=682.2, num_updates=11100, lr=0.000262, gnorm=4.742, clip=0, train_wall=6, gb_free=76.2, wall=1534 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:39:45]    INFO >> epoch 008:    391 / 1539 loss=3.753, wps=5817.2, ups=7.62, wpb=763.4, bsz=763.4, num_updates=11150, lr=0.000262, gnorm=4.446, clip=0, train_wall=6, gb_free=74, wall=1541 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:39:51]    INFO >> epoch 008:    441 / 1539 loss=3.637, wps=5575.6, ups=7.79, wpb=715.7, bsz=715.7, num_updates=11200, lr=0.000262, gnorm=4.728, clip=0, train_wall=6, gb_free=75.9, wall=1547 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:39:58]    INFO >> epoch 008:    491 / 1539 loss=3.686, wps=6090, ups=7.09, wpb=859.2, bsz=859.2, num_updates=11250, lr=0.000262, gnorm=4.664, clip=0, train_wall=6, gb_free=77.5, wall=1554 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:40:06]    INFO >> epoch 008:    541 / 1539 loss=3.735, wps=5401, ups=8.01, wpb=674.3, bsz=674.3, num_updates=11300, lr=0.000262, gnorm=4.049, clip=0, train_wall=6, gb_free=76.5, wall=1561 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:40:12]    INFO >> epoch 008:    591 / 1539 loss=3.736, wps=5399.6, ups=8.04, wpb=671.5, bsz=671.5, num_updates=11350, lr=0.000262, gnorm=4.443, clip=0, train_wall=6, gb_free=75.7, wall=1567 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:40:18]    INFO >> epoch 008:    641 / 1539 loss=3.724, wps=5346.7, ups=8.15, wpb=655.9, bsz=655.9, num_updates=11400, lr=0.000262, gnorm=4.328, clip=0, train_wall=6, gb_free=77, wall=1573 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:40:24]    INFO >> epoch 008:    691 / 1539 loss=3.661, wps=5585.8, ups=8.04, wpb=694.7, bsz=694.7, num_updates=11450, lr=0.000262, gnorm=4.254, clip=0, train_wall=6, gb_free=72.7, wall=1579 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:40:31]    INFO >> epoch 008:    741 / 1539 loss=3.805, wps=5142.8, ups=7.92, wpb=649.1, bsz=649.1, num_updates=11500, lr=0.000262, gnorm=3.695, clip=0, train_wall=6, gb_free=73.9, wall=1585 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:40:39]    INFO >> epoch 008:    791 / 1539 loss=3.661, wps=5448.2, ups=7.71, wpb=706.4, bsz=706.4, num_updates=11550, lr=0.000262, gnorm=4.359, clip=0, train_wall=6, gb_free=76.5, wall=1592 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:40:45]    INFO >> epoch 008:    841 / 1539 loss=3.709, wps=5565.1, ups=8.19, wpb=679.8, bsz=679.8, num_updates=11600, lr=0.000262, gnorm=4.258, clip=0, train_wall=6, gb_free=76.9, wall=1598 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:40:51]    INFO >> epoch 008:    891 / 1539 loss=3.776, wps=5160, ups=7.83, wpb=658.7, bsz=658.7, num_updates=11650, lr=0.000262, gnorm=3.971, clip=0, train_wall=6, gb_free=75.4, wall=1604 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:40:58]    INFO >> epoch 008:    941 / 1539 loss=3.392, wps=5868.4, ups=7.53, wpb=779.3, bsz=779.3, num_updates=11700, lr=0.000262, gnorm=4.848, clip=0, train_wall=6, gb_free=74.9, wall=1611 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:41:04] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 1.25 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 2.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:41:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:41:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:41:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 15           |        cudaMalloc retries: 22        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77826 MiB |  77886 MiB | 200066 GiB | 199990 GiB |
|       from large pool |  77388 MiB |  77448 MiB | 197841 GiB | 197766 GiB |
|       from small pool |    438 MiB |    439 MiB |   2225 GiB |   2224 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77826 MiB |  77886 MiB | 200066 GiB | 199990 GiB |
|       from large pool |  77388 MiB |  77448 MiB | 197841 GiB | 197766 GiB |
|       from small pool |    438 MiB |    439 MiB |   2225 GiB |   2224 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77575 MiB |  77635 MiB | 199059 GiB | 198984 GiB |
|       from large pool |  77139 MiB |  77199 MiB | 196838 GiB | 196762 GiB |
|       from small pool |    436 MiB |    437 MiB |   2221 GiB |   2221 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80506 MiB |  80506 MiB | 253540 MiB | 173034 MiB |
|       from large pool |  80058 MiB |  80058 MiB | 251540 MiB | 171482 MiB |
|       from small pool |    448 MiB |    448 MiB |   2000 MiB |   1552 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2679 MiB |   4073 MiB | 150766 GiB | 150764 GiB |
|       from large pool |   2669 MiB |   4069 MiB | 148176 GiB | 148174 GiB |
|       from small pool |      9 MiB |     22 MiB |   2590 GiB |   2590 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    8218    |    8219    |   25530 K  |   25522 K  |
|       from large pool |    1023    |    1024    |   11567 K  |   11566 K  |
|       from small pool |    7195    |    7196    |   13963 K  |   13956 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    8218    |    8219    |   25530 K  |   25522 K  |
|       from large pool |    1023    |    1024    |   11567 K  |   11566 K  |
|       from small pool |    7195    |    7196    |   13963 K  |   13956 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     839    |     839    |    3819    |    2980    |
|       from large pool |     615    |     615    |    2819    |    2204    |
|       from small pool |     224    |     224    |    1000    |     776    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     345    |     348    |   15068 K  |   15068 K  |
|       from large pool |     159    |     159    |    8369 K  |    8369 K  |
|       from small pool |     186    |     189    |    6698 K  |    6698 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:41:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:41:04] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:41:05]    INFO >> epoch 008:    992 / 1539 loss=3.781, wps=4628.9, ups=7.41, wpb=625, bsz=625, num_updates=11750, lr=0.000262, gnorm=4.344, clip=0, train_wall=6, gb_free=77.9, wall=1618 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:41:13]    INFO >> epoch 008:   1042 / 1539 loss=3.552, wps=5796.1, ups=7.18, wpb=807.1, bsz=807.1, num_updates=11800, lr=0.000262, gnorm=5.274, clip=0, train_wall=6, gb_free=76.2, wall=1625 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:41:19]    INFO >> epoch 008:   1092 / 1539 loss=3.699, wps=5202.9, ups=7.78, wpb=669.2, bsz=669.2, num_updates=11850, lr=0.000262, gnorm=4.252, clip=0, train_wall=6, gb_free=75.1, wall=1631 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:41:25]    INFO >> epoch 008:   1142 / 1539 loss=3.8, wps=4873.2, ups=8.08, wpb=603.1, bsz=603.1, num_updates=11900, lr=0.000262, gnorm=4.118, clip=0, train_wall=6, gb_free=75.9, wall=1637 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:41:30] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 9.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 75.53 GiB is allocated by PyTorch, and 3.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:41:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:41:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:41:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 16           |        cudaMalloc retries: 24        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77283 MiB |  77343 MiB | 202941 GiB | 202865 GiB |
|       from large pool |  76939 MiB |  76999 MiB | 200685 GiB | 200610 GiB |
|       from small pool |    343 MiB |    344 MiB |   2256 GiB |   2255 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77283 MiB |  77343 MiB | 202941 GiB | 202865 GiB |
|       from large pool |  76939 MiB |  76999 MiB | 200685 GiB | 200610 GiB |
|       from small pool |    343 MiB |    344 MiB |   2256 GiB |   2255 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76965 MiB |  77024 MiB | 201919 GiB | 201844 GiB |
|       from large pool |  76624 MiB |  76683 MiB | 199666 GiB | 199591 GiB |
|       from small pool |    341 MiB |    342 MiB |   2252 GiB |   2252 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80498 MiB |  80506 MiB | 253602 MiB | 173104 MiB |
|       from large pool |  80118 MiB |  80118 MiB | 251600 MiB | 171482 MiB |
|       from small pool |    380 MiB |    448 MiB |   2002 MiB |   1622 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3154 MiB |   6329 MiB | 152977 GiB | 152974 GiB |
|       from large pool |   3118 MiB |   6296 MiB | 150349 GiB | 150346 GiB |
|       from small pool |     36 MiB |     37 MiB |   2627 GiB |   2627 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6537    |    6540    |   25901 K  |   25895 K  |
|       from large pool |     876    |     877    |   11743 K  |   11742 K  |
|       from small pool |    5661    |    5664    |   14158 K  |   14152 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    6537    |    6540    |   25901 K  |   25895 K  |
|       from large pool |     876    |     877    |   11743 K  |   11742 K  |
|       from small pool |    5661    |    5664    |   14158 K  |   14152 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     806    |     839    |    3821    |    3015    |
|       from large pool |     616    |     616    |    2820    |    2204    |
|       from small pool |     190    |     224    |    1001    |     811    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     548    |     549    |   15285 K  |   15285 K  |
|       from large pool |     203    |     203    |    8496 K  |    8496 K  |
|       from small pool |     345    |     346    |    6789 K  |    6788 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:41:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:41:30] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:41:33]    INFO >> epoch 008:   1193 / 1539 loss=3.815, wps=4703.7, ups=6.65, wpb=707.8, bsz=707.8, num_updates=11950, lr=0.000262, gnorm=4.264, clip=0, train_wall=6, gb_free=67.8, wall=1645 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:41:40]    INFO >> epoch 008:   1243 / 1539 loss=3.483, wps=6273.6, ups=6.84, wpb=916.7, bsz=916.7, num_updates=12000, lr=0.000262, gnorm=4.374, clip=0, train_wall=7, gb_free=74.5, wall=1652 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:41:48]    INFO >> epoch 008:   1293 / 1539 loss=3.614, wps=5904, ups=7.69, wpb=767.6, bsz=767.6, num_updates=12050, lr=0.000262, gnorm=4.765, clip=0, train_wall=6, gb_free=74.4, wall=1659 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:41:55]    INFO >> epoch 008:   1343 / 1539 loss=3.671, wps=5528.7, ups=7.68, wpb=719.6, bsz=719.6, num_updates=12100, lr=0.000262, gnorm=4.769, clip=0, train_wall=6, gb_free=76.3, wall=1665 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:42:02]    INFO >> epoch 008:   1393 / 1539 loss=3.878, wps=5607.6, ups=7.05, wpb=795.2, bsz=795.2, num_updates=12150, lr=0.000262, gnorm=3.526, clip=0, train_wall=6, gb_free=75.6, wall=1672 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:42:08]    INFO >> epoch 008:   1443 / 1539 loss=3.734, wps=6127.1, ups=7.93, wpb=772.6, bsz=772.6, num_updates=12200, lr=0.000262, gnorm=4.572, clip=0, train_wall=6, gb_free=74.6, wall=1679 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:42:15]    INFO >> epoch 008:   1493 / 1539 loss=3.686, wps=5166.4, ups=7.54, wpb=684.9, bsz=684.9, num_updates=12250, lr=0.000262, gnorm=4.758, clip=0, train_wall=6, gb_free=72.5, wall=1685 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:42:22]    INFO >> epoch 008 | loss 3.686 | wps 5273.3 | ups 7.32 | wpb 720.8 | bsz 720.8 | num_updates 12296 | lr 0.000262 | gnorm 4.448 | clip 0 | train_wall 182 | gb_free 77.2 | wall 1691 (progress_bar.py:267, print())[0m
[33m[2025-11-21 00:42:22] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:42:31]    INFO >> epoch 008 | valid on 'valid' subset | loss 3.859 | wps 17166.7 | wpb 5412.5 | bsz 5412.5 | num_updates 12296 | best_loss 5.506 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 00:42:31]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 00:42:31]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/checkpoints/checkpoint_last.pt (epoch 8 @ 12296 updates, score 3.859) (writing took 0.011932 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 00:42:31] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:42:32]    INFO >> epoch 009:      4 / 1539 loss=3.664, wps=2193.9, ups=3.18, wpb=690, bsz=690, num_updates=12300, lr=0.000227, gnorm=4.247, clip=0, train_wall=6, gb_free=78.3, wall=1701 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:42:38]    INFO >> epoch 009:     54 / 1539 loss=3.775, wps=4829.5, ups=8.22, wpb=587.9, bsz=587.9, num_updates=12350, lr=0.000227, gnorm=4.122, clip=0, train_wall=6, gb_free=76.5, wall=1707 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:42:44]    INFO >> epoch 009:    104 / 1539 loss=3.693, wps=5927.3, ups=7.8, wpb=760.2, bsz=760.2, num_updates=12400, lr=0.000227, gnorm=4.072, clip=0, train_wall=6, gb_free=74.1, wall=1713 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:42:52]    INFO >> epoch 009:    154 / 1539 loss=3.69, wps=5812.6, ups=8.03, wpb=723.8, bsz=723.8, num_updates=12450, lr=0.000227, gnorm=4.067, clip=0, train_wall=6, gb_free=75.8, wall=1720 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:42:58]    INFO >> epoch 009:    204 / 1539 loss=3.614, wps=5915, ups=7.9, wpb=748.9, bsz=748.9, num_updates=12500, lr=0.000227, gnorm=4.203, clip=0, train_wall=6, gb_free=76.7, wall=1726 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:43:04]    INFO >> epoch 009:    254 / 1539 loss=3.675, wps=6099.8, ups=8.29, wpb=736, bsz=736, num_updates=12550, lr=0.000227, gnorm=4.18, clip=0, train_wall=6, gb_free=75.2, wall=1732 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:43:11]    INFO >> epoch 009:    304 / 1539 loss=3.641, wps=5279.5, ups=7.65, wpb=689.9, bsz=689.9, num_updates=12600, lr=0.000227, gnorm=4.948, clip=0, train_wall=6, gb_free=75, wall=1739 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:43:18]    INFO >> epoch 009:    354 / 1539 loss=3.621, wps=5971.4, ups=7.27, wpb=820.9, bsz=820.9, num_updates=12650, lr=0.000227, gnorm=4.927, clip=0, train_wall=6, gb_free=75.3, wall=1745 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:43:20] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 9.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 75.53 GiB is allocated by PyTorch, and 3.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:43:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:43:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:43:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 17           |        cudaMalloc retries: 26        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77283 MiB |  77343 MiB | 217361 GiB | 217285 GiB |
|       from large pool |  76939 MiB |  76999 MiB | 214943 GiB | 214867 GiB |
|       from small pool |    343 MiB |    344 MiB |   2418 GiB |   2417 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77283 MiB |  77343 MiB | 217361 GiB | 217285 GiB |
|       from large pool |  76939 MiB |  76999 MiB | 214943 GiB | 214867 GiB |
|       from small pool |    343 MiB |    344 MiB |   2418 GiB |   2417 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76965 MiB |  77024 MiB | 216273 GiB | 216198 GiB |
|       from large pool |  76624 MiB |  76683 MiB | 213858 GiB | 213784 GiB |
|       from small pool |    341 MiB |    342 MiB |   2414 GiB |   2414 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80498 MiB |  80500 MiB | 253718 MiB | 173220 MiB |
|       from large pool |  80118 MiB |  80118 MiB | 251660 MiB | 171542 MiB |
|       from small pool |    380 MiB |    434 MiB |   2058 MiB |   1678 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3154 MiB |   6330 MiB | 163150 GiB | 163147 GiB |
|       from large pool |   3118 MiB |   6296 MiB | 160338 GiB | 160335 GiB |
|       from small pool |     36 MiB |     38 MiB |   2811 GiB |   2811 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6537    |    6540    |   27675 K  |   27669 K  |
|       from large pool |     876    |     877    |   12495 K  |   12495 K  |
|       from small pool |    5661    |    5664    |   15179 K  |   15173 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    6537    |    6540    |   27675 K  |   27669 K  |
|       from large pool |     876    |     877    |   12495 K  |   12495 K  |
|       from small pool |    5661    |    5664    |   15179 K  |   15173 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     806    |     832    |    3850    |    3044    |
|       from large pool |     616    |     616    |    2821    |    2205    |
|       from small pool |     190    |     217    |    1029    |     839    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     553    |     553    |   16338 K  |   16338 K  |
|       from large pool |     203    |     203    |    9041 K  |    9040 K  |
|       from small pool |     350    |     350    |    7297 K  |    7297 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:43:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:43:20] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:43:26]    INFO >> epoch 009:    405 / 1539 loss=3.645, wps=4814.2, ups=7.04, wpb=684, bsz=684, num_updates=12700, lr=0.000227, gnorm=4.469, clip=0, train_wall=6, gb_free=73.6, wall=1753 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:43:32]    INFO >> epoch 009:    455 / 1539 loss=3.711, wps=5800.4, ups=8.19, wpb=708.3, bsz=708.3, num_updates=12750, lr=0.000227, gnorm=4.251, clip=0, train_wall=6, gb_free=77.7, wall=1759 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:43:38]    INFO >> epoch 009:    505 / 1539 loss=3.688, wps=6062.6, ups=7.93, wpb=764.3, bsz=764.3, num_updates=12800, lr=0.000227, gnorm=4.212, clip=0, train_wall=6, gb_free=76.6, wall=1765 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:43:45]    INFO >> epoch 009:    555 / 1539 loss=3.549, wps=5784.2, ups=7.63, wpb=757.6, bsz=757.6, num_updates=12850, lr=0.000227, gnorm=4.429, clip=0, train_wall=6, gb_free=76.2, wall=1772 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:43:48] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 1.25 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 2.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:43:48] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:43:48] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:43:48] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 18           |        cudaMalloc retries: 27        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77826 MiB |  77886 MiB | 220570 GiB | 220494 GiB |
|       from large pool |  77388 MiB |  77448 MiB | 218117 GiB | 218041 GiB |
|       from small pool |    438 MiB |    439 MiB |   2453 GiB |   2453 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77826 MiB |  77886 MiB | 220570 GiB | 220494 GiB |
|       from large pool |  77388 MiB |  77448 MiB | 218117 GiB | 218041 GiB |
|       from small pool |    438 MiB |    439 MiB |   2453 GiB |   2453 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77575 MiB |  77635 MiB | 219466 GiB | 219390 GiB |
|       from large pool |  77139 MiB |  77199 MiB | 217016 GiB | 216940 GiB |
|       from small pool |    436 MiB |    437 MiB |   2449 GiB |   2449 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80506 MiB |  80506 MiB | 253786 MiB | 173280 MiB |
|       from large pool |  80058 MiB |  80058 MiB | 251660 MiB | 171602 MiB |
|       from small pool |    448 MiB |    448 MiB |   2126 MiB |   1678 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2679 MiB |   4073 MiB | 165649 GiB | 165647 GiB |
|       from large pool |   2669 MiB |   4069 MiB | 162795 GiB | 162793 GiB |
|       from small pool |      9 MiB |     22 MiB |   2854 GiB |   2854 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    8218    |    8219    |   28092 K  |   28084 K  |
|       from large pool |    1023    |    1024    |   12690 K  |   12689 K  |
|       from small pool |    7195    |    7196    |   15401 K  |   15394 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    8218    |    8219    |   28092 K  |   28084 K  |
|       from large pool |    1023    |    1024    |   12690 K  |   12689 K  |
|       from small pool |    7195    |    7196    |   15401 K  |   15394 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     839    |     839    |    3884    |    3045    |
|       from large pool |     615    |     615    |    2821    |    2206    |
|       from small pool |     224    |     224    |    1063    |     839    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     344    |     347    |   16584 K  |   16584 K  |
|       from large pool |     159    |     159    |    9182 K  |    9182 K  |
|       from small pool |     185    |     188    |    7401 K  |    7401 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:43:48] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:43:48] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:43:52]    INFO >> epoch 009:    606 / 1539 loss=3.734, wps=4845.1, ups=7.23, wpb=670.4, bsz=670.4, num_updates=12900, lr=0.000227, gnorm=4.117, clip=0, train_wall=6, gb_free=78.5, wall=1778 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:44:00]    INFO >> epoch 009:    656 / 1539 loss=3.749, wps=5202.5, ups=7.74, wpb=672.1, bsz=672.1, num_updates=12950, lr=0.000227, gnorm=3.891, clip=0, train_wall=6, gb_free=77, wall=1785 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:44:06]    INFO >> epoch 009:    706 / 1539 loss=3.722, wps=5377.9, ups=7.84, wpb=685.7, bsz=685.7, num_updates=13000, lr=0.000227, gnorm=4.013, clip=0, train_wall=6, gb_free=76.1, wall=1791 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:44:13]    INFO >> epoch 009:    756 / 1539 loss=3.717, wps=4914.8, ups=7.67, wpb=641.1, bsz=641.1, num_updates=13050, lr=0.000227, gnorm=3.696, clip=0, train_wall=6, gb_free=74.7, wall=1798 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:44:19]    INFO >> epoch 009:    806 / 1539 loss=3.534, wps=5944.9, ups=7.72, wpb=769.8, bsz=769.8, num_updates=13100, lr=0.000227, gnorm=5.157, clip=0, train_wall=6, gb_free=77.2, wall=1804 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:44:26]    INFO >> epoch 009:    856 / 1539 loss=3.908, wps=4924.4, ups=7.23, wpb=681.2, bsz=681.2, num_updates=13150, lr=0.000227, gnorm=3.79, clip=0, train_wall=6, gb_free=76.4, wall=1811 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:44:33]    INFO >> epoch 009:    906 / 1539 loss=3.74, wps=5365.4, ups=8.11, wpb=661.4, bsz=661.4, num_updates=13200, lr=0.000227, gnorm=4.386, clip=0, train_wall=6, gb_free=76, wall=1817 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:44:41]    INFO >> epoch 009:    956 / 1539 loss=3.625, wps=5791.2, ups=6.75, wpb=858.4, bsz=858.4, num_updates=13250, lr=0.000227, gnorm=4.311, clip=0, train_wall=7, gb_free=77, wall=1825 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:44:47]    INFO >> epoch 009:   1006 / 1539 loss=3.63, wps=5283.6, ups=7.98, wpb=662.5, bsz=662.5, num_updates=13300, lr=0.000227, gnorm=4.296, clip=0, train_wall=6, gb_free=76, wall=1831 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:44:54]    INFO >> epoch 009:   1056 / 1539 loss=3.517, wps=5993.7, ups=7.76, wpb=772.1, bsz=772.1, num_updates=13350, lr=0.000227, gnorm=4.502, clip=0, train_wall=6, gb_free=74.4, wall=1837 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:45:00]    INFO >> epoch 009:   1106 / 1539 loss=3.693, wps=5507.8, ups=7.89, wpb=698.5, bsz=698.5, num_updates=13400, lr=0.000227, gnorm=4.527, clip=0, train_wall=6, gb_free=70.1, wall=1844 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:45:07]    INFO >> epoch 009:   1156 / 1539 loss=3.723, wps=5787.2, ups=8.07, wpb=716.9, bsz=716.9, num_updates=13450, lr=0.000227, gnorm=4.61, clip=0, train_wall=6, gb_free=77.5, wall=1850 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:45:14]    INFO >> epoch 009:   1206 / 1539 loss=3.641, wps=5624.1, ups=7.89, wpb=713, bsz=713, num_updates=13500, lr=0.000227, gnorm=4.246, clip=0, train_wall=6, gb_free=76.2, wall=1856 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:45:21]    INFO >> epoch 009:   1256 / 1539 loss=3.579, wps=6598.7, ups=7.21, wpb=915.7, bsz=915.7, num_updates=13550, lr=0.000227, gnorm=3.797, clip=0, train_wall=6, gb_free=76.6, wall=1863 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:45:27]    INFO >> epoch 009:   1306 / 1539 loss=3.797, wps=5518, ups=7.9, wpb=698.4, bsz=698.4, num_updates=13600, lr=0.000227, gnorm=3.87, clip=0, train_wall=6, gb_free=77.8, wall=1870 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:45:33]    INFO >> epoch 009:   1356 / 1539 loss=3.648, wps=5381, ups=7.97, wpb=674.8, bsz=674.8, num_updates=13650, lr=0.000227, gnorm=4.211, clip=0, train_wall=6, gb_free=74.1, wall=1876 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:45:41]    INFO >> epoch 009:   1406 / 1539 loss=3.468, wps=6172, ups=7.96, wpb=775.2, bsz=775.2, num_updates=13700, lr=0.000227, gnorm=4.758, clip=0, train_wall=6, gb_free=75.4, wall=1882 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:45:48]    INFO >> epoch 009:   1456 / 1539 loss=3.465, wps=5865.1, ups=7.02, wpb=836, bsz=836, num_updates=13750, lr=0.000227, gnorm=4.203, clip=0, train_wall=7, gb_free=77.9, wall=1889 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:45:54]    INFO >> epoch 009:   1506 / 1539 loss=3.711, wps=5010.3, ups=8.17, wpb=613.6, bsz=613.6, num_updates=13800, lr=0.000227, gnorm=4.067, clip=0, train_wall=6, gb_free=75.6, wall=1895 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:45:59]    INFO >> epoch 009 | loss 3.66 | wps 5308.4 | ups 7.36 | wpb 720.8 | bsz 720.8 | num_updates 13833 | lr 0.000227 | gnorm 4.279 | clip 0 | train_wall 181 | gb_free 77 | wall 1900 (progress_bar.py:267, print())[0m
[33m[2025-11-21 00:45:59] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:46:08]    INFO >> epoch 009 | valid on 'valid' subset | loss 3.826 | wps 16208.2 | wpb 5412.5 | bsz 5412.5 | num_updates 13833 | best_loss 5.506 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 00:46:08]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 00:46:08]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/checkpoints/checkpoint_last.pt (epoch 9 @ 13833 updates, score 3.826) (writing took 0.011596 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 00:46:08] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:46:10]    INFO >> epoch 010:     17 / 1539 loss=3.726, wps=2001.3, ups=3.1, wpb=646.5, bsz=646.5, num_updates=13850, lr=0.000193, gnorm=4.158, clip=0, train_wall=6, gb_free=75, wall=1912 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:46:19]    INFO >> epoch 010:     67 / 1539 loss=3.669, wps=5141.6, ups=7.42, wpb=692.6, bsz=692.6, num_updates=13900, lr=0.000193, gnorm=4.512, clip=0, train_wall=6, gb_free=76.9, wall=1918 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:46:25]    INFO >> epoch 010:    117 / 1539 loss=3.596, wps=5747.8, ups=8.04, wpb=715.2, bsz=715.2, num_updates=13950, lr=0.000193, gnorm=4.579, clip=0, train_wall=6, gb_free=75.4, wall=1925 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:46:31]    INFO >> epoch 010:    167 / 1539 loss=3.676, wps=5963.4, ups=8.23, wpb=724.9, bsz=724.9, num_updates=14000, lr=0.000193, gnorm=4.094, clip=0, train_wall=6, gb_free=70.4, wall=1931 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:46:38]    INFO >> epoch 010:    217 / 1539 loss=3.587, wps=5347.4, ups=7.15, wpb=747.9, bsz=747.9, num_updates=14050, lr=0.000193, gnorm=4.524, clip=0, train_wall=6, gb_free=76.4, wall=1938 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:46:46]    INFO >> epoch 010:    267 / 1539 loss=3.638, wps=5831.2, ups=7.25, wpb=804.6, bsz=804.6, num_updates=14100, lr=0.000193, gnorm=3.898, clip=0, train_wall=6, gb_free=77, wall=1945 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:46:52]    INFO >> epoch 010:    317 / 1539 loss=3.682, wps=5578.1, ups=8.07, wpb=691.3, bsz=691.3, num_updates=14150, lr=0.000193, gnorm=4.186, clip=0, train_wall=6, gb_free=77.9, wall=1951 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:46:59]    INFO >> epoch 010:    367 / 1539 loss=3.69, wps=5756.5, ups=6.96, wpb=827.5, bsz=827.5, num_updates=14200, lr=0.000193, gnorm=4.229, clip=0, train_wall=6, gb_free=76.5, wall=1958 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:47:05]    INFO >> epoch 010:    417 / 1539 loss=3.789, wps=5070.2, ups=8.25, wpb=614.7, bsz=614.7, num_updates=14250, lr=0.000193, gnorm=3.986, clip=0, train_wall=6, gb_free=76.2, wall=1964 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:47:12]    INFO >> epoch 010:    467 / 1539 loss=3.635, wps=5366.6, ups=7.96, wpb=674.4, bsz=674.4, num_updates=14300, lr=0.000193, gnorm=3.947, clip=0, train_wall=6, gb_free=76.4, wall=1970 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:47:18]    INFO >> epoch 010:    517 / 1539 loss=3.665, wps=4767.3, ups=8.09, wpb=589, bsz=589, num_updates=14350, lr=0.000193, gnorm=4.043, clip=0, train_wall=6, gb_free=76.4, wall=1976 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:47:26]    INFO >> epoch 010:    567 / 1539 loss=3.666, wps=5542.2, ups=7.8, wpb=710.4, bsz=710.4, num_updates=14400, lr=0.000193, gnorm=4.024, clip=0, train_wall=6, gb_free=75.3, wall=1983 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:47:32]    INFO >> epoch 010:    617 / 1539 loss=3.717, wps=5431.7, ups=7.79, wpb=697, bsz=697, num_updates=14450, lr=0.000193, gnorm=3.762, clip=0, train_wall=6, gb_free=76.3, wall=1989 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:47:39]    INFO >> epoch 010:    667 / 1539 loss=3.623, wps=5244, ups=7.74, wpb=677.7, bsz=677.7, num_updates=14500, lr=0.000193, gnorm=4.24, clip=0, train_wall=6, gb_free=75.9, wall=1996 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:47:45]    INFO >> epoch 010:    717 / 1539 loss=3.412, wps=6556.5, ups=7.26, wpb=902.6, bsz=902.6, num_updates=14550, lr=0.000193, gnorm=4.185, clip=0, train_wall=6, gb_free=74.1, wall=2003 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:47:53]    INFO >> epoch 010:    767 / 1539 loss=3.638, wps=5861.5, ups=7.93, wpb=739, bsz=739, num_updates=14600, lr=0.000193, gnorm=4.225, clip=0, train_wall=6, gb_free=77.5, wall=2009 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:47:59]    INFO >> epoch 010:    817 / 1539 loss=3.613, wps=5579.7, ups=8.09, wpb=690.1, bsz=690.1, num_updates=14650, lr=0.000193, gnorm=4.502, clip=0, train_wall=6, gb_free=76, wall=2015 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:48:05]    INFO >> epoch 010:    867 / 1539 loss=3.678, wps=5118.7, ups=8.24, wpb=621.3, bsz=621.3, num_updates=14700, lr=0.000193, gnorm=4.059, clip=0, train_wall=6, gb_free=78.2, wall=2021 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:48:12]    INFO >> epoch 010:    917 / 1539 loss=3.542, wps=7104.3, ups=7.54, wpb=942.8, bsz=942.8, num_updates=14750, lr=0.000193, gnorm=4.589, clip=0, train_wall=6, gb_free=76.1, wall=2028 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:48:18]    INFO >> epoch 010:    967 / 1539 loss=3.761, wps=5350.9, ups=8.54, wpb=626.6, bsz=626.6, num_updates=14800, lr=0.000193, gnorm=3.801, clip=0, train_wall=5, gb_free=76.5, wall=2034 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:48:24]    INFO >> epoch 010:   1017 / 1539 loss=3.685, wps=5341.5, ups=8.17, wpb=653.6, bsz=653.6, num_updates=14850, lr=0.000193, gnorm=4.21, clip=0, train_wall=6, gb_free=76.9, wall=2040 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:48:30]    INFO >> epoch 010:   1067 / 1539 loss=3.685, wps=6361.3, ups=8.17, wpb=778.4, bsz=778.4, num_updates=14900, lr=0.000193, gnorm=4.163, clip=0, train_wall=6, gb_free=77.7, wall=2046 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:48:36]    INFO >> epoch 010:   1117 / 1539 loss=3.624, wps=5464, ups=7.92, wpb=689.7, bsz=689.7, num_updates=14950, lr=0.000193, gnorm=4.116, clip=0, train_wall=6, gb_free=74.7, wall=2052 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:48:43]    INFO >> epoch 010:   1167 / 1539 loss=3.636, wps=5194.5, ups=8.02, wpb=647.8, bsz=647.8, num_updates=15000, lr=0.000193, gnorm=4.471, clip=0, train_wall=6, gb_free=75.1, wall=2058 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:48:45] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 1.25 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 76.06 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:48:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:48:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:48:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 19           |        cudaMalloc retries: 28        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77886 MiB |  77946 MiB | 256350 GiB | 256274 GiB |
|       from large pool |  77448 MiB |  77507 MiB | 253502 GiB | 253426 GiB |
|       from small pool |    438 MiB |    439 MiB |   2847 GiB |   2847 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77886 MiB |  77946 MiB | 256350 GiB | 256274 GiB |
|       from large pool |  77448 MiB |  77507 MiB | 253502 GiB | 253426 GiB |
|       from small pool |    438 MiB |    439 MiB |   2847 GiB |   2847 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77635 MiB |  77695 MiB | 255067 GiB | 254991 GiB |
|       from large pool |  77199 MiB |  77258 MiB | 252223 GiB | 252148 GiB |
|       from small pool |    436 MiB |    437 MiB |   2843 GiB |   2843 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80506 MiB |  80506 MiB | 253786 MiB | 173280 MiB |
|       from large pool |  80058 MiB |  80058 MiB | 251660 MiB | 171602 MiB |
|       from small pool |    448 MiB |    448 MiB |   2126 MiB |   1678 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2619 MiB |   4073 MiB | 192524 GiB | 192521 GiB |
|       from large pool |   2609 MiB |   4069 MiB | 189209 GiB | 189207 GiB |
|       from small pool |      9 MiB |     20 MiB |   3314 GiB |   3314 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    8222    |    8223    |   32657 K  |   32648 K  |
|       from large pool |    1024    |    1025    |   14787 K  |   14786 K  |
|       from small pool |    7198    |    7200    |   17869 K  |   17862 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    8222    |    8223    |   32657 K  |   32648 K  |
|       from large pool |    1024    |    1025    |   14787 K  |   14786 K  |
|       from small pool |    7198    |    7200    |   17869 K  |   17862 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     839    |     839    |    3884    |    3045    |
|       from large pool |     615    |     615    |    2821    |    2206    |
|       from small pool |     224    |     224    |    1063    |     839    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     347    |     348    |   19266 K  |   19265 K  |
|       from large pool |     159    |     159    |   10698 K  |   10697 K  |
|       from small pool |     188    |     189    |    8568 K  |    8567 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:48:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:48:45] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:48:50]    INFO >> epoch 010:   1218 / 1539 loss=3.599, wps=5949.8, ups=6.5, wpb=915.3, bsz=915.3, num_updates=15050, lr=0.000193, gnorm=4.42, clip=0, train_wall=6, gb_free=75.2, wall=2066 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:48:57]    INFO >> epoch 010:   1268 / 1539 loss=3.461, wps=5794.1, ups=7.46, wpb=776.5, bsz=776.5, num_updates=15100, lr=0.000193, gnorm=4.471, clip=0, train_wall=6, gb_free=76.3, wall=2073 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:49:06]    INFO >> epoch 010:   1318 / 1539 loss=3.733, wps=4902.8, ups=7.81, wpb=627.8, bsz=627.8, num_updates=15150, lr=0.000193, gnorm=4.136, clip=0, train_wall=6, gb_free=73.7, wall=2079 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:49:13]    INFO >> epoch 010:   1368 / 1539 loss=3.568, wps=5848.6, ups=7.65, wpb=764.9, bsz=764.9, num_updates=15200, lr=0.000193, gnorm=4.495, clip=0, train_wall=6, gb_free=77, wall=2086 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:49:19]    INFO >> epoch 010:   1418 / 1539 loss=3.592, wps=5555.9, ups=7.73, wpb=718.3, bsz=718.3, num_updates=15250, lr=0.000193, gnorm=4.243, clip=0, train_wall=6, gb_free=75.9, wall=2092 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:49:25]    INFO >> epoch 010:   1468 / 1539 loss=3.734, wps=5015.3, ups=8.2, wpb=611.6, bsz=611.6, num_updates=15300, lr=0.000193, gnorm=3.788, clip=0, train_wall=6, gb_free=77.4, wall=2098 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:49:27] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 9.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 75.53 GiB is allocated by PyTorch, and 3.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:49:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:49:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:49:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 20           |        cudaMalloc retries: 30        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77283 MiB |  77343 MiB | 260961 GiB | 260886 GiB |
|       from large pool |  76939 MiB |  76999 MiB | 258063 GiB | 257988 GiB |
|       from small pool |    343 MiB |    344 MiB |   2898 GiB |   2898 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77283 MiB |  77343 MiB | 260961 GiB | 260886 GiB |
|       from large pool |  76939 MiB |  76999 MiB | 258063 GiB | 257988 GiB |
|       from small pool |    343 MiB |    344 MiB |   2898 GiB |   2898 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76965 MiB |  77024 MiB | 259654 GiB | 259579 GiB |
|       from large pool |  76624 MiB |  76683 MiB | 256760 GiB | 256685 GiB |
|       from small pool |    341 MiB |    342 MiB |   2894 GiB |   2893 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80498 MiB |  80506 MiB | 253848 MiB | 173350 MiB |
|       from large pool |  80118 MiB |  80118 MiB | 251720 MiB | 171602 MiB |
|       from small pool |    380 MiB |    448 MiB |   2128 MiB |   1748 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3154 MiB |   6329 MiB | 196141 GiB | 196138 GiB |
|       from large pool |   3118 MiB |   6296 MiB | 192767 GiB | 192764 GiB |
|       from small pool |     36 MiB |     37 MiB |   3374 GiB |   3374 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6537    |    6540    |   33259 K  |   33252 K  |
|       from large pool |     876    |     877    |   15071 K  |   15070 K  |
|       from small pool |    5661    |    5664    |   18187 K  |   18182 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    6537    |    6540    |   33259 K  |   33252 K  |
|       from large pool |     876    |     877    |   15071 K  |   15070 K  |
|       from small pool |    5661    |    5664    |   18187 K  |   18182 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     806    |     839    |    3886    |    3080    |
|       from large pool |     616    |     616    |    2822    |    2206    |
|       from small pool |     190    |     224    |    1064    |     874    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     544    |     544    |   19618 K  |   19618 K  |
|       from large pool |     203    |     203    |   10902 K  |   10902 K  |
|       from small pool |     341    |     341    |    8716 K  |    8715 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:49:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:49:27] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:49:32]    INFO >> epoch 010:   1519 / 1539 loss=3.653, wps=5113.4, ups=7.06, wpb=724.3, bsz=724.3, num_updates=15350, lr=0.000193, gnorm=4.139, clip=0, train_wall=6, gb_free=76, wall=2105 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:49:36]    INFO >> epoch 010 | loss 3.634 | wps 5323.2 | ups 7.39 | wpb 720.8 | bsz 720.8 | num_updates 15370 | lr 0.000193 | gnorm 4.201 | clip 0 | train_wall 181 | gb_free 75.3 | wall 2108 (progress_bar.py:267, print())[0m
[33m[2025-11-21 00:49:36] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:49:46]    INFO >> epoch 010 | valid on 'valid' subset | loss 3.708 | wps 16081.7 | wpb 5412.5 | bsz 5412.5 | num_updates 15370 | best_loss 5.506 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 00:49:46]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 00:49:46]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/checkpoints/checkpoint_last.pt (epoch 10 @ 15370 updates, score 3.708) (writing took 0.009244 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 00:49:46] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:49:50]    INFO >> epoch 011:     30 / 1539 loss=3.65, wps=2230.2, ups=3.1, wpb=720, bsz=720, num_updates=15400, lr=0.000161, gnorm=4.145, clip=0, train_wall=6, gb_free=78.3, wall=2122 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:49:56]    INFO >> epoch 011:     80 / 1539 loss=3.72, wps=5281.1, ups=8.03, wpb=657.4, bsz=657.4, num_updates=15450, lr=0.000161, gnorm=4.163, clip=0, train_wall=6, gb_free=74.5, wall=2128 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:50:02]    INFO >> epoch 011:    130 / 1539 loss=3.676, wps=6096.4, ups=8.16, wpb=746.7, bsz=746.7, num_updates=15500, lr=0.000161, gnorm=4.3, clip=0, train_wall=6, gb_free=75.7, wall=2134 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:50:10]    INFO >> epoch 011:    180 / 1539 loss=3.637, wps=5672.7, ups=8.19, wpb=692.3, bsz=692.3, num_updates=15550, lr=0.000161, gnorm=3.638, clip=0, train_wall=6, gb_free=75.9, wall=2140 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:50:16]    INFO >> epoch 011:    230 / 1539 loss=3.56, wps=5979.8, ups=7.76, wpb=771, bsz=771, num_updates=15600, lr=0.000161, gnorm=4.336, clip=0, train_wall=6, gb_free=77, wall=2146 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:50:23]    INFO >> epoch 011:    280 / 1539 loss=3.666, wps=5304.8, ups=7.68, wpb=691.1, bsz=691.1, num_updates=15650, lr=0.000161, gnorm=4.06, clip=0, train_wall=6, gb_free=73.9, wall=2153 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:50:29]    INFO >> epoch 011:    330 / 1539 loss=3.551, wps=5627.7, ups=8.1, wpb=694.8, bsz=694.8, num_updates=15700, lr=0.000161, gnorm=4.246, clip=0, train_wall=6, gb_free=76.1, wall=2159 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:50:35]    INFO >> epoch 011:    380 / 1539 loss=3.603, wps=5394.2, ups=7.94, wpb=679.6, bsz=679.6, num_updates=15750, lr=0.000161, gnorm=4.064, clip=0, train_wall=6, gb_free=76.9, wall=2165 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:50:43]    INFO >> epoch 011:    430 / 1539 loss=3.698, wps=4986.3, ups=7.78, wpb=640.5, bsz=640.5, num_updates=15800, lr=0.000161, gnorm=4.012, clip=0, train_wall=6, gb_free=75.7, wall=2172 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:50:50]    INFO >> epoch 011:    480 / 1539 loss=3.72, wps=4643.4, ups=7.32, wpb=634, bsz=634, num_updates=15850, lr=0.000161, gnorm=4.047, clip=0, train_wall=6, gb_free=70.4, wall=2179 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:50:56]    INFO >> epoch 011:    530 / 1539 loss=3.857, wps=5005.6, ups=7.89, wpb=634.2, bsz=634.2, num_updates=15900, lr=0.000161, gnorm=3.69, clip=0, train_wall=6, gb_free=73.6, wall=2185 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:51:03]    INFO >> epoch 011:    580 / 1539 loss=3.395, wps=5894.1, ups=7.11, wpb=829.2, bsz=829.2, num_updates=15950, lr=0.000161, gnorm=4.414, clip=0, train_wall=6, gb_free=76.9, wall=2192 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:51:10]    INFO >> epoch 011:    630 / 1539 loss=3.528, wps=5967.5, ups=7.47, wpb=799, bsz=799, num_updates=16000, lr=0.000161, gnorm=4.251, clip=0, train_wall=6, gb_free=77.9, wall=2199 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:51:17]    INFO >> epoch 011:    680 / 1539 loss=3.803, wps=5741.6, ups=7.8, wpb=736.4, bsz=736.4, num_updates=16050, lr=0.000161, gnorm=3.812, clip=0, train_wall=6, gb_free=75.7, wall=2205 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:51:24]    INFO >> epoch 011:    730 / 1539 loss=3.591, wps=5022.6, ups=7.72, wpb=650.5, bsz=650.5, num_updates=16100, lr=0.000161, gnorm=3.791, clip=0, train_wall=6, gb_free=75.6, wall=2212 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:51:30]    INFO >> epoch 011:    780 / 1539 loss=3.679, wps=5055.7, ups=7.61, wpb=664.4, bsz=664.4, num_updates=16150, lr=0.000161, gnorm=3.963, clip=0, train_wall=6, gb_free=77.4, wall=2218 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:51:32] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 9.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 75.53 GiB is allocated by PyTorch, and 3.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:51:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:51:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:51:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 21           |        cudaMalloc retries: 32        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77283 MiB |  77343 MiB | 276457 GiB | 276382 GiB |
|       from large pool |  76939 MiB |  76999 MiB | 273381 GiB | 273306 GiB |
|       from small pool |    343 MiB |    344 MiB |   3075 GiB |   3075 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77283 MiB |  77343 MiB | 276457 GiB | 276382 GiB |
|       from large pool |  76939 MiB |  76999 MiB | 273381 GiB | 273306 GiB |
|       from small pool |    343 MiB |    344 MiB |   3075 GiB |   3075 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76965 MiB |  77024 MiB | 275075 GiB | 275000 GiB |
|       from large pool |  76624 MiB |  76683 MiB | 272004 GiB | 271929 GiB |
|       from small pool |    341 MiB |    342 MiB |   3071 GiB |   3070 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80498 MiB |  80498 MiB | 253964 MiB | 173466 MiB |
|       from large pool |  80118 MiB |  80118 MiB | 251780 MiB | 171662 MiB |
|       from small pool |    380 MiB |    434 MiB |   2184 MiB |   1804 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3154 MiB |   6329 MiB | 207163 GiB | 207159 GiB |
|       from large pool |   3118 MiB |   6296 MiB | 203586 GiB | 203583 GiB |
|       from small pool |     36 MiB |     37 MiB |   3576 GiB |   3576 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6537    |    6540    |   35231 K  |   35224 K  |
|       from large pool |     876    |     877    |   15927 K  |   15926 K  |
|       from small pool |    5661    |    5664    |   19304 K  |   19298 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    6537    |    6540    |   35231 K  |   35224 K  |
|       from large pool |     876    |     877    |   15927 K  |   15926 K  |
|       from small pool |    5661    |    5664    |   19304 K  |   19298 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     806    |     832    |    3915    |    3109    |
|       from large pool |     616    |     616    |    2823    |    2207    |
|       from small pool |     190    |     217    |    1092    |     902    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     547    |     549    |   20802 K  |   20802 K  |
|       from large pool |     203    |     203    |   11523 K  |   11522 K  |
|       from small pool |     344    |     346    |    9279 K  |    9279 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:51:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:51:32] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:51:38]    INFO >> epoch 011:    831 / 1539 loss=3.642, wps=4380.5, ups=7.11, wpb=615.7, bsz=615.7, num_updates=16200, lr=0.000161, gnorm=3.84, clip=0, train_wall=6, gb_free=76.7, wall=2225 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:51:44]    INFO >> epoch 011:    881 / 1539 loss=3.65, wps=6020.9, ups=8.06, wpb=747.5, bsz=747.5, num_updates=16250, lr=0.000161, gnorm=4.203, clip=0, train_wall=6, gb_free=74.7, wall=2231 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:51:52]    INFO >> epoch 011:    931 / 1539 loss=3.58, wps=6178.5, ups=7.71, wpb=801.4, bsz=801.4, num_updates=16300, lr=0.000161, gnorm=4.006, clip=0, train_wall=6, gb_free=75.8, wall=2238 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:51:58]    INFO >> epoch 011:    981 / 1539 loss=3.608, wps=5477, ups=8.01, wpb=683.5, bsz=683.5, num_updates=16350, lr=0.000161, gnorm=3.938, clip=0, train_wall=6, gb_free=75, wall=2244 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:52:04]    INFO >> epoch 011:   1031 / 1539 loss=3.657, wps=5357.3, ups=7.61, wpb=703.9, bsz=703.9, num_updates=16400, lr=0.000161, gnorm=3.889, clip=0, train_wall=6, gb_free=75.1, wall=2251 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:52:11] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 1.25 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 2.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:52:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:52:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:52:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 22           |        cudaMalloc retries: 33        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77826 MiB |  77886 MiB | 280967 GiB | 280891 GiB |
|       from large pool |  77388 MiB |  77448 MiB | 277841 GiB | 277765 GiB |
|       from small pool |    438 MiB |    439 MiB |   3125 GiB |   3125 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77826 MiB |  77886 MiB | 280967 GiB | 280891 GiB |
|       from large pool |  77388 MiB |  77448 MiB | 277841 GiB | 277765 GiB |
|       from small pool |    438 MiB |    439 MiB |   3125 GiB |   3125 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77575 MiB |  77635 MiB | 279562 GiB | 279486 GiB |
|       from large pool |  77139 MiB |  77199 MiB | 276441 GiB | 276365 GiB |
|       from small pool |    436 MiB |    437 MiB |   3121 GiB |   3120 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80506 MiB |  80506 MiB | 254032 MiB | 173526 MiB |
|       from large pool |  80058 MiB |  80058 MiB | 251780 MiB | 171722 MiB |
|       from small pool |    448 MiB |    448 MiB |   2252 MiB |   1804 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2679 MiB |   4073 MiB | 210652 GiB | 210649 GiB |
|       from large pool |   2669 MiB |   4069 MiB | 207015 GiB | 207013 GiB |
|       from small pool |      9 MiB |     20 MiB |   3636 GiB |   3636 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    8218    |    8219    |   35821 K  |   35813 K  |
|       from large pool |    1023    |    1024    |   16205 K  |   16204 K  |
|       from small pool |    7195    |    7196    |   19616 K  |   19609 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    8218    |    8219    |   35821 K  |   35813 K  |
|       from large pool |    1023    |    1024    |   16205 K  |   16204 K  |
|       from small pool |    7195    |    7196    |   19616 K  |   19609 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     839    |     839    |    3949    |    3110    |
|       from large pool |     615    |     615    |    2823    |    2208    |
|       from small pool |     224    |     224    |    1126    |     902    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     344    |     347    |   21150 K  |   21150 K  |
|       from large pool |     159    |     159    |   11724 K  |   11724 K  |
|       from small pool |     185    |     188    |    9425 K  |    9425 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:52:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:52:11] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:52:12]    INFO >> epoch 011:   1082 / 1539 loss=3.524, wps=5715.1, ups=7.01, wpb=814.9, bsz=814.9, num_updates=16450, lr=0.000161, gnorm=3.974, clip=0, train_wall=6, gb_free=76.2, wall=2258 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:52:18]    INFO >> epoch 011:   1132 / 1539 loss=3.238, wps=6099.5, ups=7.52, wpb=811.2, bsz=811.2, num_updates=16500, lr=0.000161, gnorm=4.337, clip=2, train_wall=6, gb_free=72.8, wall=2265 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:52:26]    INFO >> epoch 011:   1182 / 1539 loss=3.575, wps=5555.8, ups=7.7, wpb=721.4, bsz=721.4, num_updates=16550, lr=0.000161, gnorm=4.38, clip=0, train_wall=6, gb_free=76.2, wall=2271 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:52:33]    INFO >> epoch 011:   1232 / 1539 loss=3.582, wps=5233.6, ups=7.24, wpb=722.5, bsz=722.5, num_updates=16600, lr=0.000161, gnorm=3.974, clip=0, train_wall=6, gb_free=74.9, wall=2278 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:52:39]    INFO >> epoch 011:   1282 / 1539 loss=3.706, wps=6024.6, ups=7.66, wpb=786.2, bsz=786.2, num_updates=16650, lr=0.000161, gnorm=4.324, clip=0, train_wall=6, gb_free=76.1, wall=2284 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:52:47]    INFO >> epoch 011:   1332 / 1539 loss=3.717, wps=5215, ups=6.98, wpb=747, bsz=747, num_updates=16700, lr=0.000161, gnorm=3.885, clip=0, train_wall=6, gb_free=76.7, wall=2292 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:52:53]    INFO >> epoch 011:   1382 / 1539 loss=3.676, wps=5845.3, ups=7.76, wpb=753.1, bsz=753.1, num_updates=16750, lr=0.000161, gnorm=4.702, clip=0, train_wall=6, gb_free=76.7, wall=2298 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:53:01]    INFO >> epoch 011:   1432 / 1539 loss=3.727, wps=5490.5, ups=7.51, wpb=731.2, bsz=731.2, num_updates=16800, lr=0.000161, gnorm=4.019, clip=0, train_wall=6, gb_free=75.6, wall=2305 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:53:08]    INFO >> epoch 011:   1482 / 1539 loss=3.296, wps=5633.9, ups=7.22, wpb=780.7, bsz=780.7, num_updates=16850, lr=0.000161, gnorm=4.71, clip=2, train_wall=6, gb_free=76.4, wall=2312 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:53:15]    INFO >> epoch 011:   1532 / 1539 loss=3.616, wps=5705.9, ups=7.61, wpb=750.1, bsz=750.1, num_updates=16900, lr=0.000161, gnorm=3.913, clip=0, train_wall=6, gb_free=76.9, wall=2318 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:53:15]    INFO >> epoch 011 | loss 3.613 | wps 5249.4 | ups 7.28 | wpb 720.8 | bsz 720.8 | num_updates 16907 | lr 0.000161 | gnorm 4.09 | clip 0.1 | train_wall 182 | gb_free 76.8 | wall 2319 (progress_bar.py:267, print())[0m
[33m[2025-11-21 00:53:15] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:53:24]    INFO >> epoch 011 | valid on 'valid' subset | loss 3.818 | wps 16817.8 | wpb 5412.5 | bsz 5412.5 | num_updates 16907 | best_loss 5.506 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 00:53:25]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 00:53:25]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/checkpoints/checkpoint_last.pt (epoch 11 @ 16907 updates, score 3.818) (writing took 0.012256 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[32m[2025-11-21 00:53:25]    INFO >> æ—©åœ: éªŒè¯æ€§èƒ½å·²10è½®æœªæå‡ (train_enhanced.py:616, single_main())[0m
[32m[2025-11-21 00:53:25]    INFO >> è®­ç»ƒå®Œæˆï¼Œç”¨æ—¶ 2252.0 ç§’ (train_enhanced.py:626, single_main())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 00:53:25]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 00:53:25]    INFO >> æ‰€æœ‰æ—¥å¿—å·²ä¿å­˜åˆ°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs (train_enhanced.py:631, single_main())[0m
[32m[2025-11-21 00:53:25]    INFO >> 
================================================================================ (train_enhanced.py:634, single_main())[0m
[32m[2025-11-21 00:53:25]    INFO >> å¼€å§‹æµ‹è¯•... (train_enhanced.py:635, single_main())[0m
[32m[2025-11-21 00:53:25]    INFO >> ================================================================================ (train_enhanced.py:636, single_main())[0m
[32m[2025-11-21 00:53:25]    INFO >> åŠ è½½æœ€ä½³checkpoint: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/checkpoints/checkpoint_best.pt (train_enhanced.py:50, run_test_after_training())[0m
[32m[2025-11-21 00:53:25]    INFO >> æµ‹è¯•é›†: test (train_enhanced.py:51, run_test_after_training())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/checkpoint_utils.py:212: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(
[32m[2025-11-21 00:54:19]    INFO >> 
================================================================================ (train_enhanced.py:168, run_test_after_training())[0m
[32m[2025-11-21 00:54:19]    INFO >> æµ‹è¯•ç»“æžœ: (train_enhanced.py:169, run_test_after_training())[0m
[32m[2025-11-21 00:54:19]    INFO >> -------------------------------------------------------------------------------- (train_enhanced.py:170, run_test_after_training())[0m
[32m[2025-11-21 00:54:19]    INFO >> å¹³å‡Loss:      3.9709 (train_enhanced.py:171, run_test_after_training())[0m
[32m[2025-11-21 00:54:19]    INFO >> Acc@1:         18.18% (train_enhanced.py:172, run_test_after_training())[0m
[32m[2025-11-21 00:54:19]    INFO >> Acc@5:         51.89% (train_enhanced.py:173, run_test_after_training())[0m
[32m[2025-11-21 00:54:19]    INFO >> Acc@1 (å«any): 18.18% (train_enhanced.py:174, run_test_after_training())[0m
[32m[2025-11-21 00:54:19]    INFO >> Acc@5 (å«any): 51.89% (train_enhanced.py:175, run_test_after_training())[0m
[32m[2025-11-21 00:54:19]    INFO >> ================================================================================ (train_enhanced.py:176, run_test_after_training())[0m
[32m[2025-11-21 00:54:19]    INFO >> æµ‹è¯•ç»“æžœå·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/checkpoints/res.txt (train_enhanced.py:187, run_test_after_training())[0m
[32m[2025-11-21 00:54:19]    INFO >> è®­ç»ƒæ—¥å¿—å·²æ›´æ–°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs (train_enhanced.py:222, run_test_after_training())[0m
[TrainingLogger] æ—¥å¿—ç›®å½•: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs
[TrainingLogger] åŽŸå§‹è¾“å‡ºå°†ä¿å­˜åˆ°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/training_output.log
[TrainingLogger] Epoch 1 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/metrics.json
[TrainingLogger] Epoch 2 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/metrics.json
[TrainingLogger] Epoch 3 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/metrics.json
[TrainingLogger] Epoch 4 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/metrics.json
[TrainingLogger] Epoch 5 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/metrics.json
[TrainingLogger] Epoch 6 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/metrics.json
[TrainingLogger] Epoch 7 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/metrics.json
[TrainingLogger] Epoch 8 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/metrics.json
[TrainingLogger] Epoch 9 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/metrics.json
[TrainingLogger] Epoch 10 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/metrics.json
[TrainingLogger] Epoch 11 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/metrics.json

âœ“ embed_32 æˆåŠŸ

ç­‰å¾…3ç§’...

è¿›åº¦: 2/16

============================================================
å®žéªŒ: embed_96 - åµŒå…¥ç»´åº¦96 (å¢žåŠ 50%ï¼Œæå‡å®¹é‡)
æ—¶é—´: 2025-11-21 00:55:08
============================================================

[32m[2025-11-21 00:55:12]    INFO >> åŠ è½½é…ç½®: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/config.yml (train_enhanced.py:666, cli_main())[0m
[32m[2025-11-21 00:55:12]    INFO >> å•GPUè®­ç»ƒ... (train_enhanced.py:694, cli_main())[0m
[32m[2025-11-21 00:55:12]    INFO >> è®­ç»ƒæ—¥å¿—å°†ä¿å­˜åˆ°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs (train_enhanced.py:561, single_main())[0m
[32m[2025-11-21 00:55:12]    INFO >> [nodes] dictionary: 9999 types (typilus.py:102, setup_task())[0m
[32m[2025-11-21 00:55:12]    INFO >> [edges] dictionary: 0 types (typilus.py:102, setup_task())[0m
[32m[2025-11-21 00:55:12]    INFO >> [supernodes.annotation] dictionary: 99 types (typilus.py:106, setup_task())[0m
[32m[2025-11-21 00:55:23]    INFO >> Typilus(
  (encoder): GGNNEncoder(
    (node_embedding): Embedding(9999, 96, padding_idx=0)
    (node_layer): Sequential(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=96, out_features=96, bias=False)
      (2): Dropout(p=0.1, inplace=False)
    )
    (ggnns): ModuleList(
      (0): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=96, out_features=96, bias=False)
          (OCCURRENCE_OF): Linear(in_features=96, out_features=96, bias=False)
          (NEXT): Linear(in_features=96, out_features=96, bias=False)
          (SUBTOKEN_OF): Linear(in_features=96, out_features=96, bias=False)
          (COMPUTED_FROM): Linear(in_features=96, out_features=96, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=96, out_features=96, bias=False)
          (NEXT_USE): Linear(in_features=96, out_features=96, bias=False)
          (RETURNS_TO): Linear(in_features=96, out_features=96, bias=False)
          (_CHILD): Linear(in_features=96, out_features=96, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=96, out_features=96, bias=False)
          (_NEXT): Linear(in_features=96, out_features=96, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=96, out_features=96, bias=False)
          (_COMPUTED_FROM): Linear(in_features=96, out_features=96, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=96, out_features=96, bias=False)
          (_NEXT_USE): Linear(in_features=96, out_features=96, bias=False)
          (_RETURNS_TO): Linear(in_features=96, out_features=96, bias=False)
        )
        (rnn_cell): GRUCell(96, 96)
      )
      (1): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=96, out_features=96, bias=False)
          (OCCURRENCE_OF): Linear(in_features=96, out_features=96, bias=False)
          (NEXT): Linear(in_features=96, out_features=96, bias=False)
          (SUBTOKEN_OF): Linear(in_features=96, out_features=96, bias=False)
          (COMPUTED_FROM): Linear(in_features=96, out_features=96, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=96, out_features=96, bias=False)
          (NEXT_USE): Linear(in_features=96, out_features=96, bias=False)
          (RETURNS_TO): Linear(in_features=96, out_features=96, bias=False)
          (_CHILD): Linear(in_features=96, out_features=96, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=96, out_features=96, bias=False)
          (_NEXT): Linear(in_features=96, out_features=96, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=96, out_features=96, bias=False)
          (_COMPUTED_FROM): Linear(in_features=96, out_features=96, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=96, out_features=96, bias=False)
          (_NEXT_USE): Linear(in_features=96, out_features=96, bias=False)
          (_RETURNS_TO): Linear(in_features=96, out_features=96, bias=False)
        )
        (rnn_cell): GRUCell(192, 96)
      )
    )
  )
  (decoder): DenseDecoder(
    (cls_layers): Sequential(
      (0): Linear(in_features=96, out_features=96, bias=False)
      (1): Dropout(p=0.1, inplace=False)
      (2): Linear(in_features=96, out_features=99, bias=True)
    )
  )
) (train_enhanced.py:568, single_main())[0m
[32m[2025-11-21 00:55:23]    INFO >> æ¨¡åž‹: typilus, æŸå¤±å‡½æ•°: TypilusCriterion (train_enhanced.py:569, single_main())[0m
[32m[2025-11-21 00:55:23]    INFO >> æ¨¡åž‹å‚æ•°: 1422243 (å¯è®­ç»ƒ: 1422243) (train_enhanced.py:570, single_main())[0m
[32m[2025-11-21 00:55:23]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 00:55:23]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 80579 MB ; used memory = 1340 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 00:55:23]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 00:55:23]    INFO >> ä½¿ç”¨ 1 ä¸ªGPUè®­ç»ƒ (train_enhanced.py:576, single_main())[0m
[32m[2025-11-21 00:55:23]    INFO >> no existing checkpoint found /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-21 00:55:23]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[32m[2025-11-21 00:56:40]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-21 00:56:40] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-21 00:56:51]    INFO >> epoch 001:     50 / 1539 loss=5.69, wps=3905.4, ups=5.4, wpb=720, bsz=720, num_updates=50, lr=0.0004, gnorm=6.657, clip=0, train_wall=9, gb_free=71.6, wall=85 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:56:54] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 23.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.46 GiB is allocated by PyTorch, and 1.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:56:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:56:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:56:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 3         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79255 MiB |  79315 MiB |   2476 GiB |   2398 GiB |
|       from large pool |  79074 MiB |  79134 MiB |   2465 GiB |   2387 GiB |
|       from small pool |    181 MiB |    182 MiB |     10 GiB |     10 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79255 MiB |  79315 MiB |   2476 GiB |   2398 GiB |
|       from large pool |  79074 MiB |  79134 MiB |   2465 GiB |   2387 GiB |
|       from small pool |    181 MiB |    182 MiB |     10 GiB |     10 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79094 MiB |  79153 MiB |   2470 GiB |   2393 GiB |
|       from large pool |  78913 MiB |  78973 MiB |   2459 GiB |   2382 GiB |
|       from small pool |    180 MiB |    181 MiB |     10 GiB |     10 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80484 MiB |  80484 MiB | 133948 MiB |  53464 MiB |
|       from large pool |  80284 MiB |  80284 MiB | 133636 MiB |  53352 MiB |
|       from small pool |    200 MiB |    200 MiB |    312 MiB |    112 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1168 MiB |   5387 MiB |   1230 GiB |   1228 GiB |
|       from large pool |   1149 MiB |   5381 MiB |   1217 GiB |   1216 GiB |
|       from small pool |     18 MiB |     25 MiB |     12 GiB |     12 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3378    |    3381    |  122658    |  119280    |
|       from large pool |     585    |     586    |   57467    |   56882    |
|       from small pool |    2793    |    2796    |   65191    |   62398    |
|---------------------------------------------------------------------------|
| Active allocs         |    3378    |    3381    |  122658    |  119280    |
|       from large pool |     585    |     586    |   57467    |   56882    |
|       from small pool |    2793    |    2796    |   65191    |   62398    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     497    |     497    |    1002    |     505    |
|       from large pool |     397    |     420    |     846    |     449    |
|       from small pool |     100    |     100    |     156    |      56    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     272    |     273    |   74230    |   73958    |
|       from large pool |      97    |      97    |   42388    |   42291    |
|       from small pool |     175    |     176    |   31842    |   31667    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:56:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:56:54] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:57:01]    INFO >> epoch 001:    101 / 1539 loss=5.921, wps=3270.3, ups=5.36, wpb=610.7, bsz=610.7, num_updates=100, lr=0.0004, gnorm=6.296, clip=0, train_wall=7, gb_free=73.5, wall=94 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:57:11]    INFO >> epoch 001:    151 / 1539 loss=6.039, wps=4206.7, ups=5.15, wpb=816.7, bsz=816.7, num_updates=150, lr=0.0004, gnorm=6.383, clip=0, train_wall=9, gb_free=71.4, wall=104 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:57:19]    INFO >> epoch 001:    201 / 1539 loss=6.003, wps=3780.6, ups=5.89, wpb=641.7, bsz=641.7, num_updates=200, lr=0.0004, gnorm=6.141, clip=0, train_wall=8, gb_free=72.3, wall=112 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:57:29]    INFO >> epoch 001:    251 / 1539 loss=5.967, wps=3688.9, ups=5.78, wpb=637.9, bsz=637.9, num_updates=250, lr=0.0004, gnorm=6.588, clip=0, train_wall=8, gb_free=67.3, wall=121 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:57:39]    INFO >> epoch 001:    301 / 1539 loss=5.728, wps=4056.2, ups=5.16, wpb=786.5, bsz=786.5, num_updates=300, lr=0.0004, gnorm=5.832, clip=0, train_wall=9, gb_free=70.7, wall=131 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:57:47]    INFO >> epoch 001:    351 / 1539 loss=5.87, wps=3829.1, ups=5.7, wpb=671.6, bsz=671.6, num_updates=350, lr=0.0004, gnorm=5.968, clip=0, train_wall=8, gb_free=68.8, wall=139 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:57:59]    INFO >> epoch 001:    401 / 1539 loss=5.598, wps=4367.5, ups=5.13, wpb=851.7, bsz=851.7, num_updates=400, lr=0.0004, gnorm=7.122, clip=2, train_wall=9, gb_free=69.9, wall=149 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:58:05] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.77 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.55 GiB is free. Including non-PyTorch memory, this process has 77.56 GiB memory in use. Of the allocated memory 75.98 GiB is allocated by PyTorch, and 1.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:58:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:58:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:58:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 5         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77209 MiB |  78288 MiB |  17798 GiB |  17722 GiB |
|       from large pool |  77187 MiB |  78266 MiB |  17725 GiB |  17649 GiB |
|       from small pool |     22 MiB |     23 MiB |     72 GiB |     72 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77209 MiB |  78288 MiB |  17798 GiB |  17722 GiB |
|       from large pool |  77187 MiB |  78266 MiB |  17725 GiB |  17649 GiB |
|       from small pool |     22 MiB |     23 MiB |     72 GiB |     72 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77192 MiB |  78269 MiB |  17767 GiB |  17692 GiB |
|       from large pool |  77170 MiB |  78247 MiB |  17695 GiB |  17619 GiB |
|       from small pool |     22 MiB |     23 MiB |     72 GiB |     72 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78916 MiB |  80472 MiB | 203698 MiB | 124782 MiB |
|       from large pool |  78888 MiB |  80224 MiB | 203338 MiB | 124450 MiB |
|       from small pool |     28 MiB |    248 MiB |    360 MiB |    332 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1706 MiB |   5120 MiB |  14116 GiB |  14114 GiB |
|       from large pool |   1700 MiB |   5113 MiB |  14033 GiB |  14031 GiB |
|       from small pool |      5 MiB |     26 MiB |     83 GiB |     83 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     564    |     572    |     848 K  |     848 K  |
|       from large pool |     270    |     278    |     427 K  |     427 K  |
|       from small pool |     294    |     354    |     421 K  |     421 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     564    |     572    |     848 K  |     848 K  |
|       from large pool |     270    |     278    |     427 K  |     427 K  |
|       from small pool |     294    |     354    |     421 K  |     421 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     117    |     520    |    1087    |     970    |
|       from large pool |     103    |     396    |     907    |     804    |
|       from small pool |      14    |     124    |     180    |     166    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     128    |     128    |  496831    |  496703    |
|       from large pool |      99    |      99    |  300973    |  300874    |
|       from small pool |      29    |      56    |  195858    |  195829    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:58:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:58:05] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:58:15]    INFO >> epoch 001:    452 / 1539 loss=5.804, wps=1924.1, ups=3.05, wpb=631.6, bsz=631.6, num_updates=450, lr=0.0004, gnorm=4.998, clip=0, train_wall=9, gb_free=68.1, wall=166 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:58:20] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 2.69 GiB is free. Including non-PyTorch memory, this process has 76.42 GiB memory in use. Of the allocated memory 73.71 GiB is allocated by PyTorch, and 2.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:58:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:58:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:58:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 3            |        cudaMalloc retries: 7         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75044 MiB |  75474 MiB |  19592 GiB |  19518 GiB |
|       from large pool |  75018 MiB |  75448 MiB |  19513 GiB |  19439 GiB |
|       from small pool |     25 MiB |     26 MiB |     79 GiB |     79 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75044 MiB |  75474 MiB |  19592 GiB |  19518 GiB |
|       from large pool |  75018 MiB |  75448 MiB |  19513 GiB |  19439 GiB |
|       from small pool |     25 MiB |     26 MiB |     79 GiB |     79 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75024 MiB |  75454 MiB |  19559 GiB |  19485 GiB |
|       from large pool |  74999 MiB |  75428 MiB |  19480 GiB |  19406 GiB |
|       from small pool |     25 MiB |     26 MiB |     78 GiB |     78 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77748 MiB |  78948 MiB | 274718 MiB | 196970 MiB |
|       from large pool |  77716 MiB |  78888 MiB | 274326 MiB | 196610 MiB |
|       from small pool |     32 MiB |     60 MiB |    392 MiB |    360 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2703 MiB |   5140 MiB |  15997 GiB |  15994 GiB |
|       from large pool |   2697 MiB |   5133 MiB |  15907 GiB |  15904 GiB |
|       from small pool |      6 MiB |     24 MiB |     89 GiB |     89 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     565    |     572    |     926 K  |     926 K  |
|       from large pool |     265    |     272    |     469 K  |     469 K  |
|       from small pool |     300    |     354    |     457 K  |     456 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     565    |     572    |     926 K  |     926 K  |
|       from large pool |     265    |     272    |     469 K  |     469 K  |
|       from small pool |     300    |     354    |     457 K  |     456 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      82    |     133    |    1132    |    1050    |
|       from large pool |      66    |     103    |     936    |     870    |
|       from small pool |      16    |      30    |     196    |     180    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      92    |      94    |  538319    |  538227    |
|       from large pool |      59    |      61    |  327675    |  327616    |
|       from small pool |      33    |      56    |  210644    |  210611    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:58:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:58:20] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:58:27]    INFO >> epoch 001:    503 / 1539 loss=5.572, wps=2912, ups=4.21, wpb=692.2, bsz=692.2, num_updates=500, lr=0.0004, gnorm=6.616, clip=2, train_wall=9, gb_free=70.9, wall=178 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:58:38]    INFO >> epoch 001:    553 / 1539 loss=5.681, wps=3349.5, ups=5.14, wpb=651.5, bsz=651.5, num_updates=550, lr=0.0004, gnorm=5.094, clip=0, train_wall=9, gb_free=67.6, wall=187 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:58:48]    INFO >> epoch 001:    603 / 1539 loss=5.584, wps=3426.8, ups=5.07, wpb=676.1, bsz=676.1, num_updates=600, lr=0.0004, gnorm=5.62, clip=0, train_wall=9, gb_free=65.3, wall=197 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:58:59]    INFO >> epoch 001:    653 / 1539 loss=5.434, wps=3225.6, ups=4.57, wpb=705.7, bsz=705.7, num_updates=650, lr=0.0004, gnorm=5.785, clip=0, train_wall=10, gb_free=72.2, wall=208 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:59:12]    INFO >> epoch 001:    703 / 1539 loss=5.372, wps=2829.4, ups=4.16, wpb=680.4, bsz=680.4, num_updates=700, lr=0.0004, gnorm=5.336, clip=0, train_wall=11, gb_free=69.1, wall=220 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:59:23]    INFO >> epoch 001:    753 / 1539 loss=5.04, wps=3554.3, ups=4.7, wpb=756.5, bsz=756.5, num_updates=750, lr=0.0004, gnorm=7.41, clip=4, train_wall=10, gb_free=70.1, wall=231 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:59:33]    INFO >> epoch 001:    803 / 1539 loss=5.21, wps=3517.4, ups=4.85, wpb=725.4, bsz=725.4, num_updates=800, lr=0.0004, gnorm=5.739, clip=0, train_wall=10, gb_free=55.3, wall=241 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:59:45]    INFO >> epoch 001:    853 / 1539 loss=4.895, wps=3014.9, ups=4.7, wpb=641.9, bsz=641.9, num_updates=850, lr=0.0004, gnorm=5.292, clip=0, train_wall=10, gb_free=68.9, wall=252 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:59:55]    INFO >> epoch 001:    903 / 1539 loss=4.79, wps=3337.8, ups=5.12, wpb=651.9, bsz=651.9, num_updates=900, lr=0.0004, gnorm=5.034, clip=0, train_wall=9, gb_free=71.4, wall=261 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:00:05]    INFO >> epoch 001:    953 / 1539 loss=4.552, wps=3458.7, ups=4.82, wpb=717.7, bsz=717.7, num_updates=950, lr=0.0004, gnorm=6.116, clip=0, train_wall=9, gb_free=68.7, wall=272 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:00:09] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.54 GiB is free. Including non-PyTorch memory, this process has 77.58 GiB memory in use. Of the allocated memory 76.06 GiB is allocated by PyTorch, and 1.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:00:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:00:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:00:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 4            |        cudaMalloc retries: 10        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77289 MiB |  77889 MiB |  40735 GiB |  40659 GiB |
|       from large pool |  77273 MiB |  77873 MiB |  40581 GiB |  40506 GiB |
|       from small pool |     16 MiB |     24 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77289 MiB |  77889 MiB |  40735 GiB |  40659 GiB |
|       from large pool |  77273 MiB |  77873 MiB |  40581 GiB |  40506 GiB |
|       from small pool |     16 MiB |     24 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77280 MiB |  77879 MiB |  40674 GiB |  40598 GiB |
|       from large pool |  77263 MiB |  77862 MiB |  40520 GiB |  40445 GiB |
|       from small pool |     16 MiB |     24 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78932 MiB |  80482 MiB | 344502 MiB | 265570 MiB |
|       from large pool |  78904 MiB |  80376 MiB | 343982 MiB | 265078 MiB |
|       from small pool |     28 MiB |    106 MiB |    520 MiB |    492 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1642 MiB |   5057 MiB |  40343 GiB |  40342 GiB |
|       from large pool |   1630 MiB |   5044 MiB |  40170 GiB |  40168 GiB |
|       from small pool |     11 MiB |     25 MiB |    173 GiB |    173 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     526    |     533    |    1855 K  |    1854 K  |
|       from large pool |     239    |     246    |     964 K  |     964 K  |
|       from small pool |     287    |     354    |     890 K  |     890 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     526    |     533    |    1855 K  |    1854 K  |
|       from large pool |     239    |     246    |     964 K  |     964 K  |
|       from small pool |     287    |     354    |     890 K  |     890 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      74    |     118    |    1230    |    1156    |
|       from large pool |      60    |      65    |     970    |     910    |
|       from small pool |      14    |      53    |     260    |     246    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      89    |      89    |    1028 K  |    1028 K  |
|       from large pool |      61    |      61    |     630 K  |     630 K  |
|       from small pool |      28    |      55    |     397 K  |     397 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:00:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:00:09] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:00:19]    INFO >> epoch 001:   1004 / 1539 loss=4.47, wps=2618.3, ups=4.02, wpb=651.1, bsz=651.1, num_updates=1000, lr=0.0004, gnorm=6.153, clip=0, train_wall=9, gb_free=67.4, wall=284 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:00:30]    INFO >> epoch 001:   1054 / 1539 loss=4.45, wps=3663.5, ups=4.46, wpb=822.1, bsz=822.1, num_updates=1050, lr=0.0004, gnorm=7.48, clip=2, train_wall=10, gb_free=65.4, wall=296 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:00:41] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 722.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 83.25 MiB is free. Including non-PyTorch memory, this process has 79.04 GiB memory in use. Of the allocated memory 73.99 GiB is allocated by PyTorch, and 4.54 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:00:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:00:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:00:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 5            |        cudaMalloc retries: 12        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72193 MiB |  76053 MiB |  46922 GiB |  46852 GiB |
|       from large pool |  72175 MiB |  76035 MiB |  46744 GiB |  46673 GiB |
|       from small pool |     18 MiB |     21 MiB |    178 GiB |    178 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72193 MiB |  76053 MiB |  46922 GiB |  46852 GiB |
|       from large pool |  72175 MiB |  76035 MiB |  46744 GiB |  46673 GiB |
|       from small pool |     18 MiB |     21 MiB |    178 GiB |    178 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72184 MiB |  76043 MiB |  46853 GiB |  46783 GiB |
|       from large pool |  72166 MiB |  76025 MiB |  46675 GiB |  46605 GiB |
|       from small pool |     18 MiB |     21 MiB |    178 GiB |    178 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80422 MiB |  80422 MiB | 348978 MiB | 268556 MiB |
|       from large pool |  80388 MiB |  80388 MiB | 348276 MiB | 267888 MiB |
|       from small pool |     34 MiB |    210 MiB |    702 MiB |    668 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5376 MiB |  10840 MiB |  47553 GiB |  47547 GiB |
|       from large pool |   5360 MiB |  10823 MiB |  47351 GiB |  47345 GiB |
|       from small pool |     15 MiB |     31 MiB |    202 GiB |    202 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     507    |     516    |    2140 K  |    2139 K  |
|       from large pool |     220    |     229    |    1104 K  |    1103 K  |
|       from small pool |     287    |     354    |    1035 K  |    1035 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     507    |     516    |    2140 K  |    2139 K  |
|       from large pool |     220    |     229    |    1104 K  |    1103 K  |
|       from small pool |     287    |     354    |    1035 K  |    1035 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      74    |     165    |    1323    |    1249    |
|       from large pool |      57    |      60    |     972    |     915    |
|       from small pool |      17    |     105    |     351    |     334    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      83    |      83    |    1181 K  |    1181 K  |
|       from large pool |      53    |      53    |     712 K  |     712 K  |
|       from small pool |      30    |      67    |     468 K  |     468 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:00:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:00:41] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:00:43]    INFO >> epoch 001:   1105 / 1539 loss=4.715, wps=3631.4, ups=3.9, wpb=930.5, bsz=930.5, num_updates=1100, lr=0.0004, gnorm=6.031, clip=0, train_wall=11, gb_free=58.8, wall=308 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:00:55]    INFO >> epoch 001:   1155 / 1539 loss=4.364, wps=3347.8, ups=4.85, wpb=690.2, bsz=690.2, num_updates=1150, lr=0.0004, gnorm=6.36, clip=0, train_wall=9, gb_free=54.3, wall=319 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:01:05]    INFO >> epoch 001:   1205 / 1539 loss=4.398, wps=3295.8, ups=4.87, wpb=676.1, bsz=676.1, num_updates=1200, lr=0.0004, gnorm=6.425, clip=2, train_wall=9, gb_free=61.7, wall=329 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:01:16]    INFO >> epoch 001:   1255 / 1539 loss=4.378, wps=3465, ups=4.71, wpb=735.2, bsz=735.2, num_updates=1250, lr=0.0004, gnorm=6.236, clip=0, train_wall=10, gb_free=70.2, wall=339 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:01:28]    INFO >> epoch 001:   1305 / 1539 loss=4.224, wps=3351.9, ups=4.49, wpb=746.4, bsz=746.4, num_updates=1300, lr=0.0004, gnorm=5.92, clip=0, train_wall=10, gb_free=63.5, wall=351 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:01:38]    INFO >> epoch 001:   1355 / 1539 loss=4.38, wps=3205.6, ups=4.96, wpb=645.9, bsz=645.9, num_updates=1350, lr=0.0004, gnorm=5.634, clip=0, train_wall=9, gb_free=69.7, wall=361 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:01:49]    INFO >> epoch 001:   1405 / 1539 loss=4.152, wps=3200.5, ups=4.51, wpb=710.1, bsz=710.1, num_updates=1400, lr=0.0004, gnorm=5.879, clip=0, train_wall=10, gb_free=58.6, wall=372 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:02:01]    INFO >> epoch 001:   1455 / 1539 loss=4.192, wps=3428.3, ups=4.87, wpb=703.3, bsz=703.3, num_updates=1450, lr=0.0004, gnorm=7.031, clip=0, train_wall=9, gb_free=64.9, wall=382 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:02:12]    INFO >> epoch 001:   1505 / 1539 loss=4.105, wps=3134.9, ups=4.52, wpb=693.7, bsz=693.7, num_updates=1500, lr=0.0004, gnorm=6.672, clip=2, train_wall=10, gb_free=69.1, wall=393 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:02:19]    INFO >> epoch 001 | loss 5.054 | wps 3361.3 | ups 4.73 | wpb 711.1 | bsz 711.1 | num_updates 1534 | lr 0.0004 | gnorm 6.155 | clip 0.5 | train_wall 285 | gb_free 75 | wall 400 (progress_bar.py:267, print())[0m
[33m[2025-11-21 01:02:19] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:02:43]    INFO >> epoch 001 | valid on 'valid' subset | loss 4.043 | wps 6754.5 | wpb 5412.5 | bsz 5412.5 | num_updates 1534 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
[32m[2025-11-21 01:02:43]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 01:02:43]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/checkpoints/checkpoint_best.pt (epoch 1 @ 1534 updates, score 4.043) (writing took 0.022064 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 01:02:43] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-21 01:02:46]    INFO >> epoch 002:     16 / 1539 loss=4.146, wps=1108.9, ups=1.51, wpb=735.1, bsz=735.1, num_updates=1550, lr=0.0004, gnorm=6.438, clip=0, train_wall=9, gb_free=70.4, wall=426 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:02:56]    INFO >> epoch 002:     66 / 1539 loss=4.043, wps=3594.8, ups=5.46, wpb=658.3, bsz=658.3, num_updates=1600, lr=0.0004, gnorm=5.98, clip=0, train_wall=9, gb_free=68, wall=435 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:03:07]    INFO >> epoch 002:    116 / 1539 loss=4.115, wps=3479.3, ups=4.87, wpb=713.9, bsz=713.9, num_updates=1650, lr=0.0004, gnorm=5.859, clip=0, train_wall=10, gb_free=68.7, wall=446 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:03:17]    INFO >> epoch 002:    166 / 1539 loss=3.79, wps=3690.3, ups=4.97, wpb=743.2, bsz=743.2, num_updates=1700, lr=0.0004, gnorm=5.998, clip=0, train_wall=10, gb_free=68.3, wall=456 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:03:28]    INFO >> epoch 002:    216 / 1539 loss=4.24, wps=3418.2, ups=4.85, wpb=704.6, bsz=704.6, num_updates=1750, lr=0.0004, gnorm=6.338, clip=0, train_wall=10, gb_free=69.2, wall=466 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:03:40]    INFO >> epoch 002:    266 / 1539 loss=3.781, wps=3816, ups=4.36, wpb=876, bsz=876, num_updates=1800, lr=0.0004, gnorm=6.806, clip=0, train_wall=11, gb_free=61.4, wall=478 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:03:50]    INFO >> epoch 002:    316 / 1539 loss=4.051, wps=3167.9, ups=4.94, wpb=640.7, bsz=640.7, num_updates=1850, lr=0.0004, gnorm=5.868, clip=2, train_wall=10, gb_free=65.7, wall=488 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:04:01]    INFO >> epoch 002:    366 / 1539 loss=3.969, wps=3233.2, ups=4.92, wpb=656.7, bsz=656.7, num_updates=1900, lr=0.0004, gnorm=6.068, clip=0, train_wall=10, gb_free=65.3, wall=498 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:04:13]    INFO >> epoch 002:    416 / 1539 loss=3.982, wps=3156.3, ups=4.43, wpb=712.6, bsz=712.6, num_updates=1950, lr=0.0004, gnorm=5.195, clip=0, train_wall=11, gb_free=68.1, wall=509 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:04:14] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 2 has a total capacity of 79.14 GiB of which 2.87 GiB is free. Including non-PyTorch memory, this process has 76.25 GiB memory in use. Of the allocated memory 72.07 GiB is allocated by PyTorch, and 3.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:04:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:04:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:04:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 6            |        cudaMalloc retries: 13        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73200 MiB |  74277 MiB |  89318 GiB |  89246 GiB |
|       from large pool |  73184 MiB |  74260 MiB |  88966 GiB |  88894 GiB |
|       from small pool |     16 MiB |     27 MiB |    351 GiB |    351 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73200 MiB |  74277 MiB |  89318 GiB |  89246 GiB |
|       from large pool |  73184 MiB |  74260 MiB |  88966 GiB |  88894 GiB |
|       from small pool |     16 MiB |     27 MiB |    351 GiB |    351 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73189 MiB |  74265 MiB |  89199 GiB |  89127 GiB |
|       from large pool |  73173 MiB |  74248 MiB |  88848 GiB |  88776 GiB |
|       from small pool |     16 MiB |     27 MiB |    351 GiB |    351 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77566 MiB |  77746 MiB | 349154 MiB | 271588 MiB |
|       from large pool |  77536 MiB |  77536 MiB | 348276 MiB | 270740 MiB |
|       from small pool |     30 MiB |    210 MiB |    878 MiB |    848 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4365 MiB |   7738 MiB |  94371 GiB |  94367 GiB |
|       from large pool |   4351 MiB |   7724 MiB |  93979 GiB |  93974 GiB |
|       from small pool |     13 MiB |     27 MiB |    392 GiB |    392 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     524    |     532    |    4064 K  |    4064 K  |
|       from large pool |     238    |     246    |    2004 K  |    2003 K  |
|       from small pool |     286    |     356    |    2060 K  |    2060 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     524    |     532    |    4064 K  |    4064 K  |
|       from large pool |     238    |     246    |    2004 K  |    2003 K  |
|       from small pool |     286    |     356    |    2060 K  |    2060 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      71    |     161    |    1411    |    1340    |
|       from large pool |      56    |      56    |     972    |     916    |
|       from small pool |      15    |     105    |     439    |     424    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      82    |      82    |    2223 K  |    2223 K  |
|       from large pool |      52    |      52    |    1249 K  |    1249 K  |
|       from small pool |      30    |      60    |     974 K  |     974 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:04:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:04:14] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:04:25]    INFO >> epoch 002:    467 / 1539 loss=3.992, wps=3207.8, ups=4.43, wpb=724.9, bsz=724.9, num_updates=2000, lr=0.0004, gnorm=5.593, clip=0, train_wall=10, gb_free=67.4, wall=520 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:04:35]    INFO >> epoch 002:    517 / 1539 loss=4.054, wps=3383.2, ups=4.79, wpb=706.2, bsz=706.2, num_updates=2050, lr=0.0004, gnorm=5.93, clip=0, train_wall=10, gb_free=72.9, wall=531 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:04:46]    INFO >> epoch 002:    567 / 1539 loss=3.966, wps=3193.6, ups=5.02, wpb=636.1, bsz=636.1, num_updates=2100, lr=0.0004, gnorm=6.124, clip=0, train_wall=9, gb_free=59.6, wall=541 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:04:57]    INFO >> epoch 002:    617 / 1539 loss=4.039, wps=4061.5, ups=4.73, wpb=859.3, bsz=859.3, num_updates=2150, lr=0.0004, gnorm=6.298, clip=0, train_wall=10, gb_free=67.8, wall=551 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:05:07]    INFO >> epoch 002:    667 / 1539 loss=3.77, wps=3630.2, ups=4.7, wpb=773, bsz=773, num_updates=2200, lr=0.0004, gnorm=6.419, clip=2, train_wall=10, gb_free=69.3, wall=562 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:05:18]    INFO >> epoch 002:    717 / 1539 loss=3.909, wps=3708.1, ups=5.22, wpb=710.8, bsz=710.8, num_updates=2250, lr=0.0004, gnorm=5.844, clip=0, train_wall=9, gb_free=68.4, wall=572 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:05:29]    INFO >> epoch 002:    767 / 1539 loss=3.963, wps=3296.3, ups=4.9, wpb=673, bsz=673, num_updates=2300, lr=0.0004, gnorm=5.461, clip=0, train_wall=10, gb_free=67.6, wall=582 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:05:39]    INFO >> epoch 002:    817 / 1539 loss=4.112, wps=3106.2, ups=4.8, wpb=647.4, bsz=647.4, num_updates=2350, lr=0.0004, gnorm=5.34, clip=0, train_wall=10, gb_free=70.4, wall=592 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:05:51]    INFO >> epoch 002:    867 / 1539 loss=3.998, wps=3361.4, ups=4.63, wpb=726.4, bsz=726.4, num_updates=2400, lr=0.0004, gnorm=6.074, clip=0, train_wall=10, gb_free=66.5, wall=603 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:06:01]    INFO >> epoch 002:    917 / 1539 loss=3.971, wps=3253.2, ups=4.86, wpb=668.7, bsz=668.7, num_updates=2450, lr=0.0004, gnorm=5.241, clip=0, train_wall=10, gb_free=63.4, wall=613 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:06:11]    INFO >> epoch 002:    967 / 1539 loss=3.804, wps=3238.7, ups=5.13, wpb=631.4, bsz=631.4, num_updates=2500, lr=0.0004, gnorm=5.276, clip=0, train_wall=9, gb_free=72, wall=623 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:06:12] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 7.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.58 GiB is allocated by PyTorch, and 1.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:06:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:06:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:06:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 7            |        cudaMalloc retries: 15        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79383 MiB |  79443 MiB | 112747 GiB | 112669 GiB |
|       from large pool |  79199 MiB |  79259 MiB | 112306 GiB | 112229 GiB |
|       from small pool |    183 MiB |    185 MiB |    440 GiB |    440 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79383 MiB |  79443 MiB | 112747 GiB | 112669 GiB |
|       from large pool |  79199 MiB |  79259 MiB | 112306 GiB | 112229 GiB |
|       from small pool |    183 MiB |    185 MiB |    440 GiB |    440 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79334 MiB |  79394 MiB | 112598 GiB | 112521 GiB |
|       from large pool |  79151 MiB |  79211 MiB | 112158 GiB | 112081 GiB |
|       from small pool |    183 MiB |    184 MiB |    439 GiB |    439 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80498 MiB |  80500 MiB | 352134 MiB | 271636 MiB |
|       from large pool |  80296 MiB |  80296 MiB | 351036 MiB | 270740 MiB |
|       from small pool |    202 MiB |    248 MiB |   1098 MiB |    896 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1054 MiB |   5809 MiB | 122332 GiB | 122331 GiB |
|       from large pool |   1036 MiB |   5802 MiB | 121839 GiB | 121838 GiB |
|       from small pool |     18 MiB |     28 MiB |    492 GiB |    492 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3424    |    3427    |    5136 K  |    5133 K  |
|       from large pool |     589    |     590    |    2560 K  |    2560 K  |
|       from small pool |    2835    |    2838    |    2575 K  |    2573 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3424    |    3427    |    5136 K  |    5133 K  |
|       from large pool |     589    |     590    |    2560 K  |    2560 K  |
|       from small pool |    2835    |    2838    |    2575 K  |    2573 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     203    |     225    |    1567    |    1364    |
|       from large pool |     102    |     102    |    1018    |     916    |
|       from small pool |     101    |     124    |     549    |     448    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     239    |     240    |    2783 K  |    2783 K  |
|       from large pool |      59    |      61    |    1578 K  |    1578 K  |
|       from small pool |     180    |     181    |    1205 K  |    1205 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:06:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:06:12] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 01:06:19] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.77 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.05 GiB is free. Including non-PyTorch memory, this process has 78.07 GiB memory in use. Of the allocated memory 75.98 GiB is allocated by PyTorch, and 1.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:06:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:06:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:06:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 8            |        cudaMalloc retries: 17        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77208 MiB |  78284 MiB | 114057 GiB | 113982 GiB |
|       from large pool |  77186 MiB |  78262 MiB | 113612 GiB | 113537 GiB |
|       from small pool |     22 MiB |     27 MiB |    445 GiB |    445 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77208 MiB |  78284 MiB | 114057 GiB | 113982 GiB |
|       from large pool |  77186 MiB |  78262 MiB | 113612 GiB | 113537 GiB |
|       from small pool |     22 MiB |     27 MiB |    445 GiB |    445 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77192 MiB |  78269 MiB | 113907 GiB | 113832 GiB |
|       from large pool |  77170 MiB |  78247 MiB | 113463 GiB | 113387 GiB |
|       from small pool |     22 MiB |     27 MiB |    444 GiB |    444 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79434 MiB |  80438 MiB | 353222 MiB | 273788 MiB |
|       from large pool |  79404 MiB |  80236 MiB | 352124 MiB | 272720 MiB |
|       from small pool |     30 MiB |    202 MiB |   1098 MiB |   1068 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2225 MiB |   8258 MiB | 123721 GiB | 123719 GiB |
|       from large pool |   2217 MiB |   8249 MiB | 123223 GiB | 123221 GiB |
|       from small pool |      7 MiB |     31 MiB |    497 GiB |    497 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     566    |     574    |    5193 K  |    5193 K  |
|       from large pool |     270    |     278    |    2590 K  |    2590 K  |
|       from small pool |     296    |     356    |    2603 K  |    2603 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     566    |     574    |    5193 K  |    5193 K  |
|       from large pool |     270    |     278    |    2590 K  |    2590 K  |
|       from small pool |     296    |     356    |    2603 K  |    2603 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      86    |     202    |    1569    |    1483    |
|       from large pool |      71    |     101    |    1020    |     949    |
|       from small pool |      15    |     101    |     549    |     534    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     107    |     107    |    2816 K  |    2815 K  |
|       from large pool |      76    |      76    |    1597 K  |    1597 K  |
|       from small pool |      31    |      62    |    1218 K  |    1218 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:06:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:06:19] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:06:24]    INFO >> epoch 002:   1019 / 1539 loss=3.945, wps=3096.6, ups=4.5, wpb=687.7, bsz=687.7, num_updates=2550, lr=0.0004, gnorm=5.944, clip=0, train_wall=9, gb_free=70.8, wall=634 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:06:35]    INFO >> epoch 002:   1069 / 1539 loss=3.781, wps=3546.9, ups=4.61, wpb=769.7, bsz=769.7, num_updates=2600, lr=0.0004, gnorm=5.75, clip=0, train_wall=10, gb_free=71, wall=645 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:06:45]    INFO >> epoch 002:   1119 / 1539 loss=3.573, wps=3723.7, ups=4.69, wpb=794.6, bsz=794.6, num_updates=2650, lr=0.0004, gnorm=6.355, clip=0, train_wall=10, gb_free=63.2, wall=656 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:06:57]    INFO >> epoch 002:   1169 / 1539 loss=4.05, wps=3598.5, ups=4.69, wpb=767.5, bsz=767.5, num_updates=2700, lr=0.0004, gnorm=5.89, clip=2, train_wall=10, gb_free=66.6, wall=666 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:07:08]    INFO >> epoch 002:   1219 / 1539 loss=3.961, wps=3396.4, ups=4.81, wpb=706.2, bsz=706.2, num_updates=2750, lr=0.0004, gnorm=5.159, clip=2, train_wall=10, gb_free=72.9, wall=677 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:07:12] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.79 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.31 GiB is free. Including non-PyTorch memory, this process has 77.80 GiB memory in use. Of the allocated memory 68.26 GiB is allocated by PyTorch, and 9.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:07:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:07:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:07:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 9            |        cudaMalloc retries: 18        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  65551 MiB |  69899 MiB | 124430 GiB | 124366 GiB |
|       from large pool |  65534 MiB |  69881 MiB | 123944 GiB | 123880 GiB |
|       from small pool |     17 MiB |     20 MiB |    485 GiB |    485 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  65551 MiB |  69899 MiB | 124430 GiB | 124366 GiB |
|       from large pool |  65534 MiB |  69881 MiB | 123944 GiB | 123880 GiB |
|       from small pool |     17 MiB |     20 MiB |    485 GiB |    485 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  65539 MiB |  69886 MiB | 124266 GiB | 124202 GiB |
|       from large pool |  65521 MiB |  69869 MiB | 123781 GiB | 123717 GiB |
|       from small pool |     17 MiB |     20 MiB |    484 GiB |    484 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79162 MiB |  79630 MiB | 353418 MiB | 274256 MiB |
|       from large pool |  79132 MiB |  79404 MiB | 352124 MiB | 272992 MiB |
|       from small pool |     30 MiB |    226 MiB |   1294 MiB |   1264 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   9236 MiB |  11545 MiB | 135402 GiB | 135393 GiB |
|       from large pool |   9223 MiB |  11533 MiB | 134859 GiB | 134850 GiB |
|       from small pool |     12 MiB |     27 MiB |    543 GiB |    543 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     489    |     498    |    5670 K  |    5669 K  |
|       from large pool |     201    |     210    |    2830 K  |    2830 K  |
|       from small pool |     288    |     356    |    2840 K  |    2839 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     489    |     498    |    5670 K  |    5669 K  |
|       from large pool |     201    |     210    |    2830 K  |    2830 K  |
|       from small pool |     288    |     356    |    2840 K  |    2839 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      85    |     184    |    1667    |    1582    |
|       from large pool |      70    |      71    |    1020    |     950    |
|       from small pool |      15    |     113    |     647    |     632    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      88    |      88    |    3072 K  |    3072 K  |
|       from large pool |      58    |      58    |    1743 K  |    1743 K  |
|       from small pool |      30    |      61    |    1329 K  |    1328 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:07:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:07:12] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:07:19]    INFO >> epoch 002:   1270 / 1539 loss=3.801, wps=3440.6, ups=4.48, wpb=767.2, bsz=767.2, num_updates=2800, lr=0.0004, gnorm=5.615, clip=0, train_wall=10, gb_free=65.1, wall=688 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:07:28]    INFO >> epoch 002:   1320 / 1539 loss=3.882, wps=3475.1, ups=5.32, wpb=653.8, bsz=653.8, num_updates=2850, lr=0.0004, gnorm=5.099, clip=0, train_wall=9, gb_free=70.3, wall=697 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:07:40]    INFO >> epoch 002:   1370 / 1539 loss=3.896, wps=3624.7, ups=4.88, wpb=742.9, bsz=742.9, num_updates=2900, lr=0.0004, gnorm=5.267, clip=0, train_wall=10, gb_free=58.4, wall=708 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:07:50]    INFO >> epoch 002:   1420 / 1539 loss=3.818, wps=3234.4, ups=5.04, wpb=641.3, bsz=641.3, num_updates=2950, lr=0.0004, gnorm=5.422, clip=0, train_wall=9, gb_free=72.7, wall=717 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:07:54] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.72 GiB is free. Including non-PyTorch memory, this process has 77.39 GiB memory in use. Of the allocated memory 73.70 GiB is allocated by PyTorch, and 3.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:07:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:07:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:07:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 10           |        cudaMalloc retries: 19        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75039 MiB |  75469 MiB | 132770 GiB | 132697 GiB |
|       from large pool |  75013 MiB |  75443 MiB | 132253 GiB | 132180 GiB |
|       from small pool |     25 MiB |     31 MiB |    516 GiB |    516 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75039 MiB |  75469 MiB | 132770 GiB | 132697 GiB |
|       from large pool |  75013 MiB |  75443 MiB | 132253 GiB | 132180 GiB |
|       from small pool |     25 MiB |     31 MiB |    516 GiB |    516 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75024 MiB |  75454 MiB | 132596 GiB | 132522 GiB |
|       from large pool |  74999 MiB |  75428 MiB | 132080 GiB | 132007 GiB |
|       from small pool |     25 MiB |     31 MiB |    515 GiB |    515 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78740 MiB |  78842 MiB | 357472 MiB | 278732 MiB |
|       from large pool |  78708 MiB |  78708 MiB | 356074 MiB | 277366 MiB |
|       from small pool |     32 MiB |    134 MiB |   1398 MiB |   1366 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3700 MiB |  10469 MiB | 144898 GiB | 144894 GiB |
|       from large pool |   3694 MiB |  10458 MiB | 144320 GiB | 144317 GiB |
|       from small pool |      6 MiB |     27 MiB |    577 GiB |    577 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     567    |     574    |    6047 K  |    6046 K  |
|       from large pool |     265    |     272    |    3030 K  |    3029 K  |
|       from small pool |     302    |     356    |    3017 K  |    3016 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     567    |     574    |    6047 K  |    6046 K  |
|       from large pool |     265    |     272    |    3030 K  |    3029 K  |
|       from small pool |     302    |     356    |    3017 K  |    3016 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      85    |     136    |    1720    |    1635    |
|       from large pool |      69    |      69    |    1021    |     952    |
|       from small pool |      16    |      67    |     699    |     683    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      89    |      92    |    3271 K  |    3271 K  |
|       from large pool |      57    |      60    |    1865 K  |    1865 K  |
|       from small pool |      32    |      56    |    1405 K  |    1405 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:07:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:07:54] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:08:00]    INFO >> epoch 002:   1471 / 1539 loss=3.847, wps=3116.1, ups=4.72, wpb=660, bsz=660, num_updates=3000, lr=0.0004, gnorm=5.181, clip=0, train_wall=9, gb_free=67.6, wall=728 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:08:05] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.64 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.74 GiB is free. Including non-PyTorch memory, this process has 77.38 GiB memory in use. Of the allocated memory 71.67 GiB is allocated by PyTorch, and 5.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:08:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:08:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:08:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 11           |        cudaMalloc retries: 20        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69620 MiB |  73386 MiB | 134858 GiB | 134790 GiB |
|       from large pool |  69600 MiB |  73366 MiB | 134334 GiB | 134267 GiB |
|       from small pool |     19 MiB |     29 MiB |    523 GiB |    523 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69620 MiB |  73386 MiB | 134858 GiB | 134790 GiB |
|       from large pool |  69600 MiB |  73366 MiB | 134334 GiB | 134267 GiB |
|       from small pool |     19 MiB |     29 MiB |    523 GiB |    523 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  69602 MiB |  73368 MiB | 134681 GiB | 134613 GiB |
|       from large pool |  69582 MiB |  73348 MiB | 134158 GiB | 134090 GiB |
|       from small pool |     19 MiB |     29 MiB |    522 GiB |    522 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78726 MiB |  78764 MiB | 357496 MiB | 278770 MiB |
|       from large pool |  78692 MiB |  78708 MiB | 356074 MiB | 277382 MiB |
|       from small pool |     34 MiB |     56 MiB |   1422 MiB |   1388 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5013 MiB |   8691 MiB | 147257 GiB | 147252 GiB |
|       from large pool |   4999 MiB |   8676 MiB | 146671 GiB | 146667 GiB |
|       from small pool |     14 MiB |     26 MiB |    585 GiB |    585 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     649    |    6135 K  |    6135 K  |
|       from large pool |     336    |     344    |    3078 K  |    3078 K  |
|       from small pool |     305    |     356    |    3057 K  |    3057 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     649    |    6135 K  |    6135 K  |
|       from large pool |     336    |     344    |    3078 K  |    3078 K  |
|       from small pool |     305    |     356    |    3057 K  |    3057 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      85    |      97    |    1732    |    1647    |
|       from large pool |      68    |      69    |    1021    |     953    |
|       from small pool |      17    |      28    |     711    |     694    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     100    |     105    |    3317 K  |    3317 K  |
|       from large pool |      70    |      75    |    1894 K  |    1894 K  |
|       from small pool |      30    |      57    |    1422 K  |    1422 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:08:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:08:05] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:08:12]    INFO >> epoch 002:   1522 / 1539 loss=3.757, wps=3189.7, ups=4.87, wpb=655.6, bsz=655.6, num_updates=3050, lr=0.0004, gnorm=5.936, clip=2, train_wall=9, gb_free=70, wall=738 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:08:16]    INFO >> epoch 002 | loss 3.927 | wps 3188.3 | ups 4.48 | wpb 711 | bsz 711 | num_updates 3067 | lr 0.0004 | gnorm 5.782 | clip 0.3 | train_wall 297 | gb_free 68.7 | wall 742 (progress_bar.py:267, print())[0m
[33m[2025-11-21 01:08:16] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:08:38]    INFO >> epoch 002 | valid on 'valid' subset | loss 3.725 | wps 7058 | wpb 5412.5 | bsz 5412.5 | num_updates 3067 | best_loss 4.043 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 01:08:39]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 01:08:39]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/checkpoints/checkpoint_last.pt (epoch 2 @ 3067 updates, score 3.725) (writing took 0.018611 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 01:08:39] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:08:45]    INFO >> epoch 003:     33 / 1539 loss=3.828, wps=1106, ups=1.56, wpb=708.8, bsz=708.8, num_updates=3100, lr=0.000392, gnorm=6.328, clip=0, train_wall=10, gb_free=68.2, wall=770 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:08:56]    INFO >> epoch 003:     83 / 1539 loss=3.741, wps=3522.2, ups=4.72, wpb=745.9, bsz=745.9, num_updates=3150, lr=0.000392, gnorm=6.276, clip=0, train_wall=10, gb_free=68.8, wall=781 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:08:58] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.05 GiB is free. Including non-PyTorch memory, this process has 78.06 GiB memory in use. Of the allocated memory 76.06 GiB is allocated by PyTorch, and 1.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:08:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:08:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:08:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 12           |        cudaMalloc retries: 21        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77290 MiB |  77889 MiB | 147470 GiB | 147395 GiB |
|       from large pool |  77273 MiB |  77873 MiB | 146887 GiB | 146812 GiB |
|       from small pool |     16 MiB |     25 MiB |    583 GiB |    583 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77290 MiB |  77889 MiB | 147470 GiB | 147395 GiB |
|       from large pool |  77273 MiB |  77873 MiB | 146887 GiB | 146812 GiB |
|       from small pool |     16 MiB |     25 MiB |    583 GiB |    583 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77280 MiB |  77879 MiB | 147279 GiB | 147203 GiB |
|       from large pool |  77263 MiB |  77862 MiB | 146697 GiB | 146621 GiB |
|       from small pool |     16 MiB |     25 MiB |    582 GiB |    582 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79426 MiB |  79898 MiB | 362760 MiB | 283334 MiB |
|       from large pool |  79396 MiB |  79816 MiB | 361290 MiB | 281894 MiB |
|       from small pool |     30 MiB |     82 MiB |   1470 MiB |   1440 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2075 MiB |   7190 MiB | 157603 GiB | 157601 GiB |
|       from large pool |   2062 MiB |   7176 MiB | 156955 GiB | 156953 GiB |
|       from small pool |     13 MiB |     29 MiB |    647 GiB |    647 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     528    |     535    |    6691 K  |    6690 K  |
|       from large pool |     239    |     246    |    3263 K  |    3263 K  |
|       from small pool |     289    |     356    |    3427 K  |    3426 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     528    |     535    |    6691 K  |    6690 K  |
|       from large pool |     239    |     246    |    3263 K  |    3263 K  |
|       from small pool |     289    |     356    |    3427 K  |    3426 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      78    |     111    |    1759    |    1681    |
|       from large pool |      63    |      70    |    1024    |     961    |
|       from small pool |      15    |      41    |     735    |     720    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      79    |      81    |    3639 K  |    3639 K  |
|       from large pool |      50    |      52    |    2011 K  |    2011 K  |
|       from small pool |      29    |      65    |    1627 K  |    1627 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:08:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:08:58] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:09:07]    INFO >> epoch 003:    134 / 1539 loss=3.942, wps=3802.2, ups=4.51, wpb=842.2, bsz=842.2, num_updates=3200, lr=0.000392, gnorm=5.376, clip=0, train_wall=10, gb_free=70.4, wall=792 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:09:18]    INFO >> epoch 003:    184 / 1539 loss=3.94, wps=3249.8, ups=4.91, wpb=661.4, bsz=661.4, num_updates=3250, lr=0.000392, gnorm=5.465, clip=0, train_wall=10, gb_free=63.3, wall=802 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:09:25] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.79 GiB. GPU 2 has a total capacity of 79.14 GiB of which 2.76 GiB is free. Including non-PyTorch memory, this process has 76.36 GiB memory in use. Of the allocated memory 68.26 GiB is allocated by PyTorch, and 7.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:09:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:09:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:09:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 13           |        cudaMalloc retries: 22        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  65550 MiB |  69897 MiB | 152467 GiB | 152403 GiB |
|       from large pool |  65532 MiB |  69879 MiB | 151862 GiB | 151798 GiB |
|       from small pool |     17 MiB |     27 MiB |    604 GiB |    604 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  65550 MiB |  69897 MiB | 152467 GiB | 152403 GiB |
|       from large pool |  65532 MiB |  69879 MiB | 151862 GiB | 151798 GiB |
|       from small pool |     17 MiB |     27 MiB |    604 GiB |    604 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  65539 MiB |  69886 MiB | 152269 GiB | 152205 GiB |
|       from large pool |  65521 MiB |  69869 MiB | 151666 GiB | 151602 GiB |
|       from small pool |     17 MiB |     26 MiB |    603 GiB |    603 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77680 MiB |  79546 MiB | 362940 MiB | 285260 MiB |
|       from large pool |  77650 MiB |  79336 MiB | 361290 MiB | 283640 MiB |
|       from small pool |     30 MiB |    210 MiB |   1650 MiB |   1620 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   8037 MiB |  10281 MiB | 163444 GiB | 163436 GiB |
|       from large pool |   8025 MiB |  10268 MiB | 162772 GiB | 162764 GiB |
|       from small pool |     12 MiB |     33 MiB |    671 GiB |    671 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     489    |     498    |    6930 K  |    6929 K  |
|       from large pool |     201    |     210    |    3380 K  |    3379 K  |
|       from small pool |     288    |     356    |    3550 K  |    3549 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     489    |     498    |    6930 K  |    6929 K  |
|       from large pool |     201    |     210    |    3380 K  |    3379 K  |
|       from small pool |     288    |     356    |    3550 K  |    3549 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      74    |     167    |    1849    |    1775    |
|       from large pool |      59    |      62    |    1024    |     965    |
|       from small pool |      15    |     105    |     825    |     810    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      84    |      84    |    3769 K  |    3769 K  |
|       from large pool |      55    |      55    |    2081 K  |    2081 K  |
|       from small pool |      29    |      61    |    1687 K  |    1687 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:09:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:09:25] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:09:30]    INFO >> epoch 003:    235 / 1539 loss=3.756, wps=3324.8, ups=4.25, wpb=783.1, bsz=783.1, num_updates=3300, lr=0.000392, gnorm=5.016, clip=0, train_wall=10, gb_free=68.9, wall=814 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:09:41]    INFO >> epoch 003:    285 / 1539 loss=3.764, wps=3530.8, ups=4.74, wpb=744.5, bsz=744.5, num_updates=3350, lr=0.000392, gnorm=5.349, clip=0, train_wall=10, gb_free=71, wall=825 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:09:53]    INFO >> epoch 003:    335 / 1539 loss=3.804, wps=3461, ups=4.42, wpb=782.2, bsz=782.2, num_updates=3400, lr=0.000392, gnorm=5.628, clip=0, train_wall=11, gb_free=66.8, wall=836 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:10:05]    INFO >> epoch 003:    385 / 1539 loss=3.782, wps=3032.2, ups=4.43, wpb=683.8, bsz=683.8, num_updates=3450, lr=0.000392, gnorm=5.069, clip=0, train_wall=11, gb_free=64.5, wall=847 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:10:15]    INFO >> epoch 003:    435 / 1539 loss=3.778, wps=3375.2, ups=4.94, wpb=682.8, bsz=682.8, num_updates=3500, lr=0.000392, gnorm=5.834, clip=0, train_wall=10, gb_free=60.5, wall=857 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:10:26]    INFO >> epoch 003:    485 / 1539 loss=3.798, wps=3511.7, ups=4.87, wpb=721.3, bsz=721.3, num_updates=3550, lr=0.000392, gnorm=5.185, clip=0, train_wall=10, gb_free=71.8, wall=868 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:10:37]    INFO >> epoch 003:    535 / 1539 loss=3.771, wps=3795.6, ups=4.87, wpb=778.7, bsz=778.7, num_updates=3600, lr=0.000392, gnorm=5.67, clip=0, train_wall=10, gb_free=65.2, wall=878 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:10:47]    INFO >> epoch 003:    585 / 1539 loss=3.726, wps=3473.8, ups=4.84, wpb=717.5, bsz=717.5, num_updates=3650, lr=0.000392, gnorm=5.492, clip=2, train_wall=10, gb_free=72.5, wall=888 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:10:58]    INFO >> epoch 003:    635 / 1539 loss=3.773, wps=3442.2, ups=5.02, wpb=686.3, bsz=686.3, num_updates=3700, lr=0.000392, gnorm=6.235, clip=0, train_wall=9, gb_free=62.7, wall=898 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:11:09]    INFO >> epoch 003:    685 / 1539 loss=3.871, wps=3547.2, ups=4.69, wpb=755.6, bsz=755.6, num_updates=3750, lr=0.000392, gnorm=5.318, clip=0, train_wall=10, gb_free=70.2, wall=909 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:11:20]    INFO >> epoch 003:    735 / 1539 loss=3.68, wps=3550.7, ups=4.34, wpb=817.3, bsz=817.3, num_updates=3800, lr=0.000392, gnorm=4.848, clip=0, train_wall=11, gb_free=63.2, wall=920 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:11:33] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.77 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.16 GiB is free. Including non-PyTorch memory, this process has 77.96 GiB memory in use. Of the allocated memory 75.98 GiB is allocated by PyTorch, and 1.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:11:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:11:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:11:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 14           |        cudaMalloc retries: 23        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77209 MiB |  78285 MiB | 177740 GiB | 177665 GiB |
|       from large pool |  77187 MiB |  78263 MiB | 177038 GiB | 176963 GiB |
|       from small pool |     22 MiB |     24 MiB |    701 GiB |    701 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77209 MiB |  78285 MiB | 177740 GiB | 177665 GiB |
|       from large pool |  77187 MiB |  78263 MiB | 177038 GiB | 176963 GiB |
|       from small pool |     22 MiB |     24 MiB |    701 GiB |    701 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77192 MiB |  78269 MiB | 177509 GiB | 177434 GiB |
|       from large pool |  77170 MiB |  78247 MiB | 176809 GiB | 176733 GiB |
|       from small pool |     22 MiB |     24 MiB |    700 GiB |    700 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79320 MiB |  80138 MiB | 369490 MiB | 290170 MiB |
|       from large pool |  79290 MiB |  79890 MiB | 367622 MiB | 288332 MiB |
|       from small pool |     30 MiB |    248 MiB |   1868 MiB |   1838 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2050 MiB |   8193 MiB | 191704 GiB | 191702 GiB |
|       from large pool |   2042 MiB |   8184 MiB | 190923 GiB | 190921 GiB |
|       from small pool |      7 MiB |     25 MiB |    781 GiB |    781 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     566    |     574    |    8084 K  |    8083 K  |
|       from large pool |     270    |     278    |    3968 K  |    3968 K  |
|       from small pool |     296    |     356    |    4115 K  |    4115 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     566    |     574    |    8084 K  |    8083 K  |
|       from large pool |     270    |     278    |    3968 K  |    3968 K  |
|       from small pool |     296    |     356    |    4115 K  |    4115 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      91    |     210    |    1986    |    1895    |
|       from large pool |      76    |      86    |    1052    |     976    |
|       from small pool |      15    |     124    |     934    |     919    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     109    |     109    |    4399 K  |    4399 K  |
|       from large pool |      80    |      80    |    2448 K  |    2448 K  |
|       from small pool |      29    |      59    |    1950 K  |    1950 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:11:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:11:33] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:11:33]    INFO >> epoch 003:    786 / 1539 loss=3.329, wps=3457.3, ups=4.46, wpb=774.9, bsz=774.9, num_updates=3850, lr=0.000392, gnorm=5.879, clip=2, train_wall=10, gb_free=3.5, wall=931 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:11:43]    INFO >> epoch 003:    836 / 1539 loss=3.865, wps=3778.2, ups=4.95, wpb=763.4, bsz=763.4, num_updates=3900, lr=0.000392, gnorm=6.188, clip=0, train_wall=10, gb_free=63.3, wall=942 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:11:53]    INFO >> epoch 003:    886 / 1539 loss=3.791, wps=3421.9, ups=4.94, wpb=692.7, bsz=692.7, num_updates=3950, lr=0.000392, gnorm=6.114, clip=0, train_wall=10, gb_free=67, wall=952 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:12:06]    INFO >> epoch 003:    936 / 1539 loss=3.648, wps=3292.7, ups=4.47, wpb=737, bsz=737, num_updates=4000, lr=0.000392, gnorm=5.389, clip=0, train_wall=11, gb_free=67.9, wall=963 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:12:08] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 77.90 GiB memory in use. Of the allocated memory 73.70 GiB is allocated by PyTorch, and 3.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:12:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:12:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:12:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 15           |        cudaMalloc retries: 24        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75041 MiB |  75471 MiB | 184710 GiB | 184637 GiB |
|       from large pool |  75015 MiB |  75445 MiB | 183984 GiB | 183911 GiB |
|       from small pool |     25 MiB |     26 MiB |    726 GiB |    726 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75041 MiB |  75471 MiB | 184710 GiB | 184637 GiB |
|       from large pool |  75015 MiB |  75445 MiB | 183984 GiB | 183911 GiB |
|       from small pool |     25 MiB |     26 MiB |    726 GiB |    726 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75024 MiB |  75454 MiB | 184471 GiB | 184397 GiB |
|       from large pool |  74999 MiB |  75428 MiB | 183745 GiB | 183672 GiB |
|       from small pool |     25 MiB |     26 MiB |    725 GiB |    725 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79262 MiB |  79294 MiB | 369524 MiB | 290262 MiB |
|       from large pool |  79230 MiB |  79230 MiB | 367622 MiB | 288392 MiB |
|       from small pool |     32 MiB |     64 MiB |   1902 MiB |   1870 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4220 MiB |   8716 MiB | 199422 GiB | 199418 GiB |
|       from large pool |   4214 MiB |   8708 MiB | 198613 GiB | 198609 GiB |
|       from small pool |      6 MiB |     23 MiB |    809 GiB |    809 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     567    |     574    |    8388 K  |    8388 K  |
|       from large pool |     265    |     272    |    4129 K  |    4129 K  |
|       from small pool |     302    |     356    |    4258 K  |    4258 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     567    |     574    |    8388 K  |    8388 K  |
|       from large pool |     265    |     272    |    4129 K  |    4129 K  |
|       from small pool |     302    |     356    |    4258 K  |    4258 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      91    |     107    |    2003    |    1912    |
|       from large pool |      75    |      75    |    1052    |     977    |
|       from small pool |      16    |      32    |     951    |     935    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      89    |      89    |    4561 K  |    4561 K  |
|       from large pool |      57    |      57    |    2547 K  |    2547 K  |
|       from small pool |      32    |      51    |    2014 K  |    2014 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:12:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:12:08] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 01:12:14] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.64 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 77.90 GiB memory in use. Of the allocated memory 72.13 GiB is allocated by PyTorch, and 5.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:12:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:12:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:12:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 16           |        cudaMalloc retries: 25        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69479 MiB |  76766 MiB | 185678 GiB | 185610 GiB |
|       from large pool |  69459 MiB |  76746 MiB | 184949 GiB | 184881 GiB |
|       from small pool |     20 MiB |     29 MiB |    729 GiB |    729 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69479 MiB |  76766 MiB | 185678 GiB | 185610 GiB |
|       from large pool |  69459 MiB |  76746 MiB | 184949 GiB | 184881 GiB |
|       from small pool |     20 MiB |     29 MiB |    729 GiB |    729 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  69460 MiB |  76747 MiB | 185437 GiB | 185369 GiB |
|       from large pool |  69439 MiB |  76727 MiB | 184709 GiB | 184641 GiB |
|       from small pool |     20 MiB |     29 MiB |    728 GiB |    728 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79262 MiB |  79286 MiB | 369548 MiB | 290286 MiB |
|       from large pool |  79230 MiB |  79230 MiB | 367622 MiB | 288392 MiB |
|       from small pool |     32 MiB |     56 MiB |   1926 MiB |   1894 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5690 MiB |   8026 MiB | 200461 GiB | 200456 GiB |
|       from large pool |   5678 MiB |   8015 MiB | 199649 GiB | 199643 GiB |
|       from small pool |     11 MiB |     31 MiB |    812 GiB |    812 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     646    |     721    |    8427 K  |    8426 K  |
|       from large pool |     331    |     355    |    4151 K  |    4150 K  |
|       from small pool |     315    |     376    |    4276 K  |    4276 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     646    |     721    |    8427 K  |    8426 K  |
|       from large pool |     331    |     355    |    4151 K  |    4150 K  |
|       from small pool |     315    |     376    |    4276 K  |    4276 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      91    |     103    |    2015    |    1924    |
|       from large pool |      75    |      75    |    1052    |     977    |
|       from small pool |      16    |      28    |     963    |     947    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     101    |     111    |    4582 K  |    4582 K  |
|       from large pool |      73    |      77    |    2560 K  |    2560 K  |
|       from small pool |      28    |      60    |    2021 K  |    2021 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:12:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:12:14] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:12:18]    INFO >> epoch 003:    988 / 1539 loss=3.769, wps=2660.2, ups=4.24, wpb=626.8, bsz=626.8, num_updates=4050, lr=0.000392, gnorm=5.308, clip=0, train_wall=9, gb_free=61.7, wall=975 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:12:28]    INFO >> epoch 003:   1038 / 1539 loss=3.781, wps=3191.1, ups=5.04, wpb=633.3, bsz=633.3, num_updates=4100, lr=0.000392, gnorm=5.179, clip=0, train_wall=9, gb_free=72.3, wall=985 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:12:39]    INFO >> epoch 003:   1088 / 1539 loss=3.764, wps=3356.9, ups=4.95, wpb=678.6, bsz=678.6, num_updates=4150, lr=0.000392, gnorm=5.79, clip=0, train_wall=10, gb_free=66.5, wall=995 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:12:47] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 5.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.54 GiB is allocated by PyTorch, and 1.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:12:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:12:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:12:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 17           |        cudaMalloc retries: 26        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79342 MiB |  79402 MiB | 192202 GiB | 192125 GiB |
|       from large pool |  79158 MiB |  79218 MiB | 191447 GiB | 191370 GiB |
|       from small pool |    183 MiB |    184 MiB |    754 GiB |    754 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79342 MiB |  79402 MiB | 192202 GiB | 192125 GiB |
|       from large pool |  79158 MiB |  79218 MiB | 191447 GiB | 191370 GiB |
|       from small pool |    183 MiB |    184 MiB |    754 GiB |    754 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79274 MiB |  79334 MiB | 191952 GiB | 191875 GiB |
|       from large pool |  79092 MiB |  79151 MiB | 191198 GiB | 191121 GiB |
|       from small pool |    182 MiB |    183 MiB |    753 GiB |    753 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80500 MiB |  80500 MiB | 374878 MiB | 294378 MiB |
|       from large pool |  80298 MiB |  80298 MiB | 372782 MiB | 292484 MiB |
|       from small pool |    202 MiB |    202 MiB |   2096 MiB |   1894 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1097 MiB |   6815 MiB | 207757 GiB | 207756 GiB |
|       from large pool |   1079 MiB |   6811 MiB | 206916 GiB | 206915 GiB |
|       from small pool |     18 MiB |     22 MiB |    841 GiB |    841 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3413    |    3416    |    8735 K  |    8731 K  |
|       from large pool |     588    |     589    |    4311 K  |    4310 K  |
|       from small pool |    2825    |    2828    |    4424 K  |    4421 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3413    |    3416    |    8735 K  |    8731 K  |
|       from large pool |     588    |     589    |    4311 K  |    4310 K  |
|       from small pool |    2825    |    2828    |    4424 K  |    4421 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     261    |     261    |    2186    |    1925    |
|       from large pool |     160    |     160    |    1138    |     978    |
|       from small pool |     101    |     101    |    1048    |     947    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     243    |     243    |    4744 K  |    4744 K  |
|       from large pool |      69    |      72    |    2658 K  |    2658 K  |
|       from small pool |     174    |     174    |    2086 K  |    2085 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:12:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:12:47] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:12:49]    INFO >> epoch 003:   1139 / 1539 loss=3.725, wps=3407.3, ups=4.92, wpb=692.4, bsz=692.4, num_updates=4200, lr=0.000392, gnorm=4.839, clip=0, train_wall=9, gb_free=64.9, wall=1005 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:13:00]    INFO >> epoch 003:   1189 / 1539 loss=3.697, wps=3036.6, ups=4.63, wpb=655.7, bsz=655.7, num_updates=4250, lr=0.000392, gnorm=4.956, clip=0, train_wall=10, gb_free=64, wall=1016 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:13:12]    INFO >> epoch 003:   1239 / 1539 loss=3.701, wps=3371.5, ups=4.75, wpb=710.2, bsz=710.2, num_updates=4300, lr=0.000392, gnorm=6.257, clip=6, train_wall=10, gb_free=70.7, wall=1026 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:13:21]    INFO >> epoch 003:   1289 / 1539 loss=3.612, wps=3269.7, ups=5.23, wpb=625.7, bsz=625.7, num_updates=4350, lr=0.000392, gnorm=5.057, clip=0, train_wall=9, gb_free=74.4, wall=1036 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:13:32]    INFO >> epoch 003:   1339 / 1539 loss=3.701, wps=3665.6, ups=4.92, wpb=744.7, bsz=744.7, num_updates=4400, lr=0.000392, gnorm=4.835, clip=0, train_wall=10, gb_free=67.4, wall=1046 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:13:42]    INFO >> epoch 003:   1389 / 1539 loss=3.633, wps=3463.5, ups=5.25, wpb=660.3, bsz=660.3, num_updates=4450, lr=0.000392, gnorm=4.931, clip=0, train_wall=9, gb_free=69.1, wall=1055 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:13:52]    INFO >> epoch 003:   1439 / 1539 loss=3.757, wps=3371.7, ups=5.05, wpb=667.7, bsz=667.7, num_updates=4500, lr=0.000392, gnorm=4.66, clip=0, train_wall=9, gb_free=69.7, wall=1065 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:14:03]    INFO >> epoch 003:   1489 / 1539 loss=3.687, wps=3118.7, ups=4.57, wpb=682.6, bsz=682.6, num_updates=4550, lr=0.000392, gnorm=5.632, clip=2, train_wall=10, gb_free=66.1, wall=1076 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:14:15]    INFO >> epoch 003:   1539 / 1539 loss=3.738, wps=3160.7, ups=4.91, wpb=644.1, bsz=644.1, num_updates=4600, lr=0.000392, gnorm=5.459, clip=0, train_wall=10, gb_free=71.7, wall=1086 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:14:15]    INFO >> epoch 003 | loss 3.746 | wps 3164.7 | ups 4.45 | wpb 711 | bsz 711 | num_updates 4600 | lr 0.000392 | gnorm 5.449 | clip 0.4 | train_wall 300 | gb_free 71.7 | wall 1086 (progress_bar.py:267, print())[0m
[33m[2025-11-21 01:14:15] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:14:36]    INFO >> epoch 003 | valid on 'valid' subset | loss 3.843 | wps 7057.1 | wpb 5412.5 | bsz 5412.5 | num_updates 4600 | best_loss 4.043 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 01:14:37]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 01:14:37]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/checkpoints/checkpoint_last.pt (epoch 3 @ 4600 updates, score 3.843) (writing took 0.033478 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 01:14:37] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:14:49]    INFO >> epoch 004:     50 / 1539 loss=3.676, wps=1121.7, ups=1.53, wpb=734.4, bsz=734.4, num_updates=4650, lr=0.000376, gnorm=5.737, clip=0, train_wall=10, gb_free=66.9, wall=1119 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:15:01]    INFO >> epoch 004:    100 / 1539 loss=3.582, wps=3817.4, ups=4.28, wpb=892.1, bsz=892.1, num_updates=4700, lr=0.000376, gnorm=5.298, clip=0, train_wall=11, gb_free=62.5, wall=1131 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:15:13]    INFO >> epoch 004:    150 / 1539 loss=3.503, wps=3611.4, ups=3.93, wpb=920.1, bsz=920.1, num_updates=4750, lr=0.000376, gnorm=5.518, clip=0, train_wall=12, gb_free=66.2, wall=1144 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:15:24]    INFO >> epoch 004:    200 / 1539 loss=3.736, wps=3679.7, ups=5.19, wpb=708.5, bsz=708.5, num_updates=4800, lr=0.000376, gnorm=4.531, clip=0, train_wall=9, gb_free=61.9, wall=1153 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:15:35]    INFO >> epoch 004:    250 / 1539 loss=3.654, wps=3329.6, ups=4.58, wpb=726.8, bsz=726.8, num_updates=4850, lr=0.000376, gnorm=5.018, clip=0, train_wall=10, gb_free=70.2, wall=1164 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:15:45]    INFO >> epoch 004:    300 / 1539 loss=3.658, wps=3267.9, ups=5.04, wpb=649, bsz=649, num_updates=4900, lr=0.000376, gnorm=4.815, clip=0, train_wall=9, gb_free=63.4, wall=1174 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:15:57]    INFO >> epoch 004:    350 / 1539 loss=3.653, wps=3772.5, ups=4.87, wpb=774.4, bsz=774.4, num_updates=4950, lr=0.000376, gnorm=5.41, clip=0, train_wall=10, gb_free=31.6, wall=1184 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:16:07]    INFO >> epoch 004:    400 / 1539 loss=3.537, wps=3257.6, ups=4.88, wpb=667.8, bsz=667.8, num_updates=5000, lr=0.000376, gnorm=5.077, clip=0, train_wall=10, gb_free=70.1, wall=1195 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:16:12] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.64 GiB. GPU 2 has a total capacity of 79.14 GiB of which 597.25 MiB is free. Including non-PyTorch memory, this process has 78.53 GiB memory in use. Of the allocated memory 71.67 GiB is allocated by PyTorch, and 6.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:16:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:16:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:16:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 18           |        cudaMalloc retries: 27        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69626 MiB |  73392 MiB | 233801 GiB | 233733 GiB |
|       from large pool |  69606 MiB |  73372 MiB | 232874 GiB | 232806 GiB |
|       from small pool |     19 MiB |     28 MiB |    926 GiB |    926 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69626 MiB |  73392 MiB | 233801 GiB | 233733 GiB |
|       from large pool |  69606 MiB |  73372 MiB | 232874 GiB | 232806 GiB |
|       from small pool |     19 MiB |     28 MiB |    926 GiB |    926 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  69602 MiB |  73368 MiB | 233497 GiB | 233429 GiB |
|       from large pool |  69582 MiB |  73348 MiB | 232571 GiB | 232503 GiB |
|       from small pool |     19 MiB |     28 MiB |    925 GiB |    925 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79908 MiB |  80464 MiB | 374902 MiB | 294994 MiB |
|       from large pool |  79878 MiB |  80238 MiB | 372782 MiB | 292904 MiB |
|       from small pool |     30 MiB |    226 MiB |   2120 MiB |   2090 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6069 MiB |  11416 MiB | 247510 GiB | 247504 GiB |
|       from large pool |   6059 MiB |  11405 MiB | 246480 GiB | 246474 GiB |
|       from small pool |     10 MiB |     27 MiB |   1030 GiB |   1030 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     649    |   10636 K  |   10636 K  |
|       from large pool |     336    |     344    |    5190 K  |    5189 K  |
|       from small pool |     305    |     356    |    5446 K  |    5446 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     649    |   10636 K  |   10636 K  |
|       from large pool |     336    |     344    |    5190 K  |    5189 K  |
|       from small pool |     305    |     356    |    5446 K  |    5446 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     168    |     272    |    2198    |    2030    |
|       from large pool |     153    |     159    |    1138    |     985    |
|       from small pool |      15    |     113    |    1060    |    1045    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     151    |     153    |    5829 K  |    5829 K  |
|       from large pool |     122    |     124    |    3233 K  |    3232 K  |
|       from small pool |      29    |      53    |    2596 K  |    2596 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:16:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:16:12] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:16:18]    INFO >> epoch 004:    451 / 1539 loss=3.761, wps=2880.2, ups=4.43, wpb=650.1, bsz=650.1, num_updates=5050, lr=0.000376, gnorm=4.93, clip=0, train_wall=10, gb_free=70.1, wall=1206 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:16:30]    INFO >> epoch 004:    501 / 1539 loss=3.613, wps=3339.2, ups=4.65, wpb=717.9, bsz=717.9, num_updates=5100, lr=0.000376, gnorm=4.907, clip=0, train_wall=10, gb_free=70.6, wall=1217 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:16:41]    INFO >> epoch 004:    551 / 1539 loss=3.724, wps=3210.5, ups=4.69, wpb=684.3, bsz=684.3, num_updates=5150, lr=0.000376, gnorm=5.346, clip=0, train_wall=10, gb_free=69.6, wall=1227 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:16:51] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.77 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.07 GiB is free. Including non-PyTorch memory, this process has 78.04 GiB memory in use. Of the allocated memory 75.98 GiB is allocated by PyTorch, and 1.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:16:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:16:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:16:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 19           |        cudaMalloc retries: 29        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77207 MiB |  78283 MiB | 241162 GiB | 241086 GiB |
|       from large pool |  77185 MiB |  78261 MiB | 240210 GiB | 240134 GiB |
|       from small pool |     22 MiB |     26 MiB |    952 GiB |    952 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77207 MiB |  78283 MiB | 241162 GiB | 241086 GiB |
|       from large pool |  77185 MiB |  78261 MiB | 240210 GiB | 240134 GiB |
|       from small pool |     22 MiB |     26 MiB |    952 GiB |    952 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77192 MiB |  78269 MiB | 240847 GiB | 240771 GiB |
|       from large pool |  77170 MiB |  78247 MiB | 239896 GiB | 239821 GiB |
|       from small pool |     22 MiB |     26 MiB |    950 GiB |    950 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79408 MiB |  79408 MiB | 383330 MiB | 303922 MiB |
|       from large pool |  79378 MiB |  79378 MiB | 381174 MiB | 301796 MiB |
|       from small pool |     30 MiB |     66 MiB |   2156 MiB |   2126 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2200 MiB |   8992 MiB | 254910 GiB | 254908 GiB |
|       from large pool |   2192 MiB |   8983 MiB | 253851 GiB | 253849 GiB |
|       from small pool |      7 MiB |     29 MiB |   1059 GiB |   1059 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     566    |     574    |   10954 K  |   10953 K  |
|       from large pool |     270    |     278    |    5362 K  |    5362 K  |
|       from small pool |     296    |     356    |    5591 K  |    5591 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     566    |     574    |   10954 K  |   10953 K  |
|       from large pool |     270    |     278    |    5362 K  |    5362 K  |
|       from small pool |     296    |     356    |    5591 K  |    5591 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      94    |     184    |    2223    |    2129    |
|       from large pool |      79    |     151    |    1145    |    1066    |
|       from small pool |      15    |      33    |    1078    |    1063    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     109    |     111    |    6002 K  |    6001 K  |
|       from large pool |      80    |      82    |    3344 K  |    3344 K  |
|       from small pool |      29    |      61    |    2657 K  |    2657 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:16:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:16:51] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:16:52]    INFO >> epoch 004:    602 / 1539 loss=3.575, wps=2846.3, ups=4.55, wpb=625.7, bsz=625.7, num_updates=5200, lr=0.000376, gnorm=4.825, clip=0, train_wall=10, gb_free=70.3, wall=1238 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:17:03]    INFO >> epoch 004:    652 / 1539 loss=3.612, wps=3507.8, ups=5.19, wpb=676.2, bsz=676.2, num_updates=5250, lr=0.000376, gnorm=4.741, clip=0, train_wall=9, gb_free=73.2, wall=1248 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:17:11] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 2 has a total capacity of 79.14 GiB of which 3.25 GiB is free. Including non-PyTorch memory, this process has 75.87 GiB memory in use. Of the allocated memory 72.07 GiB is allocated by PyTorch, and 3.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:17:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:17:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:17:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 20           |        cudaMalloc retries: 30        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73199 MiB |  74276 MiB | 244882 GiB | 244810 GiB |
|       from large pool |  73182 MiB |  74259 MiB | 243916 GiB | 243845 GiB |
|       from small pool |     16 MiB |     30 MiB |    965 GiB |    965 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73199 MiB |  74276 MiB | 244882 GiB | 244810 GiB |
|       from large pool |  73182 MiB |  74259 MiB | 243916 GiB | 243845 GiB |
|       from small pool |     16 MiB |     30 MiB |    965 GiB |    965 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73189 MiB |  74265 MiB | 244562 GiB | 244490 GiB |
|       from large pool |  73173 MiB |  74248 MiB | 243597 GiB | 243526 GiB |
|       from small pool |     16 MiB |     30 MiB |    964 GiB |    964 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77178 MiB |  79436 MiB | 383358 MiB | 306180 MiB |
|       from large pool |  77146 MiB |  79378 MiB | 381174 MiB | 304028 MiB |
|       from small pool |     32 MiB |     58 MiB |   2184 MiB |   2152 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3918 MiB |   6687 MiB | 258963 GiB | 258959 GiB |
|       from large pool |   3903 MiB |   6672 MiB | 257888 GiB | 257884 GiB |
|       from small pool |     15 MiB |     31 MiB |   1074 GiB |   1074 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     527    |     535    |   11123 K  |   11123 K  |
|       from large pool |     238    |     246    |    5453 K  |    5453 K  |
|       from small pool |     289    |     356    |    5669 K  |    5669 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     527    |     535    |   11123 K  |   11123 K  |
|       from large pool |     238    |     246    |    5453 K  |    5453 K  |
|       from small pool |     289    |     356    |    5669 K  |    5669 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      82    |     108    |    2237    |    2155    |
|       from large pool |      66    |      79    |    1145    |    1079    |
|       from small pool |      16    |      29    |    1092    |    1076    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      89    |      89    |    6090 K  |    6090 K  |
|       from large pool |      60    |      60    |    3400 K  |    3400 K  |
|       from small pool |      29    |      59    |    2690 K  |    2690 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:17:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:17:11] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:17:14]    INFO >> epoch 004:    703 / 1539 loss=3.643, wps=2936.1, ups=4.63, wpb=634, bsz=634, num_updates=5300, lr=0.000376, gnorm=4.868, clip=0, train_wall=9, gb_free=58.4, wall=1259 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:17:22] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 3.31 GiB is free. Including non-PyTorch memory, this process has 75.81 GiB memory in use. Of the allocated memory 73.70 GiB is allocated by PyTorch, and 1.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:17:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:17:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:17:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 21           |        cudaMalloc retries: 31        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75038 MiB |  75468 MiB | 246895 GiB | 246822 GiB |
|       from large pool |  75012 MiB |  75442 MiB | 245922 GiB | 245849 GiB |
|       from small pool |     25 MiB |     26 MiB |    972 GiB |    972 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75038 MiB |  75468 MiB | 246895 GiB | 246822 GiB |
|       from large pool |  75012 MiB |  75442 MiB | 245922 GiB | 245849 GiB |
|       from small pool |     25 MiB |     26 MiB |    972 GiB |    972 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75024 MiB |  75454 MiB | 246573 GiB | 246499 GiB |
|       from large pool |  74999 MiB |  75428 MiB | 245602 GiB | 245528 GiB |
|       from small pool |     25 MiB |     26 MiB |    971 GiB |    971 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77116 MiB |  77142 MiB | 383382 MiB | 306266 MiB |
|       from large pool |  77086 MiB |  77086 MiB | 381174 MiB | 304088 MiB |
|       from small pool |     30 MiB |     56 MiB |   2208 MiB |   2178 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2077 MiB |   9083 MiB | 261223 GiB | 261221 GiB |
|       from large pool |   2073 MiB |   9077 MiB | 260141 GiB | 260139 GiB |
|       from small pool |      4 MiB |     19 MiB |   1082 GiB |   1082 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     567    |     574    |   11207 K  |   11206 K  |
|       from large pool |     265    |     272    |    5499 K  |    5499 K  |
|       from small pool |     302    |     348    |    5707 K  |    5707 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     567    |     574    |   11207 K  |   11206 K  |
|       from large pool |     265    |     272    |    5499 K  |    5499 K  |
|       from small pool |     302    |     348    |    5707 K  |    5707 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      80    |      93    |    2249    |    2169    |
|       from large pool |      65    |      65    |    1145    |    1080    |
|       from small pool |      15    |      28    |    1104    |    1089    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      84    |      86    |    6133 K  |    6133 K  |
|       from large pool |      56    |      58    |    3427 K  |    3427 K  |
|       from small pool |      28    |      49    |    2706 K  |    2706 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:17:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:17:22] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:17:25]    INFO >> epoch 004:    754 / 1539 loss=3.547, wps=2863, ups=4.43, wpb=645.8, bsz=645.8, num_updates=5350, lr=0.000376, gnorm=4.313, clip=0, train_wall=10, gb_free=68.8, wall=1270 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:17:37]    INFO >> epoch 004:    804 / 1539 loss=3.595, wps=3434.1, ups=4.91, wpb=699.4, bsz=699.4, num_updates=5400, lr=0.000376, gnorm=4.675, clip=0, train_wall=10, gb_free=69.9, wall=1280 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:17:47]    INFO >> epoch 004:    854 / 1539 loss=3.54, wps=3624.1, ups=4.89, wpb=741.8, bsz=741.8, num_updates=5450, lr=0.000376, gnorm=5.283, clip=0, train_wall=10, gb_free=69.7, wall=1290 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:17:59]    INFO >> epoch 004:    904 / 1539 loss=3.634, wps=3280.2, ups=4.25, wpb=771.9, bsz=771.9, num_updates=5500, lr=0.000376, gnorm=5.656, clip=0, train_wall=11, gb_free=67.5, wall=1302 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:18:10]    INFO >> epoch 004:    954 / 1539 loss=3.591, wps=3263.6, ups=5.06, wpb=645.3, bsz=645.3, num_updates=5550, lr=0.000376, gnorm=4.521, clip=0, train_wall=9, gb_free=69.8, wall=1312 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:18:21]    INFO >> epoch 004:   1004 / 1539 loss=3.579, wps=3357.8, ups=4.32, wpb=777.9, bsz=777.9, num_updates=5600, lr=0.000376, gnorm=5.564, clip=0, train_wall=11, gb_free=57.3, wall=1324 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:18:32]    INFO >> epoch 004:   1054 / 1539 loss=3.78, wps=3135.5, ups=4.9, wpb=639.5, bsz=639.5, num_updates=5650, lr=0.000376, gnorm=4.513, clip=0, train_wall=10, gb_free=68.8, wall=1334 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:18:44]    INFO >> epoch 004:   1104 / 1539 loss=3.654, wps=3319.1, ups=4.73, wpb=701.1, bsz=701.1, num_updates=5700, lr=0.000376, gnorm=4.481, clip=0, train_wall=10, gb_free=68.8, wall=1344 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:18:54]    INFO >> epoch 004:   1154 / 1539 loss=3.589, wps=3436, ups=4.65, wpb=738.7, bsz=738.7, num_updates=5750, lr=0.000376, gnorm=4.978, clip=2, train_wall=10, gb_free=60.2, wall=1355 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:19:04]    INFO >> epoch 004:   1204 / 1539 loss=3.661, wps=3480.8, ups=4.99, wpb=697.7, bsz=697.7, num_updates=5800, lr=0.000376, gnorm=5.847, clip=0, train_wall=9, gb_free=66.7, wall=1365 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:19:14]    INFO >> epoch 004:   1254 / 1539 loss=3.56, wps=3237.6, ups=5.08, wpb=637, bsz=637, num_updates=5850, lr=0.000376, gnorm=4.786, clip=0, train_wall=9, gb_free=70.9, wall=1375 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:19:26]    INFO >> epoch 004:   1304 / 1539 loss=3.628, wps=3046.9, ups=4.6, wpb=662.8, bsz=662.8, num_updates=5900, lr=0.000376, gnorm=4.223, clip=0, train_wall=10, gb_free=69.5, wall=1386 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:19:37]    INFO >> epoch 004:   1354 / 1539 loss=3.521, wps=3769.8, ups=4.68, wpb=806.1, bsz=806.1, num_updates=5950, lr=0.000376, gnorm=5.215, clip=0, train_wall=10, gb_free=70.7, wall=1397 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:19:46] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.79 GiB. GPU 2 has a total capacity of 79.14 GiB of which 2.72 GiB is free. Including non-PyTorch memory, this process has 76.39 GiB memory in use. Of the allocated memory 68.26 GiB is allocated by PyTorch, and 7.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:19:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:19:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:19:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 22           |        cudaMalloc retries: 32        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  65549 MiB |  69896 MiB | 274575 GiB | 274510 GiB |
|       from large pool |  65531 MiB |  69879 MiB | 273498 GiB | 273434 GiB |
|       from small pool |     17 MiB |     19 MiB |   1076 GiB |   1076 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  65549 MiB |  69896 MiB | 274575 GiB | 274510 GiB |
|       from large pool |  65531 MiB |  69879 MiB | 273498 GiB | 273434 GiB |
|       from small pool |     17 MiB |     19 MiB |   1076 GiB |   1076 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  65539 MiB |  69886 MiB | 274217 GiB | 274153 GiB |
|       from large pool |  65521 MiB |  69869 MiB | 273142 GiB | 273078 GiB |
|       from small pool |     17 MiB |     19 MiB |   1075 GiB |   1075 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77718 MiB |  80186 MiB | 386452 MiB | 308734 MiB |
|       from large pool |  77690 MiB |  79938 MiB | 384026 MiB | 306336 MiB |
|       from small pool |     28 MiB |    248 MiB |   2426 MiB |   2398 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   9316 MiB |  13924 MiB | 293000 GiB | 292991 GiB |
|       from large pool |   9306 MiB |  13913 MiB | 291800 GiB | 291791 GiB |
|       from small pool |     10 MiB |     18 MiB |   1199 GiB |   1199 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     489    |     498    |   12468 K  |   12468 K  |
|       from large pool |     201    |     210    |    6157 K  |    6157 K  |
|       from small pool |     288    |     336    |    6311 K  |    6311 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     489    |     498    |   12468 K  |   12468 K  |
|       from large pool |     201    |     210    |    6157 K  |    6157 K  |
|       from small pool |     288    |     336    |    6311 K  |    6311 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      76    |     190    |    2359    |    2283    |
|       from large pool |      62    |      66    |    1146    |    1084    |
|       from small pool |      14    |     124    |    1213    |    1199    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      85    |      85    |    6799 K  |    6799 K  |
|       from large pool |      57    |      57    |    3823 K  |    3823 K  |
|       from small pool |      28    |      46    |    2976 K  |    2975 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:19:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:19:46] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:19:50]    INFO >> epoch 004:   1405 / 1539 loss=3.709, wps=3316.3, ups=4.29, wpb=772.2, bsz=772.2, num_updates=6000, lr=0.000376, gnorm=4.975, clip=0, train_wall=10, gb_free=72.6, wall=1408 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:20:00]    INFO >> epoch 004:   1455 / 1539 loss=3.633, wps=3401.2, ups=4.88, wpb=697.1, bsz=697.1, num_updates=6050, lr=0.000376, gnorm=4.982, clip=0, train_wall=10, gb_free=67.5, wall=1419 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:20:11]    INFO >> epoch 004:   1505 / 1539 loss=3.701, wps=3441.2, ups=4.86, wpb=708.2, bsz=708.2, num_updates=6100, lr=0.000376, gnorm=4.84, clip=0, train_wall=10, gb_free=70.4, wall=1429 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:20:12] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 5.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.54 GiB is allocated by PyTorch, and 1.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:20:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:20:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:20:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 23           |        cudaMalloc retries: 33        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79342 MiB |  79402 MiB | 279532 GiB | 279454 GiB |
|       from large pool |  79159 MiB |  79219 MiB | 278435 GiB | 278358 GiB |
|       from small pool |    183 MiB |    184 MiB |   1096 GiB |   1096 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79342 MiB |  79402 MiB | 279532 GiB | 279454 GiB |
|       from large pool |  79159 MiB |  79219 MiB | 278435 GiB | 278358 GiB |
|       from small pool |    183 MiB |    184 MiB |   1096 GiB |   1096 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79274 MiB |  79334 MiB | 279168 GiB | 279090 GiB |
|       from large pool |  79092 MiB |  79151 MiB | 278073 GiB | 277995 GiB |
|       from small pool |    182 MiB |    183 MiB |   1095 GiB |   1094 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80500 MiB |  80502 MiB | 392088 MiB | 311588 MiB |
|       from large pool |  80298 MiB |  80298 MiB | 389486 MiB | 309188 MiB |
|       from small pool |    202 MiB |    204 MiB |   2602 MiB |   2400 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1097 MiB |   6556 MiB | 298767 GiB | 298766 GiB |
|       from large pool |   1078 MiB |   6551 MiB | 297545 GiB | 297544 GiB |
|       from small pool |     18 MiB |     20 MiB |   1222 GiB |   1221 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3413    |    3416    |   12701 K  |   12698 K  |
|       from large pool |     588    |     589    |    6275 K  |    6274 K  |
|       from small pool |    2825    |    2828    |    6426 K  |    6423 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3413    |    3416    |   12701 K  |   12698 K  |
|       from large pool |     588    |     589    |    6275 K  |    6274 K  |
|       from small pool |    2825    |    2828    |    6426 K  |    6423 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     253    |     254    |    2538    |    2285    |
|       from large pool |     152    |     152    |    1237    |    1085    |
|       from small pool |     101    |     102    |    1301    |    1200    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     240    |     240    |    6923 K  |    6922 K  |
|       from large pool |      60    |      63    |    3894 K  |    3894 K  |
|       from small pool |     180    |     180    |    3028 K  |    3028 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:20:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:20:12] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:20:17]    INFO >> epoch 004 | loss 3.625 | wps 3121.7 | ups 4.39 | wpb 711 | bsz 711 | num_updates 6133 | lr 0.000376 | gnorm 5.005 | clip 0.1 | train_wall 304 | gb_free 65.2 | wall 1436 (progress_bar.py:267, print())[0m
[33m[2025-11-21 01:20:17] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:20:41]    INFO >> epoch 004 | valid on 'valid' subset | loss 3.774 | wps 6872.9 | wpb 5412.5 | bsz 5412.5 | num_updates 6133 | best_loss 4.043 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 01:20:41]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 01:20:41]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/checkpoints/checkpoint_last.pt (epoch 4 @ 6133 updates, score 3.774) (writing took 0.026904 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 01:20:41] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:20:45]    INFO >> epoch 005:     17 / 1539 loss=3.641, wps=945.5, ups=1.53, wpb=618.7, bsz=618.7, num_updates=6150, lr=0.000354, gnorm=4.908, clip=0, train_wall=9, gb_free=74, wall=1462 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:20:47] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.73 GiB is free. Including non-PyTorch memory, this process has 77.39 GiB memory in use. Of the allocated memory 69.84 GiB is allocated by PyTorch, and 7.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:20:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:20:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:20:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 24           |        cudaMalloc retries: 34        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  71090 MiB |  72154 MiB | 288040 GiB | 287970 GiB |
|       from large pool |  71064 MiB |  72130 MiB | 286898 GiB | 286829 GiB |
|       from small pool |     25 MiB |     26 MiB |   1141 GiB |   1141 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  71090 MiB |  72154 MiB | 288040 GiB | 287970 GiB |
|       from large pool |  71064 MiB |  72130 MiB | 286898 GiB | 286829 GiB |
|       from small pool |     25 MiB |     26 MiB |   1141 GiB |   1141 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  71075 MiB |  72140 MiB | 287666 GiB | 287597 GiB |
|       from large pool |  71049 MiB |  72116 MiB | 286526 GiB | 286457 GiB |
|       from small pool |     25 MiB |     26 MiB |   1139 GiB |   1139 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78736 MiB |  80440 MiB | 392088 MiB | 313352 MiB |
|       from large pool |  78706 MiB |  80238 MiB | 389486 MiB | 310780 MiB |
|       from small pool |     30 MiB |    202 MiB |   2602 MiB |   2572 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   7525 MiB |   7525 MiB | 304274 GiB | 304266 GiB |
|       from large pool |   7521 MiB |   7521 MiB | 303006 GiB | 302998 GiB |
|       from small pool |      4 MiB |     25 MiB |   1267 GiB |   1267 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     566    |     574    |   13081 K  |   13080 K  |
|       from large pool |     264    |     272    |    6368 K  |    6368 K  |
|       from small pool |     302    |     356    |    6712 K  |    6712 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     566    |     574    |   13081 K  |   13080 K  |
|       from large pool |     264    |     272    |    6368 K  |    6368 K  |
|       from small pool |     302    |     356    |    6712 K  |    6712 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     144    |     252    |    2538    |    2394    |
|       from large pool |     129    |     151    |    1237    |    1108    |
|       from small pool |      15    |     101    |    1301    |    1286    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     132    |     133    |    7166 K  |    7166 K  |
|       from large pool |     102    |     103    |    3958 K  |    3957 K  |
|       from small pool |      30    |      55    |    3208 K  |    3208 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:20:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:20:47] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:20:55]    INFO >> epoch 005:     68 / 1539 loss=3.619, wps=3454.8, ups=4.96, wpb=696.6, bsz=696.6, num_updates=6200, lr=0.000354, gnorm=4.223, clip=0, train_wall=9, gb_free=68.2, wall=1472 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:21:06]    INFO >> epoch 005:    118 / 1539 loss=3.607, wps=3357, ups=4.93, wpb=681.6, bsz=681.6, num_updates=6250, lr=0.000354, gnorm=4.327, clip=0, train_wall=10, gb_free=68, wall=1482 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:21:16]    INFO >> epoch 005:    168 / 1539 loss=3.624, wps=3269.3, ups=5.24, wpb=623.6, bsz=623.6, num_updates=6300, lr=0.000354, gnorm=4.207, clip=0, train_wall=9, gb_free=67.8, wall=1491 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:21:26]    INFO >> epoch 005:    218 / 1539 loss=3.425, wps=3263.8, ups=4.82, wpb=676.8, bsz=676.8, num_updates=6350, lr=0.000354, gnorm=5.108, clip=0, train_wall=10, gb_free=71.3, wall=1502 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:21:39]    INFO >> epoch 005:    268 / 1539 loss=3.559, wps=3842.3, ups=4.19, wpb=918.1, bsz=918.1, num_updates=6400, lr=0.000354, gnorm=5.338, clip=2, train_wall=11, gb_free=58.5, wall=1514 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:21:49]    INFO >> epoch 005:    318 / 1539 loss=3.564, wps=3379.3, ups=5.06, wpb=667.8, bsz=667.8, num_updates=6450, lr=0.000354, gnorm=4.499, clip=0, train_wall=9, gb_free=68.8, wall=1524 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:22:00]    INFO >> epoch 005:    368 / 1539 loss=3.398, wps=3851.8, ups=4.86, wpb=792.1, bsz=792.1, num_updates=6500, lr=0.000354, gnorm=4.623, clip=0, train_wall=10, gb_free=70.9, wall=1534 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:22:08] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 39.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 77.28 GiB is allocated by PyTorch, and 1.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:22:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:22:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:22:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 25           |        cudaMalloc retries: 36        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79070 MiB |  79130 MiB | 303678 GiB | 303601 GiB |
|       from large pool |  78889 MiB |  78949 MiB | 302473 GiB | 302396 GiB |
|       from small pool |    180 MiB |    182 MiB |   1205 GiB |   1204 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79070 MiB |  79130 MiB | 303678 GiB | 303601 GiB |
|       from large pool |  78889 MiB |  78949 MiB | 302473 GiB | 302396 GiB |
|       from small pool |    180 MiB |    182 MiB |   1205 GiB |   1204 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79034 MiB |  79093 MiB | 303282 GiB | 303205 GiB |
|       from large pool |  78854 MiB |  78913 MiB | 302079 GiB | 302002 GiB |
|       from small pool |    180 MiB |    181 MiB |   1203 GiB |   1203 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80466 MiB |  80466 MiB | 393988 MiB | 313522 MiB |
|       from large pool |  80266 MiB |  80266 MiB | 391166 MiB | 310900 MiB |
|       from small pool |    200 MiB |    248 MiB |   2822 MiB |   2622 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1335 MiB |   8746 MiB | 320558 GiB | 320557 GiB |
|       from large pool |   1316 MiB |   8741 MiB | 319217 GiB | 319216 GiB |
|       from small pool |     19 MiB |     22 MiB |   1340 GiB |   1340 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3369    |    3372    |   13833 K  |   13830 K  |
|       from large pool |     584    |     585    |    6748 K  |    6747 K  |
|       from small pool |    2785    |    2788    |    7085 K  |    7082 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3369    |    3372    |   13833 K  |   13830 K  |
|       from large pool |     584    |     585    |    6748 K  |    6747 K  |
|       from small pool |    2785    |    2788    |    7085 K  |    7082 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     255    |     278    |    2676    |    2421    |
|       from large pool |     155    |     155    |    1265    |    1110    |
|       from small pool |     100    |     124    |    1411    |    1311    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     284    |     284    |    7587 K  |    7587 K  |
|       from large pool |     109    |     109    |    4204 K  |    4204 K  |
|       from small pool |     175    |     175    |    3382 K  |    3382 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:22:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:22:08] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:22:11]    INFO >> epoch 005:    419 / 1539 loss=3.605, wps=3183.1, ups=4.82, wpb=660.8, bsz=660.8, num_updates=6550, lr=0.000354, gnorm=4.568, clip=0, train_wall=9, gb_free=65.1, wall=1544 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:22:21]    INFO >> epoch 005:    469 / 1539 loss=3.621, wps=3118.5, ups=4.93, wpb=632.8, bsz=632.8, num_updates=6600, lr=0.000354, gnorm=4.464, clip=0, train_wall=10, gb_free=54, wall=1554 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:22:32]    INFO >> epoch 005:    519 / 1539 loss=3.45, wps=3328.7, ups=4.71, wpb=706, bsz=706, num_updates=6650, lr=0.000354, gnorm=4.419, clip=0, train_wall=10, gb_free=65.4, wall=1565 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:22:43]    INFO >> epoch 005:    569 / 1539 loss=3.663, wps=3418.7, ups=5.18, wpb=660.3, bsz=660.3, num_updates=6700, lr=0.000354, gnorm=4.944, clip=0, train_wall=9, gb_free=68.7, wall=1575 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:22:47] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.77 GiB. GPU 2 has a total capacity of 79.14 GiB of which 843.25 MiB is free. Including non-PyTorch memory, this process has 78.29 GiB memory in use. Of the allocated memory 75.98 GiB is allocated by PyTorch, and 1.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:22:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:22:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:22:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 26           |        cudaMalloc retries: 38        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77203 MiB |  78281 MiB | 311472 GiB | 311397 GiB |
|       from large pool |  77181 MiB |  78259 MiB | 310240 GiB | 310164 GiB |
|       from small pool |     22 MiB |     23 MiB |   1232 GiB |   1232 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77203 MiB |  78281 MiB | 311472 GiB | 311397 GiB |
|       from large pool |  77181 MiB |  78259 MiB | 310240 GiB | 310164 GiB |
|       from small pool |     22 MiB |     23 MiB |   1232 GiB |   1232 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77192 MiB |  78269 MiB | 311065 GiB | 310990 GiB |
|       from large pool |  77170 MiB |  78247 MiB | 309834 GiB | 309759 GiB |
|       from small pool |     22 MiB |     23 MiB |   1230 GiB |   1230 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79662 MiB |  80406 MiB | 398456 MiB | 318794 MiB |
|       from large pool |  79634 MiB |  80206 MiB | 395634 MiB | 316000 MiB |
|       from small pool |     28 MiB |    200 MiB |   2822 MiB |   2794 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2398 MiB |   9199 MiB | 328538 GiB | 328536 GiB |
|       from large pool |   2392 MiB |   9192 MiB | 327166 GiB | 327164 GiB |
|       from small pool |      5 MiB |     27 MiB |   1371 GiB |   1371 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     566    |     574    |   14176 K  |   14176 K  |
|       from large pool |     270    |     278    |    6933 K  |    6933 K  |
|       from small pool |     296    |     356    |    7243 K  |    7242 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     566    |     574    |   14176 K  |   14176 K  |
|       from large pool |     270    |     278    |    6933 K  |    6933 K  |
|       from small pool |     296    |     356    |    7243 K  |    7242 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      88    |     254    |    2680    |    2592    |
|       from large pool |      74    |     154    |    1269    |    1195    |
|       from small pool |      14    |     100    |    1411    |    1397    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     107    |     108    |    7774 K  |    7774 K  |
|       from large pool |      79    |      80    |    4324 K  |    4324 K  |
|       from small pool |      28    |      59    |    3449 K  |    3449 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:22:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:22:47] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:22:54]    INFO >> epoch 005:    620 / 1539 loss=3.545, wps=3162.7, ups=4.42, wpb=715.3, bsz=715.3, num_updates=6750, lr=0.000354, gnorm=4.772, clip=0, train_wall=10, gb_free=67.3, wall=1586 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:23:10]    INFO >> epoch 005:    670 / 1539 loss=3.69, wps=2648, ups=3.28, wpb=808.5, bsz=808.5, num_updates=6800, lr=0.000354, gnorm=5.381, clip=0, train_wall=15, gb_free=69, wall=1601 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:23:21]    INFO >> epoch 005:    720 / 1539 loss=3.573, wps=3458.5, ups=5.08, wpb=681.1, bsz=681.1, num_updates=6850, lr=0.000354, gnorm=5.406, clip=0, train_wall=9, gb_free=67.4, wall=1611 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:23:31]    INFO >> epoch 005:    770 / 1539 loss=3.505, wps=3677.8, ups=5.04, wpb=729.6, bsz=729.6, num_updates=6900, lr=0.000354, gnorm=4.53, clip=0, train_wall=9, gb_free=65.7, wall=1621 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:23:35] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.64 GiB. GPU 2 has a total capacity of 79.14 GiB of which 899.25 MiB is free. Including non-PyTorch memory, this process has 78.24 GiB memory in use. Of the allocated memory 72.14 GiB is allocated by PyTorch, and 5.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:23:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:23:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:23:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 27           |        cudaMalloc retries: 39        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69479 MiB |  76767 MiB | 319958 GiB | 319890 GiB |
|       from large pool |  69459 MiB |  76746 MiB | 318694 GiB | 318626 GiB |
|       from small pool |     20 MiB |     27 MiB |   1264 GiB |   1264 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69479 MiB |  76767 MiB | 319958 GiB | 319890 GiB |
|       from large pool |  69459 MiB |  76746 MiB | 318694 GiB | 318626 GiB |
|       from small pool |     20 MiB |     27 MiB |   1264 GiB |   1264 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  69460 MiB |  76747 MiB | 319539 GiB | 319472 GiB |
|       from large pool |  69439 MiB |  76727 MiB | 318277 GiB | 318209 GiB |
|       from small pool |     20 MiB |     26 MiB |   1262 GiB |   1262 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79606 MiB |  79646 MiB | 398500 MiB | 318894 MiB |
|       from large pool |  79574 MiB |  79574 MiB | 395634 MiB | 316060 MiB |
|       from small pool |     32 MiB |     72 MiB |   2866 MiB |   2834 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1942 MiB |   8675 MiB | 338046 GiB | 338044 GiB |
|       from large pool |   1930 MiB |   8664 MiB | 336638 GiB | 336637 GiB |
|       from small pool |     11 MiB |     31 MiB |   1407 GiB |   1407 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     646    |     721    |   14558 K  |   14557 K  |
|       from large pool |     331    |     355    |    7132 K  |    7132 K  |
|       from small pool |     315    |     376    |    7425 K  |    7424 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     646    |     721    |   14558 K  |   14557 K  |
|       from large pool |     331    |     355    |    7132 K  |    7132 K  |
|       from small pool |     315    |     376    |    7425 K  |    7424 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      89    |     109    |    2702    |    2613    |
|       from large pool |      73    |      73    |    1269    |    1196    |
|       from small pool |      16    |      36    |    1433    |    1417    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     104    |     116    |    7976 K  |    7976 K  |
|       from large pool |      73    |      79    |    4446 K  |    4446 K  |
|       from small pool |      31    |      62    |    3530 K  |    3530 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:23:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:23:35] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:23:42]    INFO >> epoch 005:    821 / 1539 loss=3.588, wps=2935.6, ups=4.6, wpb=637.8, bsz=637.8, num_updates=6950, lr=0.000354, gnorm=3.965, clip=0, train_wall=9, gb_free=69.7, wall=1632 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:23:53]    INFO >> epoch 005:    871 / 1539 loss=3.391, wps=3421.9, ups=4.84, wpb=707, bsz=707, num_updates=7000, lr=0.000354, gnorm=4.574, clip=0, train_wall=10, gb_free=69.1, wall=1642 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:24:04]    INFO >> epoch 005:    921 / 1539 loss=3.56, wps=3013.7, ups=4.71, wpb=640.5, bsz=640.5, num_updates=7050, lr=0.000354, gnorm=4.31, clip=0, train_wall=10, gb_free=69.9, wall=1653 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:24:10] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 2 has a total capacity of 79.14 GiB of which 317.25 MiB is free. Including non-PyTorch memory, this process has 78.81 GiB memory in use. Of the allocated memory 76.06 GiB is allocated by PyTorch, and 2.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:24:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:24:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:24:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 28           |        cudaMalloc retries: 41        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77289 MiB |  77889 MiB | 326772 GiB | 326696 GiB |
|       from large pool |  77272 MiB |  77872 MiB | 325484 GiB | 325408 GiB |
|       from small pool |     16 MiB |     30 MiB |   1288 GiB |   1288 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77289 MiB |  77889 MiB | 326772 GiB | 326696 GiB |
|       from large pool |  77272 MiB |  77872 MiB | 325484 GiB | 325408 GiB |
|       from small pool |     16 MiB |     30 MiB |   1288 GiB |   1288 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77280 MiB |  77879 MiB | 326344 GiB | 326269 GiB |
|       from large pool |  77263 MiB |  77862 MiB | 325058 GiB | 324983 GiB |
|       from small pool |     16 MiB |     30 MiB |   1286 GiB |   1286 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80188 MiB |  80188 MiB | 407846 MiB | 327658 MiB |
|       from large pool |  80158 MiB |  80158 MiB | 404942 MiB | 324784 MiB |
|       from small pool |     30 MiB |     70 MiB |   2904 MiB |   2874 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2838 MiB |   6449 MiB | 345633 GiB | 345630 GiB |
|       from large pool |   2825 MiB |   6435 MiB | 344199 GiB | 344196 GiB |
|       from small pool |     13 MiB |     25 MiB |   1434 GiB |   1434 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     528    |     535    |   14859 K  |   14858 K  |
|       from large pool |     239    |     246    |    7295 K  |    7295 K  |
|       from small pool |     289    |     356    |    7563 K  |    7563 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     528    |     535    |   14859 K  |   14858 K  |
|       from large pool |     239    |     246    |    7295 K  |    7295 K  |
|       from small pool |     289    |     356    |    7563 K  |    7563 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      81    |     109    |    2725    |    2644    |
|       from large pool |      66    |      74    |    1273    |    1207    |
|       from small pool |      15    |      35    |    1452    |    1437    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      90    |      90    |    8135 K  |    8134 K  |
|       from large pool |      63    |      63    |    4546 K  |    4545 K  |
|       from small pool |      27    |      50    |    3589 K  |    3588 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:24:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:24:10] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:24:15]    INFO >> epoch 005:    972 / 1539 loss=3.556, wps=3187.2, ups=4.56, wpb=698.3, bsz=698.3, num_updates=7100, lr=0.000354, gnorm=4.658, clip=0, train_wall=10, gb_free=51, wall=1664 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:24:28]    INFO >> epoch 005:   1022 / 1539 loss=3.278, wps=3916.9, ups=4.16, wpb=941.7, bsz=941.7, num_updates=7150, lr=0.000354, gnorm=4.862, clip=0, train_wall=11, gb_free=67.7, wall=1676 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:24:39]    INFO >> epoch 005:   1072 / 1539 loss=3.477, wps=3469.9, ups=4.49, wpb=773.6, bsz=773.6, num_updates=7200, lr=0.000354, gnorm=3.871, clip=0, train_wall=11, gb_free=72.2, wall=1687 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:24:49]    INFO >> epoch 005:   1122 / 1539 loss=3.484, wps=3186.9, ups=5.04, wpb=632.1, bsz=632.1, num_updates=7250, lr=0.000354, gnorm=4.799, clip=0, train_wall=9, gb_free=70.8, wall=1697 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:24:50] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.81 GiB. GPU 2 has a total capacity of 79.14 GiB of which 899.25 MiB is free. Including non-PyTorch memory, this process has 78.24 GiB memory in use. Of the allocated memory 67.00 GiB is allocated by PyTorch, and 10.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:24:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:24:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:24:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 29           |        cudaMalloc retries: 43        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  51065 MiB |  72747 MiB | 334660 GiB | 334611 GiB |
|       from large pool |  51047 MiB |  72730 MiB | 333341 GiB | 333291 GiB |
|       from small pool |     17 MiB |     27 MiB |   1319 GiB |   1319 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  51065 MiB |  72747 MiB | 334660 GiB | 334611 GiB |
|       from large pool |  51047 MiB |  72730 MiB | 333341 GiB | 333291 GiB |
|       from small pool |     17 MiB |     27 MiB |   1319 GiB |   1319 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  51057 MiB |  72738 MiB | 334223 GiB | 334173 GiB |
|       from large pool |  51040 MiB |  72720 MiB | 332905 GiB | 332856 GiB |
|       from small pool |     17 MiB |     27 MiB |   1317 GiB |   1317 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79606 MiB |  80324 MiB | 410894 MiB | 331288 MiB |
|       from large pool |  79578 MiB |  80098 MiB | 407794 MiB | 328216 MiB |
|       from small pool |     28 MiB |    226 MiB |   3100 MiB |   3072 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  16420 MiB |  19680 MiB | 354755 GiB | 354739 GiB |
|       from large pool |  16410 MiB |  19670 MiB | 353285 GiB | 353269 GiB |
|       from small pool |     10 MiB |     33 MiB |   1470 GiB |   1470 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     454    |     504    |   15221 K  |   15220 K  |
|       from large pool |     166    |     215    |    7474 K  |    7474 K  |
|       from small pool |     288    |     356    |    7746 K  |    7746 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     454    |     504    |   15221 K  |   15220 K  |
|       from large pool |     166    |     215    |    7474 K  |    7474 K  |
|       from small pool |     288    |     356    |    7746 K  |    7746 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      74    |     178    |    2824    |    2750    |
|       from large pool |      60    |      65    |    1274    |    1214    |
|       from small pool |      14    |     113    |    1550    |    1536    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      91    |      93    |    8330 K  |    8330 K  |
|       from large pool |      63    |      65    |    4652 K  |    4652 K  |
|       from small pool |      28    |      59    |    3677 K  |    3677 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:24:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:24:50] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:25:03]    INFO >> epoch 005:   1173 / 1539 loss=3.538, wps=3291.1, ups=4.17, wpb=788.8, bsz=788.8, num_updates=7300, lr=0.000354, gnorm=4.972, clip=0, train_wall=10, gb_free=71.2, wall=1709 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:25:13]    INFO >> epoch 005:   1223 / 1539 loss=3.618, wps=3062.6, ups=4.84, wpb=632.5, bsz=632.5, num_updates=7350, lr=0.000354, gnorm=4.077, clip=0, train_wall=10, gb_free=66.2, wall=1719 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:25:22]    INFO >> epoch 005:   1273 / 1539 loss=3.573, wps=3758.4, ups=5.62, wpb=669.3, bsz=669.3, num_updates=7400, lr=0.000354, gnorm=4.114, clip=0, train_wall=8, gb_free=72, wall=1728 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:25:34]    INFO >> epoch 005:   1323 / 1539 loss=3.473, wps=3182.5, ups=4.81, wpb=661.6, bsz=661.6, num_updates=7450, lr=0.000354, gnorm=4.377, clip=0, train_wall=10, gb_free=71, wall=1738 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:25:43]    INFO >> epoch 005:   1373 / 1539 loss=3.555, wps=3497.6, ups=5.11, wpb=684.4, bsz=684.4, num_updates=7500, lr=0.000354, gnorm=4.736, clip=0, train_wall=9, gb_free=70.3, wall=1748 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:25:53]    INFO >> epoch 005:   1423 / 1539 loss=3.658, wps=3272.8, ups=4.99, wpb=655.3, bsz=655.3, num_updates=7550, lr=0.000354, gnorm=4.542, clip=0, train_wall=9, gb_free=63.9, wall=1758 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:26:06]    INFO >> epoch 005:   1473 / 1539 loss=3.465, wps=3773.1, ups=4.61, wpb=819, bsz=819, num_updates=7600, lr=0.000354, gnorm=5.29, clip=0, train_wall=10, gb_free=71.4, wall=1769 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:26:17]    INFO >> epoch 005:   1523 / 1539 loss=3.636, wps=3322.9, ups=4.41, wpb=754.3, bsz=754.3, num_updates=7650, lr=0.000354, gnorm=4.524, clip=2, train_wall=11, gb_free=66, wall=1780 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:26:21]    INFO >> epoch 005 | loss 3.54 | wps 3127.6 | ups 4.4 | wpb 711 | bsz 711 | num_updates 7666 | lr 0.000354 | gnorm 4.609 | clip 0.1 | train_wall 304 | gb_free 56 | wall 1784 (progress_bar.py:267, print())[0m
[33m[2025-11-21 01:26:21] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:26:44]    INFO >> epoch 005 | valid on 'valid' subset | loss 3.718 | wps 6831.4 | wpb 5412.5 | bsz 5412.5 | num_updates 7666 | best_loss 4.043 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 01:26:45]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 01:26:45]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/checkpoints/checkpoint_last.pt (epoch 5 @ 7666 updates, score 3.718) (writing took 0.026611 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 01:26:45] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:26:51]    INFO >> epoch 006:     34 / 1539 loss=3.492, wps=1101.4, ups=1.51, wpb=727.7, bsz=727.7, num_updates=7700, lr=0.000327, gnorm=4.561, clip=0, train_wall=10, gb_free=64.6, wall=1813 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:27:03]    INFO >> epoch 006:     84 / 1539 loss=3.66, wps=3615.4, ups=4.45, wpb=813, bsz=813, num_updates=7750, lr=0.000327, gnorm=5.209, clip=2, train_wall=11, gb_free=66.7, wall=1825 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:27:14]    INFO >> epoch 006:    134 / 1539 loss=3.6, wps=3564.4, ups=5.05, wpb=706.4, bsz=706.4, num_updates=7800, lr=0.000327, gnorm=4.79, clip=0, train_wall=9, gb_free=65.1, wall=1835 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:27:25]    INFO >> epoch 006:    184 / 1539 loss=3.49, wps=3073.1, ups=4.56, wpb=674.2, bsz=674.2, num_updates=7850, lr=0.000327, gnorm=4.801, clip=0, train_wall=10, gb_free=67.5, wall=1846 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:27:26] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 722.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 141.25 MiB is free. Including non-PyTorch memory, this process has 78.98 GiB memory in use. Of the allocated memory 73.99 GiB is allocated by PyTorch, and 4.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:27:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:27:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:27:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 30           |        cudaMalloc retries: 44        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72193 MiB |  76054 MiB | 366624 GiB | 366553 GiB |
|       from large pool |  72175 MiB |  76036 MiB | 365172 GiB | 365102 GiB |
|       from small pool |     18 MiB |     21 MiB |   1451 GiB |   1451 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72193 MiB |  76054 MiB | 366624 GiB | 366553 GiB |
|       from large pool |  72175 MiB |  76036 MiB | 365172 GiB | 365102 GiB |
|       from small pool |     18 MiB |     21 MiB |   1451 GiB |   1451 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72184 MiB |  76043 MiB | 366151 GiB | 366081 GiB |
|       from large pool |  72166 MiB |  76025 MiB | 364702 GiB | 364631 GiB |
|       from small pool |     18 MiB |     21 MiB |   1449 GiB |   1449 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80364 MiB |  80438 MiB | 423846 MiB | 343482 MiB |
|       from large pool |  80332 MiB |  80332 MiB | 420668 MiB | 340336 MiB |
|       from small pool |     32 MiB |    106 MiB |   3178 MiB |   3146 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5318 MiB |  14599 MiB | 388940 GiB | 388935 GiB |
|       from large pool |   5304 MiB |  14584 MiB | 387326 GiB | 387321 GiB |
|       from small pool |     13 MiB |     31 MiB |   1614 GiB |   1614 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     509    |     518    |   16649 K  |   16649 K  |
|       from large pool |     220    |     229    |    8116 K  |    8116 K  |
|       from small pool |     289    |     356    |    8533 K  |    8533 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     509    |     518    |   16649 K  |   16649 K  |
|       from large pool |     220    |     229    |    8116 K  |    8116 K  |
|       from small pool |     289    |     356    |    8533 K  |    8533 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      76    |     113    |    2869    |    2793    |
|       from large pool |      60    |      60    |    1280    |    1220    |
|       from small pool |      16    |      53    |    1589    |    1573    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      89    |      91    |    9116 K  |    9116 K  |
|       from large pool |      58    |      60    |    5038 K  |    5038 K  |
|       from small pool |      31    |      68    |    4078 K  |    4078 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:27:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:27:26] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 01:27:34] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.77 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.33 GiB is free. Including non-PyTorch memory, this process has 77.79 GiB memory in use. Of the allocated memory 75.97 GiB is allocated by PyTorch, and 1.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:27:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:27:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:27:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 31           |        cudaMalloc retries: 45        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77203 MiB |  78280 MiB | 368250 GiB | 368174 GiB |
|       from large pool |  77181 MiB |  78257 MiB | 366793 GiB | 366718 GiB |
|       from small pool |     22 MiB |     23 MiB |   1456 GiB |   1456 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77203 MiB |  78280 MiB | 368250 GiB | 368174 GiB |
|       from large pool |  77181 MiB |  78257 MiB | 366793 GiB | 366718 GiB |
|       from small pool |     22 MiB |     23 MiB |   1456 GiB |   1456 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77192 MiB |  78269 MiB | 367775 GiB | 367700 GiB |
|       from large pool |  77170 MiB |  78247 MiB | 366321 GiB | 366245 GiB |
|       from small pool |     22 MiB |     23 MiB |   1454 GiB |   1454 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79144 MiB |  79172 MiB | 425506 MiB | 346362 MiB |
|       from large pool |  79112 MiB |  79112 MiB | 422300 MiB | 343188 MiB |
|       from small pool |     32 MiB |     60 MiB |   3206 MiB |   3174 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1940 MiB |   6463 MiB | 390819 GiB | 390817 GiB |
|       from large pool |   1930 MiB |   6453 MiB | 389199 GiB | 389197 GiB |
|       from small pool |      9 MiB |     31 MiB |   1619 GiB |   1619 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     566    |     574    |   16713 K  |   16712 K  |
|       from large pool |     270    |     278    |    8150 K  |    8149 K  |
|       from small pool |     296    |     356    |    8563 K  |    8562 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     566    |     574    |   16713 K  |   16712 K  |
|       from large pool |     270    |     278    |    8150 K  |    8149 K  |
|       from small pool |     296    |     356    |    8563 K  |    8562 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      78    |      92    |    2886    |    2808    |
|       from large pool |      62    |      62    |    1283    |    1221    |
|       from small pool |      16    |      30    |    1603    |    1587    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     101    |     101    |    9150 K  |    9150 K  |
|       from large pool |      69    |      69    |    5058 K  |    5058 K  |
|       from small pool |      32    |      69    |    4092 K  |    4092 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:27:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:27:34] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:27:38]    INFO >> epoch 006:    236 / 1539 loss=3.564, wps=2748.2, ups=3.84, wpb=716.2, bsz=716.2, num_updates=7900, lr=0.000327, gnorm=4.184, clip=0, train_wall=11, gb_free=72.2, wall=1859 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:27:49]    INFO >> epoch 006:    286 / 1539 loss=3.454, wps=3530.9, ups=5.17, wpb=683.2, bsz=683.2, num_updates=7950, lr=0.000327, gnorm=4.353, clip=0, train_wall=9, gb_free=69.8, wall=1868 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:28:00]    INFO >> epoch 006:    336 / 1539 loss=3.337, wps=3559.4, ups=4.49, wpb=792.7, bsz=792.7, num_updates=8000, lr=0.000327, gnorm=5.388, clip=0, train_wall=10, gb_free=67, wall=1879 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:28:10]    INFO >> epoch 006:    386 / 1539 loss=3.6, wps=3104.7, ups=4.76, wpb=651.9, bsz=651.9, num_updates=8050, lr=0.000327, gnorm=3.992, clip=0, train_wall=10, gb_free=73.3, wall=1890 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:28:22]    INFO >> epoch 006:    436 / 1539 loss=3.579, wps=3804.8, ups=5.01, wpb=759.3, bsz=759.3, num_updates=8100, lr=0.000327, gnorm=3.939, clip=0, train_wall=9, gb_free=67.2, wall=1900 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:28:23] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 2 has a total capacity of 79.14 GiB of which 3.51 GiB is free. Including non-PyTorch memory, this process has 75.60 GiB memory in use. Of the allocated memory 72.07 GiB is allocated by PyTorch, and 3.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:28:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:28:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:28:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 32           |        cudaMalloc retries: 46        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73198 MiB |  74274 MiB | 377481 GiB | 377409 GiB |
|       from large pool |  73181 MiB |  74258 MiB | 375988 GiB | 375916 GiB |
|       from small pool |     16 MiB |     27 MiB |   1492 GiB |   1492 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73198 MiB |  74274 MiB | 377481 GiB | 377409 GiB |
|       from large pool |  73181 MiB |  74258 MiB | 375988 GiB | 375916 GiB |
|       from small pool |     16 MiB |     27 MiB |   1492 GiB |   1492 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73189 MiB |  74265 MiB | 376995 GiB | 376923 GiB |
|       from large pool |  73173 MiB |  74248 MiB | 375504 GiB | 375432 GiB |
|       from small pool |     16 MiB |     27 MiB |   1490 GiB |   1490 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  76906 MiB |  79322 MiB | 425684 MiB | 348778 MiB |
|       from large pool |  76876 MiB |  79112 MiB | 422300 MiB | 345424 MiB |
|       from small pool |     30 MiB |    210 MiB |   3384 MiB |   3354 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3707 MiB |   6243 MiB | 401658 GiB | 401654 GiB |
|       from large pool |   3694 MiB |   6229 MiB | 399997 GiB | 399994 GiB |
|       from small pool |     13 MiB |     35 MiB |   1660 GiB |   1660 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     527    |     535    |   17145 K  |   17145 K  |
|       from large pool |     238    |     246    |    8370 K  |    8370 K  |
|       from small pool |     289    |     356    |    8774 K  |    8774 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     527    |     535    |   17145 K  |   17145 K  |
|       from large pool |     238    |     246    |    8370 K  |    8370 K  |
|       from small pool |     289    |     356    |    8774 K  |    8774 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      72    |     167    |    2975    |    2903    |
|       from large pool |      57    |      62    |    1283    |    1226    |
|       from small pool |      15    |     105    |    1692    |    1677    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      90    |      90    |    9379 K  |    9379 K  |
|       from large pool |      63    |      63    |    5189 K  |    5189 K  |
|       from small pool |      27    |      60    |    4189 K  |    4189 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:28:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:28:23] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:28:33]    INFO >> epoch 006:    487 / 1539 loss=3.358, wps=3469.8, ups=4.43, wpb=783.9, bsz=783.9, num_updates=8150, lr=0.000327, gnorm=4.766, clip=0, train_wall=10, gb_free=60.2, wall=1911 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:28:44]    INFO >> epoch 006:    537 / 1539 loss=3.548, wps=2966.9, ups=4.49, wpb=660.1, bsz=660.1, num_updates=8200, lr=0.000327, gnorm=4.84, clip=0, train_wall=11, gb_free=66.4, wall=1922 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:28:56]    INFO >> epoch 006:    587 / 1539 loss=3.655, wps=3647.8, ups=4.83, wpb=754.9, bsz=754.9, num_updates=8250, lr=0.000327, gnorm=3.979, clip=0, train_wall=10, gb_free=68.5, wall=1933 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:29:06]    INFO >> epoch 006:    637 / 1539 loss=3.449, wps=3370.8, ups=4.94, wpb=682.4, bsz=682.4, num_updates=8300, lr=0.000327, gnorm=4.605, clip=0, train_wall=10, gb_free=63.2, wall=1943 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:29:16]    INFO >> epoch 006:    687 / 1539 loss=3.504, wps=3458.3, ups=4.91, wpb=704.8, bsz=704.8, num_updates=8350, lr=0.000327, gnorm=4.112, clip=0, train_wall=10, gb_free=67.5, wall=1953 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:29:27] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 7.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.52 GiB is allocated by PyTorch, and 1.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:29:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:29:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:29:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 33           |        cudaMalloc retries: 48        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79323 MiB |  79383 MiB | 389681 GiB | 389604 GiB |
|       from large pool |  79139 MiB |  79199 MiB | 388139 GiB | 388062 GiB |
|       from small pool |    183 MiB |    184 MiB |   1542 GiB |   1542 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79323 MiB |  79383 MiB | 389681 GiB | 389604 GiB |
|       from large pool |  79139 MiB |  79199 MiB | 388139 GiB | 388062 GiB |
|       from small pool |    183 MiB |    184 MiB |   1542 GiB |   1542 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79274 MiB |  79334 MiB | 389179 GiB | 389102 GiB |
|       from large pool |  79092 MiB |  79151 MiB | 387639 GiB | 387562 GiB |
|       from small pool |    182 MiB |    183 MiB |   1539 GiB |   1539 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80498 MiB |  80500 MiB | 429324 MiB | 348826 MiB |
|       from large pool |  80296 MiB |  80296 MiB | 425720 MiB | 345424 MiB |
|       from small pool |    202 MiB |    248 MiB |   3604 MiB |   3402 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1114 MiB |   6546 MiB | 416293 GiB | 416292 GiB |
|       from large pool |   1096 MiB |   6542 MiB | 414577 GiB | 414576 GiB |
|       from small pool |     18 MiB |     27 MiB |   1716 GiB |   1716 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3413    |    3416    |   17724 K  |   17721 K  |
|       from large pool |     588    |     589    |    8664 K  |    8664 K  |
|       from small pool |    2825    |    2828    |    9059 K  |    9056 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3413    |    3416    |   17724 K  |   17721 K  |
|       from large pool |     588    |     589    |    8664 K  |    8664 K  |
|       from small pool |    2825    |    2828    |    9059 K  |    9056 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     215    |     237    |    3142    |    2927    |
|       from large pool |     114    |     114    |    1340    |    1226    |
|       from small pool |     101    |     124    |    1802    |    1701    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     243    |     243    |    9684 K  |    9684 K  |
|       from large pool |      63    |      65    |    5364 K  |    5364 K  |
|       from small pool |     180    |     180    |    4320 K  |    4320 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:29:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:29:27] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:29:28]    INFO >> epoch 006:    738 / 1539 loss=3.464, wps=2831.4, ups=4.72, wpb=599.7, bsz=599.7, num_updates=8400, lr=0.000327, gnorm=4.304, clip=0, train_wall=9, gb_free=68.8, wall=1964 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:29:38]    INFO >> epoch 006:    788 / 1539 loss=3.515, wps=3510.4, ups=5.31, wpb=661.1, bsz=661.1, num_updates=8450, lr=0.000327, gnorm=4.096, clip=0, train_wall=9, gb_free=71.7, wall=1973 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:29:48]    INFO >> epoch 006:    838 / 1539 loss=3.482, wps=3308.1, ups=4.82, wpb=687, bsz=687, num_updates=8500, lr=0.000327, gnorm=4.081, clip=0, train_wall=10, gb_free=67.2, wall=1983 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:30:00]    INFO >> epoch 006:    888 / 1539 loss=3.567, wps=3364.5, ups=4.77, wpb=705.5, bsz=705.5, num_updates=8550, lr=0.000327, gnorm=4.2, clip=0, train_wall=10, gb_free=66.2, wall=1994 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:30:12]    INFO >> epoch 006:    938 / 1539 loss=3.418, wps=3811.9, ups=4.07, wpb=936.3, bsz=936.3, num_updates=8600, lr=0.000327, gnorm=5.354, clip=0, train_wall=12, gb_free=54.2, wall=2006 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:30:22]    INFO >> epoch 006:    988 / 1539 loss=3.56, wps=2893.2, ups=5.05, wpb=572.7, bsz=572.7, num_updates=8650, lr=0.000327, gnorm=3.949, clip=0, train_wall=9, gb_free=71, wall=2016 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:30:33] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 239.25 MiB is free. Including non-PyTorch memory, this process has 78.88 GiB memory in use. Of the allocated memory 73.70 GiB is allocated by PyTorch, and 4.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:30:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:30:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:30:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 34           |        cudaMalloc retries: 49        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75039 MiB |  75469 MiB | 402362 GiB | 402289 GiB |
|       from large pool |  75013 MiB |  75443 MiB | 400771 GiB | 400698 GiB |
|       from small pool |     25 MiB |     29 MiB |   1590 GiB |   1590 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75039 MiB |  75469 MiB | 402362 GiB | 402289 GiB |
|       from large pool |  75013 MiB |  75443 MiB | 400771 GiB | 400698 GiB |
|       from small pool |     25 MiB |     29 MiB |   1590 GiB |   1590 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75024 MiB |  75454 MiB | 401841 GiB | 401768 GiB |
|       from large pool |  74999 MiB |  75428 MiB | 400253 GiB | 400180 GiB |
|       from small pool |     25 MiB |     29 MiB |   1588 GiB |   1588 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80266 MiB |  80462 MiB | 429348 MiB | 349082 MiB |
|       from large pool |  80236 MiB |  80236 MiB | 425720 MiB | 345484 MiB |
|       from small pool |     30 MiB |    226 MiB |   3628 MiB |   3598 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5226 MiB |   7149 MiB | 429696 GiB | 429691 GiB |
|       from large pool |   5222 MiB |   7141 MiB | 427925 GiB | 427920 GiB |
|       from small pool |      4 MiB |     27 MiB |   1770 GiB |   1770 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     567    |     574    |   18303 K  |   18303 K  |
|       from large pool |     265    |     272    |    8964 K  |    8964 K  |
|       from small pool |     302    |     356    |    9339 K  |    9339 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     567    |     574    |   18303 K  |   18303 K  |
|       from large pool |     265    |     272    |    8964 K  |    8964 K  |
|       from small pool |     302    |     356    |    9339 K  |    9339 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     128    |     226    |    3154    |    3026    |
|       from large pool |     113    |     113    |    1340    |    1227    |
|       from small pool |      15    |     113    |    1814    |    1799    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     117    |     121    |   10006 K  |   10006 K  |
|       from large pool |      88    |      92    |    5558 K  |    5558 K  |
|       from small pool |      29    |      58    |    4447 K  |    4447 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:30:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:30:33] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:30:35]    INFO >> epoch 006:   1039 / 1539 loss=3.547, wps=3203.4, ups=4.23, wpb=758, bsz=758, num_updates=8700, lr=0.000327, gnorm=4.468, clip=0, train_wall=10, gb_free=61.7, wall=2028 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:30:46]    INFO >> epoch 006:   1089 / 1539 loss=3.54, wps=3319.7, ups=4.69, wpb=707.3, bsz=707.3, num_updates=8750, lr=0.000327, gnorm=4.446, clip=0, train_wall=10, gb_free=64.1, wall=2039 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:30:57]    INFO >> epoch 006:   1139 / 1539 loss=3.376, wps=3326.5, ups=4.3, wpb=774.5, bsz=774.5, num_updates=8800, lr=0.000327, gnorm=4.172, clip=0, train_wall=11, gb_free=59.9, wall=2050 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:31:08] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.64 GiB. GPU 2 has a total capacity of 79.14 GiB of which 239.25 MiB is free. Including non-PyTorch memory, this process has 78.88 GiB memory in use. Of the allocated memory 72.14 GiB is allocated by PyTorch, and 6.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:31:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:31:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:31:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 35           |        cudaMalloc retries: 50        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69480 MiB |  76767 MiB | 409215 GiB | 409147 GiB |
|       from large pool |  69459 MiB |  76747 MiB | 407600 GiB | 407532 GiB |
|       from small pool |     20 MiB |     26 MiB |   1615 GiB |   1615 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69480 MiB |  76767 MiB | 409215 GiB | 409147 GiB |
|       from large pool |  69459 MiB |  76747 MiB | 407600 GiB | 407532 GiB |
|       from small pool |     20 MiB |     26 MiB |   1615 GiB |   1615 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  69460 MiB |  76747 MiB | 408686 GiB | 408618 GiB |
|       from large pool |  69439 MiB |  76727 MiB | 407073 GiB | 407005 GiB |
|       from small pool |     20 MiB |     25 MiB |   1612 GiB |   1612 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80266 MiB |  80370 MiB | 429452 MiB | 349186 MiB |
|       from large pool |  80236 MiB |  80236 MiB | 425720 MiB | 345484 MiB |
|       from small pool |     30 MiB |    134 MiB |   3732 MiB |   3702 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6693 MiB |   9030 MiB | 436955 GiB | 436948 GiB |
|       from large pool |   6684 MiB |   9020 MiB | 435156 GiB | 435150 GiB |
|       from small pool |      9 MiB |     20 MiB |   1798 GiB |   1798 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     646    |     721    |   18601 K  |   18601 K  |
|       from large pool |     331    |     355    |    9118 K  |    9118 K  |
|       from small pool |     315    |     376    |    9483 K  |    9483 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     646    |     721    |   18601 K  |   18601 K  |
|       from large pool |     331    |     355    |    9118 K  |    9118 K  |
|       from small pool |     315    |     376    |    9483 K  |    9483 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     128    |     180    |    3206    |    3078    |
|       from large pool |     113    |     113    |    1340    |    1227    |
|       from small pool |      15    |      67    |    1866    |    1851    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     133    |     144    |   10170 K  |   10170 K  |
|       from large pool |     106    |     111    |    5656 K  |    5656 K  |
|       from small pool |      27    |      48    |    4513 K  |    4513 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:31:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:31:08] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:31:10]    INFO >> epoch 006:   1190 / 1539 loss=3.561, wps=3239.8, ups=4.42, wpb=732.7, bsz=732.7, num_updates=8850, lr=0.000327, gnorm=5.114, clip=0, train_wall=10, gb_free=69.3, wall=2061 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:31:20]    INFO >> epoch 006:   1240 / 1539 loss=3.466, wps=3569.5, ups=5.15, wpb=692.9, bsz=692.9, num_updates=8900, lr=0.000327, gnorm=4.709, clip=0, train_wall=9, gb_free=63.8, wall=2071 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:31:30]    INFO >> epoch 006:   1290 / 1539 loss=3.6, wps=2944.6, ups=4.76, wpb=618.3, bsz=618.3, num_updates=8950, lr=0.000327, gnorm=4.146, clip=0, train_wall=10, gb_free=46.9, wall=2082 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:31:42]    INFO >> epoch 006:   1340 / 1539 loss=3.455, wps=3442.4, ups=4.82, wpb=714.5, bsz=714.5, num_updates=9000, lr=0.000327, gnorm=4.423, clip=0, train_wall=10, gb_free=74.5, wall=2092 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:31:52]    INFO >> epoch 006:   1390 / 1539 loss=3.535, wps=3296, ups=4.96, wpb=665, bsz=665, num_updates=9050, lr=0.000327, gnorm=3.818, clip=0, train_wall=10, gb_free=70.7, wall=2102 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:32:03]    INFO >> epoch 006:   1440 / 1539 loss=3.399, wps=3413.9, ups=4.7, wpb=726.4, bsz=726.4, num_updates=9100, lr=0.000327, gnorm=5.045, clip=0, train_wall=10, gb_free=66.7, wall=2113 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:32:15]    INFO >> epoch 006:   1490 / 1539 loss=3.403, wps=3134.9, ups=4.39, wpb=714.4, bsz=714.4, num_updates=9150, lr=0.000327, gnorm=4.449, clip=0, train_wall=11, gb_free=67.3, wall=2124 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:32:26]    INFO >> epoch 006 | loss 3.503 | wps 3112.4 | ups 4.38 | wpb 711 | bsz 711 | num_updates 9199 | lr 0.000327 | gnorm 4.452 | clip 0.1 | train_wall 305 | gb_free 68.5 | wall 2134 (progress_bar.py:267, print())[0m
[33m[2025-11-21 01:32:26] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:32:48]    INFO >> epoch 006 | valid on 'valid' subset | loss 3.719 | wps 7116.7 | wpb 5412.5 | bsz 5412.5 | num_updates 9199 | best_loss 4.043 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 01:32:49]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 01:32:49]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/checkpoints/checkpoint_last.pt (epoch 6 @ 9199 updates, score 3.719) (writing took 0.019999 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 01:32:49] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:32:49]    INFO >> epoch 007:      1 / 1539 loss=3.452, wps=1040.3, ups=1.55, wpb=671.1, bsz=671.1, num_updates=9200, lr=0.000295, gnorm=3.728, clip=0, train_wall=10, gb_free=63.9, wall=2156 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:32:54] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 381.25 MiB is free. Including non-PyTorch memory, this process has 78.74 GiB memory in use. Of the allocated memory 73.70 GiB is allocated by PyTorch, and 4.54 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:32:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:32:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:32:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 36           |        cudaMalloc retries: 51        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75039 MiB |  75470 MiB | 431340 GiB | 431266 GiB |
|       from large pool |  75013 MiB |  75444 MiB | 429630 GiB | 429557 GiB |
|       from small pool |     25 MiB |     27 MiB |   1709 GiB |   1709 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75039 MiB |  75470 MiB | 431340 GiB | 431266 GiB |
|       from large pool |  75013 MiB |  75444 MiB | 429630 GiB | 429557 GiB |
|       from small pool |     25 MiB |     27 MiB |   1709 GiB |   1709 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75024 MiB |  75454 MiB | 430783 GiB | 430710 GiB |
|       from large pool |  74999 MiB |  75428 MiB | 429076 GiB | 429003 GiB |
|       from small pool |     25 MiB |     27 MiB |   1706 GiB |   1706 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80124 MiB |  80166 MiB | 433444 MiB | 353320 MiB |
|       from large pool |  80094 MiB |  80094 MiB | 429670 MiB | 349576 MiB |
|       from small pool |     30 MiB |     72 MiB |   3774 MiB |   3744 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5084 MiB |   7149 MiB | 456969 GiB | 456964 GiB |
|       from large pool |   5080 MiB |   7141 MiB | 455070 GiB | 455065 GiB |
|       from small pool |      4 MiB |     33 MiB |   1899 GiB |   1899 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     567    |     574    |   19588 K  |   19588 K  |
|       from large pool |     265    |     272    |    9536 K  |    9535 K  |
|       from small pool |     302    |     356    |   10052 K  |   10052 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     567    |     574    |   19588 K  |   19588 K  |
|       from large pool |     265    |     272    |    9536 K  |    9535 K  |
|       from small pool |     302    |     356    |   10052 K  |   10052 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     128    |     149    |    3228    |    3100    |
|       from large pool |     113    |     113    |    1341    |    1228    |
|       from small pool |      15    |      36    |    1887    |    1872    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     116    |     120    |   10743 K  |   10743 K  |
|       from large pool |      87    |      91    |    5929 K  |    5929 K  |
|       from small pool |      29    |      64    |    4813 K  |    4813 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:32:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:32:54] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 01:33:00] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 2 has a total capacity of 79.14 GiB of which 3.99 GiB is free. Including non-PyTorch memory, this process has 75.13 GiB memory in use. Of the allocated memory 72.07 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:33:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:33:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:33:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 37           |        cudaMalloc retries: 53        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73198 MiB |  74274 MiB | 432472 GiB | 432400 GiB |
|       from large pool |  73181 MiB |  74258 MiB | 430759 GiB | 430687 GiB |
|       from small pool |     16 MiB |     31 MiB |   1713 GiB |   1713 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73198 MiB |  74274 MiB | 432472 GiB | 432400 GiB |
|       from large pool |  73181 MiB |  74258 MiB | 430759 GiB | 430687 GiB |
|       from small pool |     16 MiB |     31 MiB |   1713 GiB |   1713 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73189 MiB |  74265 MiB | 431914 GiB | 431842 GiB |
|       from large pool |  73173 MiB |  74248 MiB | 430203 GiB | 430132 GiB |
|       from small pool |     16 MiB |     31 MiB |   1710 GiB |   1710 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  76424 MiB |  80152 MiB | 437564 MiB | 361140 MiB |
|       from large pool |  76392 MiB |  80094 MiB | 433762 MiB | 357370 MiB |
|       from small pool |     32 MiB |     58 MiB |   3802 MiB |   3770 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3225 MiB |   6365 MiB | 458153 GiB | 458150 GiB |
|       from large pool |   3210 MiB |   6349 MiB | 456249 GiB | 456246 GiB |
|       from small pool |     15 MiB |     26 MiB |   1903 GiB |   1903 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     527    |     535    |   19636 K  |   19636 K  |
|       from large pool |     238    |     246    |    9561 K  |    9561 K  |
|       from small pool |     289    |     356    |   10075 K  |   10075 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     527    |     535    |   19636 K  |   19636 K  |
|       from large pool |     238    |     246    |    9561 K  |    9561 K  |
|       from small pool |     289    |     356    |   10075 K  |   10075 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      73    |     142    |    3243    |    3170    |
|       from large pool |      57    |     113    |    1342    |    1285    |
|       from small pool |      16    |      29    |    1901    |    1885    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      93    |      93    |   10769 K  |   10769 K  |
|       from large pool |      64    |      64    |    5946 K  |    5945 K  |
|       from small pool |      29    |      55    |    4823 K  |    4823 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:33:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:33:00] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:33:00]    INFO >> epoch 007:     53 / 1539 loss=3.464, wps=2928.2, ups=4.41, wpb=664.4, bsz=664.4, num_updates=9250, lr=0.000295, gnorm=4.334, clip=0, train_wall=9, gb_free=71.1, wall=2168 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:33:10]    INFO >> epoch 007:    103 / 1539 loss=3.535, wps=3491.1, ups=5.09, wpb=686.3, bsz=686.3, num_updates=9300, lr=0.000295, gnorm=4.207, clip=0, train_wall=9, gb_free=46.9, wall=2178 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:33:24]    INFO >> epoch 007:    153 / 1539 loss=3.462, wps=3343.2, ups=4.18, wpb=799.7, bsz=799.7, num_updates=9350, lr=0.000295, gnorm=4.433, clip=0, train_wall=11, gb_free=69.4, wall=2190 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:33:34]    INFO >> epoch 007:    203 / 1539 loss=3.499, wps=3463.7, ups=4.58, wpb=756.8, bsz=756.8, num_updates=9400, lr=0.000295, gnorm=4.158, clip=0, train_wall=10, gb_free=66.1, wall=2200 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:33:46]    INFO >> epoch 007:    253 / 1539 loss=3.455, wps=3507.6, ups=4.37, wpb=802.1, bsz=802.1, num_updates=9450, lr=0.000295, gnorm=4.26, clip=0, train_wall=11, gb_free=64, wall=2212 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:33:57]    INFO >> epoch 007:    303 / 1539 loss=3.407, wps=3637.1, ups=5.32, wpb=683.1, bsz=683.1, num_updates=9500, lr=0.000295, gnorm=4.308, clip=0, train_wall=9, gb_free=67.4, wall=2221 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:34:02] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.77 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.22 GiB is free. Including non-PyTorch memory, this process has 77.90 GiB memory in use. Of the allocated memory 75.97 GiB is allocated by PyTorch, and 1.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:34:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:34:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:34:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 38           |        cudaMalloc retries: 54        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77203 MiB |  78280 MiB | 444295 GiB | 444220 GiB |
|       from large pool |  77181 MiB |  78257 MiB | 442537 GiB | 442462 GiB |
|       from small pool |     22 MiB |     23 MiB |   1758 GiB |   1758 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77203 MiB |  78280 MiB | 444295 GiB | 444220 GiB |
|       from large pool |  77181 MiB |  78257 MiB | 442537 GiB | 442462 GiB |
|       from small pool |     22 MiB |     23 MiB |   1758 GiB |   1758 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77192 MiB |  78269 MiB | 443722 GiB | 443647 GiB |
|       from large pool |  77170 MiB |  78247 MiB | 441966 GiB | 441891 GiB |
|       from small pool |     22 MiB |     23 MiB |   1755 GiB |   1755 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79258 MiB |  79454 MiB | 440594 MiB | 361336 MiB |
|       from large pool |  79228 MiB |  79228 MiB | 436598 MiB | 357370 MiB |
|       from small pool |     30 MiB |    226 MiB |   3996 MiB |   3966 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2054 MiB |  10490 MiB | 472289 GiB | 472287 GiB |
|       from large pool |   2046 MiB |  10481 MiB | 470335 GiB | 470333 GiB |
|       from small pool |      7 MiB |     26 MiB |   1954 GiB |   1954 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     566    |     574    |   20176 K  |   20176 K  |
|       from large pool |     270    |     278    |    9839 K  |    9839 K  |
|       from small pool |     296    |     342    |   10337 K  |   10336 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     566    |     574    |   20176 K  |   20176 K  |
|       from large pool |     270    |     278    |    9839 K  |    9839 K  |
|       from small pool |     296    |     342    |   10337 K  |   10336 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      73    |     171    |    3341    |    3268    |
|       from large pool |      58    |      58    |    1343    |    1285    |
|       from small pool |      15    |     113    |    1998    |    1983    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      93    |      95    |   11053 K  |   11053 K  |
|       from large pool |      63    |      65    |    6111 K  |    6111 K  |
|       from small pool |      30    |      52    |    4942 K  |    4942 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:34:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:34:02] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 01:34:04] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.64 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.22 GiB is free. Including non-PyTorch memory, this process has 77.90 GiB memory in use. Of the allocated memory 72.14 GiB is allocated by PyTorch, and 5.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:34:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:34:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:34:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 39           |        cudaMalloc retries: 55        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69480 MiB |  76767 MiB | 444787 GiB | 444719 GiB |
|       from large pool |  69459 MiB |  76747 MiB | 443027 GiB | 442959 GiB |
|       from small pool |     20 MiB |     28 MiB |   1759 GiB |   1759 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69480 MiB |  76767 MiB | 444787 GiB | 444719 GiB |
|       from large pool |  69459 MiB |  76747 MiB | 443027 GiB | 442959 GiB |
|       from small pool |     20 MiB |     28 MiB |   1759 GiB |   1759 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  69460 MiB |  76747 MiB | 444213 GiB | 444145 GiB |
|       from large pool |  69439 MiB |  76727 MiB | 442456 GiB | 442388 GiB |
|       from small pool |     20 MiB |     28 MiB |   1756 GiB |   1756 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79260 MiB |  79284 MiB | 440620 MiB | 361360 MiB |
|       from large pool |  79228 MiB |  79228 MiB | 436598 MiB | 357370 MiB |
|       from small pool |     32 MiB |     56 MiB |   4022 MiB |   3990 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5687 MiB |   8024 MiB | 472847 GiB | 472842 GiB |
|       from large pool |   5676 MiB |   8012 MiB | 470891 GiB | 470886 GiB |
|       from small pool |     11 MiB |     29 MiB |   1955 GiB |   1955 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     646    |     721    |   20192 K  |   20191 K  |
|       from large pool |     331    |     355    |    9847 K  |    9847 K  |
|       from small pool |     315    |     376    |   10344 K  |   10344 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     646    |     721    |   20192 K  |   20191 K  |
|       from large pool |     331    |     355    |    9847 K  |    9847 K  |
|       from small pool |     315    |     376    |   10344 K  |   10344 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      74    |      86    |    3354    |    3280    |
|       from large pool |      58    |      58    |    1343    |    1285    |
|       from small pool |      16    |      28    |    2011    |    1995    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      99    |     109    |   11062 K  |   11061 K  |
|       from large pool |      69    |      74    |    6116 K  |    6116 K  |
|       from small pool |      30    |      59    |    4945 K  |    4945 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:34:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:34:04] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:34:09]    INFO >> epoch 007:    355 / 1539 loss=3.524, wps=2610.7, ups=4.05, wpb=644.9, bsz=644.9, num_updates=9550, lr=0.000295, gnorm=3.86, clip=0, train_wall=10, gb_free=68.4, wall=2234 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:34:19]    INFO >> epoch 007:    405 / 1539 loss=3.562, wps=2969.6, ups=4.89, wpb=607.6, bsz=607.6, num_updates=9600, lr=0.000295, gnorm=4.111, clip=0, train_wall=10, gb_free=60.1, wall=2244 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:34:31]    INFO >> epoch 007:    455 / 1539 loss=3.449, wps=3367.4, ups=4.76, wpb=707.3, bsz=707.3, num_updates=9650, lr=0.000295, gnorm=3.566, clip=0, train_wall=10, gb_free=70.3, wall=2254 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:34:35] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.81 GiB. GPU 2 has a total capacity of 79.14 GiB of which 2.43 GiB is free. Including non-PyTorch memory, this process has 76.69 GiB memory in use. Of the allocated memory 67.00 GiB is allocated by PyTorch, and 9.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:34:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:34:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:34:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 40           |        cudaMalloc retries: 56        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  51066 MiB |  72749 MiB | 450547 GiB | 450497 GiB |
|       from large pool |  51049 MiB |  72732 MiB | 448767 GiB | 448717 GiB |
|       from small pool |     17 MiB |     24 MiB |   1779 GiB |   1779 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  51066 MiB |  72749 MiB | 450547 GiB | 450497 GiB |
|       from large pool |  51049 MiB |  72732 MiB | 448767 GiB | 448717 GiB |
|       from small pool |     17 MiB |     24 MiB |   1779 GiB |   1779 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  51057 MiB |  72738 MiB | 449966 GiB | 449916 GiB |
|       from large pool |  51040 MiB |  72720 MiB | 448189 GiB | 448139 GiB |
|       from small pool |     17 MiB |     23 MiB |   1776 GiB |   1776 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78018 MiB |  78066 MiB | 443518 MiB | 365500 MiB |
|       from large pool |  77988 MiB |  77988 MiB | 439450 MiB | 361462 MiB |
|       from small pool |     30 MiB |     78 MiB |   4068 MiB |   4038 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  15915 MiB |  19147 MiB | 479741 GiB | 479725 GiB |
|       from large pool |  15902 MiB |  19134 MiB | 477763 GiB | 477747 GiB |
|       from small pool |     12 MiB |     25 MiB |   1977 GiB |   1977 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     454    |     504    |   20445 K  |   20445 K  |
|       from large pool |     166    |     215    |    9984 K  |    9984 K  |
|       from small pool |     288    |     356    |   10461 K  |   10460 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     454    |     504    |   20445 K  |   20445 K  |
|       from large pool |     166    |     215    |    9984 K  |    9984 K  |
|       from small pool |     288    |     356    |   10461 K  |   10460 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      73    |      97    |    3378    |    3305    |
|       from large pool |      58    |      58    |    1344    |    1286    |
|       from small pool |      15    |      39    |    2034    |    2019    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      86    |      91    |   11193 K  |   11193 K  |
|       from large pool |      57    |      62    |    6198 K  |    6198 K  |
|       from small pool |      29    |      61    |    4995 K  |    4995 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:34:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:34:35] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:34:42]    INFO >> epoch 007:    506 / 1539 loss=3.426, wps=3250.1, ups=4.39, wpb=740, bsz=740, num_updates=9700, lr=0.000295, gnorm=4.312, clip=0, train_wall=10, gb_free=67.5, wall=2266 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:34:52]    INFO >> epoch 007:    556 / 1539 loss=3.479, wps=3371.2, ups=5.06, wpb=666, bsz=666, num_updates=9750, lr=0.000295, gnorm=4.251, clip=0, train_wall=9, gb_free=69.8, wall=2276 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:35:04]    INFO >> epoch 007:    606 / 1539 loss=3.535, wps=3116.6, ups=4.87, wpb=640.3, bsz=640.3, num_updates=9800, lr=0.000295, gnorm=3.905, clip=0, train_wall=10, gb_free=70.7, wall=2286 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:35:15]    INFO >> epoch 007:    656 / 1539 loss=3.36, wps=3852.1, ups=4.75, wpb=811.4, bsz=811.4, num_updates=9850, lr=0.000295, gnorm=5.081, clip=0, train_wall=10, gb_free=67.6, wall=2296 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:35:25]    INFO >> epoch 007:    706 / 1539 loss=3.35, wps=3571.3, ups=4.8, wpb=744.1, bsz=744.1, num_updates=9900, lr=0.000295, gnorm=3.893, clip=0, train_wall=10, gb_free=71.7, wall=2307 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:35:37]    INFO >> epoch 007:    756 / 1539 loss=3.55, wps=3198.4, ups=4.81, wpb=665.4, bsz=665.4, num_updates=9950, lr=0.000295, gnorm=3.927, clip=0, train_wall=10, gb_free=73.5, wall=2317 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:35:47]    INFO >> epoch 007:    806 / 1539 loss=3.554, wps=3184.8, ups=4.93, wpb=646, bsz=646, num_updates=10000, lr=0.000295, gnorm=3.766, clip=0, train_wall=10, gb_free=72.4, wall=2327 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:35:58]    INFO >> epoch 007:    856 / 1539 loss=3.454, wps=3462, ups=4.57, wpb=756.9, bsz=756.9, num_updates=10050, lr=0.000295, gnorm=4.121, clip=0, train_wall=10, gb_free=68.7, wall=2338 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:36:09]    INFO >> epoch 007:    906 / 1539 loss=3.439, wps=3318.6, ups=4.96, wpb=668.4, bsz=668.4, num_updates=10100, lr=0.000295, gnorm=4.258, clip=0, train_wall=10, gb_free=71.8, wall=2348 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:36:20]    INFO >> epoch 007:    956 / 1539 loss=3.513, wps=3487.1, ups=4.62, wpb=754.1, bsz=754.1, num_updates=10150, lr=0.000295, gnorm=4.266, clip=0, train_wall=10, gb_free=55.4, wall=2359 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:36:30]    INFO >> epoch 007:   1006 / 1539 loss=3.401, wps=3324.3, ups=4.88, wpb=681.8, bsz=681.8, num_updates=10200, lr=0.000295, gnorm=3.922, clip=0, train_wall=10, gb_free=70, wall=2370 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:36:43]    INFO >> epoch 007:   1056 / 1539 loss=3.502, wps=3299.7, ups=4.37, wpb=754.4, bsz=754.4, num_updates=10250, lr=0.000295, gnorm=4.403, clip=0, train_wall=11, gb_free=64.4, wall=2381 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:36:53]    INFO >> epoch 007:   1106 / 1539 loss=3.404, wps=3452.6, ups=5.06, wpb=682.8, bsz=682.8, num_updates=10300, lr=0.000295, gnorm=3.668, clip=0, train_wall=9, gb_free=74.1, wall=2391 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:37:05]    INFO >> epoch 007:   1156 / 1539 loss=3.338, wps=3463.5, ups=4.26, wpb=813, bsz=813, num_updates=10350, lr=0.000295, gnorm=4.555, clip=0, train_wall=11, gb_free=46.6, wall=2403 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:37:16]    INFO >> epoch 007:   1206 / 1539 loss=3.366, wps=3606.6, ups=4.8, wpb=750.9, bsz=750.9, num_updates=10400, lr=0.000295, gnorm=4.501, clip=0, train_wall=10, gb_free=68.2, wall=2413 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:37:27]    INFO >> epoch 007:   1256 / 1539 loss=3.384, wps=3465.1, ups=4.75, wpb=728.9, bsz=728.9, num_updates=10450, lr=0.000295, gnorm=4.917, clip=0, train_wall=10, gb_free=68.1, wall=2423 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:37:36]    INFO >> epoch 007:   1306 / 1539 loss=3.402, wps=3324.4, ups=5.21, wpb=637.5, bsz=637.5, num_updates=10500, lr=0.000295, gnorm=4.323, clip=0, train_wall=9, gb_free=68.5, wall=2433 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:37:48]    INFO >> epoch 007:   1356 / 1539 loss=3.561, wps=3357.6, ups=5.12, wpb=656.2, bsz=656.2, num_updates=10550, lr=0.000295, gnorm=4.221, clip=0, train_wall=9, gb_free=68.9, wall=2443 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:37:58]    INFO >> epoch 007:   1406 / 1539 loss=3.503, wps=3443.8, ups=4.75, wpb=725.3, bsz=725.3, num_updates=10600, lr=0.000295, gnorm=3.781, clip=0, train_wall=10, gb_free=69.9, wall=2453 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:38:03] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 35.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 77.27 GiB is allocated by PyTorch, and 1.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:38:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:38:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:38:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 41           |        cudaMalloc retries: 58        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79065 MiB |  79125 MiB | 490895 GiB | 490818 GiB |
|       from large pool |  78885 MiB |  78945 MiB | 488961 GiB | 488884 GiB |
|       from small pool |    180 MiB |    181 MiB |   1934 GiB |   1933 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79065 MiB |  79125 MiB | 490895 GiB | 490818 GiB |
|       from large pool |  78885 MiB |  78945 MiB | 488961 GiB | 488884 GiB |
|       from small pool |    180 MiB |    181 MiB |   1934 GiB |   1933 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78974 MiB |  79033 MiB | 490262 GiB | 490185 GiB |
|       from large pool |  78794 MiB |  78854 MiB | 488330 GiB | 488253 GiB |
|       from small pool |    179 MiB |    180 MiB |   1931 GiB |   1931 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80470 MiB |  80472 MiB | 457058 MiB | 376588 MiB |
|       from large pool |  80272 MiB |  80272 MiB | 452770 MiB | 372498 MiB |
|       from small pool |    198 MiB |    248 MiB |   4288 MiB |   4090 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1344 MiB |   8920 MiB | 525956 GiB | 525954 GiB |
|       from large pool |   1326 MiB |   8915 MiB | 523804 GiB | 523803 GiB |
|       from small pool |     17 MiB |     29 MiB |   2151 GiB |   2151 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3358    |    3361    |   22300 K  |   22296 K  |
|       from large pool |     583    |     584    |   10946 K  |   10945 K  |
|       from small pool |    2775    |    2778    |   11353 K  |   11351 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3358    |    3361    |   22300 K  |   22296 K  |
|       from large pool |     583    |     584    |   10946 K  |   10945 K  |
|       from small pool |    2775    |    2778    |   11353 K  |   11351 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     376    |     400    |    3710    |    3334    |
|       from large pool |     277    |     277    |    1566    |    1289    |
|       from small pool |      99    |     124    |    2144    |    2045    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     295    |     296    |   12181 K  |   12181 K  |
|       from large pool |     123    |     124    |    6787 K  |    6786 K  |
|       from small pool |     172    |     173    |    5394 K  |    5394 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:38:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:38:03] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:38:09]    INFO >> epoch 007:   1457 / 1539 loss=3.394, wps=3170.3, ups=4.43, wpb=715.8, bsz=715.8, num_updates=10650, lr=0.000295, gnorm=4.719, clip=0, train_wall=10, gb_free=62.9, wall=2465 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:38:21]    INFO >> epoch 007:   1507 / 1539 loss=3.471, wps=3430.5, ups=4.83, wpb=710.2, bsz=710.2, num_updates=10700, lr=0.000295, gnorm=4.376, clip=0, train_wall=10, gb_free=68.4, wall=2475 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:38:28]    INFO >> epoch 007 | loss 3.451 | wps 3132.8 | ups 4.41 | wpb 711 | bsz 711 | num_updates 10732 | lr 0.000295 | gnorm 4.193 | clip 0 | train_wall 303 | gb_free 65.3 | wall 2482 (progress_bar.py:267, print())[0m
[33m[2025-11-21 01:38:28] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:38:50]    INFO >> epoch 007 | valid on 'valid' subset | loss 3.756 | wps 6898.3 | wpb 5412.5 | bsz 5412.5 | num_updates 10732 | best_loss 4.043 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 01:38:51]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 01:38:51]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/checkpoints/checkpoint_last.pt (epoch 7 @ 10732 updates, score 3.756) (writing took 0.029432 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 01:38:51] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:38:56]    INFO >> epoch 008:     18 / 1539 loss=3.254, wps=1218.6, ups=1.49, wpb=818.3, bsz=818.3, num_updates=10750, lr=0.000262, gnorm=3.501, clip=0, train_wall=11, gb_free=71.4, wall=2509 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:39:06]    INFO >> epoch 008:     68 / 1539 loss=3.425, wps=3391.6, ups=4.83, wpb=702.3, bsz=702.3, num_updates=10800, lr=0.000262, gnorm=4.17, clip=0, train_wall=10, gb_free=68.9, wall=2519 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:39:17]    INFO >> epoch 008:    118 / 1539 loss=3.436, wps=3163.2, ups=4.71, wpb=671.2, bsz=671.2, num_updates=10850, lr=0.000262, gnorm=3.432, clip=0, train_wall=10, gb_free=71.3, wall=2530 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:39:29]    INFO >> epoch 008:    168 / 1539 loss=3.336, wps=3183.5, ups=4.71, wpb=675.6, bsz=675.6, num_updates=10900, lr=0.000262, gnorm=4.16, clip=0, train_wall=10, gb_free=66.5, wall=2540 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:39:41]    INFO >> epoch 008:    218 / 1539 loss=3.382, wps=3554.6, ups=4.28, wpb=830.8, bsz=830.8, num_updates=10950, lr=0.000262, gnorm=4.243, clip=0, train_wall=11, gb_free=68.3, wall=2552 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:39:51]    INFO >> epoch 008:    268 / 1539 loss=3.471, wps=3540.6, ups=4.78, wpb=740, bsz=740, num_updates=11000, lr=0.000262, gnorm=4.586, clip=0, train_wall=10, gb_free=67.5, wall=2562 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:40:03]    INFO >> epoch 008:    318 / 1539 loss=3.597, wps=3196.9, ups=4.62, wpb=692.2, bsz=692.2, num_updates=11050, lr=0.000262, gnorm=4.249, clip=0, train_wall=10, gb_free=70.5, wall=2573 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:40:15]    INFO >> epoch 008:    368 / 1539 loss=3.351, wps=3043.4, ups=4.32, wpb=705, bsz=705, num_updates=11100, lr=0.000262, gnorm=4.411, clip=0, train_wall=11, gb_free=69.6, wall=2585 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:40:26]    INFO >> epoch 008:    418 / 1539 loss=3.505, wps=3256.5, ups=4.51, wpb=722.1, bsz=722.1, num_updates=11150, lr=0.000262, gnorm=4.684, clip=0, train_wall=11, gb_free=73.3, wall=2596 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:40:39]    INFO >> epoch 008:    468 / 1539 loss=3.495, wps=3913.6, ups=4.45, wpb=880, bsz=880, num_updates=11200, lr=0.000262, gnorm=4.044, clip=0, train_wall=11, gb_free=42.2, wall=2607 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:40:49]    INFO >> epoch 008:    518 / 1539 loss=3.521, wps=3307.8, ups=4.83, wpb=684.3, bsz=684.3, num_updates=11250, lr=0.000262, gnorm=3.983, clip=0, train_wall=10, gb_free=62.6, wall=2617 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:41:00]    INFO >> epoch 008:    568 / 1539 loss=3.457, wps=3155.6, ups=4.68, wpb=674.5, bsz=674.5, num_updates=11300, lr=0.000262, gnorm=4.968, clip=0, train_wall=10, gb_free=69.1, wall=2628 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:41:11]    INFO >> epoch 008:    618 / 1539 loss=3.488, wps=3264.4, ups=4.95, wpb=659, bsz=659, num_updates=11350, lr=0.000262, gnorm=4.237, clip=0, train_wall=10, gb_free=73.2, wall=2638 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:41:21]    INFO >> epoch 008:    668 / 1539 loss=3.406, wps=3338.4, ups=5.17, wpb=645.6, bsz=645.6, num_updates=11400, lr=0.000262, gnorm=3.402, clip=0, train_wall=9, gb_free=69.2, wall=2648 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:41:31]    INFO >> epoch 008:    718 / 1539 loss=3.523, wps=3378.9, ups=4.8, wpb=704.5, bsz=704.5, num_updates=11450, lr=0.000262, gnorm=4.926, clip=0, train_wall=10, gb_free=69.6, wall=2658 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:41:43]    INFO >> epoch 008:    768 / 1539 loss=3.44, wps=3331.8, ups=4.74, wpb=702.4, bsz=702.4, num_updates=11500, lr=0.000262, gnorm=4.149, clip=0, train_wall=10, gb_free=64.1, wall=2669 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:41:54]    INFO >> epoch 008:    818 / 1539 loss=3.311, wps=3336.2, ups=4.63, wpb=720.8, bsz=720.8, num_updates=11550, lr=0.000262, gnorm=4.112, clip=0, train_wall=10, gb_free=66.4, wall=2680 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:42:03]    INFO >> epoch 008:    868 / 1539 loss=3.473, wps=3167.6, ups=5.46, wpb=580.1, bsz=580.1, num_updates=11600, lr=0.000262, gnorm=3.202, clip=0, train_wall=9, gb_free=65.3, wall=2689 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:42:16]    INFO >> epoch 008:    918 / 1539 loss=3.313, wps=3602.6, ups=4.45, wpb=809.8, bsz=809.8, num_updates=11650, lr=0.000262, gnorm=3.846, clip=0, train_wall=11, gb_free=58.8, wall=2700 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:42:25]    INFO >> epoch 008:    968 / 1539 loss=3.394, wps=3462.9, ups=5.08, wpb=681.2, bsz=681.2, num_updates=11700, lr=0.000262, gnorm=4.008, clip=0, train_wall=9, gb_free=64.5, wall=2710 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:42:30] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.77 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.27 GiB is free. Including non-PyTorch memory, this process has 77.85 GiB memory in use. Of the allocated memory 75.98 GiB is allocated by PyTorch, and 1.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:42:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:42:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:42:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 42           |        cudaMalloc retries: 60        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77204 MiB |  78282 MiB | 543496 GiB | 543421 GiB |
|       from large pool |  77182 MiB |  78260 MiB | 541345 GiB | 541270 GiB |
|       from small pool |     22 MiB |     26 MiB |   2150 GiB |   2150 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77204 MiB |  78282 MiB | 543496 GiB | 543421 GiB |
|       from large pool |  77182 MiB |  78260 MiB | 541345 GiB | 541270 GiB |
|       from small pool |     22 MiB |     26 MiB |   2150 GiB |   2150 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77192 MiB |  78269 MiB | 542791 GiB | 542716 GiB |
|       from large pool |  77170 MiB |  78247 MiB | 540644 GiB | 540568 GiB |
|       from small pool |     22 MiB |     26 MiB |   2147 GiB |   2147 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79204 MiB |  80460 MiB | 468608 MiB | 389404 MiB |
|       from large pool |  79172 MiB |  80212 MiB | 464270 MiB | 385098 MiB |
|       from small pool |     32 MiB |    248 MiB |   4338 MiB |   4306 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1939 MiB |   8870 MiB | 577831 GiB | 577829 GiB |
|       from large pool |   1929 MiB |   8859 MiB | 575440 GiB | 575438 GiB |
|       from small pool |      9 MiB |     27 MiB |   2390 GiB |   2390 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     566    |     574    |   24737 K  |   24736 K  |
|       from large pool |     270    |     278    |   12096 K  |   12096 K  |
|       from small pool |     296    |     356    |   12640 K  |   12640 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     566    |     574    |   24737 K  |   24736 K  |
|       from large pool |     270    |     278    |   12096 K  |   12096 K  |
|       from small pool |     296    |     356    |   12640 K  |   12640 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      92    |     400    |    3744    |    3652    |
|       from large pool |      76    |     276    |    1575    |    1499    |
|       from small pool |      16    |     124    |    2169    |    2153    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     109    |     110    |   13560 K  |   13560 K  |
|       from large pool |      78    |      79    |    7535 K  |    7535 K  |
|       from small pool |      31    |      56    |    6025 K  |    6025 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:42:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:42:30] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 01:42:36] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.01 GiB is free. Including non-PyTorch memory, this process has 78.11 GiB memory in use. Of the allocated memory 73.70 GiB is allocated by PyTorch, and 3.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:42:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:42:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:42:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 43           |        cudaMalloc retries: 62        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75039 MiB |  75470 MiB | 544600 GiB | 544526 GiB |
|       from large pool |  75014 MiB |  75444 MiB | 542445 GiB | 542372 GiB |
|       from small pool |     25 MiB |     31 MiB |   2154 GiB |   2154 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75039 MiB |  75470 MiB | 544600 GiB | 544526 GiB |
|       from large pool |  75014 MiB |  75444 MiB | 542445 GiB | 542372 GiB |
|       from small pool |     25 MiB |     31 MiB |   2154 GiB |   2154 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75024 MiB |  75454 MiB | 543894 GiB | 543820 GiB |
|       from large pool |  74999 MiB |  75428 MiB | 541742 GiB | 541669 GiB |
|       from small pool |     25 MiB |     31 MiB |   2151 GiB |   2151 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79470 MiB |  79470 MiB | 500236 MiB | 420766 MiB |
|       from large pool |  79438 MiB |  79438 MiB | 495870 MiB | 416432 MiB |
|       from small pool |     32 MiB |     60 MiB |   4366 MiB |   4334 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4430 MiB |   6436 MiB | 579037 GiB | 579033 GiB |
|       from large pool |   4423 MiB |   6426 MiB | 576643 GiB | 576638 GiB |
|       from small pool |      6 MiB |     25 MiB |   2394 GiB |   2394 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     567    |     574    |   24782 K  |   24781 K  |
|       from large pool |     265    |     272    |   12121 K  |   12120 K  |
|       from small pool |     302    |     356    |   12660 K  |   12660 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     567    |     574    |   24782 K  |   24781 K  |
|       from large pool |     265    |     272    |   12121 K  |   12120 K  |
|       from small pool |     302    |     356    |   12660 K  |   12660 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      88    |     105    |    3766    |    3678    |
|       from large pool |      72    |      75    |    1583    |    1511    |
|       from small pool |      16    |      30    |    2183    |    2167    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      89    |      90    |   13583 K  |   13583 K  |
|       from large pool |      58    |      59    |    7550 K  |    7550 K  |
|       from small pool |      31    |      50    |    6033 K  |    6033 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:42:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:42:36] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:42:38]    INFO >> epoch 008:   1020 / 1539 loss=3.358, wps=2595.9, ups=4.05, wpb=641.3, bsz=641.3, num_updates=11750, lr=0.000262, gnorm=4.076, clip=0, train_wall=9, gb_free=61.2, wall=2722 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:42:49]    INFO >> epoch 008:   1070 / 1539 loss=3.256, wps=3478.1, ups=4.88, wpb=712.9, bsz=712.9, num_updates=11800, lr=0.000262, gnorm=4.844, clip=0, train_wall=10, gb_free=68.9, wall=2732 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:43:00]    INFO >> epoch 008:   1120 / 1539 loss=3.478, wps=3114.2, ups=4.68, wpb=666, bsz=666, num_updates=11850, lr=0.000262, gnorm=4.067, clip=0, train_wall=10, gb_free=65, wall=2743 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:43:09]    INFO >> epoch 008:   1170 / 1539 loss=3.572, wps=3220.4, ups=5.36, wpb=600.9, bsz=600.9, num_updates=11900, lr=0.000262, gnorm=4.059, clip=0, train_wall=9, gb_free=63.6, wall=2752 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:43:10] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.79 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.54 GiB is free. Including non-PyTorch memory, this process has 77.58 GiB memory in use. Of the allocated memory 68.26 GiB is allocated by PyTorch, and 8.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:43:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:43:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:43:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 44           |        cudaMalloc retries: 63        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  65550 MiB |  69898 MiB | 551083 GiB | 551019 GiB |
|       from large pool |  65533 MiB |  69880 MiB | 548904 GiB | 548840 GiB |
|       from small pool |     17 MiB |     24 MiB |   2178 GiB |   2178 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  65550 MiB |  69898 MiB | 551083 GiB | 551019 GiB |
|       from large pool |  65533 MiB |  69880 MiB | 548904 GiB | 548840 GiB |
|       from small pool |     17 MiB |     24 MiB |   2178 GiB |   2178 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  65539 MiB |  69886 MiB | 550368 GiB | 550304 GiB |
|       from large pool |  65521 MiB |  69869 MiB | 548193 GiB | 548129 GiB |
|       from small pool |     17 MiB |     24 MiB |   2175 GiB |   2175 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78928 MiB |  79508 MiB | 500274 MiB | 421346 MiB |
|       from large pool |  78894 MiB |  79438 MiB | 495870 MiB | 416976 MiB |
|       from small pool |     34 MiB |     70 MiB |   4404 MiB |   4370 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   9285 MiB |  13621 MiB | 586320 GiB | 586311 GiB |
|       from large pool |   9268 MiB |  13604 MiB | 583898 GiB | 583889 GiB |
|       from small pool |     16 MiB |     24 MiB |   2421 GiB |   2421 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     489    |     498    |   25077 K  |   25076 K  |
|       from large pool |     201    |     210    |   12278 K  |   12278 K  |
|       from small pool |     288    |     348    |   12798 K  |   12798 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     489    |     498    |   25077 K  |   25076 K  |
|       from large pool |     201    |     210    |   12278 K  |   12278 K  |
|       from small pool |     288    |     348    |   12798 K  |   12798 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      88    |     107    |    3785    |    3697    |
|       from large pool |      71    |      72    |    1583    |    1512    |
|       from small pool |      17    |      35    |    2202    |    2185    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      91    |      92    |   13739 K  |   13738 K  |
|       from large pool |      60    |      61    |    7646 K  |    7646 K  |
|       from small pool |      31    |      49    |    6092 K  |    6092 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:43:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:43:10] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:43:23]    INFO >> epoch 008:   1221 / 1539 loss=3.454, wps=2937.8, ups=4.06, wpb=724.2, bsz=724.2, num_updates=11950, lr=0.000262, gnorm=3.74, clip=0, train_wall=11, gb_free=54, wall=2765 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:43:28] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 43.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 77.37 GiB is allocated by PyTorch, and 1.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:43:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:43:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:43:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 45           |        cudaMalloc retries: 64        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79165 MiB |  79225 MiB | 554331 GiB | 554253 GiB |
|       from large pool |  78983 MiB |  79043 MiB | 552140 GiB | 552063 GiB |
|       from small pool |    181 MiB |    182 MiB |   2190 GiB |   2190 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79165 MiB |  79225 MiB | 554331 GiB | 554253 GiB |
|       from large pool |  78983 MiB |  79043 MiB | 552140 GiB | 552063 GiB |
|       from small pool |    181 MiB |    182 MiB |   2190 GiB |   2190 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79094 MiB |  79153 MiB | 553612 GiB | 553535 GiB |
|       from large pool |  78913 MiB |  78973 MiB | 551425 GiB | 551348 GiB |
|       from small pool |    180 MiB |    181 MiB |   2187 GiB |   2187 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80462 MiB |  80462 MiB | 505900 MiB | 425438 MiB |
|       from large pool |  80262 MiB |  80262 MiB | 501330 MiB | 421068 MiB |
|       from small pool |    200 MiB |    200 MiB |   4570 MiB |   4370 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1236 MiB |   6833 MiB | 589952 GiB | 589951 GiB |
|       from large pool |   1218 MiB |   6826 MiB | 587517 GiB | 587515 GiB |
|       from small pool |     18 MiB |     22 MiB |   2435 GiB |   2435 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3380    |    3383    |   25216 K  |   25212 K  |
|       from large pool |     585    |     586    |   12346 K  |   12346 K  |
|       from small pool |    2795    |    2798    |   12869 K  |   12866 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3380    |    3383    |   25216 K  |   25212 K  |
|       from large pool |     585    |     586    |   12346 K  |   12346 K  |
|       from small pool |    2795    |    2798    |   12869 K  |   12866 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     261    |     261    |    3959    |    3698    |
|       from large pool |     161    |     161    |    1674    |    1513    |
|       from small pool |     100    |     100    |    2285    |    2185    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     236    |     236    |   13814 K  |   13814 K  |
|       from large pool |      63    |      64    |    7688 K  |    7688 K  |
|       from small pool |     173    |     173    |    6126 K  |    6125 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:43:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:43:28] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:43:35]    INFO >> epoch 008:   1272 / 1539 loss=3.492, wps=3523, ups=4.16, wpb=847.8, bsz=847.8, num_updates=12000, lr=0.000262, gnorm=4.759, clip=0, train_wall=11, gb_free=62, wall=2777 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:43:45]    INFO >> epoch 008:   1322 / 1539 loss=3.488, wps=3465.4, ups=5.07, wpb=684.1, bsz=684.1, num_updates=12050, lr=0.000262, gnorm=3.792, clip=0, train_wall=9, gb_free=67.7, wall=2787 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:43:56] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.35 GiB is free. Including non-PyTorch memory, this process has 77.77 GiB memory in use. Of the allocated memory 76.06 GiB is allocated by PyTorch, and 1.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:43:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:43:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:43:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 46           |        cudaMalloc retries: 66        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77291 MiB |  77890 MiB | 559563 GiB | 559488 GiB |
|       from large pool |  77274 MiB |  77873 MiB | 557354 GiB | 557278 GiB |
|       from small pool |     16 MiB |     24 MiB |   2209 GiB |   2209 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77291 MiB |  77890 MiB | 559563 GiB | 559488 GiB |
|       from large pool |  77274 MiB |  77873 MiB | 557354 GiB | 557278 GiB |
|       from small pool |     16 MiB |     24 MiB |   2209 GiB |   2209 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77280 MiB |  77879 MiB | 558837 GiB | 558762 GiB |
|       from large pool |  77263 MiB |  77862 MiB | 556631 GiB | 556556 GiB |
|       from small pool |     16 MiB |     24 MiB |   2206 GiB |   2206 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79122 MiB |  80402 MiB | 547406 MiB | 468284 MiB |
|       from large pool |  79092 MiB |  80202 MiB | 542836 MiB | 463744 MiB |
|       from small pool |     30 MiB |    200 MiB |   4570 MiB |   4540 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1830 MiB |   5537 MiB | 595394 GiB | 595392 GiB |
|       from large pool |   1817 MiB |   5522 MiB | 592937 GiB | 592936 GiB |
|       from small pool |     13 MiB |     20 MiB |   2456 GiB |   2456 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     528    |     535    |   25442 K  |   25441 K  |
|       from large pool |     239    |     246    |   12466 K  |   12466 K  |
|       from small pool |     289    |     348    |   12975 K  |   12975 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     528    |     535    |   25442 K  |   25441 K  |
|       from large pool |     239    |     246    |   12466 K  |   12466 K  |
|       from small pool |     289    |     348    |   12975 K  |   12975 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      73    |     260    |    3976    |    3903    |
|       from large pool |      58    |     160    |    1691    |    1633    |
|       from small pool |      15    |     100    |    2285    |    2270    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      90    |      90    |   13939 K  |   13939 K  |
|       from large pool |      61    |      61    |    7766 K  |    7766 K  |
|       from small pool |      29    |      49    |    6173 K  |    6173 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:43:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:43:56] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:43:59]    INFO >> epoch 008:   1373 / 1539 loss=3.311, wps=2771.5, ups=3.97, wpb=698.4, bsz=698.4, num_updates=12100, lr=0.000262, gnorm=3.793, clip=0, train_wall=10, gb_free=50.7, wall=2799 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:44:09]    INFO >> epoch 008:   1423 / 1539 loss=3.448, wps=3646.5, ups=4.69, wpb=777.5, bsz=777.5, num_updates=12150, lr=0.000262, gnorm=3.793, clip=0, train_wall=10, gb_free=67.2, wall=2810 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:44:16] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.64 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.35 GiB is free. Including non-PyTorch memory, this process has 77.77 GiB memory in use. Of the allocated memory 72.13 GiB is allocated by PyTorch, and 5.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:44:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:44:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:44:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 47           |        cudaMalloc retries: 67        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69477 MiB |  76764 MiB | 563556 GiB | 563488 GiB |
|       from large pool |  69457 MiB |  76744 MiB | 561331 GiB | 561263 GiB |
|       from small pool |     20 MiB |     32 MiB |   2224 GiB |   2224 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69477 MiB |  76764 MiB | 563556 GiB | 563488 GiB |
|       from large pool |  69457 MiB |  76744 MiB | 561331 GiB | 561263 GiB |
|       from small pool |     20 MiB |     32 MiB |   2224 GiB |   2224 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  69460 MiB |  76747 MiB | 562825 GiB | 562757 GiB |
|       from large pool |  69439 MiB |  76727 MiB | 560604 GiB | 560536 GiB |
|       from small pool |     20 MiB |     32 MiB |   2221 GiB |   2221 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79124 MiB |  79186 MiB | 547470 MiB | 468346 MiB |
|       from large pool |  79092 MiB |  79092 MiB | 542836 MiB | 463744 MiB |
|       from small pool |     32 MiB |     94 MiB |   4634 MiB |   4602 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5554 MiB |   7965 MiB | 600058 GiB | 600053 GiB |
|       from large pool |   5542 MiB |   7952 MiB | 597585 GiB | 597579 GiB |
|       from small pool |     11 MiB |     23 MiB |   2473 GiB |   2473 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     646    |     721    |   25621 K  |   25620 K  |
|       from large pool |     331    |     355    |   12557 K  |   12557 K  |
|       from small pool |     315    |     376    |   13063 K  |   13063 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     646    |     721    |   25621 K  |   25620 K  |
|       from large pool |     331    |     355    |   12557 K  |   12557 K  |
|       from small pool |     315    |     376    |   13063 K  |   13063 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      74    |     105    |    4008    |    3934    |
|       from large pool |      58    |      58    |    1691    |    1633    |
|       from small pool |      16    |      47    |    2317    |    2301    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     101    |     112    |   14033 K  |   14033 K  |
|       from large pool |      73    |      76    |    7820 K  |    7819 K  |
|       from small pool |      28    |      54    |    6213 K  |    6213 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:44:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:44:16] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:44:21]    INFO >> epoch 008:   1474 / 1539 loss=3.429, wps=3292.2, ups=4.45, wpb=740.2, bsz=740.2, num_updates=12200, lr=0.000262, gnorm=4.305, clip=0, train_wall=10, gb_free=68.9, wall=2821 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:44:32]    INFO >> epoch 008:   1524 / 1539 loss=3.386, wps=3223.5, ups=4.91, wpb=656.8, bsz=656.8, num_updates=12250, lr=0.000262, gnorm=3.672, clip=0, train_wall=10, gb_free=68.3, wall=2831 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:44:36]    INFO >> epoch 008 | loss 3.429 | wps 3093 | ups 4.35 | wpb 711 | bsz 711 | num_updates 12265 | lr 0.000262 | gnorm 4.117 | clip 0 | train_wall 306 | gb_free 72.3 | wall 2835 (progress_bar.py:267, print())[0m
[33m[2025-11-21 01:44:36] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:44:58]    INFO >> epoch 008 | valid on 'valid' subset | loss 3.766 | wps 6784.1 | wpb 5412.5 | bsz 5412.5 | num_updates 12265 | best_loss 4.043 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 01:44:58]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 01:44:58]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/checkpoints/checkpoint_last.pt (epoch 8 @ 12265 updates, score 3.766) (writing took 0.022156 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 01:44:58] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:45:06]    INFO >> epoch 009:     35 / 1539 loss=3.465, wps=982, ups=1.52, wpb=644.9, bsz=644.9, num_updates=12300, lr=0.000227, gnorm=4.034, clip=0, train_wall=10, gb_free=69.9, wall=2864 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:45:17]    INFO >> epoch 009:     85 / 1539 loss=3.4, wps=3399, ups=4.76, wpb=714.8, bsz=714.8, num_updates=12350, lr=0.000227, gnorm=4.004, clip=0, train_wall=10, gb_free=67, wall=2875 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:45:28]    INFO >> epoch 009:    135 / 1539 loss=3.39, wps=3601.2, ups=4.68, wpb=769.2, bsz=769.2, num_updates=12400, lr=0.000227, gnorm=3.553, clip=0, train_wall=10, gb_free=68.7, wall=2885 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:45:40]    INFO >> epoch 009:    185 / 1539 loss=3.463, wps=3338, ups=4.7, wpb=710, bsz=710, num_updates=12450, lr=0.000227, gnorm=4.38, clip=2, train_wall=10, gb_free=70, wall=2896 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:45:50]    INFO >> epoch 009:    235 / 1539 loss=3.429, wps=3663.1, ups=4.97, wpb=737.7, bsz=737.7, num_updates=12500, lr=0.000227, gnorm=3.83, clip=0, train_wall=9, gb_free=69.4, wall=2906 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:46:00]    INFO >> epoch 009:    285 / 1539 loss=3.349, wps=3438.5, ups=5.05, wpb=681.6, bsz=681.6, num_updates=12550, lr=0.000227, gnorm=4.231, clip=0, train_wall=9, gb_free=72.6, wall=2916 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:46:12]    INFO >> epoch 009:    335 / 1539 loss=3.292, wps=3700.1, ups=4.43, wpb=836, bsz=836, num_updates=12600, lr=0.000227, gnorm=5.084, clip=0, train_wall=11, gb_free=67.7, wall=2927 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:46:22] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.79 GiB. GPU 2 has a total capacity of 79.14 GiB of which 59.25 MiB is free. Including non-PyTorch memory, this process has 79.06 GiB memory in use. Of the allocated memory 68.26 GiB is allocated by PyTorch, and 10.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:46:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:46:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:46:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 48           |        cudaMalloc retries: 69        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  65551 MiB |  69899 MiB | 589702 GiB | 589638 GiB |
|       from large pool |  65534 MiB |  69881 MiB | 587368 GiB | 587304 GiB |
|       from small pool |     17 MiB |     22 MiB |   2333 GiB |   2333 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  65551 MiB |  69899 MiB | 589702 GiB | 589638 GiB |
|       from large pool |  65534 MiB |  69881 MiB | 587368 GiB | 587304 GiB |
|       from small pool |     17 MiB |     22 MiB |   2333 GiB |   2333 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  65539 MiB |  69886 MiB | 588942 GiB | 588878 GiB |
|       from large pool |  65521 MiB |  69869 MiB | 586611 GiB | 586547 GiB |
|       from small pool |     17 MiB |     22 MiB |   2330 GiB |   2330 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80446 MiB |  80476 MiB | 553018 MiB | 472572 MiB |
|       from large pool |  80416 MiB |  80416 MiB | 548252 MiB | 467836 MiB |
|       from small pool |     30 MiB |     60 MiB |   4766 MiB |   4736 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  10802 MiB |  14997 MiB | 626948 GiB | 626937 GiB |
|       from large pool |  10789 MiB |  14985 MiB | 624357 GiB | 624346 GiB |
|       from small pool |     12 MiB |     22 MiB |   2591 GiB |   2591 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     489    |     498    |   26781 K  |   26781 K  |
|       from large pool |     201    |     210    |   13060 K  |   13060 K  |
|       from small pool |     288    |     342    |   13721 K  |   13720 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     489    |     498    |   26781 K  |   26781 K  |
|       from large pool |     201    |     210    |   13060 K  |   13060 K  |
|       from small pool |     288    |     342    |   13721 K  |   13720 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      74    |      89    |    4076    |    4002    |
|       from large pool |      59    |      59    |    1693    |    1634    |
|       from small pool |      15    |      30    |    2383    |    2368    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      85    |      86    |   14669 K  |   14669 K  |
|       from large pool |      55    |      56    |    8125 K  |    8125 K  |
|       from small pool |      30    |      48    |    6544 K  |    6544 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:46:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:46:22] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:46:25]    INFO >> epoch 009:    386 / 1539 loss=3.519, wps=2571.9, ups=3.82, wpb=672.9, bsz=672.9, num_updates=12650, lr=0.000227, gnorm=4.206, clip=0, train_wall=12, gb_free=58.8, wall=2940 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:46:36]    INFO >> epoch 009:    436 / 1539 loss=3.442, wps=3215.3, ups=4.62, wpb=695.7, bsz=695.7, num_updates=12700, lr=0.000227, gnorm=3.916, clip=0, train_wall=10, gb_free=61.1, wall=2951 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:46:47]    INFO >> epoch 009:    486 / 1539 loss=3.311, wps=3605, ups=5.23, wpb=689.1, bsz=689.1, num_updates=12750, lr=0.000227, gnorm=3.595, clip=0, train_wall=9, gb_free=68, wall=2961 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:46:59]    INFO >> epoch 009:    536 / 1539 loss=3.302, wps=3523, ups=4.25, wpb=828.7, bsz=828.7, num_updates=12800, lr=0.000227, gnorm=4.304, clip=0, train_wall=11, gb_free=63.2, wall=2973 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:47:08] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.77 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.28 GiB is free. Including non-PyTorch memory, this process has 77.83 GiB memory in use. Of the allocated memory 75.97 GiB is allocated by PyTorch, and 1.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:47:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:47:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:47:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 49           |        cudaMalloc retries: 70        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77200 MiB |  78277 MiB | 598472 GiB | 598396 GiB |
|       from large pool |  77178 MiB |  78255 MiB | 596107 GiB | 596031 GiB |
|       from small pool |     22 MiB |     23 MiB |   2365 GiB |   2365 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77200 MiB |  78277 MiB | 598472 GiB | 598396 GiB |
|       from large pool |  77178 MiB |  78255 MiB | 596107 GiB | 596031 GiB |
|       from small pool |     22 MiB |     23 MiB |   2365 GiB |   2365 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77192 MiB |  78269 MiB | 597700 GiB | 597625 GiB |
|       from large pool |  77170 MiB |  78247 MiB | 595338 GiB | 595263 GiB |
|       from small pool |     22 MiB |     23 MiB |   2361 GiB |   2361 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79190 MiB |  79232 MiB | 555896 MiB | 476706 MiB |
|       from large pool |  79160 MiB |  79160 MiB | 551088 MiB | 471928 MiB |
|       from small pool |     30 MiB |     72 MiB |   4808 MiB |   4778 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1989 MiB |   9400 MiB | 637234 GiB | 637232 GiB |
|       from large pool |   1981 MiB |   9391 MiB | 634608 GiB | 634606 GiB |
|       from small pool |      7 MiB |     25 MiB |   2626 GiB |   2626 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     566    |     574    |   27165 K  |   27165 K  |
|       from large pool |     270    |     278    |   13263 K  |   13263 K  |
|       from small pool |     296    |     356    |   13902 K  |   13901 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     566    |     574    |   27165 K  |   27165 K  |
|       from large pool |     270    |     278    |   13263 K  |   13263 K  |
|       from small pool |     296    |     356    |   13902 K  |   13901 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      74    |      95    |    4098    |    4024    |
|       from large pool |      59    |      59    |    1694    |    1635    |
|       from small pool |      15    |      36    |    2404    |    2389    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      95    |      97    |   14871 K  |   14870 K  |
|       from large pool |      65    |      67    |    8246 K  |    8246 K  |
|       from small pool |      30    |      53    |    6624 K  |    6624 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:47:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:47:08] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:47:10]    INFO >> epoch 009:    587 / 1539 loss=3.534, wps=2837.3, ups=4.38, wpb=648.3, bsz=648.3, num_updates=12850, lr=0.000227, gnorm=3.949, clip=0, train_wall=10, gb_free=69.1, wall=2984 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:47:23]    INFO >> epoch 009:    637 / 1539 loss=3.342, wps=3355.7, ups=4.56, wpb=735.3, bsz=735.3, num_updates=12900, lr=0.000227, gnorm=4.289, clip=0, train_wall=10, gb_free=43.7, wall=2995 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:47:33]    INFO >> epoch 009:    687 / 1539 loss=3.441, wps=3299.6, ups=4.79, wpb=689.1, bsz=689.1, num_updates=12950, lr=0.000227, gnorm=4.364, clip=0, train_wall=10, gb_free=70.4, wall=3005 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:47:43]    INFO >> epoch 009:    737 / 1539 loss=3.42, wps=2888.4, ups=4.79, wpb=602.9, bsz=602.9, num_updates=13000, lr=0.000227, gnorm=3.805, clip=0, train_wall=10, gb_free=59.2, wall=3016 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:47:56]    INFO >> epoch 009:    787 / 1539 loss=3.42, wps=3582.3, ups=4.57, wpb=783.9, bsz=783.9, num_updates=13050, lr=0.000227, gnorm=4.525, clip=0, train_wall=10, gb_free=65.5, wall=3027 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:48:03] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.28 GiB is free. Including non-PyTorch memory, this process has 77.83 GiB memory in use. Of the allocated memory 76.06 GiB is allocated by PyTorch, and 1.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:48:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:48:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:48:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 50           |        cudaMalloc retries: 71        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77288 MiB |  77887 MiB | 608792 GiB | 608717 GiB |
|       from large pool |  77272 MiB |  77871 MiB | 606389 GiB | 606314 GiB |
|       from small pool |     16 MiB |     24 MiB |   2403 GiB |   2402 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77288 MiB |  77887 MiB | 608792 GiB | 608717 GiB |
|       from large pool |  77272 MiB |  77871 MiB | 606389 GiB | 606314 GiB |
|       from small pool |     16 MiB |     24 MiB |   2403 GiB |   2402 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77280 MiB |  77879 MiB | 608008 GiB | 607933 GiB |
|       from large pool |  77263 MiB |  77862 MiB | 605608 GiB | 605533 GiB |
|       from small pool |     16 MiB |     24 MiB |   2399 GiB |   2399 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79192 MiB |  79242 MiB | 555948 MiB | 476756 MiB |
|       from large pool |  79160 MiB |  79160 MiB | 551088 MiB | 471928 MiB |
|       from small pool |     32 MiB |     82 MiB |   4860 MiB |   4828 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1903 MiB |   5528 MiB | 649374 GiB | 649372 GiB |
|       from large pool |   1887 MiB |   5511 MiB | 646705 GiB | 646703 GiB |
|       from small pool |     15 MiB |     22 MiB |   2668 GiB |   2668 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     528    |     535    |   27625 K  |   27624 K  |
|       from large pool |     239    |     246    |   13507 K  |   13506 K  |
|       from small pool |     289    |     348    |   14118 K  |   14117 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     528    |     535    |   27625 K  |   27624 K  |
|       from large pool |     239    |     246    |   13507 K  |   13506 K  |
|       from small pool |     289    |     348    |   14118 K  |   14117 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      75    |     100    |    4124    |    4049    |
|       from large pool |      59    |      59    |    1694    |    1635    |
|       from small pool |      16    |      41    |    2430    |    2414    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      89    |      90    |   15109 K  |   15109 K  |
|       from large pool |      60    |      61    |    8391 K  |    8391 K  |
|       from small pool |      29    |      52    |    6718 K  |    6718 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:48:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:48:03] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:48:06]    INFO >> epoch 009:    838 / 1539 loss=3.386, wps=3086.8, ups=4.88, wpb=633, bsz=633, num_updates=13100, lr=0.000227, gnorm=3.592, clip=0, train_wall=9, gb_free=71.1, wall=3037 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:48:16]    INFO >> epoch 009:    888 / 1539 loss=3.464, wps=3306.7, ups=4.88, wpb=678.2, bsz=678.2, num_updates=13150, lr=0.000227, gnorm=3.663, clip=0, train_wall=10, gb_free=73.4, wall=3047 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:48:27]    INFO >> epoch 009:    938 / 1539 loss=3.496, wps=3251, ups=5.26, wpb=618.1, bsz=618.1, num_updates=13200, lr=0.000227, gnorm=3.615, clip=0, train_wall=9, gb_free=69.3, wall=3057 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:48:28] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.28 GiB is free. Including non-PyTorch memory, this process has 77.83 GiB memory in use. Of the allocated memory 73.70 GiB is allocated by PyTorch, and 3.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:48:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:48:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:48:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 51           |        cudaMalloc retries: 72        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75037 MiB |  75468 MiB | 613412 GiB | 613338 GiB |
|       from large pool |  75011 MiB |  75442 MiB | 610992 GiB | 610919 GiB |
|       from small pool |     25 MiB |     29 MiB |   2419 GiB |   2419 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75037 MiB |  75468 MiB | 613412 GiB | 613338 GiB |
|       from large pool |  75011 MiB |  75442 MiB | 610992 GiB | 610919 GiB |
|       from small pool |     25 MiB |     29 MiB |   2419 GiB |   2419 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75024 MiB |  75454 MiB | 612621 GiB | 612548 GiB |
|       from large pool |  74999 MiB |  75428 MiB | 610205 GiB | 610131 GiB |
|       from small pool |     25 MiB |     29 MiB |   2416 GiB |   2416 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79190 MiB |  79224 MiB | 555980 MiB | 476790 MiB |
|       from large pool |  79160 MiB |  79160 MiB | 551088 MiB | 471928 MiB |
|       from small pool |     30 MiB |     64 MiB |   4892 MiB |   4862 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4152 MiB |   8033 MiB | 654795 GiB | 654790 GiB |
|       from large pool |   4148 MiB |   8025 MiB | 652107 GiB | 652103 GiB |
|       from small pool |      4 MiB |     29 MiB |   2687 GiB |   2687 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     567    |     574    |   27839 K  |   27838 K  |
|       from large pool |     265    |     272    |   13623 K  |   13623 K  |
|       from small pool |     302    |     355    |   14216 K  |   14215 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     567    |     574    |   27839 K  |   27838 K  |
|       from large pool |     265    |     272    |   13623 K  |   13623 K  |
|       from small pool |     302    |     355    |   14216 K  |   14215 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      74    |      91    |    4140    |    4066    |
|       from large pool |      59    |      59    |    1694    |    1635    |
|       from small pool |      15    |      32    |    2446    |    2431    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      84    |      84    |   15219 K  |   15219 K  |
|       from large pool |      56    |      56    |    8461 K  |    8461 K  |
|       from small pool |      28    |      55    |    6758 K  |    6758 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:48:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:48:28] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:48:40]    INFO >> epoch 009:    989 / 1539 loss=3.293, wps=3305.3, ups=3.97, wpb=832.6, bsz=832.6, num_updates=13250, lr=0.000227, gnorm=4.277, clip=0, train_wall=11, gb_free=72.1, wall=3069 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:48:50]    INFO >> epoch 009:   1039 / 1539 loss=3.401, wps=3393.2, ups=4.71, wpb=720.7, bsz=720.7, num_updates=13300, lr=0.000227, gnorm=3.615, clip=0, train_wall=10, gb_free=68.7, wall=3080 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:49:03]    INFO >> epoch 009:   1089 / 1539 loss=3.359, wps=3089.1, ups=4.39, wpb=703.2, bsz=703.2, num_updates=13350, lr=0.000227, gnorm=3.794, clip=0, train_wall=11, gb_free=61.3, wall=3091 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:49:14]    INFO >> epoch 009:   1139 / 1539 loss=3.362, wps=3603, ups=4.67, wpb=770.8, bsz=770.8, num_updates=13400, lr=0.000227, gnorm=4.088, clip=0, train_wall=10, gb_free=71, wall=3102 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:49:24]    INFO >> epoch 009:   1189 / 1539 loss=3.434, wps=3174.8, ups=4.79, wpb=662.5, bsz=662.5, num_updates=13450, lr=0.000227, gnorm=3.943, clip=0, train_wall=10, gb_free=70.1, wall=3113 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:49:37]    INFO >> epoch 009:   1239 / 1539 loss=3.202, wps=3797.7, ups=4.44, wpb=856.2, bsz=856.2, num_updates=13500, lr=0.000227, gnorm=4.331, clip=0, train_wall=11, gb_free=63.8, wall=3124 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:49:48]    INFO >> epoch 009:   1289 / 1539 loss=3.46, wps=3560.9, ups=4.47, wpb=796.8, bsz=796.8, num_updates=13550, lr=0.000227, gnorm=4.422, clip=0, train_wall=11, gb_free=75.3, wall=3135 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:49:59]    INFO >> epoch 009:   1339 / 1539 loss=3.373, wps=3188.3, ups=4.6, wpb=692.4, bsz=692.4, num_updates=13600, lr=0.000227, gnorm=4.883, clip=0, train_wall=10, gb_free=70.1, wall=3146 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:50:11]    INFO >> epoch 009:   1389 / 1539 loss=3.28, wps=3563.3, ups=4.75, wpb=749.6, bsz=749.6, num_updates=13650, lr=0.000227, gnorm=3.568, clip=0, train_wall=10, gb_free=68.1, wall=3156 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:50:16] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 3.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.56 GiB is allocated by PyTorch, and 1.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:50:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:50:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:50:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 52           |        cudaMalloc retries: 74        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79360 MiB |  79420 MiB | 634136 GiB | 634058 GiB |
|       from large pool |  79176 MiB |  79236 MiB | 631631 GiB | 631554 GiB |
|       from small pool |    183 MiB |    185 MiB |   2504 GiB |   2504 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79360 MiB |  79420 MiB | 634136 GiB | 634058 GiB |
|       from large pool |  79176 MiB |  79236 MiB | 631631 GiB | 631554 GiB |
|       from small pool |    183 MiB |    185 MiB |   2504 GiB |   2504 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79334 MiB |  79394 MiB | 633319 GiB | 633241 GiB |
|       from large pool |  79151 MiB |  79211 MiB | 630818 GiB | 630740 GiB |
|       from small pool |    183 MiB |    184 MiB |   2501 GiB |   2500 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80502 MiB |  80504 MiB | 557340 MiB | 476838 MiB |
|       from large pool |  80300 MiB |  80300 MiB | 552228 MiB | 471928 MiB |
|       from small pool |    202 MiB |    248 MiB |   5112 MiB |   4910 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1081 MiB |   6749 MiB | 679371 GiB | 679370 GiB |
|       from large pool |   1063 MiB |   6745 MiB | 676588 GiB | 676587 GiB |
|       from small pool |     18 MiB |     29 MiB |   2783 GiB |   2783 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3424    |    3427    |   28822 K  |   28818 K  |
|       from large pool |     589    |     590    |   14110 K  |   14110 K  |
|       from small pool |    2835    |    2838    |   14711 K  |   14708 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3424    |    3427    |   28822 K  |   28818 K  |
|       from large pool |     589    |     590    |   14110 K  |   14110 K  |
|       from small pool |    2835    |    2838    |   14711 K  |   14708 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     179    |     201    |    4269    |    4090    |
|       from large pool |      78    |      78    |    1713    |    1635    |
|       from small pool |     101    |     124    |    2556    |    2455    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     244    |     245    |   15748 K  |   15748 K  |
|       from large pool |      65    |      66    |    8752 K  |    8752 K  |
|       from small pool |     179    |     180    |    6996 K  |    6996 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:50:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:50:16] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:50:22]    INFO >> epoch 009:   1440 / 1539 loss=3.347, wps=3046.8, ups=4.35, wpb=700, bsz=700, num_updates=13700, lr=0.000227, gnorm=4.146, clip=0, train_wall=10, gb_free=69, wall=3168 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:50:32]    INFO >> epoch 009:   1490 / 1539 loss=3.5, wps=3165.4, ups=5.34, wpb=592.3, bsz=592.3, num_updates=13750, lr=0.000227, gnorm=3.91, clip=0, train_wall=9, gb_free=69.5, wall=3177 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:50:43]    INFO >> epoch 009 | loss 3.393 | wps 3094.3 | ups 4.35 | wpb 711.1 | bsz 711.1 | num_updates 13799 | lr 0.000227 | gnorm 4.059 | clip 0.1 | train_wall 308 | gb_free 71.3 | wall 3187 (progress_bar.py:267, print())[0m
[33m[2025-11-21 01:50:43] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:51:04]    INFO >> epoch 009 | valid on 'valid' subset | loss 3.703 | wps 7102 | wpb 5412.5 | bsz 5412.5 | num_updates 13799 | best_loss 4.043 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 01:51:05]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 01:51:05]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/checkpoints/checkpoint_last.pt (epoch 9 @ 13799 updates, score 3.703) (writing took 0.027560 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 01:51:05] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:51:05]    INFO >> epoch 010:      1 / 1539 loss=3.459, wps=963.8, ups=1.56, wpb=616.7, bsz=616.7, num_updates=13800, lr=0.000193, gnorm=3.75, clip=0, train_wall=10, gb_free=63.9, wall=3209 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:51:09] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 235.25 MiB is free. Including non-PyTorch memory, this process has 78.89 GiB memory in use. Of the allocated memory 73.70 GiB is allocated by PyTorch, and 4.69 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:51:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:51:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:51:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 53           |        cudaMalloc retries: 75        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75041 MiB |  75471 MiB | 646461 GiB | 646387 GiB |
|       from large pool |  75015 MiB |  75446 MiB | 643898 GiB | 643825 GiB |
|       from small pool |     25 MiB |     26 MiB |   2562 GiB |   2562 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75041 MiB |  75471 MiB | 646461 GiB | 646387 GiB |
|       from large pool |  75015 MiB |  75446 MiB | 643898 GiB | 643825 GiB |
|       from small pool |     25 MiB |     26 MiB |   2562 GiB |   2562 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75024 MiB |  75454 MiB | 645630 GiB | 645557 GiB |
|       from large pool |  74999 MiB |  75428 MiB | 643072 GiB | 642998 GiB |
|       from small pool |     25 MiB |     26 MiB |   2558 GiB |   2558 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80270 MiB |  80442 MiB | 557340 MiB | 477070 MiB |
|       from large pool |  80240 MiB |  80240 MiB | 552228 MiB | 471988 MiB |
|       from small pool |     30 MiB |    202 MiB |   5112 MiB |   5082 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5228 MiB |   9019 MiB | 689286 GiB | 689281 GiB |
|       from large pool |   5224 MiB |   9012 MiB | 686443 GiB | 686438 GiB |
|       from small pool |      4 MiB |     22 MiB |   2843 GiB |   2843 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     567    |     574    |   29362 K  |   29361 K  |
|       from large pool |     265    |     272    |   14291 K  |   14291 K  |
|       from small pool |     302    |     342    |   15070 K  |   15070 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     567    |     574    |   29362 K  |   29361 K  |
|       from large pool |     265    |     272    |   14291 K  |   14291 K  |
|       from small pool |     302    |     342    |   15070 K  |   15070 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      92    |     178    |    4269    |    4177    |
|       from large pool |      77    |      77    |    1713    |    1636    |
|       from small pool |      15    |     101    |    2556    |    2541    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      91    |      91    |   16071 K  |   16071 K  |
|       from large pool |      60    |      60    |    8866 K  |    8866 K  |
|       from small pool |      31    |      52    |    7205 K  |    7205 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:51:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:51:09] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:51:17]    INFO >> epoch 010:     52 / 1539 loss=3.391, wps=3200.2, ups=4.69, wpb=681.6, bsz=681.6, num_updates=13850, lr=0.000193, gnorm=3.301, clip=0, train_wall=9, gb_free=71.1, wall=3220 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:51:27]    INFO >> epoch 010:    102 / 1539 loss=3.311, wps=3444.6, ups=4.99, wpb=690.5, bsz=690.5, num_updates=13900, lr=0.000193, gnorm=4.242, clip=0, train_wall=10, gb_free=68, wall=3230 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:51:37]    INFO >> epoch 010:    152 / 1539 loss=3.289, wps=3476.4, ups=5.13, wpb=678.2, bsz=678.2, num_updates=13950, lr=0.000193, gnorm=3.869, clip=0, train_wall=9, gb_free=60.4, wall=3240 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:51:50]    INFO >> epoch 010:    202 / 1539 loss=3.348, wps=3325.3, ups=4.19, wpb=793.6, bsz=793.6, num_updates=14000, lr=0.000193, gnorm=4.442, clip=0, train_wall=11, gb_free=68.7, wall=3252 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:52:02]    INFO >> epoch 010:    252 / 1539 loss=3.252, wps=3310.8, ups=4.29, wpb=772.3, bsz=772.3, num_updates=14050, lr=0.000193, gnorm=3.936, clip=0, train_wall=11, gb_free=62.7, wall=3263 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:52:11]    INFO >> epoch 010:    302 / 1539 loss=3.503, wps=3604.8, ups=5.1, wpb=706.8, bsz=706.8, num_updates=14100, lr=0.000193, gnorm=3.62, clip=0, train_wall=9, gb_free=67.9, wall=3273 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:52:16] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.11 GiB is free. Including non-PyTorch memory, this process has 78.01 GiB memory in use. Of the allocated memory 76.06 GiB is allocated by PyTorch, and 1.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:52:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:52:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:52:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 54           |        cudaMalloc retries: 76        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77289 MiB |  77889 MiB | 659624 GiB | 659548 GiB |
|       from large pool |  77273 MiB |  77872 MiB | 657012 GiB | 656936 GiB |
|       from small pool |     16 MiB |     19 MiB |   2612 GiB |   2612 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77289 MiB |  77889 MiB | 659624 GiB | 659548 GiB |
|       from large pool |  77273 MiB |  77872 MiB | 657012 GiB | 656936 GiB |
|       from small pool |     16 MiB |     19 MiB |   2612 GiB |   2612 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77280 MiB |  77879 MiB | 658777 GiB | 658701 GiB |
|       from large pool |  77263 MiB |  77862 MiB | 656168 GiB | 656093 GiB |
|       from small pool |     16 MiB |     19 MiB |   2608 GiB |   2608 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79368 MiB |  80466 MiB | 557536 MiB | 478168 MiB |
|       from large pool |  79340 MiB |  80240 MiB | 552228 MiB | 472888 MiB |
|       from small pool |     28 MiB |    226 MiB |   5308 MiB |   5280 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2018 MiB |   5703 MiB | 703979 GiB | 703977 GiB |
|       from large pool |   2006 MiB |   5691 MiB | 701080 GiB | 701078 GiB |
|       from small pool |     11 MiB |     18 MiB |   2899 GiB |   2899 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     528    |     535    |   29955 K  |   29955 K  |
|       from large pool |     239    |     246    |   14597 K  |   14597 K  |
|       from small pool |     289    |     342    |   15358 K  |   15357 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     528    |     535    |   29955 K  |   29955 K  |
|       from large pool |     239    |     246    |   14597 K  |   14597 K  |
|       from small pool |     289    |     342    |   15358 K  |   15357 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      76    |     190    |    4367    |    4291    |
|       from large pool |      62    |      77    |    1713    |    1651    |
|       from small pool |      14    |     113    |    2654    |    2640    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      86    |      86    |   16391 K  |   16391 K  |
|       from large pool |      57    |      57    |    9055 K  |    9055 K  |
|       from small pool |      29    |      53    |    7336 K  |    7336 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:52:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:52:16] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:52:24]    INFO >> epoch 010:    353 / 1539 loss=3.283, wps=3383.1, ups=4.35, wpb=777, bsz=777, num_updates=14150, lr=0.000193, gnorm=3.729, clip=0, train_wall=10, gb_free=72.5, wall=3285 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:52:34]    INFO >> epoch 010:    403 / 1539 loss=3.524, wps=3168.9, ups=5.03, wpb=630.2, bsz=630.2, num_updates=14200, lr=0.000193, gnorm=3.861, clip=0, train_wall=9, gb_free=69.1, wall=3294 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:52:44]    INFO >> epoch 010:    453 / 1539 loss=3.346, wps=3065.7, ups=5.01, wpb=612.4, bsz=612.4, num_updates=14250, lr=0.000193, gnorm=3.686, clip=0, train_wall=9, gb_free=67.6, wall=3304 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:52:56]    INFO >> epoch 010:    503 / 1539 loss=3.372, wps=3183.4, ups=4.8, wpb=662.8, bsz=662.8, num_updates=14300, lr=0.000193, gnorm=3.828, clip=0, train_wall=10, gb_free=72, wall=3315 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:53:05]    INFO >> epoch 010:    553 / 1539 loss=3.353, wps=3618.7, ups=5.22, wpb=693.7, bsz=693.7, num_updates=14350, lr=0.000193, gnorm=3.763, clip=0, train_wall=9, gb_free=70.5, wall=3324 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:53:16]    INFO >> epoch 010:    603 / 1539 loss=3.422, wps=3215.9, ups=4.82, wpb=667, bsz=667, num_updates=14400, lr=0.000193, gnorm=4.133, clip=0, train_wall=10, gb_free=67.2, wall=3335 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:53:27]    INFO >> epoch 010:    653 / 1539 loss=3.399, wps=3485.2, ups=5.15, wpb=676.2, bsz=676.2, num_updates=14450, lr=0.000193, gnorm=3.312, clip=0, train_wall=9, gb_free=71.2, wall=3345 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:53:37]    INFO >> epoch 010:    703 / 1539 loss=3.324, wps=3723, ups=4.95, wpb=751.6, bsz=751.6, num_updates=14500, lr=0.000193, gnorm=4.421, clip=0, train_wall=9, gb_free=65.3, wall=3355 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:53:38] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 5.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.44 GiB is allocated by PyTorch, and 1.17 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:53:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:53:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:53:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 55           |        cudaMalloc retries: 77        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79242 MiB |  79302 MiB | 675646 GiB | 675568 GiB |
|       from large pool |  79059 MiB |  79119 MiB | 672974 GiB | 672897 GiB |
|       from small pool |    182 MiB |    183 MiB |   2671 GiB |   2671 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79242 MiB |  79302 MiB | 675646 GiB | 675568 GiB |
|       from large pool |  79059 MiB |  79119 MiB | 672974 GiB | 672897 GiB |
|       from small pool |    182 MiB |    183 MiB |   2671 GiB |   2671 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79214 MiB |  79274 MiB | 674778 GiB | 674701 GiB |
|       from large pool |  79032 MiB |  79092 MiB | 672110 GiB | 672033 GiB |
|       from small pool |    181 MiB |    183 MiB |   2667 GiB |   2667 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80500 MiB |  80502 MiB | 558730 MiB | 478230 MiB |
|       from large pool |  80300 MiB |  80300 MiB | 553248 MiB | 472948 MiB |
|       from small pool |    200 MiB |    202 MiB |   5482 MiB |   5282 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1197 MiB |   6867 MiB | 722845 GiB | 722844 GiB |
|       from large pool |   1180 MiB |   6863 MiB | 719878 GiB | 719877 GiB |
|       from small pool |     17 MiB |     25 MiB |   2966 GiB |   2966 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3402    |    3405    |   30685 K  |   30681 K  |
|       from large pool |     587    |     588    |   14982 K  |   14982 K  |
|       from small pool |    2815    |    2818    |   15702 K  |   15699 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3402    |    3405    |   30685 K  |   30681 K  |
|       from large pool |     587    |     588    |   14982 K  |   14982 K  |
|       from small pool |    2815    |    2818    |   15702 K  |   15699 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     178    |     179    |    4471    |    4293    |
|       from large pool |      78    |      78    |    1730    |    1652    |
|       from small pool |     100    |     101    |    2741    |    2641    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     245    |     246    |   16775 K  |   16775 K  |
|       from large pool |      68    |      70    |    9286 K  |    9286 K  |
|       from small pool |     177    |     178    |    7489 K  |    7488 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:53:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:53:38] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:53:47]    INFO >> epoch 010:    754 / 1539 loss=3.456, wps=3149, ups=5, wpb=629.6, bsz=629.6, num_updates=14550, lr=0.000193, gnorm=3.24, clip=0, train_wall=9, gb_free=68, wall=3365 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:54:00]    INFO >> epoch 010:    804 / 1539 loss=3.437, wps=3428.7, ups=4.34, wpb=789.2, bsz=789.2, num_updates=14600, lr=0.000193, gnorm=4.094, clip=0, train_wall=11, gb_free=68, wall=3376 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:54:10]    INFO >> epoch 010:    854 / 1539 loss=3.383, wps=2974.1, ups=4.79, wpb=620.9, bsz=620.9, num_updates=14650, lr=0.000193, gnorm=3.076, clip=0, train_wall=10, gb_free=67.2, wall=3387 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:54:22]    INFO >> epoch 010:    904 / 1539 loss=3.206, wps=3853.7, ups=4.14, wpb=931.5, bsz=931.5, num_updates=14700, lr=0.000193, gnorm=5.087, clip=0, train_wall=11, gb_free=70.7, wall=3399 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:54:32]    INFO >> epoch 010:    954 / 1539 loss=3.487, wps=3448.1, ups=5.3, wpb=650.5, bsz=650.5, num_updates=14750, lr=0.000193, gnorm=3.628, clip=0, train_wall=9, gb_free=68.9, wall=3408 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:54:43]    INFO >> epoch 010:   1004 / 1539 loss=3.44, wps=3265.3, ups=5.03, wpb=649.8, bsz=649.8, num_updates=14800, lr=0.000193, gnorm=4.246, clip=0, train_wall=9, gb_free=66, wall=3418 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:54:54]    INFO >> epoch 010:   1054 / 1539 loss=3.39, wps=3463.2, ups=4.66, wpb=742.6, bsz=742.6, num_updates=14850, lr=0.000193, gnorm=4.517, clip=0, train_wall=10, gb_free=60.4, wall=3429 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:55:04]    INFO >> epoch 010:   1104 / 1539 loss=3.429, wps=3534.4, ups=4.85, wpb=728.5, bsz=728.5, num_updates=14900, lr=0.000193, gnorm=3.356, clip=0, train_wall=10, gb_free=66.6, wall=3439 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:55:16]    INFO >> epoch 010:   1154 / 1539 loss=3.362, wps=3102.2, ups=4.99, wpb=622, bsz=622, num_updates=14950, lr=0.000193, gnorm=3.454, clip=0, train_wall=9, gb_free=71.3, wall=3449 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:55:22] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.77 GiB. GPU 2 has a total capacity of 79.14 GiB of which 475.25 MiB is free. Including non-PyTorch memory, this process has 78.65 GiB memory in use. Of the allocated memory 75.98 GiB is allocated by PyTorch, and 2.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:55:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:55:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:55:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 56           |        cudaMalloc retries: 78        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77206 MiB |  78283 MiB | 695600 GiB | 695524 GiB |
|       from large pool |  77184 MiB |  78261 MiB | 692852 GiB | 692776 GiB |
|       from small pool |     22 MiB |     23 MiB |   2747 GiB |   2747 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77206 MiB |  78283 MiB | 695600 GiB | 695524 GiB |
|       from large pool |  77184 MiB |  78261 MiB | 692852 GiB | 692776 GiB |
|       from small pool |     22 MiB |     23 MiB |   2747 GiB |   2747 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77192 MiB |  78269 MiB | 694705 GiB | 694630 GiB |
|       from large pool |  77170 MiB |  78247 MiB | 691961 GiB | 691886 GiB |
|       from small pool |     22 MiB |     23 MiB |   2743 GiB |   2743 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80030 MiB |  80488 MiB | 558778 MiB | 478748 MiB |
|       from large pool |  80000 MiB |  80240 MiB | 553248 MiB | 473248 MiB |
|       from small pool |     30 MiB |    248 MiB |   5530 MiB |   5500 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2763 MiB |   8799 MiB | 745200 GiB | 745197 GiB |
|       from large pool |   2755 MiB |   8791 MiB | 742147 GiB | 742144 GiB |
|       from small pool |      7 MiB |     20 MiB |   3053 GiB |   3053 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     566    |     574    |   31600 K  |   31600 K  |
|       from large pool |     270    |     278    |   15455 K  |   15455 K  |
|       from small pool |     296    |     342    |   16145 K  |   16144 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     566    |     574    |   31600 K  |   31600 K  |
|       from large pool |     270    |     278    |   15455 K  |   15455 K  |
|       from small pool |     296    |     342    |   16145 K  |   16144 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      88    |     201    |    4495    |    4407    |
|       from large pool |      73    |      77    |    1730    |    1657    |
|       from small pool |      15    |     124    |    2765    |    2750    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      99    |      99    |   17269 K  |   17269 K  |
|       from large pool |      68    |      68    |    9579 K  |    9578 K  |
|       from small pool |      31    |      50    |    7690 K  |    7690 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:55:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:55:22] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:55:28]    INFO >> epoch 010:   1205 / 1539 loss=3.387, wps=3682.2, ups=3.97, wpb=927, bsz=927, num_updates=15000, lr=0.000193, gnorm=4.287, clip=0, train_wall=11, gb_free=63.6, wall=3462 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:55:39]    INFO >> epoch 010:   1255 / 1539 loss=3.426, wps=3134.9, ups=4.8, wpb=652.6, bsz=652.6, num_updates=15050, lr=0.000193, gnorm=4.411, clip=0, train_wall=10, gb_free=73, wall=3472 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:55:51]    INFO >> epoch 010:   1305 / 1539 loss=3.343, wps=3445.7, ups=4.52, wpb=762.6, bsz=762.6, num_updates=15100, lr=0.000193, gnorm=3.947, clip=0, train_wall=10, gb_free=72.1, wall=3483 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:56:01]    INFO >> epoch 010:   1355 / 1539 loss=3.463, wps=3453.1, ups=4.86, wpb=710.9, bsz=710.9, num_updates=15150, lr=0.000193, gnorm=3.562, clip=0, train_wall=10, gb_free=69.4, wall=3493 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:56:12]    INFO >> epoch 010:   1405 / 1539 loss=3.358, wps=3589.7, ups=4.75, wpb=755.9, bsz=755.9, num_updates=15200, lr=0.000193, gnorm=4.328, clip=0, train_wall=10, gb_free=72.6, wall=3504 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:56:23]    INFO >> epoch 010:   1455 / 1539 loss=3.472, wps=3113.8, ups=4.84, wpb=643.9, bsz=643.9, num_updates=15250, lr=0.000193, gnorm=3.306, clip=0, train_wall=10, gb_free=70.8, wall=3514 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:56:27] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.79 GiB. GPU 2 has a total capacity of 79.14 GiB of which 535.25 MiB is free. Including non-PyTorch memory, this process has 78.59 GiB memory in use. Of the allocated memory 68.26 GiB is allocated by PyTorch, and 9.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:56:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:56:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:56:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 57           |        cudaMalloc retries: 79        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  65551 MiB |  69899 MiB | 708173 GiB | 708109 GiB |
|       from large pool |  65534 MiB |  69881 MiB | 705377 GiB | 705313 GiB |
|       from small pool |     17 MiB |     29 MiB |   2795 GiB |   2795 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  65551 MiB |  69899 MiB | 708173 GiB | 708109 GiB |
|       from large pool |  65534 MiB |  69881 MiB | 705377 GiB | 705313 GiB |
|       from small pool |     17 MiB |     29 MiB |   2795 GiB |   2795 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  65539 MiB |  69886 MiB | 707261 GiB | 707197 GiB |
|       from large pool |  65521 MiB |  69869 MiB | 704469 GiB | 704405 GiB |
|       from small pool |     17 MiB |     29 MiB |   2791 GiB |   2791 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79970 MiB |  80150 MiB | 558958 MiB | 478988 MiB |
|       from large pool |  79940 MiB |  79940 MiB | 553248 MiB | 473308 MiB |
|       from small pool |     30 MiB |    210 MiB |   5710 MiB |   5680 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  10326 MiB |  14178 MiB | 759462 GiB | 759452 GiB |
|       from large pool |  10313 MiB |  14166 MiB | 756354 GiB | 756344 GiB |
|       from small pool |     12 MiB |     25 MiB |   3107 GiB |   3107 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     489    |     498    |   32176 K  |   32176 K  |
|       from large pool |     201    |     210    |   15752 K  |   15752 K  |
|       from small pool |     288    |     356    |   16424 K  |   16423 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     489    |     498    |   32176 K  |   32176 K  |
|       from large pool |     201    |     210    |   15752 K  |   15752 K  |
|       from small pool |     288    |     356    |   16424 K  |   16423 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      87    |     177    |    4585    |    4498    |
|       from large pool |      72    |      72    |    1730    |    1658    |
|       from small pool |      15    |     105    |    2855    |    2840    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      95    |      95    |   17579 K  |   17579 K  |
|       from large pool |      64    |      64    |    9761 K  |    9761 K  |
|       from small pool |      31    |      57    |    7818 K  |    7818 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:56:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:56:27] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:56:34]    INFO >> epoch 010:   1506 / 1539 loss=3.317, wps=3336.3, ups=4.87, wpb=685.6, bsz=685.6, num_updates=15300, lr=0.000193, gnorm=4.003, clip=0, train_wall=9, gb_free=64, wall=3525 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:56:41]    INFO >> epoch 010 | loss 3.377 | wps 3163.5 | ups 4.45 | wpb 711.1 | bsz 711.1 | num_updates 15333 | lr 0.000193 | gnorm 3.907 | clip 0 | train_wall 301 | gb_free 66.2 | wall 3532 (progress_bar.py:267, print())[0m
[33m[2025-11-21 01:56:41] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:57:04]    INFO >> epoch 010 | valid on 'valid' subset | loss 3.698 | wps 6978.8 | wpb 5412.5 | bsz 5412.5 | num_updates 15333 | best_loss 4.043 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 01:57:05]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 01:57:05]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/checkpoints/checkpoint_last.pt (epoch 10 @ 15333 updates, score 3.698) (writing took 0.018886 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 01:57:05] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:57:08]    INFO >> epoch 011:     17 / 1539 loss=3.333, wps=1070.1, ups=1.5, wpb=713.7, bsz=713.7, num_updates=15350, lr=0.000161, gnorm=4.348, clip=0, train_wall=11, gb_free=70.2, wall=3558 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:57:18]    INFO >> epoch 011:     67 / 1539 loss=3.414, wps=3446.2, ups=5.05, wpb=682, bsz=682, num_updates=15400, lr=0.000161, gnorm=3.583, clip=0, train_wall=9, gb_free=65.9, wall=3568 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:57:30]    INFO >> epoch 011:    117 / 1539 loss=3.439, wps=3440.8, ups=4.86, wpb=708.5, bsz=708.5, num_updates=15450, lr=0.000161, gnorm=4.035, clip=0, train_wall=10, gb_free=70.3, wall=3578 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:57:39]    INFO >> epoch 011:    167 / 1539 loss=3.323, wps=3612.2, ups=5.3, wpb=681.5, bsz=681.5, num_updates=15500, lr=0.000161, gnorm=3.57, clip=0, train_wall=9, gb_free=66.8, wall=3588 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:57:50]    INFO >> epoch 011:    217 / 1539 loss=3.392, wps=3724.5, ups=4.76, wpb=782.3, bsz=782.3, num_updates=15550, lr=0.000161, gnorm=3.808, clip=0, train_wall=10, gb_free=73.2, wall=3598 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:58:00]    INFO >> epoch 011:    267 / 1539 loss=3.296, wps=3561.5, ups=5.04, wpb=706.9, bsz=706.9, num_updates=15600, lr=0.000161, gnorm=4.211, clip=2, train_wall=9, gb_free=65, wall=3608 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:58:10]    INFO >> epoch 011:    317 / 1539 loss=3.358, wps=3401.1, ups=4.69, wpb=725.5, bsz=725.5, num_updates=15650, lr=0.000161, gnorm=3.437, clip=0, train_wall=10, gb_free=62.2, wall=3619 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:58:20]    INFO >> epoch 011:    367 / 1539 loss=3.347, wps=3352.9, ups=5.05, wpb=664, bsz=664, num_updates=15700, lr=0.000161, gnorm=3.458, clip=0, train_wall=9, gb_free=68.1, wall=3629 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:58:32]    INFO >> epoch 011:    417 / 1539 loss=3.4, wps=3461.5, ups=5.32, wpb=650.2, bsz=650.2, num_updates=15750, lr=0.000161, gnorm=4.27, clip=0, train_wall=9, gb_free=72.1, wall=3638 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:58:42]    INFO >> epoch 011:    467 / 1539 loss=3.529, wps=3263.4, ups=5.38, wpb=607, bsz=607, num_updates=15800, lr=0.000161, gnorm=3.375, clip=0, train_wall=9, gb_free=71.9, wall=3647 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:58:43] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.64 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.87 GiB is free. Including non-PyTorch memory, this process has 77.24 GiB memory in use. Of the allocated memory 72.14 GiB is allocated by PyTorch, and 4.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:58:43] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:58:43] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:58:43] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 58           |        cudaMalloc retries: 80        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69483 MiB |  76771 MiB | 736554 GiB | 736486 GiB |
|       from large pool |  69462 MiB |  76750 MiB | 733638 GiB | 733570 GiB |
|       from small pool |     20 MiB |     26 MiB |   2915 GiB |   2915 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69483 MiB |  76771 MiB | 736554 GiB | 736486 GiB |
|       from large pool |  69462 MiB |  76750 MiB | 733638 GiB | 733570 GiB |
|       from small pool |     20 MiB |     26 MiB |   2915 GiB |   2915 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  69460 MiB |  76747 MiB | 735607 GiB | 735539 GiB |
|       from large pool |  69439 MiB |  76727 MiB | 732696 GiB | 732628 GiB |
|       from small pool |     20 MiB |     25 MiB |   2911 GiB |   2911 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78586 MiB |  78638 MiB | 561718 MiB | 483132 MiB |
|       from large pool |  78556 MiB |  78556 MiB | 555956 MiB | 477400 MiB |
|       from small pool |     30 MiB |     82 MiB |   5762 MiB |   5732 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2302 MiB |   8557 MiB |    769 TiB |    769 TiB |
|       from large pool |   2293 MiB |   8546 MiB |    766 TiB |    766 TiB |
|       from small pool |      9 MiB |     24 MiB |      3 TiB |      3 TiB |
|---------------------------------------------------------------------------|
| Allocations           |     646    |     721    |   33467 K  |   33466 K  |
|       from large pool |     331    |     355    |   16326 K  |   16326 K  |
|       from small pool |     315    |     376    |   17140 K  |   17140 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     646    |     721    |   33467 K  |   33466 K  |
|       from large pool |     331    |     355    |   16326 K  |   16326 K  |
|       from small pool |     315    |     376    |   17140 K  |   17140 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      87    |     113    |    4612    |    4525    |
|       from large pool |      72    |      72    |    1731    |    1659    |
|       from small pool |      15    |      41    |    2881    |    2866    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     104    |     115    |   18291 K  |   18291 K  |
|       from large pool |      74    |      78    |   10116 K  |   10116 K  |
|       from small pool |      30    |      57    |    8175 K  |    8175 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:58:43] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:58:43] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:58:53]    INFO >> epoch 011:    518 / 1539 loss=3.511, wps=2760.4, ups=4.34, wpb=636.4, bsz=636.4, num_updates=15850, lr=0.000161, gnorm=3.44, clip=0, train_wall=10, gb_free=72.5, wall=3659 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:59:02] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 27.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.57 GiB is allocated by PyTorch, and 1.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:59:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:59:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:59:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 59           |        cudaMalloc retries: 81        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79374 MiB |  79434 MiB | 740204 GiB | 740126 GiB |
|       from large pool |  79191 MiB |  79251 MiB | 737275 GiB | 737198 GiB |
|       from small pool |    183 MiB |    184 MiB |   2928 GiB |   2928 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79374 MiB |  79434 MiB | 740204 GiB | 740126 GiB |
|       from large pool |  79191 MiB |  79251 MiB | 737275 GiB | 737198 GiB |
|       from small pool |    183 MiB |    184 MiB |   2928 GiB |   2928 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79274 MiB |  79334 MiB | 739252 GiB | 739175 GiB |
|       from large pool |  79092 MiB |  79151 MiB | 736328 GiB | 736251 GiB |
|       from small pool |    182 MiB |    183 MiB |   2924 GiB |   2924 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80478 MiB |  80478 MiB | 570410 MiB | 489932 MiB |
|       from large pool |  80276 MiB |  80276 MiB | 564476 MiB | 484200 MiB |
|       from small pool |    202 MiB |    202 MiB |   5934 MiB |   5732 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1043 MiB |   7522 MiB |    773 TiB |    773 TiB |
|       from large pool |   1024 MiB |   7519 MiB |    770 TiB |    770 TiB |
|       from small pool |     18 MiB |     20 MiB |      3 TiB |      3 TiB |
|---------------------------------------------------------------------------|
| Allocations           |    3413    |    3416    |   33629 K  |   33626 K  |
|       from large pool |     588    |     589    |   16410 K  |   16410 K  |
|       from small pool |    2825    |    2828    |   17219 K  |   17216 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3413    |    3416    |   33629 K  |   33626 K  |
|       from large pool |     588    |     589    |   16410 K  |   16410 K  |
|       from small pool |    2825    |    2828    |   17219 K  |   17216 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     313    |     313    |    4840    |    4527    |
|       from large pool |     212    |     212    |    1873    |    1661    |
|       from small pool |     101    |     101    |    2967    |    2866    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     239    |     239    |   18378 K  |   18378 K  |
|       from large pool |      64    |      64    |   10168 K  |   10167 K  |
|       from small pool |     175    |     175    |    8210 K  |    8210 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:59:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:59:02] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:59:05]    INFO >> epoch 011:    569 / 1539 loss=3.323, wps=2944.4, ups=4.76, wpb=618.5, bsz=618.5, num_updates=15900, lr=0.000161, gnorm=4.226, clip=0, train_wall=9, gb_free=68.9, wall=3669 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:59:16]    INFO >> epoch 011:    619 / 1539 loss=3.317, wps=3813.7, ups=4.51, wpb=845.6, bsz=845.6, num_updates=15950, lr=0.000161, gnorm=3.885, clip=0, train_wall=11, gb_free=68.4, wall=3680 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:59:26]    INFO >> epoch 011:    669 / 1539 loss=3.382, wps=3388.7, ups=4.92, wpb=689.1, bsz=689.1, num_updates=16000, lr=0.000161, gnorm=4.017, clip=0, train_wall=10, gb_free=63, wall=3691 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:59:38]    INFO >> epoch 011:    719 / 1539 loss=3.406, wps=3321, ups=4.64, wpb=716, bsz=716, num_updates=16050, lr=0.000161, gnorm=3.004, clip=0, train_wall=10, gb_free=55.6, wall=3701 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:59:49]    INFO >> epoch 011:    769 / 1539 loss=3.391, wps=3069.5, ups=4.69, wpb=654.3, bsz=654.3, num_updates=16100, lr=0.000161, gnorm=3.609, clip=0, train_wall=10, gb_free=68, wall=3712 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:59:54] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.79 GiB. GPU 2 has a total capacity of 79.14 GiB of which 2.76 GiB is free. Including non-PyTorch memory, this process has 76.35 GiB memory in use. Of the allocated memory 68.26 GiB is allocated by PyTorch, and 7.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:59:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:59:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:59:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 60           |        cudaMalloc retries: 83        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  65552 MiB |  69900 MiB | 750267 GiB | 750203 GiB |
|       from large pool |  65534 MiB |  69882 MiB | 747301 GiB | 747237 GiB |
|       from small pool |     17 MiB |     27 MiB |   2966 GiB |   2966 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  65552 MiB |  69900 MiB | 750267 GiB | 750203 GiB |
|       from large pool |  65534 MiB |  69882 MiB | 747301 GiB | 747237 GiB |
|       from small pool |     17 MiB |     27 MiB |   2966 GiB |   2966 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  65539 MiB |  69886 MiB | 749302 GiB | 749238 GiB |
|       from large pool |  65521 MiB |  69869 MiB | 746339 GiB | 746275 GiB |
|       from small pool |     17 MiB |     27 MiB |   2962 GiB |   2962 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77674 MiB |  80464 MiB | 576160 MiB | 498486 MiB |
|       from large pool |  77640 MiB |  80216 MiB | 570180 MiB | 492540 MiB |
|       from small pool |     34 MiB |    248 MiB |   5980 MiB |   5946 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   9209 MiB |  14242 MiB |    783 TiB |    783 TiB |
|       from large pool |   9193 MiB |  14226 MiB |    780 TiB |    780 TiB |
|       from small pool |     16 MiB |     29 MiB |      3 TiB |      3 TiB |
|---------------------------------------------------------------------------|
| Allocations           |     489    |     498    |   34088 K  |   34087 K  |
|       from large pool |     201    |     210    |   16646 K  |   16646 K  |
|       from small pool |     288    |     356    |   17441 K  |   17441 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     489    |     498    |   34088 K  |   34087 K  |
|       from large pool |     201    |     210    |   16646 K  |   16646 K  |
|       from small pool |     288    |     356    |   17441 K  |   17441 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      92    |     335    |    4865    |    4773    |
|       from large pool |      75    |     211    |    1875    |    1800    |
|       from small pool |      17    |     124    |    2990    |    2973    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      97    |      98    |   18635 K  |   18634 K  |
|       from large pool |      64    |      65    |   10322 K  |   10322 K  |
|       from small pool |      33    |      57    |    8312 K  |    8312 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:59:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:59:54] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:00:00]    INFO >> epoch 011:    820 / 1539 loss=3.352, wps=2926.8, ups=4.7, wpb=622.1, bsz=622.1, num_updates=16150, lr=0.000161, gnorm=3.864, clip=0, train_wall=9, gb_free=64.5, wall=3723 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:00:11]    INFO >> epoch 011:    870 / 1539 loss=3.361, wps=3457.3, ups=5, wpb=691, bsz=691, num_updates=16200, lr=0.000161, gnorm=4.628, clip=0, train_wall=9, gb_free=71, wall=3733 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:00:21]    INFO >> epoch 011:    920 / 1539 loss=3.434, wps=3807.9, ups=4.97, wpb=766.7, bsz=766.7, num_updates=16250, lr=0.000161, gnorm=3.369, clip=0, train_wall=9, gb_free=68.3, wall=3743 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:00:32]    INFO >> epoch 011:    970 / 1539 loss=3.143, wps=3394.9, ups=4.54, wpb=747, bsz=747, num_updates=16300, lr=0.000161, gnorm=3.776, clip=0, train_wall=10, gb_free=73.5, wall=3754 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:00:44]    INFO >> epoch 011:   1020 / 1539 loss=3.395, wps=3355.4, ups=4.9, wpb=685.3, bsz=685.3, num_updates=16350, lr=0.000161, gnorm=3.692, clip=0, train_wall=10, gb_free=69.8, wall=3764 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:00:56]    INFO >> epoch 011:   1070 / 1539 loss=3.163, wps=3544.1, ups=4.26, wpb=832.4, bsz=832.4, num_updates=16400, lr=0.000161, gnorm=3.365, clip=0, train_wall=11, gb_free=59.9, wall=3776 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:00:58] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.77 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.10 GiB is free. Including non-PyTorch memory, this process has 78.02 GiB memory in use. Of the allocated memory 75.98 GiB is allocated by PyTorch, and 1.54 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:00:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:00:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:00:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 61           |        cudaMalloc retries: 84        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77205 MiB |  78281 MiB | 762630 GiB | 762554 GiB |
|       from large pool |  77183 MiB |  78259 MiB | 759617 GiB | 759541 GiB |
|       from small pool |     22 MiB |     23 MiB |   3012 GiB |   3012 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77205 MiB |  78281 MiB | 762630 GiB | 762554 GiB |
|       from large pool |  77183 MiB |  78259 MiB | 759617 GiB | 759541 GiB |
|       from small pool |     22 MiB |     23 MiB |   3012 GiB |   3012 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77192 MiB |  78269 MiB | 761648 GiB | 761572 GiB |
|       from large pool |  77170 MiB |  78247 MiB | 758639 GiB | 758564 GiB |
|       from small pool |     22 MiB |     23 MiB |   3008 GiB |   3008 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79380 MiB |  79602 MiB | 581000 MiB | 501620 MiB |
|       from large pool |  79348 MiB |  79468 MiB | 574920 MiB | 495572 MiB |
|       from small pool |     32 MiB |    134 MiB |   6080 MiB |   6048 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2114 MiB |   9689 MiB |    796 TiB |    796 TiB |
|       from large pool |   2104 MiB |   9678 MiB |    793 TiB |    793 TiB |
|       from small pool |      9 MiB |     22 MiB |      3 TiB |      3 TiB |
|---------------------------------------------------------------------------|
| Allocations           |     566    |     574    |   34645 K  |   34645 K  |
|       from large pool |     270    |     278    |   16937 K  |   16937 K  |
|       from small pool |     296    |     342    |   17708 K  |   17708 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     566    |     574    |   34645 K  |   34645 K  |
|       from large pool |     270    |     278    |   16937 K  |   16937 K  |
|       from small pool |     296    |     342    |   17708 K  |   17708 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      92    |     145    |    4920    |    4828    |
|       from large pool |      76    |      78    |    1880    |    1804    |
|       from small pool |      16    |      67    |    3040    |    3024    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     105    |     105    |   18933 K  |   18933 K  |
|       from large pool |      75    |      75    |   10501 K  |   10501 K  |
|       from small pool |      30    |      57    |    8431 K  |    8431 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:00:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:00:58] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:01:07]    INFO >> epoch 011:   1121 / 1539 loss=3.319, wps=3432, ups=4.44, wpb=772.4, bsz=772.4, num_updates=16450, lr=0.000161, gnorm=3.941, clip=0, train_wall=10, gb_free=63.6, wall=3787 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:01:19]    INFO >> epoch 011:   1171 / 1539 loss=3.199, wps=3548.5, ups=4.64, wpb=764.3, bsz=764.3, num_updates=16500, lr=0.000161, gnorm=4.371, clip=0, train_wall=10, gb_free=69.6, wall=3798 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:01:29]    INFO >> epoch 011:   1221 / 1539 loss=3.254, wps=3381.5, ups=4.93, wpb=686.3, bsz=686.3, num_updates=16550, lr=0.000161, gnorm=3.594, clip=0, train_wall=10, gb_free=69.6, wall=3808 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:01:30] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.16 GiB is free. Including non-PyTorch memory, this process has 77.96 GiB memory in use. Of the allocated memory 73.70 GiB is allocated by PyTorch, and 3.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:01:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:01:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:01:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 62           |        cudaMalloc retries: 85        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75036 MiB |  75467 MiB | 769040 GiB | 768966 GiB |
|       from large pool |  75010 MiB |  75441 MiB | 766001 GiB | 765928 GiB |
|       from small pool |     25 MiB |     26 MiB |   3038 GiB |   3038 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75036 MiB |  75467 MiB | 769040 GiB | 768966 GiB |
|       from large pool |  75010 MiB |  75441 MiB | 766001 GiB | 765928 GiB |
|       from small pool |     25 MiB |     26 MiB |   3038 GiB |   3038 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75024 MiB |  75454 MiB | 768049 GiB | 767976 GiB |
|       from large pool |  74999 MiB |  75428 MiB | 765015 GiB | 764942 GiB |
|       from small pool |     25 MiB |     26 MiB |   3034 GiB |   3034 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79320 MiB |  79498 MiB | 581178 MiB | 501858 MiB |
|       from large pool |  79288 MiB |  79288 MiB | 574920 MiB | 495632 MiB |
|       from small pool |     32 MiB |    210 MiB |   6258 MiB |   6226 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4283 MiB |  10429 MiB |    804 TiB |    804 TiB |
|       from large pool |   4277 MiB |  10419 MiB |    800 TiB |    800 TiB |
|       from small pool |      6 MiB |     22 MiB |      3 TiB |      3 TiB |
|---------------------------------------------------------------------------|
| Allocations           |     567    |     574    |   34944 K  |   34943 K  |
|       from large pool |     265    |     272    |   17086 K  |   17086 K  |
|       from small pool |     302    |     342    |   17857 K  |   17857 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     567    |     574    |   34944 K  |   34943 K  |
|       from large pool |     265    |     272    |   17086 K  |   17086 K  |
|       from small pool |     302    |     342    |   17857 K  |   17857 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      91    |     180    |    5009    |    4918    |
|       from large pool |      75    |      75    |    1880    |    1805    |
|       from small pool |      16    |     105    |    3129    |    3113    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      91    |      91    |   19095 K  |   19095 K  |
|       from large pool |      61    |      61    |   10593 K  |   10593 K  |
|       from small pool |      30    |      49    |    8502 K  |    8502 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:01:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:01:30] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:01:39]    INFO >> epoch 011:   1272 / 1539 loss=3.445, wps=3672.4, ups=4.82, wpb=762.2, bsz=762.2, num_updates=16600, lr=0.000161, gnorm=3.929, clip=0, train_wall=9, gb_free=72.6, wall=3818 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:01:48] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.68 GiB is free. Including non-PyTorch memory, this process has 77.43 GiB memory in use. Of the allocated memory 72.07 GiB is allocated by PyTorch, and 4.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:01:48] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:01:48] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:01:48] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 63           |        cudaMalloc retries: 86        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73200 MiB |  74276 MiB | 772607 GiB | 772535 GiB |
|       from large pool |  73183 MiB |  74260 MiB | 769555 GiB | 769483 GiB |
|       from small pool |     16 MiB |     28 MiB |   3052 GiB |   3052 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73200 MiB |  74276 MiB | 772607 GiB | 772535 GiB |
|       from large pool |  73183 MiB |  74260 MiB | 769555 GiB | 769483 GiB |
|       from small pool |     16 MiB |     28 MiB |   3052 GiB |   3052 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73189 MiB |  74265 MiB | 771612 GiB | 771541 GiB |
|       from large pool |  73173 MiB |  74248 MiB | 768564 GiB | 768493 GiB |
|       from small pool |     16 MiB |     28 MiB |   3047 GiB |   3047 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78782 MiB |  79372 MiB | 581230 MiB | 502448 MiB |
|       from large pool |  78748 MiB |  79288 MiB | 574920 MiB | 496172 MiB |
|       from small pool |     34 MiB |     84 MiB |   6310 MiB |   6276 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5521 MiB |   6017 MiB |    807 TiB |    807 TiB |
|       from large pool |   5504 MiB |   5998 MiB |    804 TiB |    804 TiB |
|       from small pool |     17 MiB |     29 MiB |      3 TiB |      3 TiB |
|---------------------------------------------------------------------------|
| Allocations           |     527    |     535    |   35104 K  |   35103 K  |
|       from large pool |     238    |     246    |   17170 K  |   17170 K  |
|       from small pool |     289    |     356    |   17934 K  |   17933 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     527    |     535    |   35104 K  |   35103 K  |
|       from large pool |     238    |     246    |   17170 K  |   17170 K  |
|       from small pool |     289    |     356    |   17934 K  |   17933 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      83    |     117    |    5035    |    4952    |
|       from large pool |      66    |      75    |    1880    |    1814    |
|       from small pool |      17    |      42    |    3155    |    3138    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      88    |      89    |   19180 K  |   19180 K  |
|       from large pool |      57    |      58    |   10644 K  |   10644 K  |
|       from small pool |      31    |      64    |    8536 K  |    8536 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:01:48] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:01:48] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:01:52]    INFO >> epoch 011:   1323 / 1539 loss=3.249, wps=3157.5, ups=4.56, wpb=692.5, bsz=692.5, num_updates=16650, lr=0.000161, gnorm=3.938, clip=0, train_wall=10, gb_free=55.4, wall=3829 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:02:02]    INFO >> epoch 011:   1373 / 1539 loss=3.439, wps=3445.5, ups=4.81, wpb=716.2, bsz=716.2, num_updates=16700, lr=0.000161, gnorm=3.849, clip=0, train_wall=10, gb_free=66.7, wall=3840 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:02:13]    INFO >> epoch 011:   1423 / 1539 loss=3.487, wps=3653.8, ups=4.63, wpb=789.8, bsz=789.8, num_updates=16750, lr=0.000161, gnorm=4.551, clip=0, train_wall=10, gb_free=63.2, wall=3850 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:02:25]    INFO >> epoch 011:   1473 / 1539 loss=3.259, wps=3693.7, ups=4.8, wpb=770.2, bsz=770.2, num_updates=16800, lr=0.000161, gnorm=4.144, clip=0, train_wall=10, gb_free=65.7, wall=3861 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:02:36]    INFO >> epoch 011:   1523 / 1539 loss=3.364, wps=3337, ups=4.56, wpb=731.3, bsz=731.3, num_updates=16850, lr=0.000161, gnorm=3.731, clip=0, train_wall=10, gb_free=69.5, wall=3872 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:02:39]    INFO >> epoch 011 | loss 3.354 | wps 3181.2 | ups 4.47 | wpb 711 | bsz 711 | num_updates 16866 | lr 0.000161 | gnorm 3.815 | clip 0.1 | train_wall 299 | gb_free 70.9 | wall 3875 (progress_bar.py:267, print())[0m
[33m[2025-11-21 02:02:39] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 02:03:01]    INFO >> epoch 011 | valid on 'valid' subset | loss 3.71 | wps 7130.6 | wpb 5412.5 | bsz 5412.5 | num_updates 16866 | best_loss 4.043 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 02:03:02]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 02:03:02]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/checkpoints/checkpoint_last.pt (epoch 11 @ 16866 updates, score 3.71) (writing took 0.031879 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[32m[2025-11-21 02:03:02]    INFO >> æ—©åœ: éªŒè¯æ€§èƒ½å·²10è½®æœªæå‡ (train_enhanced.py:616, single_main())[0m
[32m[2025-11-21 02:03:02]    INFO >> è®­ç»ƒå®Œæˆï¼Œç”¨æ—¶ 3821.2 ç§’ (train_enhanced.py:626, single_main())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 02:03:02]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 02:03:02]    INFO >> æ‰€æœ‰æ—¥å¿—å·²ä¿å­˜åˆ°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs (train_enhanced.py:631, single_main())[0m
[32m[2025-11-21 02:03:02]    INFO >> 
================================================================================ (train_enhanced.py:634, single_main())[0m
[32m[2025-11-21 02:03:02]    INFO >> å¼€å§‹æµ‹è¯•... (train_enhanced.py:635, single_main())[0m
[32m[2025-11-21 02:03:02]    INFO >> ================================================================================ (train_enhanced.py:636, single_main())[0m
[32m[2025-11-21 02:03:02]    INFO >> åŠ è½½æœ€ä½³checkpoint: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/checkpoints/checkpoint_best.pt (train_enhanced.py:50, run_test_after_training())[0m
[32m[2025-11-21 02:03:02]    INFO >> æµ‹è¯•é›†: test (train_enhanced.py:51, run_test_after_training())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/checkpoint_utils.py:212: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(
[32m[2025-11-21 02:04:06]    INFO >> 
================================================================================ (train_enhanced.py:168, run_test_after_training())[0m
[32m[2025-11-21 02:04:06]    INFO >> æµ‹è¯•ç»“æžœ: (train_enhanced.py:169, run_test_after_training())[0m
[32m[2025-11-21 02:04:06]    INFO >> -------------------------------------------------------------------------------- (train_enhanced.py:170, run_test_after_training())[0m
[32m[2025-11-21 02:04:06]    INFO >> å¹³å‡Loss:      4.0728 (train_enhanced.py:171, run_test_after_training())[0m
[32m[2025-11-21 02:04:06]    INFO >> Acc@1:         20.80% (train_enhanced.py:172, run_test_after_training())[0m
[32m[2025-11-21 02:04:06]    INFO >> Acc@5:         41.85% (train_enhanced.py:173, run_test_after_training())[0m
[32m[2025-11-21 02:04:06]    INFO >> Acc@1 (å«any): 20.80% (train_enhanced.py:174, run_test_after_training())[0m
[32m[2025-11-21 02:04:06]    INFO >> Acc@5 (å«any): 41.85% (train_enhanced.py:175, run_test_after_training())[0m
[32m[2025-11-21 02:04:06]    INFO >> ================================================================================ (train_enhanced.py:176, run_test_after_training())[0m
[32m[2025-11-21 02:04:06]    INFO >> æµ‹è¯•ç»“æžœå·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/checkpoints/res.txt (train_enhanced.py:187, run_test_after_training())[0m
[32m[2025-11-21 02:04:06]    INFO >> è®­ç»ƒæ—¥å¿—å·²æ›´æ–°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs (train_enhanced.py:222, run_test_after_training())[0m
[TrainingLogger] æ—¥å¿—ç›®å½•: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs
[TrainingLogger] åŽŸå§‹è¾“å‡ºå°†ä¿å­˜åˆ°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/training_output.log
[TrainingLogger] Epoch 1 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/metrics.json
[TrainingLogger] Epoch 2 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/metrics.json
[TrainingLogger] Epoch 3 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/metrics.json
[TrainingLogger] Epoch 4 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/metrics.json
[TrainingLogger] Epoch 5 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/metrics.json
[TrainingLogger] Epoch 6 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/metrics.json
[TrainingLogger] Epoch 7 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/metrics.json
[TrainingLogger] Epoch 8 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/metrics.json
[TrainingLogger] Epoch 9 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/metrics.json
[TrainingLogger] Epoch 10 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/metrics.json
[TrainingLogger] Epoch 11 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/metrics.json

âœ“ embed_96 æˆåŠŸ

ç­‰å¾…3ç§’...

è¿›åº¦: 3/16

============================================================
å®žéªŒ: embed_128 - åµŒå…¥ç»´åº¦128 (ç¿»å€ï¼Œæµ‹è¯•å®¹é‡ä¸Šé™)
æ—¶é—´: 2025-11-21 02:04:52
============================================================

[32m[2025-11-21 02:04:54]    INFO >> åŠ è½½é…ç½®: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/config.yml (train_enhanced.py:666, cli_main())[0m
[32m[2025-11-21 02:04:54]    INFO >> å•GPUè®­ç»ƒ... (train_enhanced.py:694, cli_main())[0m
[32m[2025-11-21 02:04:54]    INFO >> è®­ç»ƒæ—¥å¿—å°†ä¿å­˜åˆ°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/logs (train_enhanced.py:561, single_main())[0m
[32m[2025-11-21 02:04:54]    INFO >> [nodes] dictionary: 9999 types (typilus.py:102, setup_task())[0m
[32m[2025-11-21 02:04:54]    INFO >> [edges] dictionary: 0 types (typilus.py:102, setup_task())[0m
[32m[2025-11-21 02:04:54]    INFO >> [supernodes.annotation] dictionary: 99 types (typilus.py:106, setup_task())[0m
[32m[2025-11-21 02:05:04]    INFO >> Typilus(
  (encoder): GGNNEncoder(
    (node_embedding): Embedding(9999, 128, padding_idx=0)
    (node_layer): Sequential(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=128, out_features=128, bias=False)
      (2): Dropout(p=0.1, inplace=False)
    )
    (ggnns): ModuleList(
      (0): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=128, out_features=128, bias=False)
          (OCCURRENCE_OF): Linear(in_features=128, out_features=128, bias=False)
          (NEXT): Linear(in_features=128, out_features=128, bias=False)
          (SUBTOKEN_OF): Linear(in_features=128, out_features=128, bias=False)
          (COMPUTED_FROM): Linear(in_features=128, out_features=128, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=128, out_features=128, bias=False)
          (NEXT_USE): Linear(in_features=128, out_features=128, bias=False)
          (RETURNS_TO): Linear(in_features=128, out_features=128, bias=False)
          (_CHILD): Linear(in_features=128, out_features=128, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=128, out_features=128, bias=False)
          (_NEXT): Linear(in_features=128, out_features=128, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=128, out_features=128, bias=False)
          (_COMPUTED_FROM): Linear(in_features=128, out_features=128, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=128, out_features=128, bias=False)
          (_NEXT_USE): Linear(in_features=128, out_features=128, bias=False)
          (_RETURNS_TO): Linear(in_features=128, out_features=128, bias=False)
        )
        (rnn_cell): GRUCell(128, 128)
      )
      (1): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=128, out_features=128, bias=False)
          (OCCURRENCE_OF): Linear(in_features=128, out_features=128, bias=False)
          (NEXT): Linear(in_features=128, out_features=128, bias=False)
          (SUBTOKEN_OF): Linear(in_features=128, out_features=128, bias=False)
          (COMPUTED_FROM): Linear(in_features=128, out_features=128, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=128, out_features=128, bias=False)
          (NEXT_USE): Linear(in_features=128, out_features=128, bias=False)
          (RETURNS_TO): Linear(in_features=128, out_features=128, bias=False)
          (_CHILD): Linear(in_features=128, out_features=128, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=128, out_features=128, bias=False)
          (_NEXT): Linear(in_features=128, out_features=128, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=128, out_features=128, bias=False)
          (_COMPUTED_FROM): Linear(in_features=128, out_features=128, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=128, out_features=128, bias=False)
          (_NEXT_USE): Linear(in_features=128, out_features=128, bias=False)
          (_RETURNS_TO): Linear(in_features=128, out_features=128, bias=False)
        )
        (rnn_cell): GRUCell(256, 128)
      )
    )
  )
  (decoder): DenseDecoder(
    (cls_layers): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=False)
      (1): Dropout(p=0.1, inplace=False)
      (2): Linear(in_features=128, out_features=99, bias=True)
    )
  )
) (train_enhanced.py:568, single_main())[0m
[32m[2025-11-21 02:05:04]    INFO >> æ¨¡åž‹: typilus, æŸå¤±å‡½æ•°: TypilusCriterion (train_enhanced.py:569, single_main())[0m
[32m[2025-11-21 02:05:04]    INFO >> æ¨¡åž‹å‚æ•°: 2096995 (å¯è®­ç»ƒ: 2096995) (train_enhanced.py:570, single_main())[0m
[32m[2025-11-21 02:05:04]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 02:05:04]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 80577 MB ; used memory = 1342 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 02:05:04]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 02:05:04]    INFO >> ä½¿ç”¨ 1 ä¸ªGPUè®­ç»ƒ (train_enhanced.py:576, single_main())[0m
[32m[2025-11-21 02:05:04]    INFO >> no existing checkpoint found /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-21 02:05:04]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[32m[2025-11-21 02:06:23]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-21 02:06:24] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-21 02:06:36]    INFO >> epoch 001:     50 / 1539 loss=5.796, wps=3096, ups=4.28, wpb=720, bsz=720, num_updates=50, lr=0.0004, gnorm=6.47, clip=0, train_wall=11, gb_free=69, wall=87 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:06:38] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 294.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 5.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 76.93 GiB is allocated by PyTorch, and 1.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:06:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:06:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:06:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 4         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78455 MiB |  78777 MiB |   3215 GiB |   3139 GiB |
|       from large pool |  78425 MiB |  78747 MiB |   3207 GiB |   3130 GiB |
|       from small pool |     30 MiB |     35 MiB |      8 GiB |      8 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78455 MiB |  78777 MiB |   3215 GiB |   3139 GiB |
|       from large pool |  78425 MiB |  78747 MiB |   3207 GiB |   3130 GiB |
|       from small pool |     30 MiB |     35 MiB |      8 GiB |      8 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78432 MiB |  78754 MiB |   3211 GiB |   3135 GiB |
|       from large pool |  78402 MiB |  78724 MiB |   3202 GiB |   3126 GiB |
|       from small pool |     30 MiB |     35 MiB |      8 GiB |      8 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80500 MiB |  80502 MiB | 160894 MiB |  80394 MiB |
|       from large pool |  80468 MiB |  80468 MiB | 160728 MiB |  80260 MiB |
|       from small pool |     32 MiB |    134 MiB |    166 MiB |    134 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2044 MiB |   8689 MiB |   1764 GiB |   1762 GiB |
|       from large pool |   2042 MiB |   8686 MiB |   1754 GiB |   1752 GiB |
|       from small pool |      1 MiB |     22 MiB |     10 GiB |     10 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     622    |     624    |  110732    |  110110    |
|       from large pool |     320    |     322    |   57791    |   57471    |
|       from small pool |     302    |     354    |   52941    |   52639    |
|---------------------------------------------------------------------------|
| Active allocs         |     622    |     624    |  110732    |  110110    |
|       from large pool |     320    |     322    |   57791    |   57471    |
|       from small pool |     302    |     354    |   52941    |   52639    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     164    |     397    |     819    |     655    |
|       from large pool |     148    |     330    |     736    |     588    |
|       from small pool |      16    |      67    |      83    |      67    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     140    |     140    |   66711    |   66571    |
|       from large pool |     116    |     116    |   42937    |   42821    |
|       from small pool |      24    |      52    |   23774    |   23750    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:06:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:06:38] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:06:47]    INFO >> epoch 001:    101 / 1539 loss=6.01, wps=2633.6, ups=4.31, wpb=610.7, bsz=610.7, num_updates=100, lr=0.0004, gnorm=5.682, clip=0, train_wall=10, gb_free=71.3, wall=99 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:07:00]    INFO >> epoch 001:    151 / 1539 loss=6.039, wps=3535.8, ups=4.33, wpb=816.7, bsz=816.7, num_updates=150, lr=0.0004, gnorm=5.787, clip=0, train_wall=11, gb_free=68.6, wall=110 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:07:10]    INFO >> epoch 001:    201 / 1539 loss=5.993, wps=3161.7, ups=4.93, wpb=641.7, bsz=641.7, num_updates=200, lr=0.0004, gnorm=5.728, clip=0, train_wall=9, gb_free=69.8, wall=120 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:07:21]    INFO >> epoch 001:    251 / 1539 loss=5.897, wps=3057.3, ups=4.79, wpb=637.9, bsz=637.9, num_updates=250, lr=0.0004, gnorm=5.742, clip=0, train_wall=10, gb_free=63.1, wall=131 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:07:29] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 15.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 77.57 GiB is allocated by PyTorch, and 1.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:07:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:07:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:07:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 7         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79375 MiB |  79433 MiB |  14949 GiB |  14871 GiB |
|       from large pool |  79283 MiB |  79342 MiB |  14907 GiB |  14830 GiB |
|       from small pool |     91 MiB |     92 MiB |     41 GiB |     41 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79375 MiB |  79433 MiB |  14949 GiB |  14871 GiB |
|       from large pool |  79283 MiB |  79342 MiB |  14907 GiB |  14830 GiB |
|       from small pool |     91 MiB |     92 MiB |     41 GiB |     41 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79313 MiB |  79372 MiB |  14929 GiB |  14851 GiB |
|       from large pool |  79221 MiB |  79280 MiB |  14887 GiB |  14810 GiB |
|       from small pool |     91 MiB |     92 MiB |     41 GiB |     41 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80490 MiB |  80490 MiB | 318886 MiB | 238396 MiB |
|       from large pool |  80388 MiB |  80388 MiB | 318586 MiB | 238198 MiB |
|       from small pool |    102 MiB |    102 MiB |    300 MiB |    198 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1054 MiB |   2171 MiB |   8073 GiB |   8072 GiB |
|       from large pool |   1044 MiB |   2163 MiB |   8024 GiB |   8023 GiB |
|       from small pool |     10 MiB |     27 MiB |     48 GiB |     48 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1794    |    1797    |  521322    |  519528    |
|       from large pool |     449    |     450    |  279296    |  278847    |
|       from small pool |    1345    |    1348    |  242026    |  240681    |
|---------------------------------------------------------------------------|
| Active allocs         |    1794    |    1797    |  521322    |  519528    |
|       from large pool |     449    |     450    |  279296    |  278847    |
|       from small pool |    1345    |    1348    |  242026    |  240681    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     279    |     368    |    1399    |    1120    |
|       from large pool |     228    |     318    |    1249    |    1021    |
|       from small pool |      51    |      51    |     150    |      99    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     220    |     221    |  313005    |  312785    |
|       from large pool |     137    |     138    |  207391    |  207254    |
|       from small pool |      83    |      83    |  105614    |  105531    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:07:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:07:29] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:07:35]    INFO >> epoch 001:    302 / 1539 loss=5.814, wps=2517.7, ups=3.85, wpb=654.2, bsz=654.2, num_updates=300, lr=0.0004, gnorm=5.11, clip=0, train_wall=10, gb_free=65.1, wall=144 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:07:46] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 1.25 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 75.99 GiB is allocated by PyTorch, and 2.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:07:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:07:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:07:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 3            |        cudaMalloc retries: 10        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77688 MiB |  77808 MiB |  19182 GiB |  19107 GiB |
|       from large pool |  77505 MiB |  77625 MiB |  19128 GiB |  19052 GiB |
|       from small pool |    182 MiB |    184 MiB |     54 GiB |     54 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77688 MiB |  77808 MiB |  19182 GiB |  19107 GiB |
|       from large pool |  77505 MiB |  77625 MiB |  19128 GiB |  19052 GiB |
|       from small pool |    182 MiB |    184 MiB |     54 GiB |     54 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77671 MiB |  77792 MiB |  19157 GiB |  19082 GiB |
|       from large pool |  77489 MiB |  77609 MiB |  19103 GiB |  19027 GiB |
|       from small pool |    182 MiB |    183 MiB |     54 GiB |     54 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80504 MiB |  80504 MiB | 320968 MiB | 240464 MiB |
|       from large pool |  80306 MiB |  80328 MiB | 320570 MiB | 240264 MiB |
|       from small pool |    198 MiB |    200 MiB |    398 MiB |    200 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2691 MiB |  13166 MiB |  11545 GiB |  11543 GiB |
|       from large pool |   2676 MiB |  13158 MiB |  11482 GiB |  11479 GiB |
|       from small pool |     15 MiB |     24 MiB |     63 GiB |     63 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3411    |    3414    |  675906    |  672495    |
|       from large pool |     588    |     590    |  358892    |  358304    |
|       from small pool |    2823    |    2826    |  317014    |  314191    |
|---------------------------------------------------------------------------|
| Active allocs         |    3411    |    3414    |  675906    |  672495    |
|       from large pool |     588    |     590    |  358892    |  358304    |
|       from small pool |    2823    |    2826    |  317014    |  314191    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     357    |     357    |    1480    |    1123    |
|       from large pool |     258    |     258    |    1281    |    1023    |
|       from small pool |      99    |     100    |     199    |     100    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     386    |     388    |  400003    |  399617    |
|       from large pool |     227    |     229    |  260520    |  260293    |
|       from small pool |     159    |     160    |  139483    |  139324    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:07:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:07:46] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:07:46]    INFO >> epoch 001:    353 / 1539 loss=5.837, wps=2969.6, ups=4.42, wpb=671.8, bsz=671.8, num_updates=350, lr=0.0004, gnorm=5.106, clip=0, train_wall=10, gb_free=4, wall=155 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:07:58]    INFO >> epoch 001:    403 / 1539 loss=5.695, wps=3135.9, ups=4.29, wpb=731.1, bsz=731.1, num_updates=400, lr=0.0004, gnorm=5.3, clip=0, train_wall=11, gb_free=65.9, wall=167 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:08:06] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.69 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.87 GiB is free. Including non-PyTorch memory, this process has 77.24 GiB memory in use. Of the allocated memory 74.78 GiB is allocated by PyTorch, and 1.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:08:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:08:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:08:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 4            |        cudaMalloc retries: 12        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  71060 MiB |  76575 MiB |  23046 GiB |  22977 GiB |
|       from large pool |  71031 MiB |  76546 MiB |  22982 GiB |  22912 GiB |
|       from small pool |     28 MiB |     29 MiB |     64 GiB |     64 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  71060 MiB |  76575 MiB |  23046 GiB |  22977 GiB |
|       from large pool |  71031 MiB |  76546 MiB |  22982 GiB |  22912 GiB |
|       from small pool |     28 MiB |     29 MiB |     64 GiB |     64 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  71047 MiB |  76562 MiB |  23017 GiB |  22948 GiB |
|       from large pool |  71018 MiB |  76533 MiB |  22952 GiB |  22883 GiB |
|       from small pool |     28 MiB |     29 MiB |     64 GiB |     64 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78586 MiB |  80380 MiB | 384792 MiB | 306206 MiB |
|       from large pool |  78554 MiB |  80182 MiB | 384392 MiB | 305838 MiB |
|       from small pool |     32 MiB |    198 MiB |    400 MiB |    368 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2799 MiB |   7225 MiB |  14759 GiB |  14757 GiB |
|       from large pool |   2796 MiB |   7220 MiB |  14684 GiB |  14682 GiB |
|       from small pool |      3 MiB |     19 MiB |     75 GiB |     75 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     487    |     496    |     806 K  |     805 K  |
|       from large pool |     195    |     204    |     430 K  |     430 K  |
|       from small pool |     292    |     354    |     375 K  |     375 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     487    |     496    |     806 K  |     805 K  |
|       from large pool |     195    |     204    |     430 K  |     430 K  |
|       from small pool |     292    |     354    |     375 K  |     375 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      93    |     355    |    1522    |    1429    |
|       from large pool |      77    |     256    |    1322    |    1245    |
|       from small pool |      16    |      99    |     200    |     184    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     106    |     107    |  473946    |  473840    |
|       from large pool |      83    |      84    |  308537    |  308454    |
|       from small pool |      23    |      48    |  165409    |  165386    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:08:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:08:06] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:08:17]    INFO >> epoch 001:    454 / 1539 loss=5.668, wps=1773.7, ups=2.81, wpb=630.9, bsz=630.9, num_updates=450, lr=0.0004, gnorm=4.406, clip=0, train_wall=15, gb_free=59.9, wall=185 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:08:21] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.14 GiB. GPU 2 has a total capacity of 79.14 GiB of which 4.88 GiB is free. Including non-PyTorch memory, this process has 74.24 GiB memory in use. Of the allocated memory 72.69 GiB is allocated by PyTorch, and 1.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:08:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:08:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:08:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 5            |        cudaMalloc retries: 14        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  67280 MiB |  74436 MiB |  25360 GiB |  25294 GiB |
|       from large pool |  67253 MiB |  74409 MiB |  25289 GiB |  25223 GiB |
|       from small pool |     27 MiB |     32 MiB |     70 GiB |     70 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  67280 MiB |  74436 MiB |  25360 GiB |  25294 GiB |
|       from large pool |  67253 MiB |  74409 MiB |  25289 GiB |  25223 GiB |
|       from small pool |     27 MiB |     32 MiB |     70 GiB |     70 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  67268 MiB |  74423 MiB |  25328 GiB |  25262 GiB |
|       from large pool |  67241 MiB |  74396 MiB |  25257 GiB |  25192 GiB |
|       from small pool |     27 MiB |     32 MiB |     70 GiB |     70 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  75512 MiB |  79164 MiB | 450274 MiB | 374762 MiB |
|       from large pool |  75480 MiB |  79096 MiB | 449836 MiB | 374356 MiB |
|       from small pool |     32 MiB |     68 MiB |    438 MiB |    406 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2963 MiB |   6815 MiB |  17266 GiB |  17263 GiB |
|       from large pool |   2958 MiB |   6810 MiB |  17184 GiB |  17181 GiB |
|       from small pool |      4 MiB |     16 MiB |     82 GiB |     82 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     487    |     496    |     884 K  |     883 K  |
|       from large pool |     195    |     204    |     473 K  |     473 K  |
|       from small pool |     292    |     354    |     410 K  |     410 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     487    |     496    |     884 K  |     883 K  |
|       from large pool |     195    |     204    |     473 K  |     473 K  |
|       from small pool |     292    |     354    |     410 K  |     410 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      68    |     110    |    1563    |    1495    |
|       from large pool |      52    |      76    |    1344    |    1292    |
|       from small pool |      16    |      34    |     219    |     203    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      72    |      72    |  514131    |  514059    |
|       from large pool |      49    |      49    |  334550    |  334501    |
|       from small pool |      23    |      47    |  179581    |  179558    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:08:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:08:21] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:08:30]    INFO >> epoch 001:    505 / 1539 loss=5.403, wps=2618.4, ups=3.78, wpb=691.8, bsz=691.8, num_updates=500, lr=0.0004, gnorm=5.966, clip=0, train_wall=10, gb_free=64.5, wall=198 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:08:43]    INFO >> epoch 001:    555 / 1539 loss=5.349, wps=2903.9, ups=4.35, wpb=668.3, bsz=668.3, num_updates=550, lr=0.0004, gnorm=4.79, clip=0, train_wall=11, gb_free=57.5, wall=209 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:08:55]    INFO >> epoch 001:    605 / 1539 loss=5.075, wps=2682.6, ups=4.16, wpb=644.6, bsz=644.6, num_updates=600, lr=0.0004, gnorm=5.659, clip=0, train_wall=11, gb_free=71.8, wall=221 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:09:09]    INFO >> epoch 001:    655 / 1539 loss=4.869, wps=2812.9, ups=3.94, wpb=714.1, bsz=714.1, num_updates=650, lr=0.0004, gnorm=6.251, clip=0, train_wall=12, gb_free=72.5, wall=234 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:09:11] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.53 GiB. GPU 2 has a total capacity of 79.14 GiB of which 2.97 GiB is free. Including non-PyTorch memory, this process has 76.14 GiB memory in use. Of the allocated memory 71.68 GiB is allocated by PyTorch, and 3.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:09:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:09:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:09:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 6            |        cudaMalloc retries: 15        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72886 MiB |  73395 MiB |  36169 GiB |  36098 GiB |
|       from large pool |  72860 MiB |  73368 MiB |  36070 GiB |  35998 GiB |
|       from small pool |     26 MiB |     32 MiB |     99 GiB |     99 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72886 MiB |  73395 MiB |  36169 GiB |  36098 GiB |
|       from large pool |  72860 MiB |  73368 MiB |  36070 GiB |  35998 GiB |
|       from small pool |     26 MiB |     32 MiB |     99 GiB |     99 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72874 MiB |  73382 MiB |  36127 GiB |  36056 GiB |
|       from large pool |  72847 MiB |  73355 MiB |  36028 GiB |  35957 GiB |
|       from small pool |     26 MiB |     32 MiB |     99 GiB |     99 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77462 MiB |  77524 MiB | 457554 MiB | 380092 MiB |
|       from large pool |  77432 MiB |  77432 MiB | 457056 MiB | 379624 MiB |
|       from small pool |     30 MiB |     92 MiB |    498 MiB |    468 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4575 MiB |   9901 MiB |  30009 GiB |  30005 GiB |
|       from large pool |   4572 MiB |   9897 MiB |  29894 GiB |  29889 GiB |
|       from small pool |      3 MiB |     17 MiB |    115 GiB |    115 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     565    |     572    |    1245 K  |    1245 K  |
|       from large pool |     266    |     272    |     671 K  |     670 K  |
|       from small pool |     299    |     354    |     574 K  |     574 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     565    |     572    |    1245 K  |    1245 K  |
|       from large pool |     266    |     272    |     671 K  |     670 K  |
|       from small pool |     299    |     354    |     574 K  |     574 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      68    |      99    |    1595    |    1527    |
|       from large pool |      53    |      53    |    1346    |    1293    |
|       from small pool |      15    |      46    |     249    |     234    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      87    |      87    |  701534    |  701447    |
|       from large pool |      62    |      62    |  451870    |  451808    |
|       from small pool |      25    |      46    |  249664    |  249639    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:09:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:09:11] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:09:22]    INFO >> epoch 001:    706 / 1539 loss=4.683, wps=2597, ups=3.83, wpb=678.1, bsz=678.1, num_updates=700, lr=0.0004, gnorm=5.431, clip=0, train_wall=12, gb_free=66.8, wall=247 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:09:36]    INFO >> epoch 001:    756 / 1539 loss=4.66, wps=2794.7, ups=3.69, wpb=756.7, bsz=756.7, num_updates=750, lr=0.0004, gnorm=5.941, clip=2, train_wall=13, gb_free=68.3, wall=261 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:09:49]    INFO >> epoch 001:    806 / 1539 loss=4.592, wps=2980.4, ups=4.11, wpb=725.5, bsz=725.5, num_updates=800, lr=0.0004, gnorm=6.408, clip=0, train_wall=11, gb_free=73.8, wall=273 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:10:02]    INFO >> epoch 001:    856 / 1539 loss=4.397, wps=2599.5, ups=3.97, wpb=655.3, bsz=655.3, num_updates=850, lr=0.0004, gnorm=6.13, clip=0, train_wall=12, gb_free=60.7, wall=285 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:10:13]    INFO >> epoch 001:    906 / 1539 loss=4.344, wps=2777.9, ups=4.42, wpb=628.4, bsz=628.4, num_updates=900, lr=0.0004, gnorm=5.146, clip=0, train_wall=10, gb_free=72.7, wall=297 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:10:27]    INFO >> epoch 001:    956 / 1539 loss=4.222, wps=2847.2, ups=3.9, wpb=730.7, bsz=730.7, num_updates=950, lr=0.0004, gnorm=6.334, clip=2, train_wall=12, gb_free=66.4, wall=310 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:10:31] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.33 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.25 GiB is free. Including non-PyTorch memory, this process has 77.87 GiB memory in use. Of the allocated memory 76.06 GiB is allocated by PyTorch, and 1.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:10:31] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:10:31] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:10:31] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 7            |        cudaMalloc retries: 17        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77084 MiB |  78547 MiB |  53046 GiB |  52971 GiB |
|       from large pool |  77060 MiB |  78523 MiB |  52902 GiB |  52827 GiB |
|       from small pool |     23 MiB |     32 MiB |    143 GiB |    143 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77084 MiB |  78547 MiB |  53046 GiB |  52971 GiB |
|       from large pool |  77060 MiB |  78523 MiB |  52902 GiB |  52827 GiB |
|       from small pool |     23 MiB |     32 MiB |    143 GiB |    143 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77063 MiB |  78526 MiB |  52989 GiB |  52914 GiB |
|       from large pool |  77040 MiB |  78502 MiB |  52845 GiB |  52770 GiB |
|       from small pool |     23 MiB |     32 MiB |    143 GiB |    143 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79224 MiB |  79224 MiB | 522512 MiB | 443288 MiB |
|       from large pool |  79196 MiB |  79196 MiB | 521926 MiB | 442730 MiB |
|       from small pool |     28 MiB |    118 MiB |    586 MiB |    558 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2139 MiB |   6699 MiB |  49721 GiB |  49718 GiB |
|       from large pool |   2135 MiB |   6693 MiB |  49553 GiB |  49551 GiB |
|       from small pool |      4 MiB |     23 MiB |    167 GiB |    167 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     486    |     494    |    1811 K  |    1810 K  |
|       from large pool |     200    |     208    |     977 K  |     977 K  |
|       from small pool |     286    |     354    |     833 K  |     833 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     486    |     494    |    1811 K  |    1810 K  |
|       from large pool |     200    |     208    |     977 K  |     977 K  |
|       from small pool |     286    |     354    |     833 K  |     833 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      71    |     112    |    1668    |    1597    |
|       from large pool |      57    |      57    |    1375    |    1318    |
|       from small pool |      14    |      59    |     293    |     279    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      62    |      62    |     996 K  |     996 K  |
|       from large pool |      43    |      43    |     634 K  |     634 K  |
|       from small pool |      19    |      54    |     362 K  |     362 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:10:31] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:10:31] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:10:42]    INFO >> epoch 001:   1007 / 1539 loss=4.235, wps=2320.4, ups=3.52, wpb=659.5, bsz=659.5, num_updates=1000, lr=0.0004, gnorm=6.447, clip=0, train_wall=11, gb_free=62.6, wall=324 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:10:57]    INFO >> epoch 001:   1057 / 1539 loss=4.235, wps=3233, ups=3.66, wpb=883.2, bsz=883.2, num_updates=1050, lr=0.0004, gnorm=7.328, clip=2, train_wall=13, gb_free=59.3, wall=337 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:11:08] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.88 GiB. GPU 2 has a total capacity of 79.14 GiB of which 923.25 MiB is free. Including non-PyTorch memory, this process has 78.21 GiB memory in use. Of the allocated memory 72.80 GiB is allocated by PyTorch, and 4.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:11:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:11:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:11:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 8            |        cudaMalloc retries: 20        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  45874 MiB |  74763 MiB |  61126 GiB |  61081 GiB |
|       from large pool |  45850 MiB |  74739 MiB |  60957 GiB |  60912 GiB |
|       from small pool |     24 MiB |     29 MiB |    168 GiB |    168 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  45874 MiB |  74763 MiB |  61126 GiB |  61081 GiB |
|       from large pool |  45850 MiB |  74739 MiB |  60957 GiB |  60912 GiB |
|       from small pool |     24 MiB |     29 MiB |    168 GiB |    168 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  45867 MiB |  74754 MiB |  61061 GiB |  61016 GiB |
|       from large pool |  45842 MiB |  74730 MiB |  60893 GiB |  60848 GiB |
|       from small pool |     24 MiB |     29 MiB |    168 GiB |    168 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79582 MiB |  79582 MiB | 534152 MiB | 454570 MiB |
|       from large pool |  79552 MiB |  79552 MiB | 533376 MiB | 453824 MiB |
|       from small pool |     30 MiB |    218 MiB |    776 MiB |    746 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  13321 MiB |  16380 MiB |  58997 GiB |  58984 GiB |
|       from large pool |  13315 MiB |  16376 MiB |  58801 GiB |  58788 GiB |
|       from small pool |      5 MiB |     29 MiB |    195 GiB |    195 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     413    |     464    |    2095 K  |    2095 K  |
|       from large pool |     128    |     178    |    1119 K  |    1119 K  |
|       from small pool |     285    |     354    |     976 K  |     976 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     413    |     464    |    2095 K  |    2095 K  |
|       from large pool |     128    |     178    |    1119 K  |    1119 K  |
|       from small pool |     285    |     354    |     976 K  |     976 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      64    |     166    |    1766    |    1702    |
|       from large pool |      49    |      57    |    1378    |    1329    |
|       from small pool |      15    |     109    |     388    |     373    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      69    |      77    |    1149 K  |    1149 K  |
|       from large pool |      48    |      56    |     717 K  |     717 K  |
|       from small pool |      21    |      63    |     431 K  |     431 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:11:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:11:08] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:11:11]    INFO >> epoch 001:   1108 / 1539 loss=4.135, wps=2976.6, ups=3.45, wpb=863.3, bsz=863.3, num_updates=1100, lr=0.0004, gnorm=6.615, clip=0, train_wall=13, gb_free=63.4, wall=352 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:11:25]    INFO >> epoch 001:   1158 / 1539 loss=4.147, wps=2848.9, ups=4.15, wpb=687, bsz=687, num_updates=1150, lr=0.0004, gnorm=6.627, clip=0, train_wall=11, gb_free=64.2, wall=364 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:11:37]    INFO >> epoch 001:   1208 / 1539 loss=4.092, wps=2875.3, ups=4.13, wpb=696.1, bsz=696.1, num_updates=1200, lr=0.0004, gnorm=6.474, clip=2, train_wall=11, gb_free=65.5, wall=376 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:11:49]    INFO >> epoch 001:   1258 / 1539 loss=4.124, wps=2862.1, ups=4.07, wpb=702.7, bsz=702.7, num_updates=1250, lr=0.0004, gnorm=6.334, clip=0, train_wall=11, gb_free=67.2, wall=388 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:12:03]    INFO >> epoch 001:   1308 / 1539 loss=4.035, wps=2869.6, ups=3.82, wpb=752, bsz=752, num_updates=1300, lr=0.0004, gnorm=6.209, clip=0, train_wall=12, gb_free=67.8, wall=401 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:12:16]    INFO >> epoch 001:   1358 / 1539 loss=4.026, wps=2725.2, ups=4.01, wpb=680.4, bsz=680.4, num_updates=1350, lr=0.0004, gnorm=6.052, clip=2, train_wall=12, gb_free=66.8, wall=414 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:12:30]    INFO >> epoch 001:   1408 / 1539 loss=3.995, wps=2690.6, ups=3.89, wpb=692.5, bsz=692.5, num_updates=1400, lr=0.0004, gnorm=6.439, clip=2, train_wall=12, gb_free=65, wall=427 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:12:42]    INFO >> epoch 001:   1458 / 1539 loss=4.114, wps=2817.1, ups=4.1, wpb=687.5, bsz=687.5, num_updates=1450, lr=0.0004, gnorm=6.009, clip=0, train_wall=11, gb_free=67.9, wall=439 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:12:55]    INFO >> epoch 001:   1508 / 1539 loss=3.898, wps=2846.7, ups=3.82, wpb=744.7, bsz=744.7, num_updates=1500, lr=0.0004, gnorm=5.712, clip=2, train_wall=12, gb_free=70.6, wall=452 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:13:04]    INFO >> epoch 001 | loss 4.809 | wps 2801.3 | ups 3.99 | wpb 702.8 | bsz 702.8 | num_updates 1531 | lr 0.0004 | gnorm 5.932 | clip 0.5 | train_wall 346 | gb_free 73.3 | wall 460 (progress_bar.py:267, print())[0m
[33m[2025-11-21 02:13:04] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 02:13:30]    INFO >> epoch 001 | valid on 'valid' subset | loss 4.012 | wps 5801.8 | wpb 5412.5 | bsz 5412.5 | num_updates 1531 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
[32m[2025-11-21 02:13:31]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 02:13:31]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/checkpoints/checkpoint_best.pt (epoch 1 @ 1531 updates, score 4.012) (writing took 0.040392 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 02:13:31] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-21 02:13:36]    INFO >> epoch 002:     19 / 1539 loss=4.033, wps=902.6, ups=1.31, wpb=691.2, bsz=691.2, num_updates=1550, lr=0.0004, gnorm=6.104, clip=0, train_wall=11, gb_free=66, wall=490 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:13:47]    INFO >> epoch 002:     69 / 1539 loss=4.006, wps=2978.5, ups=4.58, wpb=650.1, bsz=650.1, num_updates=1600, lr=0.0004, gnorm=5.249, clip=0, train_wall=10, gb_free=69.2, wall=501 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:14:00]    INFO >> epoch 002:    119 / 1539 loss=3.962, wps=2769.7, ups=3.81, wpb=727.7, bsz=727.7, num_updates=1650, lr=0.0004, gnorm=5.561, clip=0, train_wall=13, gb_free=54.1, wall=514 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:14:14]    INFO >> epoch 002:    169 / 1539 loss=3.824, wps=2954.4, ups=4.03, wpb=732.6, bsz=732.6, num_updates=1700, lr=0.0004, gnorm=4.982, clip=0, train_wall=12, gb_free=64.7, wall=527 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:14:27]    INFO >> epoch 002:    219 / 1539 loss=4.084, wps=2773.5, ups=3.92, wpb=706.9, bsz=706.9, num_updates=1750, lr=0.0004, gnorm=5.7, clip=0, train_wall=12, gb_free=66.6, wall=540 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:14:40]    INFO >> epoch 002:    269 / 1539 loss=3.807, wps=3194.5, ups=3.69, wpb=866.1, bsz=866.1, num_updates=1800, lr=0.0004, gnorm=6.352, clip=2, train_wall=13, gb_free=65.3, wall=553 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:14:53]    INFO >> epoch 002:    319 / 1539 loss=3.884, wps=2917.8, ups=4.33, wpb=673.5, bsz=673.5, num_updates=1850, lr=0.0004, gnorm=4.889, clip=0, train_wall=11, gb_free=61.2, wall=565 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:15:05]    INFO >> epoch 002:    369 / 1539 loss=3.944, wps=2688.6, ups=4.28, wpb=628.3, bsz=628.3, num_updates=1900, lr=0.0004, gnorm=6.613, clip=0, train_wall=11, gb_free=63, wall=576 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:15:19] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.33 GiB. GPU 2 has a total capacity of 79.14 GiB of which 379.25 MiB is free. Including non-PyTorch memory, this process has 78.75 GiB memory in use. Of the allocated memory 76.05 GiB is allocated by PyTorch, and 2.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:15:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:15:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:15:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 9            |        cudaMalloc retries: 22        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77076 MiB |  78539 MiB | 116933 GiB | 116857 GiB |
|       from large pool |  77052 MiB |  78515 MiB | 116596 GiB | 116521 GiB |
|       from small pool |     23 MiB |     35 MiB |    336 GiB |    336 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77076 MiB |  78539 MiB | 116933 GiB | 116857 GiB |
|       from large pool |  77052 MiB |  78515 MiB | 116596 GiB | 116521 GiB |
|       from small pool |     23 MiB |     35 MiB |    336 GiB |    336 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77063 MiB |  78526 MiB | 116819 GiB | 116743 GiB |
|       from large pool |  77040 MiB |  78502 MiB | 116482 GiB | 116407 GiB |
|       from small pool |     23 MiB |     35 MiB |    336 GiB |    336 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80126 MiB |  80126 MiB | 571320 MiB | 491194 MiB |
|       from large pool |  80092 MiB |  80092 MiB | 570356 MiB | 490264 MiB |
|       from small pool |     34 MiB |    218 MiB |    964 MiB |    930 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3049 MiB |   7257 MiB | 121651 GiB | 121648 GiB |
|       from large pool |   3039 MiB |   7245 MiB | 121266 GiB | 121263 GiB |
|       from small pool |     10 MiB |     31 MiB |    385 GiB |    385 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     488    |     496    |    4020 K  |    4019 K  |
|       from large pool |     200    |     208    |    2038 K  |    2038 K  |
|       from small pool |     288    |     356    |    1981 K  |    1981 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     488    |     496    |    4020 K  |    4019 K  |
|       from large pool |     200    |     208    |    2038 K  |    2038 K  |
|       from small pool |     288    |     356    |    1981 K  |    1981 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      67    |     414    |    2128    |    2061    |
|       from large pool |      50    |     305    |    1646    |    1596    |
|       from small pool |      17    |     109    |     482    |     465    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      72    |      72    |    2200 K  |    2200 K  |
|       from large pool |      47    |      47    |    1274 K  |    1274 K  |
|       from small pool |      25    |      66    |     925 K  |     925 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:15:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:15:19] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:15:20]    INFO >> epoch 002:    420 / 1539 loss=3.863, wps=2599.5, ups=3.63, wpb=715.9, bsz=715.9, num_updates=1950, lr=0.0004, gnorm=6.418, clip=2, train_wall=12, gb_free=67.5, wall=590 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:15:33]    INFO >> epoch 002:    470 / 1539 loss=3.915, wps=2984.9, ups=4.02, wpb=742.3, bsz=742.3, num_updates=2000, lr=0.0004, gnorm=5.372, clip=0, train_wall=12, gb_free=62.3, wall=603 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:15:45]    INFO >> epoch 002:    520 / 1539 loss=3.969, wps=2831.9, ups=4.04, wpb=700.7, bsz=700.7, num_updates=2050, lr=0.0004, gnorm=6.544, clip=0, train_wall=12, gb_free=62.7, wall=615 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:15:58]    INFO >> epoch 002:    570 / 1539 loss=3.886, wps=2691, ups=4.17, wpb=645.8, bsz=645.8, num_updates=2100, lr=0.0004, gnorm=6.134, clip=0, train_wall=11, gb_free=66.8, wall=627 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:16:00] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 7.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.88 GiB is allocated by PyTorch, and 751.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:16:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:16:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:16:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 10           |        cudaMalloc retries: 23        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79687 MiB |  79746 MiB | 125815 GiB | 125737 GiB |
|       from large pool |  79591 MiB |  79650 MiB | 125454 GiB | 125376 GiB |
|       from small pool |     95 MiB |     96 MiB |    361 GiB |    360 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79687 MiB |  79746 MiB | 125815 GiB | 125737 GiB |
|       from large pool |  79591 MiB |  79650 MiB | 125454 GiB | 125376 GiB |
|       from small pool |     95 MiB |     96 MiB |    361 GiB |    360 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79669 MiB |  79728 MiB | 125693 GiB | 125615 GiB |
|       from large pool |  79575 MiB |  79633 MiB | 125332 GiB | 125254 GiB |
|       from small pool |     94 MiB |     96 MiB |    360 GiB |    360 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80498 MiB |  80498 MiB | 571692 MiB | 491194 MiB |
|       from large pool |  80392 MiB |  80392 MiB | 570656 MiB | 490264 MiB |
|       from small pool |    106 MiB |    106 MiB |   1036 MiB |    930 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory | 768884 KiB |   6931 MiB | 132313 GiB | 132312 GiB |
|       from large pool | 757847 KiB |   6929 MiB | 131899 GiB | 131898 GiB |
|       from small pool |  11036 KiB |     27 MiB |    413 GiB |    413 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1862    |    1865    |    4326 K  |    4324 K  |
|       from large pool |     455    |     456    |    2203 K  |    2202 K  |
|       from small pool |    1407    |    1410    |    2123 K  |    2121 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1862    |    1865    |    4326 K  |    4324 K  |
|       from large pool |     455    |     456    |    2203 K  |    2202 K  |
|       from small pool |    1407    |    1410    |    2123 K  |    2121 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     108    |     108    |    2169    |    2061    |
|       from large pool |      55    |      55    |    1651    |    1596    |
|       from small pool |      53    |      53    |     518    |     465    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     147    |     148    |    2361 K  |    2360 K  |
|       from large pool |      59    |      60    |    1371 K  |    1371 K  |
|       from small pool |      88    |      88    |     989 K  |     989 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:16:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:16:00] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:16:11]    INFO >> epoch 002:    621 / 1539 loss=3.853, wps=2951.2, ups=4.03, wpb=732.8, bsz=732.8, num_updates=2150, lr=0.0004, gnorm=5.198, clip=0, train_wall=11, gb_free=63.6, wall=639 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:16:26]    INFO >> epoch 002:    671 / 1539 loss=3.654, wps=2965.3, ups=3.67, wpb=809.1, bsz=809.1, num_updates=2200, lr=0.0004, gnorm=6.789, clip=0, train_wall=13, gb_free=68.6, wall=653 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:16:37]    INFO >> epoch 002:    721 / 1539 loss=3.896, wps=2857.8, ups=4.37, wpb=653.4, bsz=653.4, num_updates=2250, lr=0.0004, gnorm=5.479, clip=0, train_wall=11, gb_free=69.8, wall=664 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:16:50]    INFO >> epoch 002:    771 / 1539 loss=3.751, wps=2657, ups=3.87, wpb=685.8, bsz=685.8, num_updates=2300, lr=0.0004, gnorm=5.91, clip=2, train_wall=12, gb_free=49.7, wall=677 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:17:03]    INFO >> epoch 002:    821 / 1539 loss=3.983, wps=2643.2, ups=4.19, wpb=630.3, bsz=630.3, num_updates=2350, lr=0.0004, gnorm=4.814, clip=0, train_wall=11, gb_free=64, wall=689 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:17:17]    INFO >> epoch 002:    871 / 1539 loss=3.9, wps=2656.2, ups=3.57, wpb=744.1, bsz=744.1, num_updates=2400, lr=0.0004, gnorm=6.199, clip=0, train_wall=13, gb_free=56.9, wall=703 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:17:30]    INFO >> epoch 002:    921 / 1539 loss=3.825, wps=2564.1, ups=3.94, wpb=650, bsz=650, num_updates=2450, lr=0.0004, gnorm=5.232, clip=0, train_wall=12, gb_free=58.3, wall=716 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:17:43] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 588.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 161.25 MiB is free. Including non-PyTorch memory, this process has 78.96 GiB memory in use. Of the allocated memory 76.35 GiB is allocated by PyTorch, and 2.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:17:43] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:17:43] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:17:43] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 11           |        cudaMalloc retries: 24        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77860 MiB |  78182 MiB | 147670 GiB | 147594 GiB |
|       from large pool |  77830 MiB |  78152 MiB | 147251 GiB | 147175 GiB |
|       from small pool |     30 MiB |     36 MiB |    418 GiB |    418 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77860 MiB |  78182 MiB | 147670 GiB | 147594 GiB |
|       from large pool |  77830 MiB |  78152 MiB | 147251 GiB | 147175 GiB |
|       from small pool |     30 MiB |     36 MiB |    418 GiB |    418 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77846 MiB |  78168 MiB | 147526 GiB | 147450 GiB |
|       from large pool |  77816 MiB |  78138 MiB | 147108 GiB | 147032 GiB |
|       from small pool |     30 MiB |     36 MiB |    418 GiB |    418 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80344 MiB |  80438 MiB | 571692 MiB | 491348 MiB |
|       from large pool |  80312 MiB |  80332 MiB | 570656 MiB | 490344 MiB |
|       from small pool |     32 MiB |    106 MiB |   1036 MiB |   1004 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2483 MiB |   6999 MiB | 158118 GiB | 158116 GiB |
|       from large pool |   2481 MiB |   6995 MiB | 157638 GiB | 157635 GiB |
|       from small pool |      1 MiB |     24 MiB |    480 GiB |    480 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     623    |     626    |    5060 K  |    5060 K  |
|       from large pool |     319    |     322    |    2603 K  |    2602 K  |
|       from small pool |     304    |     356    |    2457 K  |    2457 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     623    |     626    |    5060 K  |    5060 K  |
|       from large pool |     319    |     322    |    2603 K  |    2602 K  |
|       from small pool |     304    |     356    |    2457 K  |    2457 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      69    |     107    |    2169    |    2100    |
|       from large pool |      53    |      54    |    1651    |    1598    |
|       from small pool |      16    |      53    |     518    |     502    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      72    |      78    |    2746 K  |    2746 K  |
|       from large pool |      50    |      56    |    1609 K  |    1609 K  |
|       from small pool |      22    |      51    |    1136 K  |    1136 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:17:43] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:17:43] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:17:44]    INFO >> epoch 002:    972 / 1539 loss=3.75, wps=2572.3, ups=4.02, wpb=639.3, bsz=639.3, num_updates=2500, lr=0.0004, gnorm=4.934, clip=0, train_wall=11, gb_free=3.6, wall=728 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:17:51] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.69 GiB. GPU 2 has a total capacity of 79.14 GiB of which 161.25 MiB is free. Including non-PyTorch memory, this process has 78.96 GiB memory in use. Of the allocated memory 74.78 GiB is allocated by PyTorch, and 3.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:17:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:17:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:17:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 12           |        cudaMalloc retries: 25        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  71058 MiB |  76573 MiB | 149329 GiB | 149260 GiB |
|       from large pool |  71029 MiB |  76544 MiB | 148906 GiB | 148837 GiB |
|       from small pool |     28 MiB |     35 MiB |    423 GiB |    423 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  71058 MiB |  76573 MiB | 149329 GiB | 149260 GiB |
|       from large pool |  71029 MiB |  76544 MiB | 148906 GiB | 148837 GiB |
|       from small pool |     28 MiB |     35 MiB |    423 GiB |    423 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  71047 MiB |  76562 MiB | 149184 GiB | 149115 GiB |
|       from large pool |  71018 MiB |  76533 MiB | 148762 GiB | 148692 GiB |
|       from small pool |     28 MiB |     35 MiB |    422 GiB |    422 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80344 MiB |  80380 MiB | 571728 MiB | 491384 MiB |
|       from large pool |  80312 MiB |  80312 MiB | 570656 MiB | 490344 MiB |
|       from small pool |     32 MiB |     68 MiB |   1072 MiB |   1040 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3831 MiB |  10291 MiB | 160092 GiB | 160088 GiB |
|       from large pool |   3828 MiB |  10287 MiB | 159606 GiB | 159603 GiB |
|       from small pool |      3 MiB |     25 MiB |    485 GiB |    485 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     489    |     498    |    5117 K  |    5117 K  |
|       from large pool |     195    |     204    |    2633 K  |    2633 K  |
|       from small pool |     294    |     356    |    2484 K  |    2484 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     489    |     498    |    5117 K  |    5117 K  |
|       from large pool |     195    |     204    |    2633 K  |    2633 K  |
|       from small pool |     294    |     356    |    2484 K  |    2484 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      69    |      87    |    2187    |    2118    |
|       from large pool |      53    |      53    |    1651    |    1598    |
|       from small pool |      16    |      34    |     536    |     520    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      80    |      81    |    2777 K  |    2776 K  |
|       from large pool |      56    |      57    |    1628 K  |    1628 K  |
|       from small pool |      24    |      51    |    1148 K  |    1148 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:17:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:17:51] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:17:56]    INFO >> epoch 002:   1023 / 1539 loss=3.592, wps=3003.8, ups=4, wpb=750.3, bsz=750.3, num_updates=2550, lr=0.0004, gnorm=6.45, clip=2, train_wall=11, gb_free=61.6, wall=741 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:18:10]    INFO >> epoch 002:   1073 / 1539 loss=3.793, wps=2914.9, ups=4.17, wpb=699.1, bsz=699.1, num_updates=2600, lr=0.0004, gnorm=5.967, clip=0, train_wall=11, gb_free=60.9, wall=753 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:18:17] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 39.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 77.23 GiB is allocated by PyTorch, and 1.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:18:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:18:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:18:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 13           |        cudaMalloc retries: 26        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79026 MiB |  79087 MiB | 155048 GiB | 154970 GiB |
|       from large pool |  78830 MiB |  78890 MiB | 154607 GiB | 154530 GiB |
|       from small pool |    196 MiB |    197 MiB |    440 GiB |    440 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79026 MiB |  79087 MiB | 155048 GiB | 154970 GiB |
|       from large pool |  78830 MiB |  78890 MiB | 154607 GiB | 154530 GiB |
|       from small pool |    196 MiB |    197 MiB |    440 GiB |    440 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79006 MiB |  79066 MiB | 154897 GiB | 154820 GiB |
|       from large pool |  78811 MiB |  78871 MiB | 154457 GiB | 154381 GiB |
|       from small pool |    195 MiB |    196 MiB |    439 GiB |    439 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80466 MiB |  80466 MiB | 577304 MiB | 496838 MiB |
|       from large pool |  80252 MiB |  80252 MiB | 576050 MiB | 495798 MiB |
|       from small pool |    214 MiB |    214 MiB |   1254 MiB |   1040 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1377 MiB |   7520 MiB | 166928 GiB | 166927 GiB |
|       from large pool |   1359 MiB |   7519 MiB | 166423 GiB | 166422 GiB |
|       from small pool |     17 MiB |     24 MiB |    505 GiB |    505 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3655    |    3658    |    5322 K  |    5318 K  |
|       from large pool |     610    |     611    |    2737 K  |    2737 K  |
|       from small pool |    3045    |    3048    |    2584 K  |    2581 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3655    |    3658    |    5322 K  |    5318 K  |
|       from large pool |     610    |     611    |    2737 K  |    2737 K  |
|       from small pool |    3045    |    3048    |    2584 K  |    2581 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     246    |     246    |    2365    |    2119    |
|       from large pool |     139    |     139    |    1738    |    1599    |
|       from small pool |     107    |     107    |     627    |     520    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     321    |     322    |    2886 K  |    2886 K  |
|       from large pool |     149    |     150    |    1690 K  |    1690 K  |
|       from small pool |     172    |     172    |    1196 K  |    1196 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:18:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:18:17] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:18:22]    INFO >> epoch 002:   1124 / 1539 loss=3.875, wps=2656, ups=4, wpb=663.3, bsz=663.3, num_updates=2650, lr=0.0004, gnorm=5.228, clip=0, train_wall=11, gb_free=65.9, wall=765 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:18:35]    INFO >> epoch 002:   1174 / 1539 loss=3.898, wps=3027.2, ups=3.88, wpb=779.7, bsz=779.7, num_updates=2700, lr=0.0004, gnorm=5.967, clip=0, train_wall=12, gb_free=60.5, wall=778 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:18:49]    INFO >> epoch 002:   1224 / 1539 loss=3.878, wps=2863.1, ups=3.94, wpb=727.6, bsz=727.6, num_updates=2750, lr=0.0004, gnorm=5.833, clip=0, train_wall=12, gb_free=59.1, wall=791 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:18:53] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.71 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.71 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 69.30 GiB is allocated by PyTorch, and 7.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:18:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:18:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:18:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 14           |        cudaMalloc retries: 28        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  65166 MiB |  70963 MiB | 162818 GiB | 162754 GiB |
|       from large pool |  65141 MiB |  70938 MiB | 162356 GiB | 162293 GiB |
|       from small pool |     24 MiB |     28 MiB |    461 GiB |    461 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  65166 MiB |  70963 MiB | 162818 GiB | 162754 GiB |
|       from large pool |  65141 MiB |  70938 MiB | 162356 GiB | 162293 GiB |
|       from small pool |     24 MiB |     28 MiB |    461 GiB |    461 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  65155 MiB |  70952 MiB | 162660 GiB | 162596 GiB |
|       from large pool |  65131 MiB |  70927 MiB | 162199 GiB | 162135 GiB |
|       from small pool |     24 MiB |     28 MiB |    461 GiB |    461 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78756 MiB |  80404 MiB | 581108 MiB | 502352 MiB |
|       from large pool |  78726 MiB |  80190 MiB | 579854 MiB | 501128 MiB |
|       from small pool |     30 MiB |    214 MiB |   1254 MiB |   1224 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   9785 MiB |  13210 MiB | 175068 GiB | 175059 GiB |
|       from large pool |   9780 MiB |  13204 MiB | 174538 GiB | 174528 GiB |
|       from small pool |      5 MiB |     23 MiB |    530 GiB |    530 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     450    |     459    |    5584 K  |    5584 K  |
|       from large pool |     163    |     172    |    2875 K  |    2875 K  |
|       from small pool |     287    |     356    |    2708 K  |    2708 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     450    |     459    |    5584 K  |    5584 K  |
|       from large pool |     163    |     172    |    2875 K  |    2875 K  |
|       from small pool |     287    |     356    |    2708 K  |    2708 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      69    |     245    |    2366    |    2297    |
|       from large pool |      54    |     138    |    1739    |    1685    |
|       from small pool |      15    |     107    |     627    |     612    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      75    |      75    |    3035 K  |    3035 K  |
|       from large pool |      54    |      54    |    1781 K  |    1781 K  |
|       from small pool |      21    |      58    |    1254 K  |    1254 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:18:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:18:53] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:19:02]    INFO >> epoch 002:   1275 / 1539 loss=3.749, wps=2872.8, ups=3.75, wpb=766.9, bsz=766.9, num_updates=2800, lr=0.0004, gnorm=5.462, clip=0, train_wall=12, gb_free=62.2, wall=804 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:19:15]    INFO >> epoch 002:   1325 / 1539 loss=3.788, wps=2832, ups=4.43, wpb=639.1, bsz=639.1, num_updates=2850, lr=0.0004, gnorm=5.143, clip=0, train_wall=11, gb_free=67.5, wall=816 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:19:27]    INFO >> epoch 002:   1375 / 1539 loss=3.829, wps=2920.2, ups=3.99, wpb=731.7, bsz=731.7, num_updates=2900, lr=0.0004, gnorm=4.958, clip=0, train_wall=12, gb_free=68.7, wall=828 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:19:39]    INFO >> epoch 002:   1425 / 1539 loss=3.723, wps=2873, ups=4.41, wpb=651.9, bsz=651.9, num_updates=2950, lr=0.0004, gnorm=5.453, clip=0, train_wall=11, gb_free=66.8, wall=839 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:19:42] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.14 GiB. GPU 2 has a total capacity of 79.14 GiB of which 463.25 MiB is free. Including non-PyTorch memory, this process has 78.66 GiB memory in use. Of the allocated memory 72.69 GiB is allocated by PyTorch, and 5.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:19:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:19:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:19:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 15           |        cudaMalloc retries: 29        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  67280 MiB |  74436 MiB | 173782 GiB | 173717 GiB |
|       from large pool |  67253 MiB |  74409 MiB | 173291 GiB | 173225 GiB |
|       from small pool |     27 MiB |     39 MiB |    491 GiB |    491 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  67280 MiB |  74436 MiB | 173782 GiB | 173717 GiB |
|       from large pool |  67253 MiB |  74409 MiB | 173291 GiB | 173225 GiB |
|       from small pool |     27 MiB |     39 MiB |    491 GiB |    491 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  67268 MiB |  74423 MiB | 173614 GiB | 173548 GiB |
|       from large pool |  67241 MiB |  74396 MiB | 173123 GiB | 173057 GiB |
|       from small pool |     27 MiB |     39 MiB |    491 GiB |    491 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80042 MiB |  80324 MiB | 586480 MiB | 506438 MiB |
|       from large pool |  80010 MiB |  80190 MiB | 585122 MiB | 505112 MiB |
|       from small pool |     32 MiB |    134 MiB |   1358 MiB |   1326 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   7493 MiB |  10544 MiB | 187946 GiB | 187939 GiB |
|       from large pool |   7488 MiB |  10539 MiB | 187382 GiB | 187374 GiB |
|       from small pool |      4 MiB |     23 MiB |    564 GiB |    564 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     489    |     498    |    5961 K  |    5960 K  |
|       from large pool |     195    |     204    |    3079 K  |    3079 K  |
|       from small pool |     294    |     356    |    2881 K  |    2881 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     489    |     498    |    5961 K  |    5960 K  |
|       from large pool |     195    |     204    |    3079 K  |    3079 K  |
|       from small pool |     294    |     356    |    2881 K  |    2881 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      67    |     121    |    2419    |    2352    |
|       from large pool |      51    |      54    |    1740    |    1689    |
|       from small pool |      16    |      67    |     679    |     663    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      68    |      69    |    3234 K  |    3234 K  |
|       from large pool |      43    |      44    |    1903 K  |    1903 K  |
|       from small pool |      25    |      52    |    1330 K  |    1330 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:19:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:19:42] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:19:53]    INFO >> epoch 002:   1476 / 1539 loss=3.757, wps=2608.4, ups=3.93, wpb=664.5, bsz=664.5, num_updates=3000, lr=0.0004, gnorm=5.348, clip=0, train_wall=11, gb_free=70.1, wall=852 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:19:56] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.53 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.19 GiB is free. Including non-PyTorch memory, this process has 77.93 GiB memory in use. Of the allocated memory 76.08 GiB is allocated by PyTorch, and 1.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:19:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:19:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:19:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 16           |        cudaMalloc retries: 30        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72887 MiB |  77908 MiB | 176495 GiB | 176424 GiB |
|       from large pool |  72860 MiB |  77881 MiB | 175997 GiB | 175925 GiB |
|       from small pool |     26 MiB |     37 MiB |    498 GiB |    498 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72887 MiB |  77908 MiB | 176495 GiB | 176424 GiB |
|       from large pool |  72860 MiB |  77881 MiB | 175997 GiB | 175925 GiB |
|       from small pool |     26 MiB |     37 MiB |    498 GiB |    498 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72874 MiB |  77895 MiB | 176324 GiB | 176253 GiB |
|       from large pool |  72847 MiB |  77867 MiB | 175826 GiB | 175755 GiB |
|       from small pool |     26 MiB |     37 MiB |    497 GiB |    497 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79286 MiB |  79320 MiB | 591026 MiB | 511740 MiB |
|       from large pool |  79256 MiB |  79256 MiB | 589636 MiB | 510380 MiB |
|       from small pool |     30 MiB |     64 MiB |   1390 MiB |   1360 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1884 MiB |   9213 MiB | 191124 GiB | 191122 GiB |
|       from large pool |   1881 MiB |   9209 MiB | 190551 GiB | 190550 GiB |
|       from small pool |      3 MiB |     22 MiB |    572 GiB |    572 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     567    |     576    |    6049 K  |    6048 K  |
|       from large pool |     266    |     274    |    3128 K  |    3128 K  |
|       from small pool |     301    |     356    |    2920 K  |    2920 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     567    |     576    |    6049 K  |    6048 K  |
|       from large pool |     266    |     274    |    3128 K  |    3128 K  |
|       from small pool |     301    |     356    |    2920 K  |    2920 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      67    |      84    |    2437    |    2370    |
|       from large pool |      52    |      52    |    1742    |    1690    |
|       from small pool |      15    |      32    |     695    |     680    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      79    |      79    |    3280 K  |    3280 K  |
|       from large pool |      54    |      54    |    1933 K  |    1932 K  |
|       from small pool |      25    |      52    |    1347 K  |    1347 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:19:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:19:56] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:20:06]    INFO >> epoch 002:   1527 / 1539 loss=3.676, wps=2579.5, ups=3.95, wpb=653.8, bsz=653.8, num_updates=3050, lr=0.0004, gnorm=5.435, clip=2, train_wall=11, gb_free=59.3, wall=865 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:20:09]    INFO >> epoch 002 | loss 3.844 | wps 2635.2 | ups 3.75 | wpb 702.8 | bsz 702.8 | num_updates 3062 | lr 0.0004 | gnorm 5.678 | clip 0.3 | train_wall 358 | gb_free 65 | wall 868 (progress_bar.py:267, print())[0m
[33m[2025-11-21 02:20:09] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 02:20:36]    INFO >> epoch 002 | valid on 'valid' subset | loss 3.813 | wps 5885.4 | wpb 5412.5 | bsz 5412.5 | num_updates 3062 | best_loss 4.012 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 02:20:36]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 02:20:36]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/checkpoints/checkpoint_last.pt (epoch 2 @ 3062 updates, score 3.813) (writing took 0.029649 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 02:20:36] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 02:20:45]    INFO >> epoch 003:     38 / 1539 loss=3.787, wps=919.8, ups=1.3, wpb=704.9, bsz=704.9, num_updates=3100, lr=0.000392, gnorm=6.116, clip=0, train_wall=11, gb_free=64.3, wall=903 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:20:59]    INFO >> epoch 003:     88 / 1539 loss=3.851, wps=2959.5, ups=3.95, wpb=749.4, bsz=749.4, num_updates=3150, lr=0.000392, gnorm=5.097, clip=0, train_wall=12, gb_free=65.2, wall=916 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:21:01] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.33 GiB. GPU 2 has a total capacity of 79.14 GiB of which 339.25 MiB is free. Including non-PyTorch memory, this process has 78.79 GiB memory in use. Of the allocated memory 76.05 GiB is allocated by PyTorch, and 2.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:21:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:21:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:21:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 17           |        cudaMalloc retries: 31        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77076 MiB |  78539 MiB | 192953 GiB | 192878 GiB |
|       from large pool |  77052 MiB |  78515 MiB | 192396 GiB | 192320 GiB |
|       from small pool |     23 MiB |     33 MiB |    557 GiB |    557 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77076 MiB |  78539 MiB | 192953 GiB | 192878 GiB |
|       from large pool |  77052 MiB |  78515 MiB | 192396 GiB | 192320 GiB |
|       from small pool |     23 MiB |     33 MiB |    557 GiB |    557 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77063 MiB |  78526 MiB | 192770 GiB | 192695 GiB |
|       from large pool |  77040 MiB |  78502 MiB | 192213 GiB | 192138 GiB |
|       from small pool |     23 MiB |     33 MiB |    556 GiB |    556 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80166 MiB |  80288 MiB | 596542 MiB | 516376 MiB |
|       from large pool |  80134 MiB |  80196 MiB | 595090 MiB | 514956 MiB |
|       from small pool |     32 MiB |     92 MiB |   1452 MiB |   1420 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3089 MiB |   8262 MiB | 205215 GiB | 205212 GiB |
|       from large pool |   3081 MiB |   8252 MiB | 204580 GiB | 204577 GiB |
|       from small pool |      8 MiB |     21 MiB |    635 GiB |    635 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     488    |     496    |    6604 K  |    6604 K  |
|       from large pool |     200    |     208    |    3317 K  |    3317 K  |
|       from small pool |     288    |     356    |    3286 K  |    3286 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     488    |     496    |    6604 K  |    6604 K  |
|       from large pool |     200    |     208    |    3317 K  |    3317 K  |
|       from small pool |     288    |     356    |    3286 K  |    3286 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      66    |      97    |    2469    |    2403    |
|       from large pool |      50    |      51    |    1743    |    1693    |
|       from small pool |      16    |      46    |     726    |     710    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      74    |      74    |    3605 K  |    3604 K  |
|       from large pool |      51    |      51    |    2054 K  |    2054 K  |
|       from small pool |      23    |      55    |    1550 K  |    1550 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:21:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:21:01] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:21:12]    INFO >> epoch 003:    139 / 1539 loss=3.714, wps=3233.7, ups=3.9, wpb=828.3, bsz=828.3, num_updates=3200, lr=0.000392, gnorm=5.333, clip=2, train_wall=11, gb_free=66.3, wall=929 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:21:25]    INFO >> epoch 003:    189 / 1539 loss=3.826, wps=2682.3, ups=4, wpb=670.6, bsz=670.6, num_updates=3250, lr=0.000392, gnorm=5.849, clip=2, train_wall=12, gb_free=61.9, wall=941 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:21:31] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.71 GiB. GPU 2 has a total capacity of 79.14 GiB of which 339.25 MiB is free. Including non-PyTorch memory, this process has 78.79 GiB memory in use. Of the allocated memory 69.30 GiB is allocated by PyTorch, and 8.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:21:31] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:21:31] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:21:31] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 18           |        cudaMalloc retries: 32        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  65166 MiB |  70962 MiB | 199474 GiB | 199410 GiB |
|       from large pool |  65141 MiB |  70938 MiB | 198896 GiB | 198832 GiB |
|       from small pool |     24 MiB |     35 MiB |    577 GiB |    577 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  65166 MiB |  70962 MiB | 199474 GiB | 199410 GiB |
|       from large pool |  65141 MiB |  70938 MiB | 198896 GiB | 198832 GiB |
|       from small pool |     24 MiB |     35 MiB |    577 GiB |    577 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  65155 MiB |  70952 MiB | 199285 GiB | 199221 GiB |
|       from large pool |  65131 MiB |  70927 MiB | 198708 GiB | 198644 GiB |
|       from small pool |     24 MiB |     35 MiB |    577 GiB |    577 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80166 MiB |  80352 MiB | 596728 MiB | 516562 MiB |
|       from large pool |  80134 MiB |  80134 MiB | 595090 MiB | 514956 MiB |
|       from small pool |     32 MiB |    218 MiB |   1638 MiB |   1606 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   9545 MiB |  12909 MiB | 212998 GiB | 212989 GiB |
|       from large pool |   9538 MiB |  12902 MiB | 212339 GiB | 212330 GiB |
|       from small pool |      7 MiB |     19 MiB |    659 GiB |    659 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     450    |     459    |    6843 K  |    6843 K  |
|       from large pool |     163    |     172    |    3436 K  |    3436 K  |
|       from small pool |     287    |     356    |    3407 K  |    3407 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     450    |     459    |    6843 K  |    6843 K  |
|       from large pool |     163    |     172    |    3436 K  |    3436 K  |
|       from small pool |     287    |     356    |    3407 K  |    3407 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      66    |     159    |    2562    |    2496    |
|       from large pool |      50    |      50    |    1743    |    1693    |
|       from small pool |      16    |     109    |     819    |     803    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      70    |      71    |    3734 K  |    3734 K  |
|       from large pool |      49    |      50    |    2124 K  |    2124 K  |
|       from small pool |      21    |      48    |    1610 K  |    1610 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:21:31] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:21:31] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:21:39]    INFO >> epoch 003:    240 / 1539 loss=3.588, wps=3002, ups=3.82, wpb=784.9, bsz=784.9, num_updates=3300, lr=0.000392, gnorm=5.444, clip=0, train_wall=12, gb_free=64.4, wall=954 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:21:52]    INFO >> epoch 003:    290 / 1539 loss=3.68, wps=2882.8, ups=3.91, wpb=737.7, bsz=737.7, num_updates=3350, lr=0.000392, gnorm=5.966, clip=0, train_wall=12, gb_free=66.4, wall=967 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:21:57] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 3.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.94 GiB is allocated by PyTorch, and 693.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:21:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:21:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:21:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 19           |        cudaMalloc retries: 33        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79749 MiB |  79808 MiB | 205292 GiB | 205214 GiB |
|       from large pool |  79653 MiB |  79712 MiB | 204698 GiB | 204620 GiB |
|       from small pool |     95 MiB |     97 MiB |    594 GiB |    594 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79749 MiB |  79808 MiB | 205292 GiB | 205214 GiB |
|       from large pool |  79653 MiB |  79712 MiB | 204698 GiB | 204620 GiB |
|       from small pool |     95 MiB |     97 MiB |    594 GiB |    594 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79729 MiB |  79788 MiB | 205098 GiB | 205021 GiB |
|       from large pool |  79633 MiB |  79692 MiB | 204505 GiB | 204427 GiB |
|       from small pool |     95 MiB |     96 MiB |    593 GiB |    593 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80502 MiB |  80502 MiB | 602518 MiB | 522016 MiB |
|       from large pool |  80396 MiB |  80396 MiB | 600806 MiB | 520410 MiB |
|       from small pool |    106 MiB |    106 MiB |   1712 MiB |   1606 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory | 709316 KiB |   6475 MiB | 219900 GiB | 219899 GiB |
|       from large pool | 698892 KiB |   6472 MiB | 219222 GiB | 219221 GiB |
|       from small pool |  10423 KiB |     20 MiB |    678 GiB |    678 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1873    |    1876    |    7043 K  |    7042 K  |
|       from large pool |     456    |     457    |    3542 K  |    3542 K  |
|       from small pool |    1417    |    1420    |    3501 K  |    3499 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1873    |    1876    |    7043 K  |    7042 K  |
|       from large pool |     456    |     457    |    3542 K  |    3542 K  |
|       from small pool |    1417    |    1420    |    3501 K  |    3499 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     170    |     170    |    2667    |    2497    |
|       from large pool |     117    |     117    |    1811    |    1694    |
|       from small pool |      53    |      53    |     856    |     803    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     193    |     194    |    3839 K  |    3839 K  |
|       from large pool |     109    |     110    |    2187 K  |    2187 K  |
|       from small pool |      84    |      84    |    1651 K  |    1651 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:21:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:21:57] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:22:06]    INFO >> epoch 003:    341 / 1539 loss=3.841, wps=2599.3, ups=3.81, wpb=681.6, bsz=681.6, num_updates=3400, lr=0.000392, gnorm=4.823, clip=0, train_wall=12, gb_free=64.1, wall=980 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:22:19]    INFO >> epoch 003:    391 / 1539 loss=3.724, wps=2509.5, ups=3.8, wpb=660.7, bsz=660.7, num_updates=3450, lr=0.000392, gnorm=4.883, clip=0, train_wall=13, gb_free=59.1, wall=993 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:22:32]    INFO >> epoch 003:    441 / 1539 loss=3.692, wps=2700.1, ups=4.02, wpb=672.2, bsz=672.2, num_updates=3500, lr=0.000392, gnorm=5.145, clip=0, train_wall=12, gb_free=67.3, wall=1006 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:22:47]    INFO >> epoch 003:    491 / 1539 loss=3.565, wps=2954, ups=3.74, wpb=790.6, bsz=790.6, num_updates=3550, lr=0.000392, gnorm=5.026, clip=0, train_wall=13, gb_free=66, wall=1019 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:22:59]    INFO >> epoch 003:    541 / 1539 loss=3.541, wps=2939.3, ups=4.07, wpb=722.3, bsz=722.3, num_updates=3600, lr=0.000392, gnorm=5.384, clip=2, train_wall=12, gb_free=65, wall=1031 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:23:13]    INFO >> epoch 003:    591 / 1539 loss=3.689, wps=2710.2, ups=3.87, wpb=700.5, bsz=700.5, num_updates=3650, lr=0.000392, gnorm=5.475, clip=0, train_wall=12, gb_free=64.3, wall=1044 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:23:25]    INFO >> epoch 003:    641 / 1539 loss=3.659, wps=2947.1, ups=4.12, wpb=714.5, bsz=714.5, num_updates=3700, lr=0.000392, gnorm=5.704, clip=0, train_wall=11, gb_free=65.2, wall=1056 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:23:38]    INFO >> epoch 003:    691 / 1539 loss=3.772, wps=2864.3, ups=3.87, wpb=740.7, bsz=740.7, num_updates=3750, lr=0.000392, gnorm=4.794, clip=0, train_wall=12, gb_free=55.6, wall=1069 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:23:54]    INFO >> epoch 003:    741 / 1539 loss=3.497, wps=2824, ups=3.51, wpb=804.4, bsz=804.4, num_updates=3800, lr=0.000392, gnorm=5.494, clip=0, train_wall=14, gb_free=65.5, wall=1084 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:24:02] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 1.25 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 73.15 GiB is allocated by PyTorch, and 5.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:24:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:24:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:24:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 20           |        cudaMalloc retries: 34        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  74902 MiB |  74962 MiB | 231935 GiB | 231861 GiB |
|       from large pool |  74747 MiB |  74808 MiB | 231267 GiB | 231194 GiB |
|       from small pool |    154 MiB |    155 MiB |    667 GiB |    667 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  74902 MiB |  74962 MiB | 231935 GiB | 231861 GiB |
|       from large pool |  74747 MiB |  74808 MiB | 231267 GiB | 231194 GiB |
|       from small pool |    154 MiB |    155 MiB |    667 GiB |    667 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  74880 MiB |  74940 MiB | 231714 GiB | 231641 GiB |
|       from large pool |  74726 MiB |  74786 MiB | 231047 GiB | 230974 GiB |
|       from small pool |    154 MiB |    155 MiB |    666 GiB |    666 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80504 MiB |  80504 MiB | 602580 MiB | 522076 MiB |
|       from large pool |  80336 MiB |  80336 MiB | 600806 MiB | 520470 MiB |
|       from small pool |    168 MiB |    168 MiB |   1774 MiB |   1606 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5601 MiB |   9448 MiB | 248344 GiB | 248339 GiB |
|       from large pool |   5588 MiB |   9445 MiB | 247581 GiB | 247575 GiB |
|       from small pool |     13 MiB |     19 MiB |    763 GiB |    763 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2903    |    2904    |    7951 K  |    7948 K  |
|       from large pool |     542    |     543    |    4023 K  |    4022 K  |
|       from small pool |    2361    |    2362    |    3928 K  |    3926 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2903    |    2904    |    7951 K  |    7948 K  |
|       from large pool |     542    |     543    |    4023 K  |    4022 K  |
|       from small pool |    2361    |    2362    |    3928 K  |    3926 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     200    |     200    |    2698    |    2498    |
|       from large pool |     116    |     116    |    1811    |    1695    |
|       from small pool |      84    |      84    |     887    |     803    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     225    |     227    |    4343 K  |    4343 K  |
|       from large pool |      93    |      94    |    2498 K  |    2498 K  |
|       from small pool |     132    |     134    |    1845 K  |    1845 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:24:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:24:02] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 02:24:05] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.69 GiB. GPU 2 has a total capacity of 79.14 GiB of which 3.42 GiB is free. Including non-PyTorch memory, this process has 75.70 GiB memory in use. Of the allocated memory 70.17 GiB is allocated by PyTorch, and 5.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:24:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:24:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:24:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 21           |        cudaMalloc retries: 35        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  71059 MiB |  71851 MiB | 232422 GiB | 232352 GiB |
|       from large pool |  71030 MiB |  71822 MiB | 231753 GiB | 231684 GiB |
|       from small pool |     28 MiB |     32 MiB |    668 GiB |    668 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  71059 MiB |  71851 MiB | 232422 GiB | 232352 GiB |
|       from large pool |  71030 MiB |  71822 MiB | 231753 GiB | 231684 GiB |
|       from small pool |     28 MiB |     32 MiB |    668 GiB |    668 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  71047 MiB |  71838 MiB | 232201 GiB | 232131 GiB |
|       from large pool |  71018 MiB |  71809 MiB | 231533 GiB | 231464 GiB |
|       from small pool |     28 MiB |     32 MiB |    667 GiB |    667 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77008 MiB |  80504 MiB | 602580 MiB | 525572 MiB |
|       from large pool |  76976 MiB |  80336 MiB | 600806 MiB | 523830 MiB |
|       from small pool |     32 MiB |    168 MiB |   1774 MiB |   1742 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5948 MiB |  12288 MiB | 248879 GiB | 248873 GiB |
|       from large pool |   5945 MiB |  12284 MiB | 248114 GiB | 248108 GiB |
|       from small pool |      3 MiB |     23 MiB |    764 GiB |    764 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     489    |     496    |    7965 K  |    7964 K  |
|       from large pool |     195    |     202    |    4030 K  |    4030 K  |
|       from small pool |     294    |     356    |    3934 K  |    3934 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     489    |     496    |    7965 K  |    7964 K  |
|       from large pool |     195    |     202    |    4030 K  |    4030 K  |
|       from small pool |     294    |     356    |    3934 K  |    3934 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      76    |     200    |    2698    |    2622    |
|       from large pool |      60    |     116    |    1811    |    1751    |
|       from small pool |      16    |      84    |     887    |     871    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      83    |      83    |    4351 K  |    4351 K  |
|       from large pool |      60    |      60    |    2503 K  |    2503 K  |
|       from small pool |      23    |      56    |    1848 K  |    1848 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:24:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:24:05] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:24:07]    INFO >> epoch 003:    793 / 1539 loss=3.621, wps=2537.1, ups=3.86, wpb=656.8, bsz=656.8, num_updates=3850, lr=0.000392, gnorm=5.316, clip=0, train_wall=11, gb_free=65.3, wall=1097 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:24:20]    INFO >> epoch 003:    843 / 1539 loss=3.668, wps=3036.8, ups=4.02, wpb=755.9, bsz=755.9, num_updates=3900, lr=0.000392, gnorm=5.769, clip=0, train_wall=12, gb_free=65.4, wall=1109 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:24:33]    INFO >> epoch 003:    893 / 1539 loss=3.711, wps=2778.2, ups=3.85, wpb=721.4, bsz=721.4, num_updates=3950, lr=0.000392, gnorm=5.345, clip=0, train_wall=12, gb_free=48.2, wall=1122 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:24:46]    INFO >> epoch 003:    943 / 1539 loss=3.627, wps=2851.9, ups=3.92, wpb=727.2, bsz=727.2, num_updates=4000, lr=0.000392, gnorm=5.595, clip=0, train_wall=12, gb_free=62.3, wall=1135 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:24:47] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.14 GiB. GPU 2 has a total capacity of 79.14 GiB of which 3.09 GiB is free. Including non-PyTorch memory, this process has 76.03 GiB memory in use. Of the allocated memory 72.69 GiB is allocated by PyTorch, and 2.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:24:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:24:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:24:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 22           |        cudaMalloc retries: 36        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  67281 MiB |  74436 MiB | 241574 GiB | 241508 GiB |
|       from large pool |  67253 MiB |  74409 MiB | 240881 GiB | 240815 GiB |
|       from small pool |     27 MiB |     31 MiB |    692 GiB |    692 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  67281 MiB |  74436 MiB | 241574 GiB | 241508 GiB |
|       from large pool |  67253 MiB |  74409 MiB | 240881 GiB | 240815 GiB |
|       from small pool |     27 MiB |     31 MiB |    692 GiB |    692 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  67268 MiB |  74423 MiB | 241344 GiB | 241278 GiB |
|       from large pool |  67241 MiB |  74396 MiB | 240652 GiB | 240587 GiB |
|       from small pool |     27 MiB |     31 MiB |    691 GiB |    691 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77344 MiB |  78364 MiB | 603936 MiB | 526592 MiB |
|       from large pool |  77312 MiB |  78294 MiB | 602124 MiB | 524812 MiB |
|       from small pool |     32 MiB |     70 MiB |   1812 MiB |   1780 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2728 MiB |  11560 MiB | 259369 GiB | 259367 GiB |
|       from large pool |   2724 MiB |  11557 MiB | 258576 GiB | 258574 GiB |
|       from small pool |      4 MiB |     23 MiB |    792 GiB |    792 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     489    |     498    |    8269 K  |    8268 K  |
|       from large pool |     195    |     204    |    4194 K  |    4194 K  |
|       from small pool |     294    |     356    |    4074 K  |    4074 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     489    |     498    |    8269 K  |    8268 K  |
|       from large pool |     195    |     204    |    4194 K  |    4194 K  |
|       from small pool |     294    |     356    |    4074 K  |    4074 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      68    |      96    |    2718    |    2650    |
|       from large pool |      52    |      61    |    1812    |    1760    |
|       from small pool |      16    |      35    |     906    |     890    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      66    |      68    |    4512 K  |    4511 K  |
|       from large pool |      43    |      45    |    2602 K  |    2601 K  |
|       from small pool |      23    |      54    |    1910 K  |    1910 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:24:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:24:47] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 02:24:54] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.53 GiB. GPU 2 has a total capacity of 79.14 GiB of which 3.20 GiB is free. Including non-PyTorch memory, this process has 75.92 GiB memory in use. Of the allocated memory 71.68 GiB is allocated by PyTorch, and 3.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:24:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:24:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:24:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 23           |        cudaMalloc retries: 37        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72887 MiB |  73395 MiB | 242779 GiB | 242707 GiB |
|       from large pool |  72860 MiB |  73368 MiB | 242083 GiB | 242012 GiB |
|       from small pool |     26 MiB |     37 MiB |    695 GiB |    695 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72887 MiB |  73395 MiB | 242779 GiB | 242707 GiB |
|       from large pool |  72860 MiB |  73368 MiB | 242083 GiB | 242012 GiB |
|       from small pool |     26 MiB |     37 MiB |    695 GiB |    695 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72874 MiB |  73382 MiB | 242548 GiB | 242477 GiB |
|       from large pool |  72847 MiB |  73355 MiB | 241853 GiB | 241782 GiB |
|       from small pool |     26 MiB |     37 MiB |    694 GiB |    694 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77228 MiB |  77260 MiB | 611186 MiB | 533958 MiB |
|       from large pool |  77198 MiB |  77198 MiB | 609344 MiB | 532146 MiB |
|       from small pool |     30 MiB |     62 MiB |   1842 MiB |   1812 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4340 MiB |   9743 MiB | 260780 GiB | 260776 GiB |
|       from large pool |   4337 MiB |   9739 MiB | 259984 GiB | 259980 GiB |
|       from small pool |      3 MiB |     22 MiB |    796 GiB |    796 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     567    |     574    |    8307 K  |    8307 K  |
|       from large pool |     266    |     272    |    4216 K  |    4216 K  |
|       from small pool |     301    |     356    |    4091 K  |    4090 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     567    |     574    |    8307 K  |    8307 K  |
|       from large pool |     266    |     272    |    4216 K  |    4216 K  |
|       from small pool |     301    |     356    |    4091 K  |    4090 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      65    |      81    |    2735    |    2670    |
|       from large pool |      50    |      50    |    1814    |    1764    |
|       from small pool |      15    |      31    |     921    |     906    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      79    |      79    |    4531 K  |    4531 K  |
|       from large pool |      58    |      58    |    2614 K  |    2614 K  |
|       from small pool |      21    |      49    |    1916 K  |    1916 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:24:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:24:54] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:25:01]    INFO >> epoch 003:    995 / 1539 loss=3.619, wps=2339.3, ups=3.79, wpb=617.6, bsz=617.6, num_updates=4050, lr=0.000392, gnorm=5.501, clip=2, train_wall=11, gb_free=69.6, wall=1148 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:25:12]    INFO >> epoch 003:   1045 / 1539 loss=3.748, wps=2813.4, ups=4.29, wpb=656.6, bsz=656.6, num_updates=4100, lr=0.000392, gnorm=4.762, clip=0, train_wall=11, gb_free=61.3, wall=1160 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:25:26]    INFO >> epoch 003:   1095 / 1539 loss=3.689, wps=2841.4, ups=4.16, wpb=683.1, bsz=683.1, num_updates=4150, lr=0.000392, gnorm=4.712, clip=0, train_wall=11, gb_free=60.9, wall=1172 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:25:34] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 588.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 337.25 MiB is free. Including non-PyTorch memory, this process has 78.79 GiB memory in use. Of the allocated memory 76.35 GiB is allocated by PyTorch, and 1.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:25:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:25:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:25:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 24           |        cudaMalloc retries: 38        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77863 MiB |  78185 MiB | 251362 GiB | 251286 GiB |
|       from large pool |  77833 MiB |  78155 MiB | 250643 GiB | 250567 GiB |
|       from small pool |     30 MiB |     33 MiB |    718 GiB |    718 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77863 MiB |  78185 MiB | 251362 GiB | 251286 GiB |
|       from large pool |  77833 MiB |  78155 MiB | 250643 GiB | 250567 GiB |
|       from small pool |     30 MiB |     33 MiB |    718 GiB |    718 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77846 MiB |  78168 MiB | 251123 GiB | 251047 GiB |
|       from large pool |  77816 MiB |  78138 MiB | 250405 GiB | 250329 GiB |
|       from small pool |     30 MiB |     33 MiB |    718 GiB |    718 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80168 MiB |  80206 MiB | 614164 MiB | 533996 MiB |
|       from large pool |  80136 MiB |  80136 MiB | 612282 MiB | 532146 MiB |
|       from small pool |     32 MiB |     70 MiB |   1882 MiB |   1850 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2304 MiB |   7429 MiB | 271033 GiB | 271031 GiB |
|       from large pool |   2302 MiB |   7426 MiB | 270210 GiB | 270208 GiB |
|       from small pool |      1 MiB |     24 MiB |    823 GiB |    823 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     623    |     626    |    8603 K  |    8602 K  |
|       from large pool |     319    |     322    |    4378 K  |    4378 K  |
|       from small pool |     304    |     348    |    4224 K  |    4224 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     623    |     626    |    8603 K  |    8602 K  |
|       from large pool |     319    |     322    |    4378 K  |    4378 K  |
|       from small pool |     304    |     348    |    4224 K  |    4224 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      70    |      89    |    2759    |    2689    |
|       from large pool |      54    |      54    |    1818    |    1764    |
|       from small pool |      16    |      35    |     941    |     925    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      76    |      79    |    4685 K  |    4685 K  |
|       from large pool |      55    |      58    |    2711 K  |    2711 K  |
|       from small pool |      21    |      53    |    1973 K  |    1973 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:25:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:25:34] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:25:38]    INFO >> epoch 003:   1146 / 1539 loss=3.616, wps=2659.5, ups=4.16, wpb=639.3, bsz=639.3, num_updates=4200, lr=0.000392, gnorm=4.934, clip=0, train_wall=11, gb_free=72.5, wall=1184 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:25:51]    INFO >> epoch 003:   1196 / 1539 loss=3.621, wps=2656.6, ups=3.81, wpb=697.2, bsz=697.2, num_updates=4250, lr=0.000392, gnorm=4.288, clip=0, train_wall=12, gb_free=57.4, wall=1197 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:26:05]    INFO >> epoch 003:   1246 / 1539 loss=3.667, wps=2825.6, ups=4.03, wpb=701.3, bsz=701.3, num_updates=4300, lr=0.000392, gnorm=4.923, clip=0, train_wall=12, gb_free=68.6, wall=1209 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:26:16]    INFO >> epoch 003:   1296 / 1539 loss=3.563, wps=2614.1, ups=4.4, wpb=593.7, bsz=593.7, num_updates=4350, lr=0.000392, gnorm=5.091, clip=0, train_wall=11, gb_free=66.7, wall=1221 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:26:30]    INFO >> epoch 003:   1346 / 1539 loss=3.582, wps=3144, ups=4.05, wpb=775.5, bsz=775.5, num_updates=4400, lr=0.000392, gnorm=5.349, clip=0, train_wall=12, gb_free=65.6, wall=1233 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:26:41]    INFO >> epoch 003:   1396 / 1539 loss=3.595, wps=2975.6, ups=4.46, wpb=667.8, bsz=667.8, num_updates=4450, lr=0.000392, gnorm=4.545, clip=0, train_wall=11, gb_free=67.1, wall=1244 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:26:52]    INFO >> epoch 003:   1446 / 1539 loss=3.627, wps=2847.7, ups=4.37, wpb=652.3, bsz=652.3, num_updates=4500, lr=0.000392, gnorm=4.835, clip=0, train_wall=11, gb_free=67.5, wall=1256 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:27:07]    INFO >> epoch 003:   1496 / 1539 loss=3.618, wps=2573, ups=3.83, wpb=671.1, bsz=671.1, num_updates=4550, lr=0.000392, gnorm=5.263, clip=0, train_wall=12, gb_free=63.9, wall=1269 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:27:17]    INFO >> epoch 003 | loss 3.664 | wps 2619.8 | ups 3.73 | wpb 702.8 | bsz 702.8 | num_updates 4593 | lr 0.000392 | gnorm 5.188 | clip 0.3 | train_wall 361 | gb_free 68.9 | wall 1279 (progress_bar.py:267, print())[0m
[33m[2025-11-21 02:27:17] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 02:27:45]    INFO >> epoch 003 | valid on 'valid' subset | loss 3.781 | wps 5641.6 | wpb 5412.5 | bsz 5412.5 | num_updates 4593 | best_loss 4.012 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 02:27:45]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 02:27:46]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/checkpoints/checkpoint_last.pt (epoch 3 @ 4593 updates, score 3.781) (writing took 0.030802 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 02:27:46] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 02:27:47]    INFO >> epoch 004:      7 / 1539 loss=3.68, wps=819.6, ups=1.28, wpb=639.9, bsz=639.9, num_updates=4600, lr=0.000376, gnorm=4.871, clip=0, train_wall=11, gb_free=62.2, wall=1308 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:28:00]    INFO >> epoch 004:     57 / 1539 loss=3.433, wps=3008.2, ups=3.84, wpb=783.2, bsz=783.2, num_updates=4650, lr=0.000376, gnorm=5.207, clip=0, train_wall=12, gb_free=67, wall=1321 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:28:13] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 29.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.41 GiB is allocated by PyTorch, and 1.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:28:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:28:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:28:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 25           |        cudaMalloc retries: 39        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79207 MiB |  79267 MiB | 287464 GiB | 287387 GiB |
|       from large pool |  79009 MiB |  79069 MiB | 286632 GiB | 286555 GiB |
|       from small pool |    198 MiB |    199 MiB |    831 GiB |    831 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79207 MiB |  79267 MiB | 287464 GiB | 287387 GiB |
|       from large pool |  79009 MiB |  79069 MiB | 286632 GiB | 286555 GiB |
|       from small pool |    198 MiB |    199 MiB |    831 GiB |    831 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79188 MiB |  79249 MiB | 287195 GiB | 287117 GiB |
|       from large pool |  78991 MiB |  79051 MiB | 286364 GiB | 286287 GiB |
|       from small pool |    197 MiB |    198 MiB |    830 GiB |    830 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80476 MiB |  80476 MiB | 614472 MiB | 533996 MiB |
|       from large pool |  80260 MiB |  80260 MiB | 612406 MiB | 532146 MiB |
|       from small pool |    216 MiB |    216 MiB |   2066 MiB |   1850 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1206 MiB |   7345 MiB | 307792 GiB | 307791 GiB |
|       from large pool |   1188 MiB |   7343 MiB | 306844 GiB | 306843 GiB |
|       from small pool |     17 MiB |     22 MiB |    948 GiB |    948 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3688    |    3691    |    9842 K  |    9838 K  |
|       from large pool |     613    |     614    |    4934 K  |    4934 K  |
|       from small pool |    3075    |    3078    |    4907 K  |    4904 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3688    |    3691    |    9842 K  |    9838 K  |
|       from large pool |     613    |     614    |    4934 K  |    4934 K  |
|       from small pool |    3075    |    3078    |    4907 K  |    4904 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     164    |     164    |    2853    |    2689    |
|       from large pool |      56    |      56    |    1820    |    1764    |
|       from small pool |     108    |     108    |    1033    |     925    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     237    |     238    |    5364 K  |    5363 K  |
|       from large pool |      63    |      64    |    3047 K  |    3047 K  |
|       from small pool |     174    |     174    |    2317 K  |    2316 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:28:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:28:13] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:28:16]    INFO >> epoch 004:    108 / 1539 loss=3.572, wps=2746.4, ups=3.38, wpb=811.4, bsz=811.4, num_updates=4700, lr=0.000376, gnorm=5.919, clip=2, train_wall=14, gb_free=63.9, wall=1335 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:28:31]    INFO >> epoch 004:    158 / 1539 loss=3.456, wps=2874.7, ups=3.35, wpb=858, bsz=858, num_updates=4750, lr=0.000376, gnorm=5.486, clip=0, train_wall=14, gb_free=54.1, wall=1350 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:28:45]    INFO >> epoch 004:    208 / 1539 loss=3.483, wps=2926.7, ups=4, wpb=732.5, bsz=732.5, num_updates=4800, lr=0.000376, gnorm=4.933, clip=0, train_wall=12, gb_free=68.7, wall=1363 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:28:57]    INFO >> epoch 004:    258 / 1539 loss=3.734, wps=2764, ups=4.04, wpb=683.6, bsz=683.6, num_updates=4850, lr=0.000376, gnorm=5.573, clip=2, train_wall=12, gb_free=67.5, wall=1375 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:29:09]    INFO >> epoch 004:    308 / 1539 loss=3.58, wps=3067.3, ups=4.43, wpb=692.3, bsz=692.3, num_updates=4900, lr=0.000376, gnorm=4.184, clip=0, train_wall=11, gb_free=65, wall=1387 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:29:22]    INFO >> epoch 004:    358 / 1539 loss=3.544, wps=3109.7, ups=4.21, wpb=738.7, bsz=738.7, num_updates=4950, lr=0.000376, gnorm=4.951, clip=0, train_wall=11, gb_free=65.5, wall=1398 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:29:34]    INFO >> epoch 004:    408 / 1539 loss=3.603, wps=2777.6, ups=4.15, wpb=669.8, bsz=669.8, num_updates=5000, lr=0.000376, gnorm=4.466, clip=0, train_wall=11, gb_free=66.1, wall=1410 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:29:38] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.53 GiB. GPU 2 has a total capacity of 79.14 GiB of which 277.25 MiB is free. Including non-PyTorch memory, this process has 78.85 GiB memory in use. Of the allocated memory 76.08 GiB is allocated by PyTorch, and 2.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:29:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:29:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:29:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 26           |        cudaMalloc retries: 40        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72887 MiB |  77908 MiB | 305940 GiB | 305869 GiB |
|       from large pool |  72860 MiB |  77881 MiB | 305055 GiB | 304983 GiB |
|       from small pool |     26 MiB |     36 MiB |    885 GiB |    885 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72887 MiB |  77908 MiB | 305940 GiB | 305869 GiB |
|       from large pool |  72860 MiB |  77881 MiB | 305055 GiB | 304983 GiB |
|       from small pool |     26 MiB |     36 MiB |    885 GiB |    885 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72874 MiB |  77895 MiB | 305653 GiB | 305582 GiB |
|       from large pool |  72847 MiB |  77867 MiB | 304769 GiB | 304697 GiB |
|       from small pool |     26 MiB |     36 MiB |    884 GiB |    884 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80228 MiB |  80416 MiB | 614474 MiB | 534246 MiB |
|       from large pool |  80198 MiB |  80198 MiB | 612406 MiB | 532208 MiB |
|       from small pool |     30 MiB |    218 MiB |   2068 MiB |   2038 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1886 MiB |  10861 MiB | 329437 GiB | 329435 GiB |
|       from large pool |   1883 MiB |  10857 MiB | 328426 GiB | 328424 GiB |
|       from small pool |      3 MiB |     18 MiB |   1011 GiB |   1011 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     567    |     576    |   10494 K  |   10494 K  |
|       from large pool |     266    |     274    |    5274 K  |    5273 K  |
|       from small pool |     301    |     356    |    5220 K  |    5220 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     567    |     576    |   10494 K  |   10494 K  |
|       from large pool |     266    |     274    |    5274 K  |    5273 K  |
|       from small pool |     301    |     356    |    5220 K  |    5220 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      70    |     164    |    2854    |    2784    |
|       from large pool |      55    |      55    |    1820    |    1765    |
|       from small pool |      15    |     109    |    1034    |    1019    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      85    |      85    |    5712 K  |    5712 K  |
|       from large pool |      60    |      60    |    3248 K  |    3248 K  |
|       from small pool |      25    |      56    |    2463 K  |    2463 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:29:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:29:38] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:29:47]    INFO >> epoch 004:    459 / 1539 loss=3.592, wps=2457.3, ups=3.74, wpb=657, bsz=657, num_updates=5050, lr=0.000376, gnorm=4.801, clip=0, train_wall=12, gb_free=57.5, wall=1424 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:30:01]    INFO >> epoch 004:    509 / 1539 loss=3.571, wps=2963.2, ups=4.01, wpb=739.2, bsz=739.2, num_updates=5100, lr=0.000376, gnorm=5.329, clip=2, train_wall=12, gb_free=59, wall=1436 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:30:14]    INFO >> epoch 004:    559 / 1539 loss=3.651, wps=2686.8, ups=4.03, wpb=666.2, bsz=666.2, num_updates=5150, lr=0.000376, gnorm=4.906, clip=0, train_wall=12, gb_free=62.3, wall=1449 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:30:23] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.69 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.90 GiB is free. Including non-PyTorch memory, this process has 77.21 GiB memory in use. Of the allocated memory 74.77 GiB is allocated by PyTorch, and 1.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:30:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:30:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:30:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 27           |        cudaMalloc retries: 41        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  71054 MiB |  76569 MiB | 315614 GiB | 315545 GiB |
|       from large pool |  71025 MiB |  76540 MiB | 314704 GiB | 314635 GiB |
|       from small pool |     28 MiB |     34 MiB |    909 GiB |    909 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  71054 MiB |  76569 MiB | 315614 GiB | 315545 GiB |
|       from large pool |  71025 MiB |  76540 MiB | 314704 GiB | 314635 GiB |
|       from small pool |     28 MiB |     34 MiB |    909 GiB |    909 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  71047 MiB |  76562 MiB | 315318 GiB | 315248 GiB |
|       from large pool |  71018 MiB |  76533 MiB | 314409 GiB | 314340 GiB |
|       from small pool |     28 MiB |     34 MiB |    908 GiB |    908 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78558 MiB |  78592 MiB | 618292 MiB | 539734 MiB |
|       from large pool |  78524 MiB |  78524 MiB | 616186 MiB | 537662 MiB |
|       from small pool |     34 MiB |     68 MiB |   2106 MiB |   2072 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3723 MiB |  11767 MiB | 340697 GiB | 340693 GiB |
|       from large pool |   3718 MiB |  11761 MiB | 339657 GiB | 339653 GiB |
|       from small pool |      5 MiB |     19 MiB |   1039 GiB |   1039 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     489    |     498    |   10812 K  |   10811 K  |
|       from large pool |     195    |     204    |    5450 K  |    5449 K  |
|       from small pool |     294    |     356    |    5362 K  |    5362 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     489    |     498    |   10812 K  |   10811 K  |
|       from large pool |     195    |     204    |    5450 K  |    5449 K  |
|       from small pool |     294    |     356    |    5362 K  |    5362 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      72    |      89    |    2874    |    2802    |
|       from large pool |      55    |      55    |    1821    |    1766    |
|       from small pool |      17    |      34    |    1053    |    1036    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      79    |      79    |    5876 K  |    5876 K  |
|       from large pool |      56    |      56    |    3353 K  |    3353 K  |
|       from small pool |      23    |      52    |    2522 K  |    2522 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:30:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:30:23] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:30:28]    INFO >> epoch 004:    610 / 1539 loss=3.597, wps=2450.1, ups=3.93, wpb=623, bsz=623, num_updates=5200, lr=0.000376, gnorm=4.61, clip=0, train_wall=11, gb_free=66.6, wall=1461 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:30:38]    INFO >> epoch 004:    660 / 1539 loss=3.507, wps=3040.7, ups=4.61, wpb=660.1, bsz=660.1, num_updates=5250, lr=0.000376, gnorm=4.731, clip=0, train_wall=10, gb_free=64.6, wall=1472 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:30:47] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.33 GiB. GPU 2 has a total capacity of 79.14 GiB of which 337.25 MiB is free. Including non-PyTorch memory, this process has 78.79 GiB memory in use. Of the allocated memory 76.05 GiB is allocated by PyTorch, and 2.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:30:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:30:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:30:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 28           |        cudaMalloc retries: 42        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77076 MiB |  78539 MiB | 320492 GiB | 320417 GiB |
|       from large pool |  77052 MiB |  78515 MiB | 319569 GiB | 319494 GiB |
|       from small pool |     23 MiB |     38 MiB |    923 GiB |    923 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77076 MiB |  78539 MiB | 320492 GiB | 320417 GiB |
|       from large pool |  77052 MiB |  78515 MiB | 319569 GiB | 319494 GiB |
|       from small pool |     23 MiB |     38 MiB |    923 GiB |    923 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77063 MiB |  78526 MiB | 320191 GiB | 320116 GiB |
|       from large pool |  77040 MiB |  78502 MiB | 319269 GiB | 319194 GiB |
|       from small pool |     23 MiB |     38 MiB |    921 GiB |    921 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80168 MiB |  80266 MiB | 623780 MiB | 543612 MiB |
|       from large pool |  80140 MiB |  80198 MiB | 621640 MiB | 541500 MiB |
|       from small pool |     28 MiB |     68 MiB |   2140 MiB |   2112 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3091 MiB |  11411 MiB | 346395 GiB | 346392 GiB |
|       from large pool |   3087 MiB |  11406 MiB | 345340 GiB | 345337 GiB |
|       from small pool |      4 MiB |     23 MiB |   1054 GiB |   1054 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     488    |     496    |   10981 K  |   10980 K  |
|       from large pool |     200    |     208    |    5543 K  |    5543 K  |
|       from small pool |     288    |     356    |    5438 K  |    5437 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     488    |     496    |   10981 K  |   10980 K  |
|       from large pool |     200    |     208    |    5543 K  |    5543 K  |
|       from small pool |     288    |     356    |    5438 K  |    5437 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      68    |      89    |    2892    |    2824    |
|       from large pool |      54    |      55    |    1822    |    1768    |
|       from small pool |      14    |      34    |    1070    |    1056    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      65    |      65    |    5963 K  |    5963 K  |
|       from large pool |      44    |      44    |    3409 K  |    3409 K  |
|       from small pool |      21    |      48    |    2554 K  |    2554 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:30:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:30:47] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:30:52]    INFO >> epoch 004:    711 / 1539 loss=3.612, wps=2503.8, ups=3.84, wpb=652.3, bsz=652.3, num_updates=5300, lr=0.000376, gnorm=5.524, clip=0, train_wall=12, gb_free=65.8, wall=1485 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:31:00] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.14 GiB. GPU 2 has a total capacity of 79.14 GiB of which 333.25 MiB is free. Including non-PyTorch memory, this process has 78.79 GiB memory in use. Of the allocated memory 72.69 GiB is allocated by PyTorch, and 5.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:31:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:31:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:31:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 29           |        cudaMalloc retries: 43        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  67280 MiB |  74436 MiB | 323097 GiB | 323031 GiB |
|       from large pool |  67252 MiB |  74409 MiB | 322167 GiB | 322101 GiB |
|       from small pool |     27 MiB |     32 MiB |    929 GiB |    929 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  67280 MiB |  74436 MiB | 323097 GiB | 323031 GiB |
|       from large pool |  67252 MiB |  74409 MiB | 322167 GiB | 322101 GiB |
|       from small pool |     27 MiB |     32 MiB |    929 GiB |    929 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  67268 MiB |  74423 MiB | 322793 GiB | 322727 GiB |
|       from large pool |  67241 MiB |  74396 MiB | 321865 GiB | 321799 GiB |
|       from small pool |     27 MiB |     32 MiB |    928 GiB |    928 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80172 MiB |  80208 MiB | 623820 MiB | 543648 MiB |
|       from large pool |  80140 MiB |  80140 MiB | 621640 MiB | 541500 MiB |
|       from small pool |     32 MiB |     68 MiB |   2180 MiB |   2148 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   7437 MiB |  11746 MiB | 349414 GiB | 349407 GiB |
|       from large pool |   7433 MiB |  11742 MiB | 348352 GiB | 348344 GiB |
|       from small pool |      4 MiB |     17 MiB |   1062 GiB |   1062 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     489    |     498    |   11065 K  |   11064 K  |
|       from large pool |     195    |     204    |    5590 K  |    5589 K  |
|       from small pool |     294    |     348    |    5474 K  |    5474 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     489    |     498    |   11065 K  |   11064 K  |
|       from large pool |     195    |     204    |    5590 K  |    5589 K  |
|       from small pool |     294    |     348    |    5474 K  |    5474 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      70    |      88    |    2912    |    2842    |
|       from large pool |      54    |      54    |    1822    |    1768    |
|       from small pool |      16    |      34    |    1090    |    1074    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      73    |      73    |    6006 K  |    6006 K  |
|       from large pool |      47    |      47    |    3437 K  |    3437 K  |
|       from small pool |      26    |      47    |    2569 K  |    2569 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:31:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:31:00] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:31:06]    INFO >> epoch 004:    762 / 1539 loss=3.518, wps=2539.5, ups=3.82, wpb=664.3, bsz=664.3, num_updates=5350, lr=0.000376, gnorm=3.732, clip=0, train_wall=12, gb_free=65.1, wall=1498 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:31:18]    INFO >> epoch 004:    812 / 1539 loss=3.5, wps=2786.3, ups=4.16, wpb=669.6, bsz=669.6, num_updates=5400, lr=0.000376, gnorm=4.257, clip=0, train_wall=11, gb_free=59.3, wall=1510 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:31:31]    INFO >> epoch 004:    862 / 1539 loss=3.427, wps=3116.3, ups=3.99, wpb=780.8, bsz=780.8, num_updates=5450, lr=0.000376, gnorm=5.108, clip=0, train_wall=12, gb_free=60.6, wall=1523 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:31:45]    INFO >> epoch 004:    912 / 1539 loss=3.603, wps=2763.6, ups=3.79, wpb=729.4, bsz=729.4, num_updates=5500, lr=0.000376, gnorm=5.286, clip=0, train_wall=13, gb_free=61.4, wall=1536 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:31:56]    INFO >> epoch 004:    962 / 1539 loss=3.497, wps=2889.7, ups=4.4, wpb=656.4, bsz=656.4, num_updates=5550, lr=0.000376, gnorm=4.205, clip=0, train_wall=11, gb_free=68.2, wall=1547 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:32:02] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 51.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 77.59 GiB is allocated by PyTorch, and 1001.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:32:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:32:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:32:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 30           |        cudaMalloc retries: 44        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79393 MiB |  79452 MiB | 336939 GiB | 336862 GiB |
|       from large pool |  79301 MiB |  79360 MiB | 335973 GiB | 335895 GiB |
|       from small pool |     92 MiB |     93 MiB |    966 GiB |    966 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79393 MiB |  79452 MiB | 336939 GiB | 336862 GiB |
|       from large pool |  79301 MiB |  79360 MiB | 335973 GiB | 335895 GiB |
|       from small pool |     92 MiB |     93 MiB |    966 GiB |    966 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79372 MiB |  79431 MiB | 336623 GiB | 336545 GiB |
|       from large pool |  79280 MiB |  79339 MiB | 335657 GiB | 335580 GiB |
|       from small pool |     91 MiB |     93 MiB |    965 GiB |    965 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80454 MiB |  80454 MiB | 629556 MiB | 549102 MiB |
|       from large pool |  80352 MiB |  80352 MiB | 627306 MiB | 546954 MiB |
|       from small pool |    102 MiB |    102 MiB |   2250 MiB |   2148 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1000 MiB |   6489 MiB | 365593 GiB | 365592 GiB |
|       from large pool |    990 MiB |   6485 MiB | 364488 GiB | 364487 GiB |
|       from small pool |      9 MiB |     20 MiB |   1105 GiB |   1105 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1807    |    1810    |   11531 K  |   11529 K  |
|       from large pool |     450    |     451    |    5842 K  |    5841 K  |
|       from small pool |    1357    |    1360    |    5689 K  |    5687 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1807    |    1810    |   11531 K  |   11529 K  |
|       from large pool |     450    |     451    |    5842 K  |    5841 K  |
|       from small pool |    1357    |    1360    |    5689 K  |    5687 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     166    |     166    |    3009    |    2843    |
|       from large pool |     115    |     115    |    1884    |    1769    |
|       from small pool |      51    |      51    |    1125    |    1074    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     200    |     201    |    6249 K  |    6248 K  |
|       from large pool |     118    |     119    |    3586 K  |    3585 K  |
|       from small pool |      82    |      83    |    2663 K  |    2662 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:32:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:32:02] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:32:11]    INFO >> epoch 004:   1013 / 1539 loss=3.639, wps=2537.3, ups=3.89, wpb=652.1, bsz=652.1, num_updates=5600, lr=0.000376, gnorm=4.815, clip=0, train_wall=12, gb_free=64.2, wall=1560 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:32:23]    INFO >> epoch 004:   1063 / 1539 loss=3.626, wps=2761.8, ups=4, wpb=690.7, bsz=690.7, num_updates=5650, lr=0.000376, gnorm=4.924, clip=0, train_wall=12, gb_free=31.1, wall=1573 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:32:35]    INFO >> epoch 004:   1113 / 1539 loss=3.55, wps=2659.2, ups=4.08, wpb=652.4, bsz=652.4, num_updates=5700, lr=0.000376, gnorm=4.365, clip=0, train_wall=12, gb_free=44.9, wall=1585 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:32:49]    INFO >> epoch 004:   1163 / 1539 loss=3.547, wps=2916.5, ups=4.03, wpb=724.3, bsz=724.3, num_updates=5750, lr=0.000376, gnorm=5.393, clip=2, train_wall=12, gb_free=54.7, wall=1598 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:33:01]    INFO >> epoch 004:   1213 / 1539 loss=3.645, wps=2878.9, ups=4.18, wpb=689.5, bsz=689.5, num_updates=5800, lr=0.000376, gnorm=5.027, clip=0, train_wall=11, gb_free=51, wall=1610 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:33:14]    INFO >> epoch 004:   1263 / 1539 loss=3.457, wps=2825, ups=4.29, wpb=659, bsz=659, num_updates=5850, lr=0.000376, gnorm=5.007, clip=0, train_wall=11, gb_free=61.2, wall=1621 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:33:27]    INFO >> epoch 004:   1313 / 1539 loss=3.447, wps=2764.6, ups=3.83, wpb=722.5, bsz=722.5, num_updates=5900, lr=0.000376, gnorm=4.595, clip=0, train_wall=12, gb_free=65.9, wall=1634 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:33:40]    INFO >> epoch 004:   1363 / 1539 loss=3.503, wps=2909.3, ups=3.97, wpb=732.7, bsz=732.7, num_updates=5950, lr=0.000376, gnorm=5.318, clip=0, train_wall=12, gb_free=67.4, wall=1647 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:33:49] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.71 GiB. GPU 2 has a total capacity of 79.14 GiB of which 271.25 MiB is free. Including non-PyTorch memory, this process has 78.85 GiB memory in use. Of the allocated memory 69.30 GiB is allocated by PyTorch, and 9.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:33:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:33:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:33:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 31           |        cudaMalloc retries: 46        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  65169 MiB |  70967 MiB | 359439 GiB | 359375 GiB |
|       from large pool |  65145 MiB |  70942 MiB | 358410 GiB | 358347 GiB |
|       from small pool |     24 MiB |     27 MiB |   1028 GiB |   1028 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  65169 MiB |  70967 MiB | 359439 GiB | 359375 GiB |
|       from large pool |  65145 MiB |  70942 MiB | 358410 GiB | 358347 GiB |
|       from small pool |     24 MiB |     27 MiB |   1028 GiB |   1028 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  65155 MiB |  70952 MiB | 359098 GiB | 359035 GiB |
|       from large pool |  65131 MiB |  70927 MiB | 358071 GiB | 358007 GiB |
|       from small pool |     24 MiB |     27 MiB |   1027 GiB |   1027 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80234 MiB |  80410 MiB | 633376 MiB | 553142 MiB |
|       from large pool |  80204 MiB |  80292 MiB | 631110 MiB | 550906 MiB |
|       from small pool |     30 MiB |    118 MiB |   2266 MiB |   2236 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  11200 MiB |  14921 MiB | 389179 GiB | 389168 GiB |
|       from large pool |  11194 MiB |  14916 MiB | 388001 GiB | 387990 GiB |
|       from small pool |      5 MiB |     15 MiB |   1177 GiB |   1177 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     450    |     459    |   12306 K  |   12306 K  |
|       from large pool |     163    |     172    |    6258 K  |    6258 K  |
|       from small pool |     287    |     336    |    6048 K  |    6048 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     450    |     459    |   12306 K  |   12306 K  |
|       from large pool |     163    |     172    |    6258 K  |    6258 K  |
|       from small pool |     287    |     336    |    6048 K  |    6048 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      74    |     173    |    3018    |    2944    |
|       from large pool |      59    |     114    |    1885    |    1826    |
|       from small pool |      15    |      59    |    1133    |    1118    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      72    |      74    |    6679 K  |    6679 K  |
|       from large pool |      51    |      53    |    3856 K  |    3856 K  |
|       from small pool |      21    |      47    |    2822 K  |    2822 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:33:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:33:49] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:33:54]    INFO >> epoch 004:   1414 / 1539 loss=3.644, wps=2993.1, ups=3.81, wpb=785.7, bsz=785.7, num_updates=6000, lr=0.000376, gnorm=5.56, clip=0, train_wall=12, gb_free=61.9, wall=1660 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:34:07]    INFO >> epoch 004:   1464 / 1539 loss=3.575, wps=2980.4, ups=4.08, wpb=731, bsz=731, num_updates=6050, lr=0.000376, gnorm=5.708, clip=0, train_wall=12, gb_free=59, wall=1672 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:34:18] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 588.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 25.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.46 GiB is allocated by PyTorch, and 1.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:34:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:34:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:34:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 32           |        cudaMalloc retries: 48        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78815 MiB |  79321 MiB | 365944 GiB | 365867 GiB |
|       from large pool |  78785 MiB |  79291 MiB | 364898 GiB | 364821 GiB |
|       from small pool |     30 MiB |     30 MiB |   1046 GiB |   1046 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78815 MiB |  79321 MiB | 365944 GiB | 365867 GiB |
|       from large pool |  78785 MiB |  79291 MiB | 364898 GiB | 364821 GiB |
|       from small pool |     30 MiB |     30 MiB |   1046 GiB |   1046 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78797 MiB |  79303 MiB | 365597 GiB | 365520 GiB |
|       from large pool |  78766 MiB |  79273 MiB | 364552 GiB | 364475 GiB |
|       from small pool |     30 MiB |     30 MiB |   1044 GiB |   1044 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80480 MiB |  80480 MiB | 637546 MiB | 557066 MiB |
|       from large pool |  80448 MiB |  80448 MiB | 635218 MiB | 554770 MiB |
|       from small pool |     32 MiB |     92 MiB |   2328 MiB |   2296 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1664 MiB |   6620 MiB | 396714 GiB | 396713 GiB |
|       from large pool |   1662 MiB |   6618 MiB | 395517 GiB | 395515 GiB |
|       from small pool |      1 MiB |     15 MiB |   1197 GiB |   1197 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     628    |     632    |   12527 K  |   12526 K  |
|       from large pool |     324    |     328    |    6377 K  |    6377 K  |
|       from small pool |     304    |     342    |    6149 K  |    6149 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     628    |     632    |   12527 K  |   12526 K  |
|       from large pool |     324    |     328    |    6377 K  |    6377 K  |
|       from small pool |     304    |     342    |    6149 K  |    6149 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      76    |     105    |    3052    |    2976    |
|       from large pool |      60    |      60    |    1888    |    1828    |
|       from small pool |      16    |      46    |    1164    |    1148    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      79    |      79    |    6795 K  |    6795 K  |
|       from large pool |      58    |      58    |    3928 K  |    3928 K  |
|       from small pool |      21    |      43    |    2867 K  |    2867 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:34:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:34:18] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:34:21]    INFO >> epoch 004:   1515 / 1539 loss=3.686, wps=2589.3, ups=3.93, wpb=658.8, bsz=658.8, num_updates=6100, lr=0.000376, gnorm=5.099, clip=0, train_wall=12, gb_free=69.6, wall=1685 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:34:25]    INFO >> epoch 004 | loss 3.558 | wps 2617 | ups 3.72 | wpb 702.8 | bsz 702.8 | num_updates 6124 | lr 0.000376 | gnorm 4.969 | clip 0.3 | train_wall 360 | gb_free 60.3 | wall 1690 (progress_bar.py:267, print())[0m
[33m[2025-11-21 02:34:25] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 02:34:52]    INFO >> epoch 004 | valid on 'valid' subset | loss 3.742 | wps 5740.5 | wpb 5412.5 | bsz 5412.5 | num_updates 6124 | best_loss 4.012 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 02:34:52]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 02:34:52]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/checkpoints/checkpoint_last.pt (epoch 4 @ 6124 updates, score 3.742) (writing took 0.039744 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 02:34:52] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[33m[2025-11-21 02:35:01] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.14 GiB. GPU 2 has a total capacity of 79.14 GiB of which 2.04 GiB is free. Including non-PyTorch memory, this process has 77.08 GiB memory in use. Of the allocated memory 72.69 GiB is allocated by PyTorch, and 3.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:35:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:35:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:35:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 33           |        cudaMalloc retries: 51        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  67282 MiB |  74438 MiB | 376974 GiB | 376908 GiB |
|       from large pool |  67255 MiB |  74411 MiB | 375883 GiB | 375817 GiB |
|       from small pool |     27 MiB |     28 MiB |   1090 GiB |   1090 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  67282 MiB |  74438 MiB | 376974 GiB | 376908 GiB |
|       from large pool |  67255 MiB |  74411 MiB | 375883 GiB | 375817 GiB |
|       from small pool |     27 MiB |     28 MiB |   1090 GiB |   1090 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  67268 MiB |  74423 MiB | 376618 GiB | 376552 GiB |
|       from large pool |  67241 MiB |  74396 MiB | 375529 GiB | 375463 GiB |
|       from small pool |     27 MiB |     28 MiB |   1089 GiB |   1089 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78418 MiB |  79432 MiB | 721890 MiB | 643472 MiB |
|       from large pool |  78386 MiB |  79362 MiB | 719524 MiB | 641138 MiB |
|       from small pool |     32 MiB |     70 MiB |   2366 MiB |   2334 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5867 MiB |   7813 MiB | 403069 GiB | 403063 GiB |
|       from large pool |   5862 MiB |   7808 MiB | 401825 GiB | 401820 GiB |
|       from small pool |      4 MiB |     21 MiB |   1243 GiB |   1243 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     489    |     498    |   12906 K  |   12906 K  |
|       from large pool |     195    |     204    |    6471 K  |    6471 K  |
|       from small pool |     294    |     356    |    6434 K  |    6434 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     489    |     498    |   12906 K  |   12906 K  |
|       from large pool |     195    |     204    |    6471 K  |    6471 K  |
|       from small pool |     294    |     356    |    6434 K  |    6434 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     107    |     170    |    3174    |    3067    |
|       from large pool |      91    |     135    |    1991    |    1900    |
|       from small pool |      16    |      35    |    1183    |    1167    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      98    |      98    |    7035 K  |    7035 K  |
|       from large pool |      74    |      74    |    3995 K  |    3995 K  |
|       from small pool |      24    |      51    |    3039 K  |    3039 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:35:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:35:01] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:35:01]    INFO >> epoch 005:     27 / 1539 loss=3.533, wps=816.7, ups=1.27, wpb=642.5, bsz=642.5, num_updates=6150, lr=0.000354, gnorm=5.026, clip=0, train_wall=11, gb_free=7.3, wall=1724 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:35:13]    INFO >> epoch 005:     77 / 1539 loss=3.62, wps=2809.2, ups=4.19, wpb=670.3, bsz=670.3, num_updates=6200, lr=0.000354, gnorm=4.514, clip=0, train_wall=11, gb_free=67.2, wall=1736 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:35:25]    INFO >> epoch 005:    127 / 1539 loss=3.441, wps=2886, ups=4.25, wpb=678.6, bsz=678.6, num_updates=6250, lr=0.000354, gnorm=4.495, clip=0, train_wall=11, gb_free=62.3, wall=1748 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:35:38]    INFO >> epoch 005:    177 / 1539 loss=3.549, wps=2766.9, ups=4.23, wpb=654.8, bsz=654.8, num_updates=6300, lr=0.000354, gnorm=4.135, clip=0, train_wall=11, gb_free=59.6, wall=1760 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:35:50]    INFO >> epoch 005:    227 / 1539 loss=3.452, wps=2748.3, ups=4.24, wpb=648.9, bsz=648.9, num_updates=6350, lr=0.000354, gnorm=4.342, clip=0, train_wall=11, gb_free=70.2, wall=1772 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:35:54] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 39.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 76.95 GiB is allocated by PyTorch, and 1.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:35:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:35:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:35:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 34           |        cudaMalloc retries: 52        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78735 MiB |  78794 MiB | 388523 GiB | 388447 GiB |
|       from large pool |  78649 MiB |  78708 MiB | 387402 GiB | 387325 GiB |
|       from small pool |     85 MiB |     86 MiB |   1121 GiB |   1121 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78735 MiB |  78794 MiB | 388523 GiB | 388447 GiB |
|       from large pool |  78649 MiB |  78708 MiB | 387402 GiB | 387325 GiB |
|       from small pool |     85 MiB |     86 MiB |   1121 GiB |   1121 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78718 MiB |  78777 MiB | 388155 GiB | 388078 GiB |
|       from large pool |  78633 MiB |  78691 MiB | 387035 GiB | 386958 GiB |
|       from small pool |     85 MiB |     86 MiB |   1120 GiB |   1119 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80466 MiB |  80468 MiB | 729208 MiB | 648742 MiB |
|       from large pool |  80372 MiB |  80372 MiB | 726778 MiB | 646406 MiB |
|       from small pool |     94 MiB |     96 MiB |   2430 MiB |   2336 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1670 MiB |   9923 MiB | 414439 GiB | 414438 GiB |
|       from large pool |   1662 MiB |   9916 MiB | 413161 GiB | 413159 GiB |
|       from small pool |      8 MiB |     19 MiB |   1278 GiB |   1278 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1686    |    1689    |   13303 K  |   13302 K  |
|       from large pool |     439    |     440    |    6690 K  |    6690 K  |
|       from small pool |    1247    |    1250    |    6613 K  |    6612 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1686    |    1689    |   13303 K  |   13302 K  |
|       from large pool |     439    |     440    |    6690 K  |    6690 K  |
|       from small pool |    1247    |    1250    |    6613 K  |    6612 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     195    |     196    |    3264    |    3069    |
|       from large pool |     148    |     148    |    2049    |    1901    |
|       from small pool |      47    |      48    |    1215    |    1168    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     206    |     207    |    7251 K  |    7251 K  |
|       from large pool |     133    |     134    |    4137 K  |    4137 K  |
|       from small pool |      73    |      73    |    3114 K  |    3114 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:35:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:35:54] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:36:05]    INFO >> epoch 005:    278 / 1539 loss=3.476, wps=2943.8, ups=3.53, wpb=833.4, bsz=833.4, num_updates=6400, lr=0.000354, gnorm=4.835, clip=0, train_wall=13, gb_free=70.6, wall=1786 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:36:18]    INFO >> epoch 005:    328 / 1539 loss=3.424, wps=2642.4, ups=4.07, wpb=649.4, bsz=649.4, num_updates=6450, lr=0.000354, gnorm=5.018, clip=0, train_wall=12, gb_free=61.4, wall=1798 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:36:30]    INFO >> epoch 005:    378 / 1539 loss=3.435, wps=3108, ups=3.95, wpb=786.5, bsz=786.5, num_updates=6500, lr=0.000354, gnorm=4.613, clip=0, train_wall=12, gb_free=63.2, wall=1811 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:36:38] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.15 GiB. GPU 2 has a total capacity of 79.14 GiB of which 429.25 MiB is free. Including non-PyTorch memory, this process has 78.70 GiB memory in use. Of the allocated memory 74.88 GiB is allocated by PyTorch, and 3.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:36:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:36:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:36:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 35           |        cudaMalloc retries: 55        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  64347 MiB |  76675 MiB | 397371 GiB | 397308 GiB |
|       from large pool |  64317 MiB |  76645 MiB | 396222 GiB | 396159 GiB |
|       from small pool |     29 MiB |     33 MiB |   1148 GiB |   1148 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  64347 MiB |  76675 MiB | 397371 GiB | 397308 GiB |
|       from large pool |  64317 MiB |  76645 MiB | 396222 GiB | 396159 GiB |
|       from small pool |     29 MiB |     33 MiB |   1148 GiB |   1148 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  64334 MiB |  76659 MiB | 396992 GiB | 396930 GiB |
|       from large pool |  64304 MiB |  76629 MiB | 395845 GiB | 395782 GiB |
|       from small pool |     29 MiB |     33 MiB |   1147 GiB |   1147 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80076 MiB |  80502 MiB | 731678 MiB | 651602 MiB |
|       from large pool |  80044 MiB |  80284 MiB | 729124 MiB | 649080 MiB |
|       from small pool |     32 MiB |    218 MiB |   2554 MiB |   2522 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   7184 MiB |  10978 MiB | 422282 GiB | 422274 GiB |
|       from large pool |   7182 MiB |  10975 MiB | 420971 GiB | 420964 GiB |
|       from small pool |      2 MiB |     23 MiB |   1310 GiB |   1310 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     571    |     622    |   13627 K  |   13626 K  |
|       from large pool |     268    |     318    |    6855 K  |    6855 K  |
|       from small pool |     303    |     348    |    6772 K  |    6771 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     571    |     622    |   13627 K  |   13626 K  |
|       from large pool |     268    |     318    |    6855 K  |    6855 K  |
|       from small pool |     303    |     348    |    6772 K  |    6771 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     119    |     254    |    3327    |    3208    |
|       from large pool |     103    |     145    |    2050    |    1947    |
|       from small pool |      16    |     109    |    1277    |    1261    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     108    |     112    |    7441 K  |    7441 K  |
|       from large pool |      84    |      88    |    4250 K  |    4250 K  |
|       from small pool |      24    |      57    |    3191 K  |    3191 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:36:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:36:38] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:36:44]    INFO >> epoch 005:    429 / 1539 loss=3.535, wps=2621.8, ups=4.05, wpb=647.1, bsz=647.1, num_updates=6550, lr=0.000354, gnorm=4.654, clip=0, train_wall=11, gb_free=61.2, wall=1823 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:36:57]    INFO >> epoch 005:    479 / 1539 loss=3.496, wps=2439.9, ups=3.97, wpb=614.5, bsz=614.5, num_updates=6600, lr=0.000354, gnorm=4.212, clip=0, train_wall=12, gb_free=62, wall=1836 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:37:11]    INFO >> epoch 005:    529 / 1539 loss=3.472, wps=2859.3, ups=3.96, wpb=722.2, bsz=722.2, num_updates=6650, lr=0.000354, gnorm=4.319, clip=0, train_wall=12, gb_free=54.4, wall=1848 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:37:23]    INFO >> epoch 005:    579 / 1539 loss=3.645, wps=2760.1, ups=4.2, wpb=657.6, bsz=657.6, num_updates=6700, lr=0.000354, gnorm=4.72, clip=0, train_wall=11, gb_free=55.6, wall=1860 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:37:25] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 946.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 773.25 MiB is free. Including non-PyTorch memory, this process has 78.36 GiB memory in use. Of the allocated memory 73.86 GiB is allocated by PyTorch, and 4.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:37:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:37:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:37:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 36           |        cudaMalloc retries: 57        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  71064 MiB |  75635 MiB | 407615 GiB | 407546 GiB |
|       from large pool |  71035 MiB |  75606 MiB | 406440 GiB | 406370 GiB |
|       from small pool |     28 MiB |     30 MiB |   1175 GiB |   1175 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  71064 MiB |  75635 MiB | 407615 GiB | 407546 GiB |
|       from large pool |  71035 MiB |  75606 MiB | 406440 GiB | 406370 GiB |
|       from small pool |     28 MiB |     30 MiB |   1175 GiB |   1175 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  71047 MiB |  75617 MiB | 407226 GiB | 407156 GiB |
|       from large pool |  71018 MiB |  75588 MiB | 406052 GiB | 405982 GiB |
|       from small pool |     28 MiB |     30 MiB |   1173 GiB |   1173 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79732 MiB |  79732 MiB | 741456 MiB | 661724 MiB |
|       from large pool |  79698 MiB |  79698 MiB | 738856 MiB | 659158 MiB |
|       from small pool |     34 MiB |     78 MiB |   2600 MiB |   2566 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3793 MiB |  13460 MiB | 432207 GiB | 432204 GiB |
|       from large pool |   3788 MiB |  13454 MiB | 430866 GiB | 430863 GiB |
|       from small pool |      5 MiB |     27 MiB |   1341 GiB |   1341 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     489    |     497    |   13970 K  |   13969 K  |
|       from large pool |     195    |     203    |    7044 K  |    7044 K  |
|       from small pool |     294    |     356    |    6925 K  |    6925 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     489    |     497    |   13970 K  |   13969 K  |
|       from large pool |     195    |     203    |    7044 K  |    7044 K  |
|       from small pool |     294    |     356    |    6925 K  |    6925 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     111    |     139    |    3355    |    3244    |
|       from large pool |      94    |     100    |    2055    |    1961    |
|       from small pool |      17    |      39    |    1300    |    1283    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      93    |      97    |    7629 K  |    7629 K  |
|       from large pool |      71    |      75    |    4373 K  |    4373 K  |
|       from small pool |      22    |      54    |    3256 K  |    3256 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:37:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:37:25] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:37:37]    INFO >> epoch 005:    630 / 1539 loss=3.464, wps=2616.6, ups=3.51, wpb=745.2, bsz=745.2, num_updates=6750, lr=0.000354, gnorm=4.092, clip=0, train_wall=13, gb_free=46.6, wall=1874 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:37:55]    INFO >> epoch 005:    680 / 1539 loss=3.587, wps=2306.1, ups=3.04, wpb=758.4, bsz=758.4, num_updates=6800, lr=0.000354, gnorm=4.857, clip=0, train_wall=16, gb_free=68.5, wall=1891 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:38:07]    INFO >> epoch 005:    730 / 1539 loss=3.483, wps=2784.8, ups=3.94, wpb=706.6, bsz=706.6, num_updates=6850, lr=0.000354, gnorm=4.793, clip=2, train_wall=12, gb_free=59.5, wall=1904 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:38:21]    INFO >> epoch 005:    780 / 1539 loss=3.519, wps=3096.7, ups=4.07, wpb=760.9, bsz=760.9, num_updates=6900, lr=0.000354, gnorm=4.865, clip=2, train_wall=12, gb_free=67.3, wall=1916 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:38:23] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.53 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.99 GiB is free. Including non-PyTorch memory, this process has 77.12 GiB memory in use. Of the allocated memory 71.68 GiB is allocated by PyTorch, and 4.95 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:38:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:38:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:38:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 37           |        cudaMalloc retries: 58        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72887 MiB |  73396 MiB | 418749 GiB | 418678 GiB |
|       from large pool |  72860 MiB |  73368 MiB | 417543 GiB | 417471 GiB |
|       from small pool |     26 MiB |     35 MiB |   1206 GiB |   1206 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72887 MiB |  73396 MiB | 418749 GiB | 418678 GiB |
|       from large pool |  72860 MiB |  73368 MiB | 417543 GiB | 417471 GiB |
|       from small pool |     26 MiB |     35 MiB |   1206 GiB |   1206 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72874 MiB |  73382 MiB | 418348 GiB | 418277 GiB |
|       from large pool |  72847 MiB |  73355 MiB | 417143 GiB | 417072 GiB |
|       from small pool |     26 MiB |     35 MiB |   1204 GiB |   1204 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78466 MiB |  78508 MiB | 745106 MiB | 666640 MiB |
|       from large pool |  78434 MiB |  78434 MiB | 742466 MiB | 664032 MiB |
|       from small pool |     32 MiB |     74 MiB |   2640 MiB |   2608 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5578 MiB |  13299 MiB | 443413 GiB | 443407 GiB |
|       from large pool |   5573 MiB |  13293 MiB | 442036 GiB | 442031 GiB |
|       from small pool |      5 MiB |     23 MiB |   1376 GiB |   1376 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     567    |     574    |   14351 K  |   14350 K  |
|       from large pool |     266    |     272    |    7247 K  |    7247 K  |
|       from small pool |     301    |     356    |    7103 K  |    7102 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     567    |     574    |   14351 K  |   14350 K  |
|       from large pool |     266    |     272    |    7247 K  |    7247 K  |
|       from small pool |     301    |     356    |    7103 K  |    7102 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     107    |     128    |    3376    |    3269    |
|       from large pool |      91    |      91    |    2056    |    1965    |
|       from small pool |      16    |      37    |    1320    |    1304    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     105    |     107    |    7838 K  |    7838 K  |
|       from large pool |      81    |      83    |    4502 K  |    4502 K  |
|       from small pool |      24    |      53    |    3335 K  |    3335 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:38:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:38:23] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:38:34]    INFO >> epoch 005:    831 / 1539 loss=3.501, wps=2320.4, ups=3.95, wpb=588.2, bsz=588.2, num_updates=6950, lr=0.000354, gnorm=3.94, clip=0, train_wall=11, gb_free=66.4, wall=1929 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:38:47]    INFO >> epoch 005:    881 / 1539 loss=3.304, wps=2890.8, ups=3.8, wpb=760.5, bsz=760.5, num_updates=7000, lr=0.000354, gnorm=4.321, clip=0, train_wall=12, gb_free=62.7, wall=1942 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:39:01]    INFO >> epoch 005:    931 / 1539 loss=3.604, wps=2368, ups=3.89, wpb=608.4, bsz=608.4, num_updates=7050, lr=0.000354, gnorm=4.282, clip=2, train_wall=12, gb_free=72.8, wall=1955 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:39:08] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.33 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.98 GiB is free. Including non-PyTorch memory, this process has 77.14 GiB memory in use. Of the allocated memory 76.06 GiB is allocated by PyTorch, and 594.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:39:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:39:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:39:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 38           |        cudaMalloc retries: 60        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77089 MiB |  78551 MiB | 427717 GiB | 427642 GiB |
|       from large pool |  77065 MiB |  78527 MiB | 426488 GiB | 426413 GiB |
|       from small pool |     23 MiB |     38 MiB |   1229 GiB |   1229 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77089 MiB |  78551 MiB | 427717 GiB | 427642 GiB |
|       from large pool |  77065 MiB |  78527 MiB | 426488 GiB | 426413 GiB |
|       from small pool |     23 MiB |     38 MiB |   1229 GiB |   1229 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77063 MiB |  78526 MiB | 427306 GiB | 427231 GiB |
|       from large pool |  77040 MiB |  78502 MiB | 426079 GiB | 426003 GiB |
|       from small pool |     23 MiB |     38 MiB |   1227 GiB |   1227 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78482 MiB |  79234 MiB |    793 GiB | 733656 MiB |
|       from large pool |  78450 MiB |  79202 MiB |    790 GiB | 731012 MiB |
|       from small pool |     32 MiB |     68 MiB |      2 GiB |   2644 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1392 MiB |   6631 MiB | 452287 GiB | 452286 GiB |
|       from large pool |   1384 MiB |   6620 MiB | 450884 GiB | 450883 GiB |
|       from small pool |      8 MiB |     26 MiB |   1402 GiB |   1402 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     488    |     496    |   14651 K  |   14651 K  |
|       from large pool |     200    |     208    |    7413 K  |    7413 K  |
|       from small pool |     288    |     356    |    7238 K  |    7237 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     488    |     496    |   14651 K  |   14651 K  |
|       from large pool |     200    |     208    |    7413 K  |    7413 K  |
|       from small pool |     288    |     356    |    7238 K  |    7237 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      87    |     125    |    3424    |    3337    |
|       from large pool |      71    |      91    |    2086    |    2015    |
|       from small pool |      16    |      34    |    1338    |    1322    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      68    |      69    |    8003 K  |    8003 K  |
|       from large pool |      48    |      49    |    4610 K  |    4610 K  |
|       from small pool |      20    |      52    |    3393 K  |    3393 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:39:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:39:08] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 02:39:15] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 41.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 77.47 GiB is allocated by PyTorch, and 1.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:39:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:39:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:39:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 39           |        cudaMalloc retries: 61        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79271 MiB |  79331 MiB | 429338 GiB | 429260 GiB |
|       from large pool |  79072 MiB |  79132 MiB | 428103 GiB | 428026 GiB |
|       from small pool |    198 MiB |    200 MiB |   1234 GiB |   1234 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79271 MiB |  79331 MiB | 429338 GiB | 429260 GiB |
|       from large pool |  79072 MiB |  79132 MiB | 428103 GiB | 428026 GiB |
|       from small pool |    198 MiB |    200 MiB |   1234 GiB |   1234 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79249 MiB |  79309 MiB | 428926 GiB | 428848 GiB |
|       from large pool |  79051 MiB |  79111 MiB | 427692 GiB | 427615 GiB |
|       from small pool |    198 MiB |    199 MiB |   1233 GiB |   1232 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80464 MiB |  80466 MiB |    795 GiB | 733658 MiB |
|       from large pool |  80248 MiB |  80248 MiB |    792 GiB | 731012 MiB |
|       from small pool |    216 MiB |    218 MiB |      2 GiB |   2646 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1130 MiB |   9271 MiB | 454051 GiB | 454050 GiB |
|       from large pool |   1113 MiB |   9263 MiB | 452641 GiB | 452640 GiB |
|       from small pool |     17 MiB |     25 MiB |   1409 GiB |   1409 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3699    |    3702    |   14710 K  |   14707 K  |
|       from large pool |     614    |     615    |    7440 K  |    7439 K  |
|       from small pool |    3085    |    3088    |    7270 K  |    7267 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3699    |    3702    |   14710 K  |   14707 K  |
|       from large pool |     614    |     615    |    7440 K  |    7439 K  |
|       from small pool |    3085    |    3088    |    7270 K  |    7267 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     208    |     209    |    3546    |    3338    |
|       from large pool |     100    |     100    |    2115    |    2015    |
|       from small pool |     108    |     109    |    1431    |    1323    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     262    |     263    |    8036 K  |    8035 K  |
|       from large pool |      88    |      89    |    4626 K  |    4626 K  |
|       from small pool |     174    |     174    |    3409 K  |    3409 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:39:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:39:15] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:39:17]    INFO >> epoch 005:    983 / 1539 loss=3.398, wps=2214.2, ups=3.08, wpb=719.9, bsz=719.9, num_updates=7100, lr=0.000354, gnorm=4.393, clip=0, train_wall=13, gb_free=62.1, wall=1971 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:39:33]    INFO >> epoch 005:   1033 / 1539 loss=3.474, wps=2951.1, ups=3.53, wpb=837.1, bsz=837.1, num_updates=7150, lr=0.000354, gnorm=4.88, clip=0, train_wall=13, gb_free=64.4, wall=1985 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:39:45]    INFO >> epoch 005:   1083 / 1539 loss=3.41, wps=3017.3, ups=4.03, wpb=749, bsz=749, num_updates=7200, lr=0.000354, gnorm=4.86, clip=0, train_wall=12, gb_free=69.5, wall=1997 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:39:57] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.71 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.61 GiB is free. Including non-PyTorch memory, this process has 77.50 GiB memory in use. Of the allocated memory 69.30 GiB is allocated by PyTorch, and 7.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:39:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:39:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:39:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 40           |        cudaMalloc retries: 63        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  65166 MiB |  70962 MiB | 437915 GiB | 437851 GiB |
|       from large pool |  65141 MiB |  70937 MiB | 436657 GiB | 436593 GiB |
|       from small pool |     24 MiB |     35 MiB |   1258 GiB |   1258 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  65166 MiB |  70962 MiB | 437915 GiB | 437851 GiB |
|       from large pool |  65141 MiB |  70937 MiB | 436657 GiB | 436593 GiB |
|       from small pool |     24 MiB |     35 MiB |   1258 GiB |   1258 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  65155 MiB |  70952 MiB | 437494 GiB | 437431 GiB |
|       from large pool |  65131 MiB |  70927 MiB | 436237 GiB | 436174 GiB |
|       from small pool |     24 MiB |     35 MiB |   1256 GiB |   1256 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78854 MiB |  80402 MiB |    802 GiB | 742876 MiB |
|       from large pool |  78820 MiB |  80186 MiB |    799 GiB | 740048 MiB |
|       from small pool |     34 MiB |    216 MiB |      2 GiB |   2828 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   9323 MiB |  17390 MiB | 462648 GiB | 462639 GiB |
|       from large pool |   9314 MiB |  17381 MiB | 461212 GiB | 461203 GiB |
|       from small pool |      9 MiB |     20 MiB |   1436 GiB |   1436 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     450    |     459    |   15004 K  |   15003 K  |
|       from large pool |     163    |     172    |    7594 K  |    7594 K  |
|       from small pool |     287    |     356    |    7409 K  |    7409 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     450    |     459    |   15004 K  |   15003 K  |
|       from large pool |     163    |     172    |    7594 K  |    7594 K  |
|       from small pool |     287    |     356    |    7409 K  |    7409 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      72    |     207    |    3548    |    3476    |
|       from large pool |      55    |      99    |    2117    |    2062    |
|       from small pool |      17    |     108    |    1431    |    1414    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      71    |      75    |    8200 K  |    8200 K  |
|       from large pool |      47    |      51    |    4726 K  |    4726 K  |
|       from small pool |      24    |      52    |    3474 K  |    3474 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:39:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:39:57] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:40:00]    INFO >> epoch 005:   1134 / 1539 loss=3.444, wps=2346.3, ups=3.7, wpb=634.8, bsz=634.8, num_updates=7250, lr=0.000354, gnorm=4.346, clip=0, train_wall=12, gb_free=56.7, wall=2011 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:40:13]    INFO >> epoch 005:   1184 / 1539 loss=3.598, wps=2883.5, ups=3.83, wpb=752.5, bsz=752.5, num_updates=7300, lr=0.000354, gnorm=4.783, clip=2, train_wall=12, gb_free=52.8, wall=2024 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:40:25]    INFO >> epoch 005:   1234 / 1539 loss=3.57, wps=2702.8, ups=4.09, wpb=660.4, bsz=660.4, num_updates=7350, lr=0.000354, gnorm=4.292, clip=0, train_wall=12, gb_free=66.5, wall=2036 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:40:38]    INFO >> epoch 005:   1284 / 1539 loss=3.484, wps=3016.8, ups=4.43, wpb=680.6, bsz=680.6, num_updates=7400, lr=0.000354, gnorm=4.416, clip=0, train_wall=11, gb_free=70.3, wall=2047 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:40:51]    INFO >> epoch 005:   1334 / 1539 loss=3.441, wps=2573.2, ups=3.88, wpb=662.6, bsz=662.6, num_updates=7450, lr=0.000354, gnorm=3.798, clip=0, train_wall=12, gb_free=68.3, wall=2060 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:41:04]    INFO >> epoch 005:   1384 / 1539 loss=3.516, wps=2800.7, ups=4.27, wpb=655.2, bsz=655.2, num_updates=7500, lr=0.000354, gnorm=4.184, clip=0, train_wall=11, gb_free=62.9, wall=2072 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:41:17]    INFO >> epoch 005:   1434 / 1539 loss=3.491, wps=2620.8, ups=3.84, wpb=682, bsz=682, num_updates=7550, lr=0.000354, gnorm=4.936, clip=0, train_wall=12, gb_free=65.2, wall=2085 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:41:31]    INFO >> epoch 005:   1484 / 1539 loss=3.44, wps=2834.8, ups=3.59, wpb=789.1, bsz=789.1, num_updates=7600, lr=0.000354, gnorm=4.467, clip=0, train_wall=13, gb_free=65, wall=2099 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:41:46]    INFO >> epoch 005:   1534 / 1539 loss=3.568, wps=2789.5, ups=3.7, wpb=754.9, bsz=754.9, num_updates=7650, lr=0.000354, gnorm=4.666, clip=0, train_wall=13, gb_free=67.6, wall=2113 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:41:47]    INFO >> epoch 005 | loss 3.494 | wps 2536.8 | ups 3.61 | wpb 702.8 | bsz 702.8 | num_updates 7655 | lr 0.000354 | gnorm 4.507 | clip 0.3 | train_wall 371 | gb_free 48.5 | wall 2114 (progress_bar.py:267, print())[0m
[33m[2025-11-21 02:41:47] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 02:42:17]    INFO >> epoch 005 | valid on 'valid' subset | loss 3.685 | wps 5245.8 | wpb 5412.5 | bsz 5412.5 | num_updates 7655 | best_loss 4.012 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 02:42:18]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 02:42:18]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/checkpoints/checkpoint_last.pt (epoch 5 @ 7655 updates, score 3.685) (writing took 0.034783 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 02:42:18] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 02:42:30]    INFO >> epoch 006:     45 / 1539 loss=3.517, wps=977.7, ups=1.17, wpb=832.2, bsz=832.2, num_updates=7700, lr=0.000327, gnorm=4.155, clip=0, train_wall=13, gb_free=63, wall=2155 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:42:43]    INFO >> epoch 006:     95 / 1539 loss=3.476, wps=2681.3, ups=3.71, wpb=723.5, bsz=723.5, num_updates=7750, lr=0.000327, gnorm=5.005, clip=0, train_wall=13, gb_free=66.3, wall=2169 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:42:57]    INFO >> epoch 006:    145 / 1539 loss=3.494, wps=2800.8, ups=3.9, wpb=718.9, bsz=718.9, num_updates=7800, lr=0.000327, gnorm=4.087, clip=0, train_wall=12, gb_free=60.5, wall=2181 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:43:08] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.71 GiB. GPU 2 has a total capacity of 79.14 GiB of which 2.16 GiB is free. Including non-PyTorch memory, this process has 76.95 GiB memory in use. Of the allocated memory 69.30 GiB is allocated by PyTorch, and 7.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:43:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:43:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:43:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 41           |        cudaMalloc retries: 64        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  65166 MiB |  70962 MiB | 479916 GiB | 479853 GiB |
|       from large pool |  65141 MiB |  70937 MiB | 478529 GiB | 478465 GiB |
|       from small pool |     24 MiB |     29 MiB |   1387 GiB |   1387 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  65166 MiB |  70962 MiB | 479916 GiB | 479853 GiB |
|       from large pool |  65141 MiB |  70937 MiB | 478529 GiB | 478465 GiB |
|       from small pool |     24 MiB |     29 MiB |   1387 GiB |   1387 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  65155 MiB |  70952 MiB | 479460 GiB | 479397 GiB |
|       from large pool |  65131 MiB |  70927 MiB | 478075 GiB | 478011 GiB |
|       from small pool |     24 MiB |     29 MiB |   1385 GiB |   1385 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78290 MiB |  78378 MiB |    806 GiB | 747328 MiB |
|       from large pool |  78260 MiB |  78260 MiB |    803 GiB | 744412 MiB |
|       from small pool |     30 MiB |    118 MiB |      2 GiB |   2916 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   9257 MiB |  12638 MiB | 507417 GiB | 507408 GiB |
|       from large pool |   9252 MiB |  12632 MiB | 505837 GiB | 505828 GiB |
|       from small pool |      5 MiB |     25 MiB |   1580 GiB |   1580 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     450    |     459    |   16432 K  |   16432 K  |
|       from large pool |     163    |     172    |    8249 K  |    8249 K  |
|       from small pool |     287    |     356    |    8183 K  |    8183 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     450    |     459    |   16432 K  |   16432 K  |
|       from large pool |     163    |     172    |    8249 K  |    8249 K  |
|       from small pool |     287    |     356    |    8183 K  |    8183 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      67    |     111    |    3591    |    3524    |
|       from large pool |      52    |      52    |    2118    |    2066    |
|       from small pool |      15    |      59    |    1473    |    1458    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      68    |      69    |    8995 K  |    8995 K  |
|       from large pool |      47    |      48    |    5124 K  |    5124 K  |
|       from small pool |      21    |      55    |    3871 K  |    3871 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:43:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:43:08] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:43:11]    INFO >> epoch 006:    196 / 1539 loss=3.497, wps=2396.3, ups=3.69, wpb=648.7, bsz=648.7, num_updates=7850, lr=0.000327, gnorm=4.311, clip=0, train_wall=12, gb_free=64.8, wall=2195 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:43:19] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.69 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.32 GiB is free. Including non-PyTorch memory, this process has 77.79 GiB memory in use. Of the allocated memory 74.78 GiB is allocated by PyTorch, and 2.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:43:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:43:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:43:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 42           |        cudaMalloc retries: 65        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  71055 MiB |  76570 MiB | 481997 GiB | 481928 GiB |
|       from large pool |  71026 MiB |  76541 MiB | 480604 GiB | 480535 GiB |
|       from small pool |     28 MiB |     30 MiB |   1392 GiB |   1392 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  71055 MiB |  76570 MiB | 481997 GiB | 481928 GiB |
|       from large pool |  71026 MiB |  76541 MiB | 480604 GiB | 480535 GiB |
|       from small pool |     28 MiB |     30 MiB |   1392 GiB |   1392 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  71047 MiB |  76562 MiB | 481540 GiB | 481470 GiB |
|       from large pool |  71018 MiB |  76533 MiB | 480149 GiB | 480079 GiB |
|       from small pool |     28 MiB |     30 MiB |   1390 GiB |   1390 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79152 MiB |  79184 MiB |    810 GiB | 751226 MiB |
|       from large pool |  79120 MiB |  79120 MiB |    808 GiB | 748278 MiB |
|       from small pool |     32 MiB |     64 MiB |      2 GiB |   2948 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3370 MiB |   9890 MiB | 509879 GiB | 509876 GiB |
|       from large pool |   3367 MiB |   9886 MiB | 508293 GiB | 508290 GiB |
|       from small pool |      3 MiB |     25 MiB |   1586 GiB |   1585 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     489    |     498    |   16495 K  |   16495 K  |
|       from large pool |     195    |     204    |    8283 K  |    8283 K  |
|       from small pool |     294    |     356    |    8212 K  |    8212 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     489    |     498    |   16495 K  |   16495 K  |
|       from large pool |     195    |     204    |    8283 K  |    8283 K  |
|       from small pool |     294    |     356    |    8212 K  |    8212 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      68    |      84    |    3610    |    3542    |
|       from large pool |      52    |      52    |    2120    |    2068    |
|       from small pool |      16    |      32    |    1490    |    1474    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      85    |      86    |    9028 K  |    9028 K  |
|       from large pool |      60    |      61    |    5144 K  |    5144 K  |
|       from small pool |      25    |      54    |    3884 K  |    3884 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:43:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:43:19] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:43:26]    INFO >> epoch 006:    247 / 1539 loss=3.39, wps=2594.1, ups=3.59, wpb=722.3, bsz=722.3, num_updates=7900, lr=0.000327, gnorm=4.954, clip=0, train_wall=13, gb_free=68.6, wall=2209 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:43:38]    INFO >> epoch 006:    297 / 1539 loss=3.371, wps=2907.8, ups=4.17, wpb=697.5, bsz=697.5, num_updates=7950, lr=0.000327, gnorm=4.51, clip=0, train_wall=11, gb_free=68.9, wall=2221 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:43:53]    INFO >> epoch 006:    347 / 1539 loss=3.394, wps=2900.3, ups=3.53, wpb=822.6, bsz=822.6, num_updates=8000, lr=0.000327, gnorm=4.91, clip=0, train_wall=14, gb_free=61.9, wall=2235 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:44:06]    INFO >> epoch 006:    397 / 1539 loss=3.439, wps=2567.1, ups=4.12, wpb=622.6, bsz=622.6, num_updates=8050, lr=0.000327, gnorm=4.292, clip=0, train_wall=12, gb_free=53.7, wall=2247 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:44:17] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.33 GiB. GPU 2 has a total capacity of 79.14 GiB of which 937.25 MiB is free. Including non-PyTorch memory, this process has 78.20 GiB memory in use. Of the allocated memory 76.05 GiB is allocated by PyTorch, and 1.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:44:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:44:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:44:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 43           |        cudaMalloc retries: 67        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77076 MiB |  78539 MiB | 494128 GiB | 494052 GiB |
|       from large pool |  77052 MiB |  78515 MiB | 492700 GiB | 492624 GiB |
|       from small pool |     23 MiB |     35 MiB |   1428 GiB |   1428 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77076 MiB |  78539 MiB | 494128 GiB | 494052 GiB |
|       from large pool |  77052 MiB |  78515 MiB | 492700 GiB | 492624 GiB |
|       from small pool |     23 MiB |     35 MiB |   1428 GiB |   1428 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77063 MiB |  78526 MiB | 493658 GiB | 493583 GiB |
|       from large pool |  77040 MiB |  78502 MiB | 492232 GiB | 492157 GiB |
|       from small pool |     23 MiB |     35 MiB |   1426 GiB |   1426 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79568 MiB |  79568 MiB |    819 GiB | 759604 MiB |
|       from large pool |  79538 MiB |  79538 MiB |    816 GiB | 756468 MiB |
|       from small pool |     30 MiB |    218 MiB |      3 GiB |   3136 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2491 MiB |   7022 MiB | 523646 GiB | 523644 GiB |
|       from large pool |   2485 MiB |   7016 MiB | 522019 GiB | 522016 GiB |
|       from small pool |      6 MiB |     25 MiB |   1627 GiB |   1627 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     488    |     496    |   16928 K  |   16927 K  |
|       from large pool |     200    |     208    |    8508 K  |    8508 K  |
|       from small pool |     288    |     356    |    8419 K  |    8419 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     488    |     496    |   16928 K  |   16927 K  |
|       from large pool |     200    |     208    |    8508 K  |    8508 K  |
|       from small pool |     288    |     356    |    8419 K  |    8419 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      61    |     210    |    3755    |    3694    |
|       from large pool |      46    |     101    |    2172    |    2126    |
|       from small pool |      15    |     109    |    1583    |    1568    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      63    |      64    |    9267 K  |    9267 K  |
|       from large pool |      43    |      44    |    5287 K  |    5287 K  |
|       from small pool |      20    |      56    |    3980 K  |    3980 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:44:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:44:17] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:44:19]    INFO >> epoch 006:    448 / 1539 loss=3.465, wps=2977.7, ups=3.81, wpb=781.9, bsz=781.9, num_updates=8100, lr=0.000327, gnorm=4.323, clip=0, train_wall=12, gb_free=65.3, wall=2260 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:44:33]    INFO >> epoch 006:    498 / 1539 loss=3.452, wps=2776.3, ups=3.76, wpb=739, bsz=739, num_updates=8150, lr=0.000327, gnorm=4.604, clip=0, train_wall=13, gb_free=65.2, wall=2274 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:44:46]    INFO >> epoch 006:    548 / 1539 loss=3.489, wps=2638.1, ups=3.97, wpb=664.7, bsz=664.7, num_updates=8200, lr=0.000327, gnorm=4.497, clip=0, train_wall=12, gb_free=71, wall=2286 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:44:48] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 23.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.88 GiB is allocated by PyTorch, and 736.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:44:48] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:44:48] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:44:48] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 44           |        cudaMalloc retries: 68        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79686 MiB |  79745 MiB | 500782 GiB | 500704 GiB |
|       from large pool |  79591 MiB |  79649 MiB | 499336 GiB | 499258 GiB |
|       from small pool |     95 MiB |     96 MiB |   1446 GiB |   1446 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79686 MiB |  79745 MiB | 500782 GiB | 500704 GiB |
|       from large pool |  79591 MiB |  79649 MiB | 499336 GiB | 499258 GiB |
|       from small pool |     95 MiB |     96 MiB |   1446 GiB |   1446 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79669 MiB |  79728 MiB | 500306 GiB | 500228 GiB |
|       from large pool |  79575 MiB |  79633 MiB | 498862 GiB | 498784 GiB |
|       from small pool |     94 MiB |     96 MiB |   1444 GiB |   1444 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80482 MiB |  80484 MiB |    820 GiB | 759606 MiB |
|       from large pool |  80378 MiB |  80378 MiB |    817 GiB | 756468 MiB |
|       from small pool |    104 MiB |    106 MiB |      3 GiB |   3138 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory | 753422 KiB |   6824 MiB | 531758 GiB | 531757 GiB |
|       from large pool | 744433 KiB |   6819 MiB | 530110 GiB | 530109 GiB |
|       from small pool |   8988 KiB |     21 MiB |   1648 GiB |   1648 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1862    |    1865    |   17152 K  |   17150 K  |
|       from large pool |     455    |     456    |    8627 K  |    8627 K  |
|       from small pool |    1407    |    1410    |    8524 K  |    8523 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1862    |    1865    |   17152 K  |   17150 K  |
|       from large pool |     455    |     456    |    8627 K  |    8627 K  |
|       from small pool |    1407    |    1410    |    8524 K  |    8523 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     112    |     113    |    3807    |    3695    |
|       from large pool |      60    |      60    |    2186    |    2126    |
|       from small pool |      52    |      53    |    1621    |    1569    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     146    |     147    |    9384 K  |    9384 K  |
|       from large pool |      64    |      65    |    5356 K  |    5356 K  |
|       from small pool |      82    |      83    |    4027 K  |    4027 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:44:48] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:44:48] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:44:57]    INFO >> epoch 006:    599 / 1539 loss=3.473, wps=2713.9, ups=4.37, wpb=621.5, bsz=621.5, num_updates=8250, lr=0.000327, gnorm=4.508, clip=0, train_wall=10, gb_free=66.9, wall=2298 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:45:11]    INFO >> epoch 006:    649 / 1539 loss=3.535, wps=2869, ups=4.2, wpb=682.4, bsz=682.4, num_updates=8300, lr=0.000327, gnorm=4.133, clip=0, train_wall=11, gb_free=71.3, wall=2310 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:45:23]    INFO >> epoch 006:    699 / 1539 loss=3.425, wps=2816, ups=4.13, wpb=682.4, bsz=682.4, num_updates=8350, lr=0.000327, gnorm=3.704, clip=0, train_wall=12, gb_free=66.9, wall=2322 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:45:33] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 588.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 155.25 MiB is free. Including non-PyTorch memory, this process has 78.96 GiB memory in use. Of the allocated memory 75.01 GiB is allocated by PyTorch, and 3.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:45:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:45:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:45:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 45           |        cudaMalloc retries: 69        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76539 MiB |  77845 MiB | 510013 GiB | 509938 GiB |
|       from large pool |  76509 MiB |  77815 MiB | 508541 GiB | 508467 GiB |
|       from small pool |     30 MiB |     35 MiB |   1471 GiB |   1471 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76539 MiB |  77845 MiB | 510013 GiB | 509938 GiB |
|       from large pool |  76509 MiB |  77815 MiB | 508541 GiB | 508467 GiB |
|       from small pool |     30 MiB |     35 MiB |   1471 GiB |   1471 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76526 MiB |  77832 MiB | 509527 GiB | 509452 GiB |
|       from large pool |  76495 MiB |  77801 MiB | 508058 GiB | 507983 GiB |
|       from small pool |     30 MiB |     35 MiB |   1469 GiB |   1469 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80350 MiB |  80422 MiB |    820 GiB | 759738 MiB |
|       from large pool |  80318 MiB |  80318 MiB |    817 GiB | 756528 MiB |
|       from small pool |     32 MiB |    104 MiB |      3 GiB |   3210 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3810 MiB |   9766 MiB | 542634 GiB | 542630 GiB |
|       from large pool |   3808 MiB |   9763 MiB | 540957 GiB | 540953 GiB |
|       from small pool |      1 MiB |     24 MiB |   1677 GiB |   1677 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     618    |     625    |   17475 K  |   17475 K  |
|       from large pool |     314    |     321    |    8805 K  |    8805 K  |
|       from small pool |     304    |     356    |    8670 K  |    8670 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     618    |     625    |   17475 K  |   17475 K  |
|       from large pool |     314    |     321    |    8805 K  |    8805 K  |
|       from small pool |     304    |     356    |    8670 K  |    8670 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      75    |     111    |    3807    |    3732    |
|       from large pool |      59    |      59    |    2186    |    2127    |
|       from small pool |      16    |      52    |    1621    |    1605    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      83    |      86    |    9554 K  |    9553 K  |
|       from large pool |      62    |      65    |    5464 K  |    5464 K  |
|       from small pool |      21    |      52    |    4089 K  |    4089 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:45:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:45:33] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:45:36]    INFO >> epoch 006:    750 / 1539 loss=3.434, wps=2533, ups=4.09, wpb=619.4, bsz=619.4, num_updates=8400, lr=0.000327, gnorm=4.375, clip=0, train_wall=11, gb_free=72, wall=2334 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:45:47]    INFO >> epoch 006:    800 / 1539 loss=3.402, wps=2991.4, ups=4.53, wpb=660.5, bsz=660.5, num_updates=8450, lr=0.000327, gnorm=4.024, clip=0, train_wall=11, gb_free=64.1, wall=2345 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:46:00]    INFO >> epoch 006:    850 / 1539 loss=3.365, wps=2783.6, ups=4.13, wpb=674.7, bsz=674.7, num_updates=8500, lr=0.000327, gnorm=4.666, clip=0, train_wall=12, gb_free=66.3, wall=2357 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:46:14]    INFO >> epoch 006:    900 / 1539 loss=3.557, wps=2791.8, ups=3.8, wpb=734.1, bsz=734.1, num_updates=8550, lr=0.000327, gnorm=4.481, clip=0, train_wall=12, gb_free=73.3, wall=2370 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:46:20] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 1.25 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 74.69 GiB is allocated by PyTorch, and 3.93 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:46:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:46:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:46:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 46           |        cudaMalloc retries: 70        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76479 MiB |  76539 MiB | 520440 GiB | 520365 GiB |
|       from large pool |  76308 MiB |  76368 MiB | 518940 GiB | 518865 GiB |
|       from small pool |    170 MiB |    171 MiB |   1500 GiB |   1500 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76479 MiB |  76539 MiB | 520440 GiB | 520365 GiB |
|       from large pool |  76308 MiB |  76368 MiB | 518940 GiB | 518865 GiB |
|       from small pool |    170 MiB |    171 MiB |   1500 GiB |   1500 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76458 MiB |  76518 MiB | 519944 GiB | 519869 GiB |
|       from large pool |  76288 MiB |  76348 MiB | 518446 GiB | 518371 GiB |
|       from small pool |    170 MiB |    171 MiB |   1498 GiB |   1498 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80504 MiB |  80504 MiB |    820 GiB | 759738 MiB |
|       from large pool |  80318 MiB |  80318 MiB |    817 GiB | 756528 MiB |
|       from small pool |    186 MiB |    186 MiB |      3 GiB |   3210 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4024 MiB |   7525 MiB | 554963 GiB | 554959 GiB |
|       from large pool |   4009 MiB |   7523 MiB | 553252 GiB | 553248 GiB |
|       from small pool |     15 MiB |     18 MiB |   1711 GiB |   1711 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3191    |    3192    |   17838 K  |   17835 K  |
|       from large pool |     568    |     569    |    8999 K  |    8999 K  |
|       from small pool |    2623    |    2624    |    8838 K  |    8836 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3191    |    3192    |   17838 K  |   17835 K  |
|       from large pool |     568    |     569    |    8999 K  |    8999 K  |
|       from small pool |    2623    |    2624    |    8838 K  |    8836 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     152    |     152    |    3884    |    3732    |
|       from large pool |      59    |      59    |    2186    |    2127    |
|       from small pool |      93    |      93    |    1698    |    1605    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     200    |     202    |    9745 K  |    9745 K  |
|       from large pool |      51    |      51    |    5582 K  |    5582 K  |
|       from small pool |     149    |     151    |    4162 K  |    4162 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:46:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:46:20] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:46:28]    INFO >> epoch 006:    951 / 1539 loss=3.483, wps=2817.3, ups=3.61, wpb=780.5, bsz=780.5, num_updates=8600, lr=0.000327, gnorm=4.308, clip=0, train_wall=13, gb_free=60, wall=2384 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:46:42]    INFO >> epoch 006:   1001 / 1539 loss=3.413, wps=2639, ups=3.96, wpb=666.2, bsz=666.2, num_updates=8650, lr=0.000327, gnorm=4.02, clip=0, train_wall=12, gb_free=56.8, wall=2397 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:46:49] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.14 GiB. GPU 2 has a total capacity of 79.14 GiB of which 875.25 MiB is free. Including non-PyTorch memory, this process has 78.26 GiB memory in use. Of the allocated memory 72.69 GiB is allocated by PyTorch, and 5.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:46:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:46:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:46:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 47           |        cudaMalloc retries: 71        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  67276 MiB |  74432 MiB | 526549 GiB | 526483 GiB |
|       from large pool |  67249 MiB |  74405 MiB | 525032 GiB | 524966 GiB |
|       from small pool |     27 MiB |     37 MiB |   1516 GiB |   1516 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  67276 MiB |  74432 MiB | 526549 GiB | 526483 GiB |
|       from large pool |  67249 MiB |  74405 MiB | 525032 GiB | 524966 GiB |
|       from small pool |     27 MiB |     37 MiB |   1516 GiB |   1516 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  67268 MiB |  74423 MiB | 526047 GiB | 525981 GiB |
|       from large pool |  67241 MiB |  74396 MiB | 524532 GiB | 524467 GiB |
|       from small pool |     27 MiB |     37 MiB |   1514 GiB |   1514 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79630 MiB |  80504 MiB |    820 GiB | 760612 MiB |
|       from large pool |  79598 MiB |  80318 MiB |    817 GiB | 757248 MiB |
|       from small pool |     32 MiB |    186 MiB |      3 GiB |   3364 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6839 MiB |  10391 MiB | 562193 GiB | 562186 GiB |
|       from large pool |   6834 MiB |  10385 MiB | 560463 GiB | 560456 GiB |
|       from small pool |      4 MiB |     24 MiB |   1730 GiB |   1730 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     489    |     498    |   18042 K  |   18042 K  |
|       from large pool |     195    |     204    |    9108 K  |    9108 K  |
|       from small pool |     294    |     356    |    8934 K  |    8933 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     489    |     498    |   18042 K  |   18042 K  |
|       from large pool |     195    |     204    |    9108 K  |    9108 K  |
|       from small pool |     294    |     356    |    8934 K  |    8933 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      63    |     152    |    3884    |    3821    |
|       from large pool |      47    |      59    |    2186    |    2139    |
|       from small pool |      16    |      93    |    1698    |    1682    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      73    |      74    |    9855 K  |    9855 K  |
|       from large pool |      48    |      49    |    5648 K  |    5648 K  |
|       from small pool |      25    |      53    |    4206 K  |    4206 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:46:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:46:49] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:46:55]    INFO >> epoch 006:   1052 / 1539 loss=3.545, wps=2674.1, ups=3.68, wpb=727.3, bsz=727.3, num_updates=8700, lr=0.000327, gnorm=4.518, clip=0, train_wall=12, gb_free=70.1, wall=2410 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:47:08]    INFO >> epoch 006:   1102 / 1539 loss=3.497, wps=2638.1, ups=4.05, wpb=651.3, bsz=651.3, num_updates=8750, lr=0.000327, gnorm=4.36, clip=0, train_wall=12, gb_free=65.9, wall=2423 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:47:23]    INFO >> epoch 006:   1152 / 1539 loss=3.417, wps=2869.1, ups=3.61, wpb=794.2, bsz=794.2, num_updates=8800, lr=0.000327, gnorm=4.456, clip=0, train_wall=13, gb_free=67, wall=2436 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:47:31] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.53 GiB. GPU 2 has a total capacity of 79.14 GiB of which 2.72 GiB is free. Including non-PyTorch memory, this process has 76.40 GiB memory in use. Of the allocated memory 71.67 GiB is allocated by PyTorch, and 4.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:47:31] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:47:31] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:47:31] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 48           |        cudaMalloc retries: 72        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72886 MiB |  73395 MiB | 535519 GiB | 535448 GiB |
|       from large pool |  72859 MiB |  73367 MiB | 533978 GiB | 533907 GiB |
|       from small pool |     26 MiB |     27 MiB |   1541 GiB |   1541 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72886 MiB |  73395 MiB | 535519 GiB | 535448 GiB |
|       from large pool |  72859 MiB |  73367 MiB | 533978 GiB | 533907 GiB |
|       from small pool |     26 MiB |     27 MiB |   1541 GiB |   1541 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72874 MiB |  73382 MiB | 535010 GiB | 534938 GiB |
|       from large pool |  72847 MiB |  73355 MiB | 533470 GiB | 533399 GiB |
|       from small pool |     26 MiB |     27 MiB |   1539 GiB |   1539 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77724 MiB |  77828 MiB |    824 GiB | 766230 MiB |
|       from large pool |  77694 MiB |  77694 MiB |    820 GiB | 762762 MiB |
|       from small pool |     30 MiB |    134 MiB |      3 GiB |   3468 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4837 MiB |  11792 MiB | 573176 GiB | 573171 GiB |
|       from large pool |   4834 MiB |  11789 MiB | 571418 GiB | 571413 GiB |
|       from small pool |      3 MiB |     16 MiB |   1758 GiB |   1758 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     567    |     574    |   18340 K  |   18339 K  |
|       from large pool |     266    |     272    |    9265 K  |    9265 K  |
|       from small pool |     301    |     342    |    9074 K  |    9074 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     567    |     574    |   18340 K  |   18339 K  |
|       from large pool |     266    |     272    |    9265 K  |    9265 K  |
|       from small pool |     301    |     342    |    9074 K  |    9074 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      61    |     113    |    3936    |    3875    |
|       from large pool |      46    |      46    |    2187    |    2141    |
|       from small pool |      15    |      67    |    1749    |    1734    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      85    |      87    |   10011 K  |   10011 K  |
|       from large pool |      62    |      64    |    5739 K  |    5739 K  |
|       from small pool |      23    |      43    |    4271 K  |    4271 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:47:31] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:47:31] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:47:36]    INFO >> epoch 006:   1203 / 1539 loss=3.521, wps=2847.2, ups=3.79, wpb=750.3, bsz=750.3, num_updates=8850, lr=0.000327, gnorm=5.39, clip=0, train_wall=12, gb_free=61.7, wall=2450 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:47:49]    INFO >> epoch 006:   1253 / 1539 loss=3.507, wps=2842.1, ups=4.42, wpb=642.9, bsz=642.9, num_updates=8900, lr=0.000327, gnorm=4.508, clip=0, train_wall=11, gb_free=68.1, wall=2461 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:48:02]    INFO >> epoch 006:   1303 / 1539 loss=3.387, wps=2649, ups=3.8, wpb=696.6, bsz=696.6, num_updates=8950, lr=0.000327, gnorm=4.007, clip=0, train_wall=12, gb_free=64.2, wall=2474 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:48:14]    INFO >> epoch 006:   1353 / 1539 loss=3.507, wps=2769.6, ups=4.29, wpb=645.9, bsz=645.9, num_updates=9000, lr=0.000327, gnorm=4.828, clip=0, train_wall=11, gb_free=63.8, wall=2486 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:48:27]    INFO >> epoch 006:   1403 / 1539 loss=3.405, wps=2902.8, ups=4.08, wpb=711.5, bsz=711.5, num_updates=9050, lr=0.000327, gnorm=4.112, clip=0, train_wall=12, gb_free=64.4, wall=2498 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:48:40]    INFO >> epoch 006:   1453 / 1539 loss=3.366, wps=2792.4, ups=3.94, wpb=708, bsz=708, num_updates=9100, lr=0.000327, gnorm=4.542, clip=0, train_wall=12, gb_free=70.2, wall=2511 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:48:53]    INFO >> epoch 006:   1503 / 1539 loss=3.363, wps=2775, ups=3.78, wpb=735.1, bsz=735.1, num_updates=9150, lr=0.000327, gnorm=4.271, clip=0, train_wall=13, gb_free=48.6, wall=2524 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:49:03]    INFO >> epoch 006 | loss 3.452 | wps 2574 | ups 3.66 | wpb 702.8 | bsz 702.8 | num_updates 9186 | lr 0.000327 | gnorm 4.406 | clip 0 | train_wall 365 | gb_free 64.6 | wall 2532 (progress_bar.py:267, print())[0m
[33m[2025-11-21 02:49:03] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 02:49:32]    INFO >> epoch 006 | valid on 'valid' subset | loss 3.689 | wps 5383.9 | wpb 5412.5 | bsz 5412.5 | num_updates 9186 | best_loss 4.012 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 02:49:32]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 02:49:32]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/checkpoints/checkpoint_last.pt (epoch 6 @ 9186 updates, score 3.689) (writing took 0.026406 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 02:49:32] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 02:49:36]    INFO >> epoch 007:     14 / 1539 loss=3.477, wps=812.1, ups=1.24, wpb=654.2, bsz=654.2, num_updates=9200, lr=0.000295, gnorm=3.502, clip=0, train_wall=11, gb_free=65.4, wall=2564 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:49:38] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.14 GiB. GPU 2 has a total capacity of 79.14 GiB of which 2.71 GiB is free. Including non-PyTorch memory, this process has 76.40 GiB memory in use. Of the allocated memory 72.69 GiB is allocated by PyTorch, and 3.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:49:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:49:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:49:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 49           |        cudaMalloc retries: 73        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  67276 MiB |  74432 MiB | 564550 GiB | 564485 GiB |
|       from large pool |  67249 MiB |  74405 MiB | 562917 GiB | 562851 GiB |
|       from small pool |     27 MiB |     35 MiB |   1633 GiB |   1633 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  67276 MiB |  74432 MiB | 564550 GiB | 564485 GiB |
|       from large pool |  67249 MiB |  74405 MiB | 562917 GiB | 562851 GiB |
|       from small pool |     27 MiB |     35 MiB |   1633 GiB |   1633 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  67268 MiB |  74423 MiB | 564018 GiB | 563953 GiB |
|       from large pool |  67241 MiB |  74396 MiB | 562387 GiB | 562321 GiB |
|       from small pool |     27 MiB |     35 MiB |   1631 GiB |   1631 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77726 MiB |  77776 MiB |    824 GiB | 766280 MiB |
|       from large pool |  77694 MiB |  77694 MiB |    820 GiB | 762762 MiB |
|       from small pool |     32 MiB |     82 MiB |      3 GiB |   3518 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4995 MiB |  11067 MiB | 603250 GiB | 603245 GiB |
|       from large pool |   4990 MiB |  11064 MiB | 601391 GiB | 601386 GiB |
|       from small pool |      4 MiB |     31 MiB |   1859 GiB |   1859 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     489    |     498    |   19327 K  |   19326 K  |
|       from large pool |     195    |     204    |    9690 K  |    9690 K  |
|       from small pool |     294    |     356    |    9636 K  |    9635 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     489    |     498    |   19327 K  |   19326 K  |
|       from large pool |     195    |     204    |    9690 K  |    9690 K  |
|       from small pool |     294    |     356    |    9636 K  |    9635 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      62    |      87    |    3962    |    3900    |
|       from large pool |      46    |      46    |    2187    |    2141    |
|       from small pool |      16    |      41    |    1775    |    1759    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      73    |      73    |   10555 K  |   10555 K  |
|       from large pool |      49    |      49    |    5990 K  |    5990 K  |
|       from small pool |      24    |      60    |    4564 K  |    4564 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:49:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:49:38] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 02:49:45] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.33 GiB. GPU 2 has a total capacity of 79.14 GiB of which 531.25 MiB is free. Including non-PyTorch memory, this process has 78.60 GiB memory in use. Of the allocated memory 76.05 GiB is allocated by PyTorch, and 2.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:49:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:49:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:49:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 50           |        cudaMalloc retries: 74        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77079 MiB |  78543 MiB | 566001 GiB | 565926 GiB |
|       from large pool |  77055 MiB |  78519 MiB | 564364 GiB | 564289 GiB |
|       from small pool |     23 MiB |     39 MiB |   1637 GiB |   1637 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77079 MiB |  78543 MiB | 566001 GiB | 565926 GiB |
|       from large pool |  77055 MiB |  78519 MiB | 564364 GiB | 564289 GiB |
|       from small pool |     23 MiB |     39 MiB |   1637 GiB |   1637 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77063 MiB |  78526 MiB | 565468 GiB | 565393 GiB |
|       from large pool |  77040 MiB |  78502 MiB | 563833 GiB | 563758 GiB |
|       from small pool |     23 MiB |     39 MiB |   1634 GiB |   1634 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79974 MiB |  80008 MiB |    831 GiB | 771768 MiB |
|       from large pool |  79944 MiB |  79944 MiB |    828 GiB | 768216 MiB |
|       from small pool |     30 MiB |     64 MiB |      3 GiB |   3552 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2894 MiB |   9028 MiB | 604964 GiB | 604962 GiB |
|       from large pool |   2888 MiB |   9021 MiB | 603101 GiB | 603098 GiB |
|       from small pool |      6 MiB |     24 MiB |   1863 GiB |   1863 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     488    |     496    |   19374 K  |   19374 K  |
|       from large pool |     200    |     208    |    9716 K  |    9716 K  |
|       from small pool |     288    |     356    |    9657 K  |    9657 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     488    |     496    |   19374 K  |   19374 K  |
|       from large pool |     200    |     208    |    9716 K  |    9716 K  |
|       from small pool |     288    |     356    |    9657 K  |    9657 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      64    |      81    |    3982    |    3918    |
|       from large pool |      49    |      49    |    2191    |    2142    |
|       from small pool |      15    |      32    |    1791    |    1776    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      62    |      62    |   10579 K  |   10579 K  |
|       from large pool |      42    |      42    |    6005 K  |    6005 K  |
|       from small pool |      20    |      55    |    4574 K  |    4573 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:49:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:49:45] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:49:49]    INFO >> epoch 007:     66 / 1539 loss=3.316, wps=2524.5, ups=3.89, wpb=648.7, bsz=648.7, num_updates=9250, lr=0.000295, gnorm=3.749, clip=0, train_wall=11, gb_free=68.2, wall=2577 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:50:03]    INFO >> epoch 007:    116 / 1539 loss=3.451, wps=2736.8, ups=3.96, wpb=690.5, bsz=690.5, num_updates=9300, lr=0.000295, gnorm=4.461, clip=0, train_wall=12, gb_free=55.6, wall=2590 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:50:12] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 31.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 836.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:50:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:50:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:50:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 51           |        cudaMalloc retries: 75        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79577 MiB |  79637 MiB | 571490 GiB | 571412 GiB |
|       from large pool |  79375 MiB |  79435 MiB | 569837 GiB | 569759 GiB |
|       from small pool |    201 MiB |    203 MiB |   1653 GiB |   1653 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79577 MiB |  79637 MiB | 571490 GiB | 571412 GiB |
|       from large pool |  79375 MiB |  79435 MiB | 569837 GiB | 569759 GiB |
|       from small pool |    201 MiB |    203 MiB |   1653 GiB |   1653 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79553 MiB |  79613 MiB | 570951 GiB | 570874 GiB |
|       from large pool |  79351 MiB |  79411 MiB | 569300 GiB | 569223 GiB |
|       from small pool |    201 MiB |    202 MiB |   1651 GiB |   1650 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80474 MiB |  80474 MiB |    832 GiB | 771768 MiB |
|       from large pool |  80254 MiB |  80254 MiB |    828 GiB | 768216 MiB |
|       from small pool |    220 MiB |    220 MiB |      3 GiB |   3552 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    834 MiB |   7099 MiB | 611567 GiB | 611566 GiB |
|       from large pool |    816 MiB |   7096 MiB | 609685 GiB | 609684 GiB |
|       from small pool |     18 MiB |     21 MiB |   1882 GiB |   1882 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3754    |    3757    |   19572 K  |   19568 K  |
|       from large pool |     619    |     620    |    9819 K  |    9819 K  |
|       from small pool |    3135    |    3138    |    9752 K  |    9749 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3754    |    3757    |   19572 K  |   19568 K  |
|       from large pool |     619    |     620    |    9819 K  |    9819 K  |
|       from small pool |    3135    |    3138    |    9752 K  |    9749 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     164    |     164    |    4082    |    3918    |
|       from large pool |      54    |      54    |    2196    |    2142    |
|       from small pool |     110    |     110    |    1886    |    1776    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     228    |     229    |   10682 K  |   10682 K  |
|       from large pool |      51    |      52    |    6065 K  |    6065 K  |
|       from small pool |     177    |     177    |    4616 K  |    4616 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:50:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:50:12] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:50:16]    INFO >> epoch 007:    167 / 1539 loss=3.463, wps=2650.2, ups=3.86, wpb=686.1, bsz=686.1, num_updates=9350, lr=0.000295, gnorm=4.36, clip=0, train_wall=12, gb_free=66, wall=2603 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:50:29]    INFO >> epoch 007:    217 / 1539 loss=3.548, wps=2958.4, ups=3.94, wpb=751.7, bsz=751.7, num_updates=9400, lr=0.000295, gnorm=3.942, clip=0, train_wall=12, gb_free=66, wall=2615 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:50:44]    INFO >> epoch 007:    267 / 1539 loss=3.425, wps=2899.6, ups=3.62, wpb=801.3, bsz=801.3, num_updates=9450, lr=0.000295, gnorm=4.106, clip=0, train_wall=13, gb_free=63.8, wall=2629 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:50:56]    INFO >> epoch 007:    317 / 1539 loss=3.319, wps=2592.2, ups=3.96, wpb=654.8, bsz=654.8, num_updates=9500, lr=0.000295, gnorm=4.227, clip=0, train_wall=12, gb_free=35.9, wall=2642 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:50:58] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.69 GiB. GPU 2 has a total capacity of 79.14 GiB of which 281.25 MiB is free. Including non-PyTorch memory, this process has 78.84 GiB memory in use. Of the allocated memory 74.78 GiB is allocated by PyTorch, and 3.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:50:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:50:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:50:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 52           |        cudaMalloc retries: 76        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  71055 MiB |  76570 MiB | 581404 GiB | 581334 GiB |
|       from large pool |  71026 MiB |  76541 MiB | 579724 GiB | 579655 GiB |
|       from small pool |     28 MiB |     32 MiB |   1679 GiB |   1679 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  71055 MiB |  76570 MiB | 581404 GiB | 581334 GiB |
|       from large pool |  71026 MiB |  76541 MiB | 579724 GiB | 579655 GiB |
|       from small pool |     28 MiB |     32 MiB |   1679 GiB |   1679 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  71047 MiB |  76562 MiB | 580856 GiB | 580787 GiB |
|       from large pool |  71018 MiB |  76533 MiB | 579179 GiB | 579109 GiB |
|       from small pool |     28 MiB |     32 MiB |   1677 GiB |   1677 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80224 MiB |  80412 MiB |    832 GiB | 772018 MiB |
|       from large pool |  80192 MiB |  80192 MiB |    828 GiB | 768278 MiB |
|       from small pool |     32 MiB |    220 MiB |      3 GiB |   3740 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3714 MiB |  10234 MiB | 623305 GiB | 623301 GiB |
|       from large pool |   3711 MiB |  10230 MiB | 621392 GiB | 621388 GiB |
|       from small pool |      3 MiB |     23 MiB |   1913 GiB |   1913 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     489    |     498    |   19905 K  |   19904 K  |
|       from large pool |     195    |     204    |    9998 K  |    9998 K  |
|       from small pool |     294    |     342    |    9906 K  |    9906 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     489    |     498    |   19905 K  |   19904 K  |
|       from large pool |     195    |     204    |    9998 K  |    9998 K  |
|       from small pool |     294    |     342    |    9906 K  |    9906 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      69    |     163    |    4082    |    4013    |
|       from large pool |      53    |      53    |    2196    |    2143    |
|       from small pool |      16    |     110    |    1886    |    1870    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      86    |      89    |   10857 K  |   10857 K  |
|       from large pool |      62    |      65    |    6171 K  |    6171 K  |
|       from small pool |      24    |      54    |    4686 K  |    4686 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:50:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:50:58] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 02:51:01] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.53 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.19 GiB is free. Including non-PyTorch memory, this process has 77.92 GiB memory in use. Of the allocated memory 76.08 GiB is allocated by PyTorch, and 1.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:51:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:51:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:51:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 53           |        cudaMalloc retries: 77        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72887 MiB |  77908 MiB | 581981 GiB | 581910 GiB |
|       from large pool |  72860 MiB |  77881 MiB | 580300 GiB | 580228 GiB |
|       from small pool |     26 MiB |     36 MiB |   1681 GiB |   1681 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72887 MiB |  77908 MiB | 581981 GiB | 581910 GiB |
|       from large pool |  72860 MiB |  77881 MiB | 580300 GiB | 580228 GiB |
|       from small pool |     26 MiB |     36 MiB |   1681 GiB |   1681 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72874 MiB |  77895 MiB | 581433 GiB | 581362 GiB |
|       from large pool |  72847 MiB |  77867 MiB | 579754 GiB | 579683 GiB |
|       from small pool |     26 MiB |     36 MiB |   1678 GiB |   1678 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79282 MiB |  79310 MiB |    836 GiB | 777500 MiB |
|       from large pool |  79252 MiB |  79252 MiB |    832 GiB | 773732 MiB |
|       from small pool |     30 MiB |     58 MiB |      3 GiB |   3768 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1880 MiB |   9013 MiB | 623953 GiB | 623952 GiB |
|       from large pool |   1877 MiB |   9009 MiB | 622039 GiB | 622037 GiB |
|       from small pool |      3 MiB |     23 MiB |   1914 GiB |   1914 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     567    |     576    |   19920 K  |   19919 K  |
|       from large pool |     266    |     274    |   10006 K  |   10006 K  |
|       from small pool |     301    |     356    |    9913 K  |    9913 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     567    |     576    |   19920 K  |   19919 K  |
|       from large pool |     266    |     274    |   10006 K  |   10006 K  |
|       from small pool |     301    |     356    |    9913 K  |    9913 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      69    |      83    |    4097    |    4028    |
|       from large pool |      54    |      54    |    2198    |    2144    |
|       from small pool |      15    |      29    |    1899    |    1884    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      84    |      85    |   10865 K  |   10865 K  |
|       from large pool |      62    |      63    |    6176 K  |    6176 K  |
|       from small pool |      22    |      49    |    4689 K  |    4689 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:51:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:51:01] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:51:11]    INFO >> epoch 007:    369 / 1539 loss=3.569, wps=2408.5, ups=3.71, wpb=648.5, bsz=648.5, num_updates=9550, lr=0.000295, gnorm=3.958, clip=0, train_wall=11, gb_free=64, wall=2655 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:51:24]    INFO >> epoch 007:    419 / 1539 loss=3.443, wps=2427.4, ups=3.88, wpb=624.8, bsz=624.8, num_updates=9600, lr=0.000295, gnorm=3.537, clip=0, train_wall=12, gb_free=61.4, wall=2668 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:51:37]    INFO >> epoch 007:    469 / 1539 loss=3.45, wps=2676.8, ups=3.72, wpb=719.1, bsz=719.1, num_updates=9650, lr=0.000295, gnorm=4.038, clip=0, train_wall=13, gb_free=68, wall=2682 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:51:38] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.88 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.07 GiB is free. Including non-PyTorch memory, this process has 78.05 GiB memory in use. Of the allocated memory 72.80 GiB is allocated by PyTorch, and 4.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:51:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:51:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:51:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 54           |        cudaMalloc retries: 80        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  45875 MiB |  74763 MiB | 589561 GiB | 589516 GiB |
|       from large pool |  45850 MiB |  74738 MiB | 587860 GiB | 587815 GiB |
|       from small pool |     24 MiB |     32 MiB |   1700 GiB |   1700 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  45875 MiB |  74763 MiB | 589561 GiB | 589516 GiB |
|       from large pool |  45850 MiB |  74738 MiB | 587860 GiB | 587815 GiB |
|       from small pool |     24 MiB |     32 MiB |   1700 GiB |   1700 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  45867 MiB |  74754 MiB | 589006 GiB | 588961 GiB |
|       from large pool |  45842 MiB |  74730 MiB | 587307 GiB | 587262 GiB |
|       from small pool |     24 MiB |     32 MiB |   1698 GiB |   1698 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79414 MiB |  79414 MiB |    847 GiB |    770 GiB |
|       from large pool |  79384 MiB |  79384 MiB |    844 GiB |    766 GiB |
|       from small pool |     30 MiB |     88 MiB |      3 GiB |      3 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  14636 MiB |  14789 MiB | 632953 GiB | 632939 GiB |
|       from large pool |  14631 MiB |  14784 MiB | 631016 GiB | 631002 GiB |
|       from small pool |      5 MiB |     19 MiB |   1937 GiB |   1937 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     412    |     463    |   20173 K  |   20172 K  |
|       from large pool |     128    |     178    |   10145 K  |   10145 K  |
|       from small pool |     284    |     356    |   10027 K  |   10027 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     412    |     463    |   20173 K  |   20172 K  |
|       from large pool |     128    |     178    |   10145 K  |   10145 K  |
|       from small pool |     284    |     356    |   10027 K  |   10027 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      65    |      97    |    4129    |    4064    |
|       from large pool |      50    |      53    |    2201    |    2151    |
|       from small pool |      15    |      44    |    1928    |    1913    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      75    |      79    |   10997 K  |   10997 K  |
|       from large pool |      53    |      57    |    6258 K  |    6258 K  |
|       from small pool |      22    |      52    |    4738 K  |    4738 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:51:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:51:38] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:51:51]    INFO >> epoch 007:    520 / 1539 loss=3.348, wps=2851.7, ups=3.94, wpb=723.4, bsz=723.4, num_updates=9700, lr=0.000295, gnorm=4.313, clip=0, train_wall=11, gb_free=55.2, wall=2694 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:52:04]    INFO >> epoch 007:    570 / 1539 loss=3.566, wps=2651.1, ups=3.91, wpb=677.4, bsz=677.4, num_updates=9750, lr=0.000295, gnorm=3.92, clip=0, train_wall=12, gb_free=67.8, wall=2707 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:52:19]    INFO >> epoch 007:    620 / 1539 loss=3.374, wps=2649.3, ups=3.88, wpb=683.7, bsz=683.7, num_updates=9800, lr=0.000295, gnorm=4.546, clip=2, train_wall=12, gb_free=53.8, wall=2720 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:52:31]    INFO >> epoch 007:    670 / 1539 loss=3.371, wps=3124.5, ups=4.1, wpb=762.3, bsz=762.3, num_updates=9850, lr=0.000295, gnorm=4.656, clip=0, train_wall=11, gb_free=68.6, wall=2732 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:52:44]    INFO >> epoch 007:    720 / 1539 loss=3.323, wps=2973.8, ups=3.66, wpb=813.5, bsz=813.5, num_updates=9900, lr=0.000295, gnorm=3.972, clip=0, train_wall=13, gb_free=61.4, wall=2746 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:52:58]    INFO >> epoch 007:    770 / 1539 loss=3.515, wps=2364.9, ups=4.07, wpb=581.5, bsz=581.5, num_updates=9950, lr=0.000295, gnorm=3.678, clip=0, train_wall=12, gb_free=72.9, wall=2758 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:53:10]    INFO >> epoch 007:    820 / 1539 loss=3.444, wps=3009.4, ups=4.01, wpb=749.7, bsz=749.7, num_updates=10000, lr=0.000295, gnorm=4.388, clip=0, train_wall=12, gb_free=64.2, wall=2771 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:53:25]    INFO >> epoch 007:    870 / 1539 loss=3.385, wps=2615.3, ups=3.87, wpb=676, bsz=676, num_updates=10050, lr=0.000295, gnorm=4.079, clip=0, train_wall=12, gb_free=66.9, wall=2783 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:53:36]    INFO >> epoch 007:    920 / 1539 loss=3.455, wps=2876.6, ups=4.31, wpb=666.7, bsz=666.7, num_updates=10100, lr=0.000295, gnorm=3.823, clip=0, train_wall=11, gb_free=67.5, wall=2795 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:53:50]    INFO >> epoch 007:    970 / 1539 loss=3.519, wps=2724, ups=3.58, wpb=760.3, bsz=760.3, num_updates=10150, lr=0.000295, gnorm=4.097, clip=0, train_wall=13, gb_free=65.5, wall=2809 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:54:04]    INFO >> epoch 007:   1020 / 1539 loss=3.424, wps=2692.9, ups=4.05, wpb=664.4, bsz=664.4, num_updates=10200, lr=0.000295, gnorm=3.989, clip=0, train_wall=12, gb_free=67.2, wall=2821 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:54:18]    INFO >> epoch 007:   1070 / 1539 loss=3.419, wps=2614.1, ups=3.46, wpb=754.5, bsz=754.5, num_updates=10250, lr=0.000295, gnorm=4.293, clip=0, train_wall=14, gb_free=65.5, wall=2836 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:54:31] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 49.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 77.65 GiB is allocated by PyTorch, and 937.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:54:31] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:54:31] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 05:57:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 55           |        cudaMalloc retries: 81        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79459 MiB |  79518 MiB | 625200 GiB | 625123 GiB |
|       from large pool |  79366 MiB |  79425 MiB | 623401 GiB | 623324 GiB |
|       from small pool |     92 MiB |     94 MiB |   1798 GiB |   1798 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79459 MiB |  79518 MiB | 625200 GiB | 625123 GiB |
|       from large pool |  79366 MiB |  79425 MiB | 623401 GiB | 623324 GiB |
|       from small pool |     92 MiB |     94 MiB |   1798 GiB |   1798 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79432 MiB |  79490 MiB | 624609 GiB | 624531 GiB |
|       from large pool |  79339 MiB |  79398 MiB | 622813 GiB | 622735 GiB |
|       from small pool |     92 MiB |     93 MiB |   1796 GiB |   1796 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80456 MiB |  80488 MiB |    867 GiB |    788 GiB |
|       from large pool |  80354 MiB |  80354 MiB |    863 GiB |    785 GiB |
|       from small pool |    102 MiB |    134 MiB |      3 GiB |      3 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    936 MiB |   6641 MiB | 674241 GiB | 674240 GiB |
|       from large pool |    927 MiB |   6638 MiB | 672190 GiB | 672189 GiB |
|       from small pool |      9 MiB |     23 MiB |   2051 GiB |   2051 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1818    |    1821    |   21400 K  |   21398 K  |
|       from large pool |     451    |     452    |   10803 K  |   10803 K  |
|       from small pool |    1367    |    1370    |   10596 K  |   10595 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1818    |    1821    |   21400 K  |   21398 K  |
|       from large pool |     451    |     452    |   10803 K  |   10803 K  |
|       from small pool |    1367    |    1370    |   10596 K  |   10595 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     194    |     210    |    4279    |    4085    |
|       from large pool |     143    |     143    |    2299    |    2156    |
|       from small pool |      51    |      67    |    1980    |    1929    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     208    |     209    |   11660 K  |   11660 K  |
|       from large pool |     126    |     127    |    6668 K  |    6668 K  |
|       from small pool |      82    |      82    |    4992 K  |    4992 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 05:57:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 05:57:50] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 05:57:52]    INFO >> epoch 007:   1121 / 1539 loss=3.31, wps=3.2, ups=0, wpb=680.3, bsz=680.3, num_updates=10300, lr=0.000295, gnorm=3.97, clip=0, train_wall=11, gb_free=61.1, wall=13409 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:58:04]    INFO >> epoch 007:   1171 / 1539 loss=3.327, wps=3384.4, ups=4.31, wpb=785.4, bsz=785.4, num_updates=10350, lr=0.000295, gnorm=4.387, clip=0, train_wall=11, gb_free=33.4, wall=13420 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:58:14]    INFO >> epoch 007:   1221 / 1539 loss=3.325, wps=3171.7, ups=4.79, wpb=662.5, bsz=662.5, num_updates=10400, lr=0.000295, gnorm=4.099, clip=0, train_wall=10, gb_free=61, wall=13431 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:58:26]    INFO >> epoch 007:   1271 / 1539 loss=3.397, wps=3298.1, ups=4.86, wpb=678.9, bsz=678.9, num_updates=10450, lr=0.000295, gnorm=4.914, clip=0, train_wall=10, gb_free=60.4, wall=13441 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:58:36]    INFO >> epoch 007:   1321 / 1539 loss=3.345, wps=3219.3, ups=4.91, wpb=655.4, bsz=655.4, num_updates=10500, lr=0.000295, gnorm=4.199, clip=0, train_wall=10, gb_free=63.7, wall=13451 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:58:46]    INFO >> epoch 007:   1371 / 1539 loss=3.503, wps=3149.7, ups=4.81, wpb=654.7, bsz=654.7, num_updates=10550, lr=0.000295, gnorm=3.849, clip=0, train_wall=10, gb_free=53.2, wall=13462 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:58:59]    INFO >> epoch 007:   1421 / 1539 loss=3.36, wps=3464, ups=4.48, wpb=772.8, bsz=772.8, num_updates=10600, lr=0.000295, gnorm=3.746, clip=0, train_wall=11, gb_free=65.5, wall=13473 (progress_bar.py:258, log())[0m
[33m[2025-11-21 05:59:00] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 588.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 209.25 MiB is free. Including non-PyTorch memory, this process has 78.91 GiB memory in use. Of the allocated memory 75.01 GiB is allocated by PyTorch, and 3.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 05:59:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 05:59:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 05:59:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 56           |        cudaMalloc retries: 83        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76539 MiB |  77845 MiB | 642636 GiB | 642561 GiB |
|       from large pool |  76509 MiB |  77815 MiB | 640789 GiB | 640715 GiB |
|       from small pool |     30 MiB |     33 MiB |   1846 GiB |   1846 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76539 MiB |  77845 MiB | 642636 GiB | 642561 GiB |
|       from large pool |  76509 MiB |  77815 MiB | 640789 GiB | 640715 GiB |
|       from small pool |     30 MiB |     33 MiB |   1846 GiB |   1846 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76526 MiB |  77832 MiB | 642026 GiB | 641951 GiB |
|       from large pool |  76495 MiB |  77801 MiB | 640181 GiB | 640107 GiB |
|       from small pool |     30 MiB |     33 MiB |   1844 GiB |   1844 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80296 MiB |  80412 MiB |    872 GiB |    793 GiB |
|       from large pool |  80264 MiB |  80294 MiB |    868 GiB |    789 GiB |
|       from small pool |     32 MiB |    118 MiB |      3 GiB |      3 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3756 MiB |   8661 MiB | 690895 GiB | 690891 GiB |
|       from large pool |   3754 MiB |   8658 MiB | 688787 GiB | 688783 GiB |
|       from small pool |      1 MiB |     18 MiB |   2107 GiB |   2107 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     618    |     625    |   21996 K  |   21995 K  |
|       from large pool |     314    |     321    |   11124 K  |   11123 K  |
|       from small pool |     304    |     356    |   10872 K  |   10872 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     618    |     625    |   21996 K  |   21995 K  |
|       from large pool |     314    |     321    |   11124 K  |   11123 K  |
|       from small pool |     304    |     356    |   10872 K  |   10872 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      95    |     201    |    4292    |    4197    |
|       from large pool |      79    |     142    |    2303    |    2224    |
|       from small pool |      16    |      59    |    1989    |    1973    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      89    |      95    |   11994 K  |   11994 K  |
|       from large pool |      68    |      74    |    6878 K  |    6878 K  |
|       from small pool |      21    |      48    |    5116 K  |    5115 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 05:59:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 05:59:00] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 05:59:10]    INFO >> epoch 007:   1472 / 1539 loss=3.379, wps=3238.1, ups=4.59, wpb=705.4, bsz=705.4, num_updates=10650, lr=0.000295, gnorm=3.523, clip=0, train_wall=10, gb_free=69.8, wall=13484 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:59:21]    INFO >> epoch 007:   1522 / 1539 loss=3.266, wps=3423.2, ups=4.32, wpb=792.7, bsz=792.7, num_updates=10700, lr=0.000295, gnorm=3.675, clip=0, train_wall=11, gb_free=67.5, wall=13495 (progress_bar.py:258, log())[0m
[32m[2025-11-21 05:59:26]    INFO >> epoch 007 | loss 3.41 | wps 98.1 | ups 0.14 | wpb 702.8 | bsz 702.8 | num_updates 10717 | lr 0.000295 | gnorm 4.07 | clip 0.1 | train_wall 353 | gb_free 60.5 | wall 13498 (progress_bar.py:267, print())[0m
[33m[2025-11-21 05:59:26] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 05:59:47]    INFO >> epoch 007 | valid on 'valid' subset | loss 3.724 | wps 7205.1 | wpb 5412.5 | bsz 5412.5 | num_updates 10717 | best_loss 4.012 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 05:59:47]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 05:59:47]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/checkpoints/checkpoint_last.pt (epoch 7 @ 10717 updates, score 3.724) (writing took 0.040121 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 05:59:47] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 05:59:55]    INFO >> epoch 008:     33 / 1539 loss=3.178, wps=1112.4, ups=1.56, wpb=712.2, bsz=712.2, num_updates=10750, lr=0.000262, gnorm=3.816, clip=0, train_wall=10, gb_free=68.4, wall=13527 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:00:08]    INFO >> epoch 008:     83 / 1539 loss=3.391, wps=3111.7, ups=4.25, wpb=733, bsz=733, num_updates=10800, lr=0.000262, gnorm=3.594, clip=0, train_wall=11, gb_free=64.1, wall=13539 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:00:20]    INFO >> epoch 008:    133 / 1539 loss=3.377, wps=2715.5, ups=4.23, wpb=642.3, bsz=642.3, num_updates=10850, lr=0.000262, gnorm=4.139, clip=0, train_wall=11, gb_free=65.9, wall=13551 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:00:34]    INFO >> epoch 008:    183 / 1539 loss=3.25, wps=2871.2, ups=3.73, wpb=769.1, bsz=769.1, num_updates=10900, lr=0.000262, gnorm=3.636, clip=0, train_wall=13, gb_free=67.3, wall=13564 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:00:47]    INFO >> epoch 008:    233 / 1539 loss=3.323, wps=3097, ups=3.78, wpb=818.8, bsz=818.8, num_updates=10950, lr=0.000262, gnorm=4.271, clip=0, train_wall=13, gb_free=39.2, wall=13577 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:01:00]    INFO >> epoch 008:    283 / 1539 loss=3.421, wps=2718.2, ups=4.09, wpb=664.8, bsz=664.8, num_updates=11000, lr=0.000262, gnorm=3.808, clip=0, train_wall=12, gb_free=67.6, wall=13590 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:01:15]    INFO >> epoch 008:    333 / 1539 loss=3.518, wps=2472.6, ups=3.51, wpb=705.1, bsz=705.1, num_updates=11050, lr=0.000262, gnorm=3.88, clip=0, train_wall=13, gb_free=63.9, wall=13604 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:01:28]    INFO >> epoch 008:    383 / 1539 loss=3.309, wps=2791.5, ups=4, wpb=698.4, bsz=698.4, num_updates=11100, lr=0.000262, gnorm=4.554, clip=0, train_wall=12, gb_free=64.7, wall=13616 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:01:41]    INFO >> epoch 008:    433 / 1539 loss=3.426, wps=2874.4, ups=4.1, wpb=700.5, bsz=700.5, num_updates=11150, lr=0.000262, gnorm=4.457, clip=0, train_wall=12, gb_free=68.3, wall=13629 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:01:45] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 17.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 77.83 GiB is allocated by PyTorch, and 795.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:01:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:01:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:01:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 57           |        cudaMalloc retries: 85        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79634 MiB |  79692 MiB | 682470 GiB | 682392 GiB |
|       from large pool |  79539 MiB |  79598 MiB | 680496 GiB | 680418 GiB |
|       from small pool |     94 MiB |     95 MiB |   1973 GiB |   1973 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79634 MiB |  79692 MiB | 682470 GiB | 682392 GiB |
|       from large pool |  79539 MiB |  79598 MiB | 680496 GiB | 680418 GiB |
|       from small pool |     94 MiB |     95 MiB |   1973 GiB |   1973 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79610 MiB |  79669 MiB | 681823 GiB | 681745 GiB |
|       from large pool |  79516 MiB |  79575 MiB | 679852 GiB | 679774 GiB |
|       from small pool |     94 MiB |     95 MiB |   1971 GiB |   1971 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80488 MiB |  80490 MiB |    872 GiB |    793 GiB |
|       from large pool |  80384 MiB |  80384 MiB |    868 GiB |    789 GiB |
|       from small pool |    104 MiB |    218 MiB |      4 GiB |      3 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    793 MiB |   7677 MiB | 727567 GiB | 727567 GiB |
|       from large pool |    784 MiB |   7672 MiB | 725317 GiB | 725316 GiB |
|       from small pool |      9 MiB |     27 MiB |   2250 GiB |   2250 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1851    |    1854    |   23384 K  |   23382 K  |
|       from large pool |     454    |     455    |   11741 K  |   11741 K  |
|       from small pool |    1397    |    1400    |   11642 K  |   11640 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1851    |    1854    |   23384 K  |   23382 K  |
|       from large pool |     454    |     455    |   11741 K  |   11741 K  |
|       from small pool |    1397    |    1400    |   11642 K  |   11640 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     133    |     188    |    4389    |    4256    |
|       from large pool |      81    |      81    |    2305    |    2224    |
|       from small pool |      52    |     109    |    2084    |    2032    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     146    |     147    |   12773 K  |   12773 K  |
|       from large pool |      64    |      65    |    7257 K  |    7257 K  |
|       from small pool |      82    |      82    |    5516 K  |    5516 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:01:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:01:45] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:01:54]    INFO >> epoch 008:    484 / 1539 loss=3.401, wps=2988.6, ups=3.8, wpb=786.8, bsz=786.8, num_updates=11200, lr=0.000262, gnorm=4.326, clip=0, train_wall=12, gb_free=68.5, wall=13642 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:02:08]    INFO >> epoch 008:    534 / 1539 loss=3.341, wps=2601.9, ups=3.79, wpb=685.9, bsz=685.9, num_updates=11250, lr=0.000262, gnorm=4.496, clip=0, train_wall=13, gb_free=64.9, wall=13655 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:02:22]    INFO >> epoch 008:    584 / 1539 loss=3.537, wps=2686.1, ups=3.98, wpb=674.5, bsz=674.5, num_updates=11300, lr=0.000262, gnorm=3.998, clip=0, train_wall=12, gb_free=54.3, wall=13667 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:02:33]    INFO >> epoch 008:    634 / 1539 loss=3.392, wps=2732.2, ups=4.28, wpb=638.8, bsz=638.8, num_updates=11350, lr=0.000262, gnorm=4.088, clip=0, train_wall=11, gb_free=61.5, wall=13679 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:02:46]    INFO >> epoch 008:    684 / 1539 loss=3.282, wps=2962.9, ups=4.34, wpb=682.8, bsz=682.8, num_updates=11400, lr=0.000262, gnorm=3.706, clip=0, train_wall=11, gb_free=65.9, wall=13691 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:02:59]    INFO >> epoch 008:    734 / 1539 loss=3.551, wps=2686.8, ups=4.01, wpb=669.6, bsz=669.6, num_updates=11450, lr=0.000262, gnorm=3.577, clip=0, train_wall=12, gb_free=61, wall=13703 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:03:12]    INFO >> epoch 008:    784 / 1539 loss=3.32, wps=2698.7, ups=3.87, wpb=697.7, bsz=697.7, num_updates=11500, lr=0.000262, gnorm=4.144, clip=0, train_wall=12, gb_free=62.1, wall=13716 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:03:25]    INFO >> epoch 008:    834 / 1539 loss=3.286, wps=2846.2, ups=4.12, wpb=691, bsz=691, num_updates=11550, lr=0.000262, gnorm=3.686, clip=0, train_wall=12, gb_free=63.4, wall=13728 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:03:37]    INFO >> epoch 008:    884 / 1539 loss=3.509, wps=2689.6, ups=4.3, wpb=626, bsz=626, num_updates=11600, lr=0.000262, gnorm=3.541, clip=0, train_wall=11, gb_free=63.2, wall=13740 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:03:40] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 1.25 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 74.21 GiB is allocated by PyTorch, and 4.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:03:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:03:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:03:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 58           |        cudaMalloc retries: 86        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75993 MiB |  76053 MiB | 706737 GiB | 706663 GiB |
|       from large pool |  75828 MiB |  75888 MiB | 704695 GiB | 704621 GiB |
|       from small pool |    165 MiB |    166 MiB |   2041 GiB |   2041 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75993 MiB |  76053 MiB | 706737 GiB | 706663 GiB |
|       from large pool |  75828 MiB |  75888 MiB | 704695 GiB | 704621 GiB |
|       from small pool |    165 MiB |    166 MiB |   2041 GiB |   2041 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75972 MiB |  76032 MiB | 706065 GiB | 705991 GiB |
|       from large pool |  75807 MiB |  75867 MiB | 704026 GiB | 703952 GiB |
|       from small pool |    165 MiB |    166 MiB |   2039 GiB |   2038 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80504 MiB |  80504 MiB |    872 GiB |    793 GiB |
|       from large pool |  80324 MiB |  80324 MiB |    868 GiB |    789 GiB |
|       from small pool |    180 MiB |    180 MiB |      4 GiB |      3 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4510 MiB |   7531 MiB | 753347 GiB | 753343 GiB |
|       from large pool |   4495 MiB |   7529 MiB | 751018 GiB | 751014 GiB |
|       from small pool |     14 MiB |     17 MiB |   2329 GiB |   2329 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3101    |    3102    |   24237 K  |   24234 K  |
|       from large pool |     560    |     561    |   12200 K  |   12199 K  |
|       from small pool |    2541    |    2542    |   12037 K  |   12034 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3101    |    3102    |   24237 K  |   24234 K  |
|       from large pool |     560    |     561    |   12200 K  |   12199 K  |
|       from small pool |    2541    |    2542    |   12037 K  |   12034 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     170    |     170    |    4427    |    4257    |
|       from large pool |      80    |      80    |    2305    |    2225    |
|       from small pool |      90    |      90    |    2122    |    2032    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     206    |     208    |   13229 K  |   13229 K  |
|       from large pool |      64    |      65    |    7538 K  |    7538 K  |
|       from small pool |     142    |     144    |    5691 K  |    5691 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:03:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:03:40] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:03:49]    INFO >> epoch 008:    935 / 1539 loss=3.31, wps=2598.5, ups=3.9, wpb=666.6, bsz=666.6, num_updates=11650, lr=0.000262, gnorm=3.858, clip=0, train_wall=12, gb_free=64.5, wall=13753 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:04:03]    INFO >> epoch 008:    985 / 1539 loss=3.363, wps=2815.7, ups=4.25, wpb=662.7, bsz=662.7, num_updates=11700, lr=0.000262, gnorm=3.549, clip=0, train_wall=11, gb_free=68.1, wall=13764 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:04:04] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.69 GiB. GPU 2 has a total capacity of 79.14 GiB of which 3.14 GiB is free. Including non-PyTorch memory, this process has 75.98 GiB memory in use. Of the allocated memory 70.17 GiB is allocated by PyTorch, and 5.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:04:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:04:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:04:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 59           |        cudaMalloc retries: 87        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  71064 MiB |  71855 MiB | 711512 GiB | 711443 GiB |
|       from large pool |  71035 MiB |  71826 MiB | 709458 GiB | 709389 GiB |
|       from small pool |     28 MiB |     34 MiB |   2054 GiB |   2054 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  71064 MiB |  71855 MiB | 711512 GiB | 711443 GiB |
|       from large pool |  71035 MiB |  71826 MiB | 709458 GiB | 709389 GiB |
|       from small pool |     28 MiB |     34 MiB |   2054 GiB |   2054 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  71047 MiB |  71838 MiB | 710835 GiB | 710766 GiB |
|       from large pool |  71018 MiB |  71809 MiB | 708784 GiB | 708714 GiB |
|       from small pool |     28 MiB |     34 MiB |   2051 GiB |   2051 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77290 MiB |  80504 MiB |    872 GiB |    796 GiB |
|       from large pool |  77256 MiB |  80324 MiB |    868 GiB |    792 GiB |
|       from small pool |     34 MiB |    180 MiB |      4 GiB |      4 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6225 MiB |  11933 MiB | 758402 GiB | 758396 GiB |
|       from large pool |   6220 MiB |  11928 MiB | 756059 GiB | 756052 GiB |
|       from small pool |      5 MiB |     18 MiB |   2343 GiB |   2343 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     489    |     496    |   24402 K  |   24401 K  |
|       from large pool |     195    |     202    |   12292 K  |   12292 K  |
|       from small pool |     294    |     356    |   12109 K  |   12109 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     489    |     496    |   24402 K  |   24401 K  |
|       from large pool |     195    |     202    |   12292 K  |   12292 K  |
|       from small pool |     294    |     356    |   12109 K  |   12109 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      88    |     170    |    4427    |    4339    |
|       from large pool |      71    |      80    |    2305    |    2234    |
|       from small pool |      17    |      90    |    2122    |    2105    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      89    |      89    |   13317 K  |   13317 K  |
|       from large pool |      65    |      65    |    7595 K  |    7594 K  |
|       from small pool |      24    |      49    |    5722 K  |    5722 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:04:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:04:04] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 06:04:10] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.14 GiB. GPU 2 has a total capacity of 79.14 GiB of which 935.25 MiB is free. Including non-PyTorch memory, this process has 78.20 GiB memory in use. Of the allocated memory 72.69 GiB is allocated by PyTorch, and 5.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:04:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:04:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:04:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 60           |        cudaMalloc retries: 89        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  67279 MiB |  74434 MiB | 712911 GiB | 712845 GiB |
|       from large pool |  67251 MiB |  74407 MiB | 710854 GiB | 710788 GiB |
|       from small pool |     27 MiB |     39 MiB |   2057 GiB |   2057 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  67279 MiB |  74434 MiB | 712911 GiB | 712845 GiB |
|       from large pool |  67251 MiB |  74407 MiB | 710854 GiB | 710788 GiB |
|       from small pool |     27 MiB |     39 MiB |   2057 GiB |   2057 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  67268 MiB |  74423 MiB | 712233 GiB | 712167 GiB |
|       from large pool |  67241 MiB |  74396 MiB | 710178 GiB | 710112 GiB |
|       from small pool |     27 MiB |     39 MiB |   2054 GiB |   2054 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79570 MiB |  79570 MiB |    877 GiB |    799 GiB |
|       from large pool |  79534 MiB |  79534 MiB |    873 GiB |    795 GiB |
|       from small pool |     36 MiB |     66 MiB |      4 GiB |      4 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6460 MiB |  11185 MiB | 759871 GiB | 759864 GiB |
|       from large pool |   6452 MiB |  11176 MiB | 757523 GiB | 757517 GiB |
|       from small pool |      8 MiB |     25 MiB |   2347 GiB |   2347 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     489    |     498    |   24446 K  |   24445 K  |
|       from large pool |     195    |     204    |   12317 K  |   12316 K  |
|       from small pool |     294    |     356    |   12129 K  |   12129 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     489    |     498    |   24446 K  |   24445 K  |
|       from large pool |     195    |     204    |   12317 K  |   12316 K  |
|       from small pool |     294    |     356    |   12129 K  |   12129 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      77    |     104    |    4444    |    4367    |
|       from large pool |      59    |      71    |    2306    |    2247    |
|       from small pool |      18    |      33    |    2138    |    2120    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      76    |      79    |   13340 K  |   13340 K  |
|       from large pool |      50    |      53    |    7610 K  |    7610 K  |
|       from small pool |      26    |      55    |    5730 K  |    5730 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:04:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:04:10] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:04:17]    INFO >> epoch 008:   1037 / 1539 loss=3.231, wps=2513, ups=3.52, wpb=714.7, bsz=714.7, num_updates=11750, lr=0.000262, gnorm=4.038, clip=0, train_wall=12, gb_free=53.2, wall=13779 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:04:30]    INFO >> epoch 008:   1087 / 1539 loss=3.339, wps=2809.7, ups=4.09, wpb=687.1, bsz=687.1, num_updates=11800, lr=0.000262, gnorm=3.774, clip=0, train_wall=12, gb_free=59.9, wall=13791 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:04:43]    INFO >> epoch 008:   1137 / 1539 loss=3.44, wps=2515.9, ups=4.09, wpb=614.6, bsz=614.6, num_updates=11850, lr=0.000262, gnorm=3.432, clip=0, train_wall=12, gb_free=56.9, wall=13803 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:04:52] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.88 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.09 GiB is free. Including non-PyTorch memory, this process has 78.03 GiB memory in use. Of the allocated memory 72.80 GiB is allocated by PyTorch, and 4.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:04:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:04:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:04:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 61           |        cudaMalloc retries: 92        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  45878 MiB |  74765 MiB | 721454 GiB | 721409 GiB |
|       from large pool |  45853 MiB |  74741 MiB | 719373 GiB | 719328 GiB |
|       from small pool |     24 MiB |     33 MiB |   2080 GiB |   2080 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  45878 MiB |  74765 MiB | 721454 GiB | 721409 GiB |
|       from large pool |  45853 MiB |  74741 MiB | 719373 GiB | 719328 GiB |
|       from small pool |     24 MiB |     33 MiB |   2080 GiB |   2080 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  45867 MiB |  74754 MiB | 720767 GiB | 720723 GiB |
|       from large pool |  45842 MiB |  74730 MiB | 718689 GiB | 718644 GiB |
|       from small pool |     24 MiB |     33 MiB |   2078 GiB |   2078 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79394 MiB |  79896 MiB |    892 GiB |    814 GiB |
|       from large pool |  79364 MiB |  79866 MiB |    888 GiB |    810 GiB |
|       from small pool |     30 MiB |     66 MiB |      4 GiB |      4 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  15607 MiB |  19270 MiB | 769755 GiB | 769740 GiB |
|       from large pool |  15602 MiB |  19265 MiB | 767380 GiB | 767365 GiB |
|       from small pool |      5 MiB |     23 MiB |   2374 GiB |   2374 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     415    |     466    |   24741 K  |   24740 K  |
|       from large pool |     128    |     178    |   12478 K  |   12478 K  |
|       from small pool |     287    |     348    |   12263 K  |   12262 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     415    |     466    |   24741 K  |   24740 K  |
|       from large pool |     128    |     178    |   12478 K  |   12478 K  |
|       from small pool |     287    |     348    |   12263 K  |   12262 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      66    |      90    |    4463    |    4397    |
|       from large pool |      51    |      57    |    2310    |    2259    |
|       from small pool |      15    |      33    |    2153    |    2138    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      70    |      78    |   13494 K  |   13494 K  |
|       from large pool |      48    |      56    |    7706 K  |    7705 K  |
|       from small pool |      22    |      53    |    5788 K  |    5788 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:04:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:04:52] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:04:55]    INFO >> epoch 008:   1188 / 1539 loss=3.413, wps=2689.2, ups=3.92, wpb=685.3, bsz=685.3, num_updates=11900, lr=0.000262, gnorm=4.415, clip=0, train_wall=11, gb_free=65.3, wall=13816 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:05:11]    INFO >> epoch 008:   1238 / 1539 loss=3.438, wps=2418.7, ups=3.46, wpb=698, bsz=698, num_updates=11950, lr=0.000262, gnorm=4.277, clip=0, train_wall=14, gb_free=63.1, wall=13830 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:05:12] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 588.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 403.25 MiB is free. Including non-PyTorch memory, this process has 78.72 GiB memory in use. Of the allocated memory 77.47 GiB is allocated by PyTorch, and 775.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:05:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:05:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:05:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 62           |        cudaMalloc retries: 93        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78819 MiB |  79326 MiB | 725702 GiB | 725625 GiB |
|       from large pool |  78789 MiB |  79296 MiB | 723611 GiB | 723534 GiB |
|       from small pool |     30 MiB |     30 MiB |   2091 GiB |   2091 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78819 MiB |  79326 MiB | 725702 GiB | 725625 GiB |
|       from large pool |  78789 MiB |  79296 MiB | 723611 GiB | 723534 GiB |
|       from small pool |     30 MiB |     30 MiB |   2091 GiB |   2091 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78797 MiB |  79303 MiB | 725012 GiB | 724935 GiB |
|       from large pool |  78766 MiB |  79273 MiB | 722923 GiB | 722846 GiB |
|       from small pool |     30 MiB |     30 MiB |   2088 GiB |   2088 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80102 MiB |  80132 MiB |    910 GiB |    832 GiB |
|       from large pool |  80070 MiB |  80070 MiB |    906 GiB |    828 GiB |
|       from small pool |     32 MiB |     62 MiB |      4 GiB |      4 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1282 MiB |   8297 MiB | 774888 GiB | 774887 GiB |
|       from large pool |   1280 MiB |   8294 MiB | 772502 GiB | 772500 GiB |
|       from small pool |      1 MiB |     16 MiB |   2386 GiB |   2386 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     628    |     632    |   24868 K  |   24868 K  |
|       from large pool |     324    |     328    |   12547 K  |   12546 K  |
|       from small pool |     304    |     335    |   12321 K  |   12321 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     628    |     632    |   24868 K  |   24868 K  |
|       from large pool |     324    |     328    |   12547 K  |   12546 K  |
|       from small pool |     304    |     335    |   12321 K  |   12321 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      90    |     105    |    4508    |    4418    |
|       from large pool |      74    |      74    |    2339    |    2265    |
|       from small pool |      16    |      31    |    2169    |    2153    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      87    |      87    |   13560 K  |   13560 K  |
|       from large pool |      65    |      65    |    7746 K  |    7746 K  |
|       from small pool |      22    |      47    |    5814 K  |    5814 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:05:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:05:12] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:05:25]    INFO >> epoch 008:   1289 / 1539 loss=3.296, wps=2866.1, ups=3.63, wpb=790, bsz=790, num_updates=12000, lr=0.000262, gnorm=4.085, clip=0, train_wall=13, gb_free=67.2, wall=13844 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:05:40]    INFO >> epoch 008:   1339 / 1539 loss=3.369, wps=2767.5, ups=3.71, wpb=745.6, bsz=745.6, num_updates=12050, lr=0.000262, gnorm=4.379, clip=0, train_wall=13, gb_free=63.9, wall=13858 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:05:45] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.33 GiB. GPU 2 has a total capacity of 79.14 GiB of which 3.11 GiB is free. Including non-PyTorch memory, this process has 76.01 GiB memory in use. Of the allocated memory 68.23 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:05:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:05:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:05:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 63           |        cudaMalloc retries: 95        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  62253 MiB |  69870 MiB | 732534 GiB | 732473 GiB |
|       from large pool |  62230 MiB |  69847 MiB | 730425 GiB | 730364 GiB |
|       from small pool |     23 MiB |     32 MiB |   2109 GiB |   2109 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  62253 MiB |  69870 MiB | 732534 GiB | 732473 GiB |
|       from large pool |  62230 MiB |  69847 MiB | 730425 GiB | 730364 GiB |
|       from small pool |     23 MiB |     32 MiB |   2109 GiB |   2109 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  62247 MiB |  69863 MiB | 731837 GiB | 731776 GiB |
|       from large pool |  62224 MiB |  69840 MiB | 729731 GiB | 729670 GiB |
|       from small pool |     23 MiB |     32 MiB |   2106 GiB |   2106 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77324 MiB |  80144 MiB |    916 GiB |    840 GiB |
|       from large pool |  77292 MiB |  80070 MiB |    911 GiB |    836 GiB |
|       from small pool |     32 MiB |     74 MiB |      4 GiB |      4 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   9322 MiB |   9322 MiB | 782267 GiB | 782258 GiB |
|       from large pool |   9313 MiB |   9313 MiB | 779859 GiB | 779850 GiB |
|       from small pool |      8 MiB |     22 MiB |   2407 GiB |   2407 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     450    |     459    |   25094 K  |   25094 K  |
|       from large pool |     163    |     172    |   12669 K  |   12669 K  |
|       from small pool |     287    |     348    |   12425 K  |   12424 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     450    |     459    |   25094 K  |   25094 K  |
|       from large pool |     163    |     172    |   12669 K  |   12669 K  |
|       from small pool |     287    |     348    |   12425 K  |   12424 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      77    |     111    |    4530    |    4453    |
|       from large pool |      61    |      74    |    2340    |    2279    |
|       from small pool |      16    |      37    |    2190    |    2174    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      78    |      78    |   13680 K  |   13680 K  |
|       from large pool |      55    |      55    |    7819 K  |    7819 K  |
|       from small pool |      23    |      49    |    5860 K  |    5860 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:05:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:05:45] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:05:53]    INFO >> epoch 008:   1390 / 1539 loss=3.324, wps=2751, ups=3.82, wpb=719.6, bsz=719.6, num_updates=12100, lr=0.000262, gnorm=3.703, clip=0, train_wall=12, gb_free=71.1, wall=13871 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:06:07]    INFO >> epoch 008:   1440 / 1539 loss=3.398, wps=2987.4, ups=3.78, wpb=789.7, bsz=789.7, num_updates=12150, lr=0.000262, gnorm=4.199, clip=0, train_wall=13, gb_free=66.3, wall=13884 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:06:10] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.53 GiB. GPU 2 has a total capacity of 79.14 GiB of which 807.25 MiB is free. Including non-PyTorch memory, this process has 78.33 GiB memory in use. Of the allocated memory 76.08 GiB is allocated by PyTorch, and 1.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:06:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:06:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:06:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 64           |        cudaMalloc retries: 96        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72886 MiB |  77907 MiB | 737728 GiB | 737657 GiB |
|       from large pool |  72859 MiB |  77880 MiB | 735604 GiB | 735532 GiB |
|       from small pool |     26 MiB |     40 MiB |   2124 GiB |   2124 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72886 MiB |  77907 MiB | 737728 GiB | 737657 GiB |
|       from large pool |  72859 MiB |  77880 MiB | 735604 GiB | 735532 GiB |
|       from small pool |     26 MiB |     40 MiB |   2124 GiB |   2124 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72874 MiB |  77895 MiB | 737026 GiB | 736955 GiB |
|       from large pool |  72847 MiB |  77867 MiB | 734904 GiB | 734833 GiB |
|       from small pool |     26 MiB |     40 MiB |   2121 GiB |   2121 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79698 MiB |  79762 MiB |    924 GiB |    846 GiB |
|       from large pool |  79668 MiB |  79668 MiB |    919 GiB |    841 GiB |
|       from small pool |     30 MiB |     94 MiB |      4 GiB |      4 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2297 MiB |  13479 MiB |    769 TiB |    769 TiB |
|       from large pool |   2294 MiB |  13475 MiB |    767 TiB |    767 TiB |
|       from small pool |      3 MiB |     25 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Allocations           |     567    |     576    |   25273 K  |   25272 K  |
|       from large pool |     266    |     274    |   12761 K  |   12761 K  |
|       from small pool |     301    |     356    |   12511 K  |   12510 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     567    |     576    |   25273 K  |   25272 K  |
|       from large pool |     266    |     274    |   12761 K  |   12761 K  |
|       from small pool |     301    |     356    |   12511 K  |   12510 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      77    |     109    |    4564    |    4487    |
|       from large pool |      62    |      62    |    2343    |    2281    |
|       from small pool |      15    |      47    |    2221    |    2206    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      92    |      92    |   13775 K  |   13775 K  |
|       from large pool |      70    |      70    |    7874 K  |    7874 K  |
|       from small pool |      22    |      59    |    5900 K  |    5900 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:06:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:06:10] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:06:21]    INFO >> epoch 008:   1491 / 1539 loss=3.407, wps=2445.7, ups=3.61, wpb=678, bsz=678, num_updates=12200, lr=0.000262, gnorm=4.134, clip=0, train_wall=12, gb_free=65.9, wall=13898 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:06:34]    INFO >> epoch 008 | loss 3.367 | wps 2612.7 | ups 3.72 | wpb 702.8 | bsz 702.8 | num_updates 12248 | lr 0.000262 | gnorm 3.988 | clip 0 | train_wall 365 | gb_free 69.8 | wall 13910 (progress_bar.py:267, print())[0m
[33m[2025-11-21 06:06:34] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 06:07:01]    INFO >> epoch 008 | valid on 'valid' subset | loss 3.773 | wps 5702.3 | wpb 5412.5 | bsz 5412.5 | num_updates 12248 | best_loss 4.012 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 06:07:02]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 06:07:02]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/checkpoints/checkpoint_last.pt (epoch 8 @ 12248 updates, score 3.773) (writing took 0.031946 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 06:07:02] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 06:07:02]    INFO >> epoch 009:      2 / 1539 loss=3.303, wps=864.5, ups=1.26, wpb=687.8, bsz=687.8, num_updates=12250, lr=0.000227, gnorm=3.875, clip=0, train_wall=12, gb_free=62.9, wall=13937 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:07:16]    INFO >> epoch 009:     52 / 1539 loss=3.481, wps=2373.3, ups=4.07, wpb=582.7, bsz=582.7, num_updates=12300, lr=0.000227, gnorm=3.778, clip=0, train_wall=12, gb_free=66, wall=13950 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:07:30]    INFO >> epoch 009:    102 / 1539 loss=3.382, wps=2721.9, ups=3.59, wpb=757.8, bsz=757.8, num_updates=12350, lr=0.000227, gnorm=3.651, clip=0, train_wall=13, gb_free=63, wall=13964 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:07:42]    INFO >> epoch 009:    152 / 1539 loss=3.275, wps=2940.4, ups=4.02, wpb=731.9, bsz=731.9, num_updates=12400, lr=0.000227, gnorm=3.887, clip=0, train_wall=12, gb_free=67.2, wall=13976 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:07:56]    INFO >> epoch 009:    202 / 1539 loss=3.422, wps=2824.7, ups=3.95, wpb=715.9, bsz=715.9, num_updates=12450, lr=0.000227, gnorm=3.725, clip=0, train_wall=12, gb_free=62.7, wall=13989 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:08:08]    INFO >> epoch 009:    252 / 1539 loss=3.409, wps=3138.6, ups=4.15, wpb=755.6, bsz=755.6, num_updates=12500, lr=0.000227, gnorm=4.249, clip=0, train_wall=11, gb_free=63.1, wall=14001 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:08:22]    INFO >> epoch 009:    302 / 1539 loss=3.353, wps=2834.1, ups=4.06, wpb=698.9, bsz=698.9, num_updates=12550, lr=0.000227, gnorm=3.919, clip=0, train_wall=12, gb_free=50.1, wall=14013 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:08:34] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.53 GiB. GPU 2 has a total capacity of 79.14 GiB of which 807.25 MiB is free. Including non-PyTorch memory, this process has 78.33 GiB memory in use. Of the allocated memory 76.08 GiB is allocated by PyTorch, and 1.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:08:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:08:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:08:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 65           |        cudaMalloc retries: 97        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72887 MiB |  77908 MiB | 770083 GiB | 770012 GiB |
|       from large pool |  72860 MiB |  77881 MiB | 767856 GiB | 767785 GiB |
|       from small pool |     26 MiB |     34 MiB |   2226 GiB |   2226 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72887 MiB |  77908 MiB | 770083 GiB | 770012 GiB |
|       from large pool |  72860 MiB |  77881 MiB | 767856 GiB | 767785 GiB |
|       from small pool |     26 MiB |     34 MiB |   2226 GiB |   2226 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72874 MiB |  77895 MiB | 769352 GiB | 769281 GiB |
|       from large pool |  72847 MiB |  77867 MiB | 767128 GiB | 767057 GiB |
|       from small pool |     26 MiB |     34 MiB |   2223 GiB |   2223 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79698 MiB |  79802 MiB |    928 GiB |    850 GiB |
|       from large pool |  79668 MiB |  79668 MiB |    924 GiB |    846 GiB |
|       from small pool |     30 MiB |    134 MiB |      4 GiB |      4 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2296 MiB |  12859 MiB |    800 TiB |    800 TiB |
|       from large pool |   2293 MiB |  12856 MiB |    798 TiB |    798 TiB |
|       from small pool |      3 MiB |     20 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Allocations           |     567    |     576    |   26380 K  |   26379 K  |
|       from large pool |     266    |     274    |   13244 K  |   13244 K  |
|       from small pool |     301    |     356    |   13135 K  |   13134 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     567    |     576    |   26380 K  |   26379 K  |
|       from large pool |     266    |     274    |   13244 K  |   13244 K  |
|       from small pool |     301    |     356    |   13135 K  |   13134 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      77    |     129    |    4618    |    4541    |
|       from large pool |      62    |      62    |    2345    |    2283    |
|       from small pool |      15    |      67    |    2273    |    2258    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      87    |      92    |   14392 K  |   14392 K  |
|       from large pool |      63    |      68    |    8167 K  |    8167 K  |
|       from small pool |      24    |      53    |    6224 K  |    6224 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:08:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:08:34] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:08:36]    INFO >> epoch 009:    353 / 1539 loss=3.272, wps=2903.2, ups=3.54, wpb=820.3, bsz=820.3, num_updates=12600, lr=0.000227, gnorm=4.236, clip=2, train_wall=13, gb_free=69.5, wall=14027 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:08:42] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.71 GiB. GPU 2 has a total capacity of 79.14 GiB of which 2.34 GiB is free. Including non-PyTorch memory, this process has 76.78 GiB memory in use. Of the allocated memory 69.30 GiB is allocated by PyTorch, and 6.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:08:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:08:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:08:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 66           |        cudaMalloc retries: 98        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  65169 MiB |  70966 MiB | 771857 GiB | 771793 GiB |
|       from large pool |  65144 MiB |  70941 MiB | 769626 GiB | 769562 GiB |
|       from small pool |     24 MiB |     30 MiB |   2230 GiB |   2230 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  65169 MiB |  70966 MiB | 771857 GiB | 771793 GiB |
|       from large pool |  65144 MiB |  70941 MiB | 769626 GiB | 769562 GiB |
|       from small pool |     24 MiB |     30 MiB |   2230 GiB |   2230 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  65155 MiB |  70952 MiB | 771125 GiB | 771061 GiB |
|       from large pool |  65131 MiB |  70927 MiB | 768897 GiB | 768833 GiB |
|       from small pool |     24 MiB |     30 MiB |   2227 GiB |   2227 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78108 MiB |  79014 MiB |    932 GiB |    856 GiB |
|       from large pool |  78076 MiB |  78958 MiB |    927 GiB |    851 GiB |
|       from small pool |     32 MiB |     56 MiB |      4 GiB |      4 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   8546 MiB |  16646 MiB |    802 TiB |    802 TiB |
|       from large pool |   8539 MiB |  16638 MiB |    800 TiB |    800 TiB |
|       from small pool |      7 MiB |     21 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Allocations           |     450    |     459    |   26431 K  |   26431 K  |
|       from large pool |     163    |     172    |   13274 K  |   13274 K  |
|       from small pool |     287    |     342    |   13157 K  |   13157 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     450    |     459    |   26431 K  |   26431 K  |
|       from large pool |     163    |     172    |   13274 K  |   13274 K  |
|       from small pool |     287    |     342    |   13157 K  |   13157 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      74    |      89    |    4632    |    4558    |
|       from large pool |      58    |      61    |    2346    |    2288    |
|       from small pool |      16    |      28    |    2286    |    2270    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      80    |      80    |   14419 K  |   14419 K  |
|       from large pool |      56    |      56    |    8185 K  |    8185 K  |
|       from small pool |      24    |      51    |    6233 K  |    6233 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:08:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:08:42] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:08:50]    INFO >> epoch 009:    404 / 1539 loss=3.354, wps=2363.5, ups=3.51, wpb=672.5, bsz=672.5, num_updates=12650, lr=0.000227, gnorm=4.112, clip=0, train_wall=13, gb_free=46.9, wall=14042 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:09:05]    INFO >> epoch 009:    454 / 1539 loss=3.299, wps=2796.8, ups=3.87, wpb=723.1, bsz=723.1, num_updates=12700, lr=0.000227, gnorm=3.709, clip=0, train_wall=12, gb_free=63.3, wall=14054 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:09:16]    INFO >> epoch 009:    504 / 1539 loss=3.204, wps=3235.3, ups=4.24, wpb=763.8, bsz=763.8, num_updates=12750, lr=0.000227, gnorm=3.719, clip=0, train_wall=11, gb_free=66.7, wall=14066 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:09:32]    INFO >> epoch 009:    554 / 1539 loss=3.339, wps=2621.4, ups=3.49, wpb=750.2, bsz=750.2, num_updates=12800, lr=0.000227, gnorm=3.983, clip=0, train_wall=14, gb_free=65.3, wall=14081 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:09:38] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.69 GiB. GPU 2 has a total capacity of 79.14 GiB of which 2.94 GiB is free. Including non-PyTorch memory, this process has 76.18 GiB memory in use. Of the allocated memory 70.16 GiB is allocated by PyTorch, and 5.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:09:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:09:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:09:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 67           |        cudaMalloc retries: 99        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  71057 MiB |  71848 MiB | 783375 GiB | 783305 GiB |
|       from large pool |  71028 MiB |  71819 MiB | 781114 GiB | 781044 GiB |
|       from small pool |     28 MiB |     31 MiB |   2261 GiB |   2261 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  71057 MiB |  71848 MiB | 783375 GiB | 783305 GiB |
|       from large pool |  71028 MiB |  71819 MiB | 781114 GiB | 781044 GiB |
|       from small pool |     28 MiB |     31 MiB |   2261 GiB |   2261 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  71047 MiB |  71838 MiB | 782632 GiB | 782563 GiB |
|       from large pool |  71018 MiB |  71809 MiB | 780374 GiB | 780305 GiB |
|       from small pool |     28 MiB |     31 MiB |   2258 GiB |   2258 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77498 MiB |  77546 MiB |    936 GiB |    860 GiB |
|       from large pool |  77464 MiB |  77464 MiB |    931 GiB |    855 GiB |
|       from small pool |     34 MiB |     82 MiB |      4 GiB |      4 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6440 MiB |  14813 MiB |    815 TiB |    815 TiB |
|       from large pool |   6435 MiB |  14807 MiB |    813 TiB |    813 TiB |
|       from small pool |      5 MiB |     31 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Allocations           |     489    |     496    |   26815 K  |   26815 K  |
|       from large pool |     195    |     202    |   13481 K  |   13481 K  |
|       from small pool |     294    |     356    |   13334 K  |   13333 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     489    |     496    |   26815 K  |   26815 K  |
|       from large pool |     195    |     202    |   13481 K  |   13481 K  |
|       from small pool |     294    |     356    |   13334 K  |   13333 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      74    |      98    |    4658    |    4584    |
|       from large pool |      57    |      57    |    2347    |    2290    |
|       from small pool |      17    |      41    |    2311    |    2294    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      87    |      87    |   14618 K  |   14618 K  |
|       from large pool |      61    |      61    |    8308 K  |    8308 K  |
|       from small pool |      26    |      62    |    6310 K  |    6310 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:09:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:09:38] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:09:45]    INFO >> epoch 009:    605 / 1539 loss=3.366, wps=2590.6, ups=3.83, wpb=676.4, bsz=676.4, num_updates=12850, lr=0.000227, gnorm=4.087, clip=0, train_wall=12, gb_free=66.1, wall=14094 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:09:58]    INFO >> epoch 009:    655 / 1539 loss=3.413, wps=2703.7, ups=4.01, wpb=673.8, bsz=673.8, num_updates=12900, lr=0.000227, gnorm=3.826, clip=0, train_wall=12, gb_free=61.4, wall=14106 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:10:12]    INFO >> epoch 009:    705 / 1539 loss=3.385, wps=2636.1, ups=3.86, wpb=683.6, bsz=683.6, num_updates=12950, lr=0.000227, gnorm=3.636, clip=0, train_wall=12, gb_free=49.7, wall=14119 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:10:24]    INFO >> epoch 009:    755 / 1539 loss=3.331, wps=2534.8, ups=3.98, wpb=636.8, bsz=636.8, num_updates=13000, lr=0.000227, gnorm=3.428, clip=0, train_wall=12, gb_free=37.1, wall=14132 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:10:38]    INFO >> epoch 009:    805 / 1539 loss=3.327, wps=3079.4, ups=3.99, wpb=771.7, bsz=771.7, num_updates=13050, lr=0.000227, gnorm=4.396, clip=0, train_wall=12, gb_free=60.9, wall=14144 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:10:43] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.33 GiB. GPU 2 has a total capacity of 79.14 GiB of which 611.25 MiB is free. Including non-PyTorch memory, this process has 78.52 GiB memory in use. Of the allocated memory 76.05 GiB is allocated by PyTorch, and 1.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:10:43] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:10:43] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:10:43] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 68           |        cudaMalloc retries: 101       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77072 MiB |  78535 MiB |    778 TiB |    778 TiB |
|       from large pool |  77048 MiB |  78511 MiB |    776 TiB |    775 TiB |
|       from small pool |     23 MiB |     32 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Active memory         |  77072 MiB |  78535 MiB |    778 TiB |    778 TiB |
|       from large pool |  77048 MiB |  78511 MiB |    776 TiB |    775 TiB |
|       from small pool |     23 MiB |     32 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77063 MiB |  78526 MiB |    777 TiB |    777 TiB |
|       from large pool |  77040 MiB |  78502 MiB |    775 TiB |    775 TiB |
|       from small pool |     23 MiB |     32 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79894 MiB |  79894 MiB |    942 GiB |    864 GiB |
|       from large pool |  79864 MiB |  79864 MiB |    937 GiB |    859 GiB |
|       from small pool |     30 MiB |     92 MiB |      4 GiB |      4 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2821 MiB |   7877 MiB |    831 TiB |    831 TiB |
|       from large pool |   2815 MiB |   7871 MiB |    828 TiB |    828 TiB |
|       from small pool |      6 MiB |     17 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Allocations           |     488    |     496    |   27275 K  |   27274 K  |
|       from large pool |     200    |     208    |   13730 K  |   13730 K  |
|       from small pool |     288    |     348    |   13544 K  |   13544 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     488    |     496    |   27275 K  |   27274 K  |
|       from large pool |     200    |     208    |   13730 K  |   13730 K  |
|       from small pool |     288    |     348    |   13544 K  |   13544 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      73    |     103    |    4689    |    4616    |
|       from large pool |      58    |      58    |    2349    |    2291    |
|       from small pool |      15    |      46    |    2340    |    2325    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      81    |      81    |   14858 K  |   14858 K  |
|       from large pool |      56    |      56    |    8456 K  |    8456 K  |
|       from small pool |      25    |      48    |    6402 K  |    6402 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:10:43] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:10:43] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:10:51]    INFO >> epoch 009:    856 / 1539 loss=3.444, wps=2432.3, ups=3.98, wpb=611, bsz=611, num_updates=13100, lr=0.000227, gnorm=3.232, clip=0, train_wall=11, gb_free=66.5, wall=14157 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:11:03]    INFO >> epoch 009:    906 / 1539 loss=3.421, wps=2691.3, ups=4.07, wpb=661.4, bsz=661.4, num_updates=13150, lr=0.000227, gnorm=3.539, clip=0, train_wall=12, gb_free=64.5, wall=14169 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:11:13] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.14 GiB. GPU 2 has a total capacity of 79.14 GiB of which 669.25 MiB is free. Including non-PyTorch memory, this process has 78.46 GiB memory in use. Of the allocated memory 72.69 GiB is allocated by PyTorch, and 5.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:11:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:11:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:11:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 69           |        cudaMalloc retries: 102       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  67279 MiB |  74434 MiB |    784 TiB |    784 TiB |
|       from large pool |  67252 MiB |  74407 MiB |    781 TiB |    781 TiB |
|       from small pool |     27 MiB |     37 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Active memory         |  67279 MiB |  74434 MiB |    784 TiB |    784 TiB |
|       from large pool |  67252 MiB |  74407 MiB |    781 TiB |    781 TiB |
|       from small pool |     27 MiB |     37 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Requested memory      |  67268 MiB |  74423 MiB |    783 TiB |    783 TiB |
|       from large pool |  67241 MiB |  74396 MiB |    781 TiB |    781 TiB |
|       from small pool |     27 MiB |     37 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79836 MiB |  79926 MiB |    942 GiB |    864 GiB |
|       from large pool |  79804 MiB |  79864 MiB |    937 GiB |    859 GiB |
|       from small pool |     32 MiB |     62 MiB |      4 GiB |      4 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3322 MiB |  10668 MiB |    838 TiB |    838 TiB |
|       from large pool |   3317 MiB |  10663 MiB |    835 TiB |    835 TiB |
|       from small pool |      4 MiB |     23 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Allocations           |     489    |     498    |   27489 K  |   27488 K  |
|       from large pool |     195    |     204    |   13849 K  |   13848 K  |
|       from small pool |     294    |     356    |   13640 K  |   13639 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     489    |     498    |   27489 K  |   27488 K  |
|       from large pool |     195    |     204    |   13849 K  |   13848 K  |
|       from small pool |     294    |     356    |   13640 K  |   13639 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      73    |      89    |    4705    |    4632    |
|       from large pool |      57    |      58    |    2349    |    2292    |
|       from small pool |      16    |      31    |    2356    |    2340    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      74    |      77    |   14968 K  |   14968 K  |
|       from large pool |      49    |      52    |    8527 K  |    8527 K  |
|       from small pool |      25    |      51    |    6441 K  |    6441 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:11:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:11:13] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 06:11:15] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 41.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 77.71 GiB is allocated by PyTorch, and 886.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:11:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:11:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:11:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 70           |        cudaMalloc retries: 103       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79519 MiB |  79577 MiB |    784 TiB |    784 TiB |
|       from large pool |  79425 MiB |  79484 MiB |    782 TiB |    782 TiB |
|       from small pool |     93 MiB |     94 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Active memory         |  79519 MiB |  79577 MiB |    784 TiB |    784 TiB |
|       from large pool |  79425 MiB |  79484 MiB |    782 TiB |    782 TiB |
|       from small pool |     93 MiB |     94 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79491 MiB |  79550 MiB |    783 TiB |    783 TiB |
|       from large pool |  79398 MiB |  79457 MiB |    781 TiB |    781 TiB |
|       from small pool |     93 MiB |     94 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80464 MiB |  80466 MiB |    951 GiB |    873 GiB |
|       from large pool |  80362 MiB |  80362 MiB |    947 GiB |    868 GiB |
|       from small pool |    102 MiB |    104 MiB |      4 GiB |      4 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    884 MiB |   6722 MiB |    838 TiB |    838 TiB |
|       from large pool |    876 MiB |   6720 MiB |    835 TiB |    835 TiB |
|       from small pool |      8 MiB |     20 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Allocations           |    1829    |    1832    |   27503 K  |   27501 K  |
|       from large pool |     452    |     453    |   13854 K  |   13854 K  |
|       from small pool |    1377    |    1380    |   13648 K  |   13647 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1829    |    1832    |   27503 K  |   27501 K  |
|       from large pool |     452    |     453    |   13854 K  |   13854 K  |
|       from small pool |    1377    |    1380    |   13648 K  |   13647 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     181    |     182    |    4816    |    4635    |
|       from large pool |     130    |     130    |    2424    |    2294    |
|       from small pool |      51    |      52    |    2392    |    2341    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     201    |     203    |   14976 K  |   14976 K  |
|       from large pool |     117    |     118    |    8530 K  |    8530 K  |
|       from small pool |      84    |      86    |    6446 K  |    6446 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:11:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:11:15] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:11:17]    INFO >> epoch 009:    958 / 1539 loss=3.33, wps=2596.2, ups=3.88, wpb=668.7, bsz=668.7, num_updates=13200, lr=0.000227, gnorm=4.281, clip=2, train_wall=11, gb_free=68.6, wall=14182 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:11:30]    INFO >> epoch 009:   1008 / 1539 loss=3.305, wps=2630.5, ups=3.88, wpb=678.8, bsz=678.8, num_updates=13250, lr=0.000227, gnorm=3.99, clip=0, train_wall=12, gb_free=58.2, wall=14195 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:11:44]    INFO >> epoch 009:   1058 / 1539 loss=3.266, wps=3039.8, ups=3.95, wpb=769.9, bsz=769.9, num_updates=13300, lr=0.000227, gnorm=3.825, clip=0, train_wall=12, gb_free=62.2, wall=14207 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:11:57]    INFO >> epoch 009:   1108 / 1539 loss=3.433, wps=2654.8, ups=3.82, wpb=695, bsz=695, num_updates=13350, lr=0.000227, gnorm=4.257, clip=0, train_wall=12, gb_free=62.3, wall=14221 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:12:10]    INFO >> epoch 009:   1158 / 1539 loss=3.414, wps=2890.3, ups=4, wpb=722.8, bsz=722.8, num_updates=13400, lr=0.000227, gnorm=3.934, clip=0, train_wall=12, gb_free=60.6, wall=14233 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:12:24]    INFO >> epoch 009:   1208 / 1539 loss=3.319, wps=2962.4, ups=3.89, wpb=760.8, bsz=760.8, num_updates=13450, lr=0.000227, gnorm=3.585, clip=0, train_wall=12, gb_free=39.2, wall=14246 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:12:26] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 1.25 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 76.11 GiB is allocated by PyTorch, and 2.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:12:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:12:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:12:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 71           |        cudaMalloc retries: 104       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77933 MiB |  77994 MiB |    799 TiB |    799 TiB |
|       from large pool |  77748 MiB |  77808 MiB |    797 TiB |    797 TiB |
|       from small pool |    185 MiB |    186 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Active memory         |  77933 MiB |  77994 MiB |    799 TiB |    799 TiB |
|       from large pool |  77748 MiB |  77808 MiB |    797 TiB |    797 TiB |
|       from small pool |    185 MiB |    186 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77914 MiB |  77974 MiB |    798 TiB |    798 TiB |
|       from large pool |  77729 MiB |  77790 MiB |    796 TiB |    796 TiB |
|       from small pool |    184 MiB |    185 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80504 MiB |  80504 MiB |    951 GiB |    873 GiB |
|       from large pool |  80302 MiB |  80302 MiB |    947 GiB |    868 GiB |
|       from small pool |    202 MiB |    202 MiB |      4 GiB |      4 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2570 MiB |   9352 MiB |    853 TiB |    853 TiB |
|       from large pool |   2553 MiB |   9350 MiB |    850 TiB |    850 TiB |
|       from small pool |     16 MiB |     24 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Allocations           |    3455    |    3456    |   28030 K  |   28027 K  |
|       from large pool |     592    |     593    |   14130 K  |   14130 K  |
|       from small pool |    2863    |    2864    |   13899 K  |   13896 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3455    |    3456    |   28030 K  |   28027 K  |
|       from large pool |     592    |     593    |   14130 K  |   14130 K  |
|       from small pool |    2863    |    2864    |   13899 K  |   13896 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     230    |     230    |    4866    |    4636    |
|       from large pool |     129    |     129    |    2424    |    2295    |
|       from small pool |     101    |     101    |    2442    |    2341    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     263    |     265    |   15271 K  |   15270 K  |
|       from large pool |     100    |     105    |    8710 K  |    8710 K  |
|       from small pool |     163    |     165    |    6560 K  |    6560 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:12:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:12:26] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:12:38]    INFO >> epoch 009:   1259 / 1539 loss=3.298, wps=2663.7, ups=3.61, wpb=738.7, bsz=738.7, num_updates=13500, lr=0.000227, gnorm=3.97, clip=0, train_wall=13, gb_free=54.1, wall=14260 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:12:52]    INFO >> epoch 009:   1309 / 1539 loss=3.494, wps=2662.4, ups=3.81, wpb=699.6, bsz=699.6, num_updates=13550, lr=0.000227, gnorm=4.23, clip=0, train_wall=12, gb_free=72.6, wall=14273 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:13:06]    INFO >> epoch 009:   1359 / 1539 loss=3.31, wps=2476.9, ups=3.75, wpb=660.2, bsz=660.2, num_updates=13600, lr=0.000227, gnorm=3.454, clip=0, train_wall=13, gb_free=68, wall=14286 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:13:09] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 1.25 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 74.77 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:13:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:13:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:13:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 72           |        cudaMalloc retries: 106       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76561 MiB |  76622 MiB |    808 TiB |    808 TiB |
|       from large pool |  76349 MiB |  76409 MiB |    805 TiB |    805 TiB |
|       from small pool |    212 MiB |    213 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Active memory         |  76561 MiB |  76622 MiB |    808 TiB |    808 TiB |
|       from large pool |  76349 MiB |  76409 MiB |    805 TiB |    805 TiB |
|       from small pool |    212 MiB |    213 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76536 MiB |  76597 MiB |    807 TiB |    807 TiB |
|       from large pool |  76324 MiB |  76384 MiB |    805 TiB |    804 TiB |
|       from small pool |    212 MiB |    213 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80504 MiB |  80504 MiB |    951 GiB |    873 GiB |
|       from large pool |  80288 MiB |  80302 MiB |    947 GiB |    868 GiB |
|       from small pool |    216 MiB |    216 MiB |      4 GiB |      4 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3942 MiB |   8602 MiB |    862 TiB |    862 TiB |
|       from large pool |   3938 MiB |   8598 MiB |    859 TiB |    859 TiB |
|       from small pool |      3 MiB |     19 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Allocations           |    3948    |    3949    |   28340 K  |   28336 K  |
|       from large pool |     637    |     638    |   14292 K  |   14292 K  |
|       from small pool |    3311    |    3312    |   14047 K  |   14044 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3948    |    3949    |   28340 K  |   28336 K  |
|       from large pool |     637    |     638    |   14292 K  |   14292 K  |
|       from small pool |    3311    |    3312    |   14047 K  |   14044 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     236    |     236    |    4873    |    4637    |
|       from large pool |     128    |     129    |    2424    |    2296    |
|       from small pool |     108    |     108    |    2449    |    2341    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     177    |     180    |   15443 K  |   15443 K  |
|       from large pool |     107    |     107    |    8814 K  |    8814 K  |
|       from small pool |      70    |      73    |    6628 K  |    6628 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:13:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:13:09] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:13:18]    INFO >> epoch 009:   1410 / 1539 loss=3.3, wps=2644.2, ups=4, wpb=661.7, bsz=661.7, num_updates=13650, lr=0.000227, gnorm=3.935, clip=0, train_wall=11, gb_free=69.3, wall=14299 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:13:20] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 588.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 23.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.46 GiB is allocated by PyTorch, and 1.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:13:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:13:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:13:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 73           |        cudaMalloc retries: 108       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78813 MiB |  79320 MiB |    810 TiB |    810 TiB |
|       from large pool |  78783 MiB |  79290 MiB |    808 TiB |    807 TiB |
|       from small pool |     30 MiB |     35 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Active memory         |  78813 MiB |  79320 MiB |    810 TiB |    810 TiB |
|       from large pool |  78783 MiB |  79290 MiB |    808 TiB |    807 TiB |
|       from small pool |     30 MiB |     35 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78797 MiB |  79303 MiB |    809 TiB |    809 TiB |
|       from large pool |  78766 MiB |  79273 MiB |    807 TiB |    807 TiB |
|       from small pool |     30 MiB |     35 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80482 MiB |  80504 MiB |    955 GiB |    876 GiB |
|       from large pool |  80450 MiB |  80450 MiB |    950 GiB |    872 GiB |
|       from small pool |     32 MiB |    216 MiB |      4 GiB |      4 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1668 MiB |   7056 MiB |    864 TiB |    864 TiB |
|       from large pool |   1666 MiB |   7054 MiB |    861 TiB |    861 TiB |
|       from small pool |      1 MiB |     18 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Allocations           |     628    |     632    |   28421 K  |   28420 K  |
|       from large pool |     324    |     328    |   14337 K  |   14337 K  |
|       from small pool |     304    |     356    |   14083 K  |   14083 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     628    |     632    |   28421 K  |   28420 K  |
|       from large pool |     324    |     328    |   14337 K  |   14337 K  |
|       from small pool |     304    |     356    |   14083 K  |   14083 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      91    |     236    |    4877    |    4786    |
|       from large pool |      75    |     128    |    2427    |    2352    |
|       from small pool |      16    |     108    |    2450    |    2434    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      90    |      92    |   15489 K  |   15488 K  |
|       from large pool |      65    |      67    |    8844 K  |    8844 K  |
|       from small pool |      25    |      50    |    6644 K  |    6644 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:13:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:13:20] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:13:33]    INFO >> epoch 009:   1461 / 1539 loss=3.379, wps=2353.1, ups=3.73, wpb=631.7, bsz=631.7, num_updates=13700, lr=0.000227, gnorm=3.483, clip=0, train_wall=12, gb_free=72.5, wall=14312 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:13:44]    INFO >> epoch 009:   1511 / 1539 loss=3.372, wps=2854.8, ups=4.51, wpb=633, bsz=633, num_updates=13750, lr=0.000227, gnorm=3.599, clip=0, train_wall=11, gb_free=65.8, wall=14323 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:13:52]    INFO >> epoch 009 | loss 3.355 | wps 2540.8 | ups 3.64 | wpb 698.7 | bsz 698.7 | num_updates 13778 | lr 0.000227 | gnorm 3.857 | clip 0.1 | train_wall 369 | gb_free 68.5 | wall 14331 (progress_bar.py:267, print())[0m
[33m[2025-11-21 06:13:52] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 06:14:21]    INFO >> epoch 009 | valid on 'valid' subset | loss 3.685 | wps 5430.3 | wpb 5412.5 | bsz 5412.5 | num_updates 13778 | best_loss 4.012 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 06:14:21]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 06:14:21]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/checkpoints/checkpoint_last.pt (epoch 9 @ 13778 updates, score 3.685) (writing took 0.042004 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 06:14:21] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[33m[2025-11-21 06:14:27] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.14 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.78 GiB is free. Including non-PyTorch memory, this process has 77.34 GiB memory in use. Of the allocated memory 72.69 GiB is allocated by PyTorch, and 4.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:14:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:14:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:14:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 74           |        cudaMalloc retries: 111       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  67275 MiB |  74431 MiB |    826 TiB |    826 TiB |
|       from large pool |  67247 MiB |  74404 MiB |    823 TiB |    823 TiB |
|       from small pool |     27 MiB |     27 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Active memory         |  67275 MiB |  74431 MiB |    826 TiB |    826 TiB |
|       from large pool |  67247 MiB |  74404 MiB |    823 TiB |    823 TiB |
|       from small pool |     27 MiB |     27 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Requested memory      |  67268 MiB |  74423 MiB |    825 TiB |    825 TiB |
|       from large pool |  67241 MiB |  74396 MiB |    822 TiB |    822 TiB |
|       from small pool |     27 MiB |     27 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78682 MiB |  78888 MiB |   1088 GiB |   1011 GiB |
|       from large pool |  78650 MiB |  78830 MiB |   1083 GiB |   1006 GiB |
|       from small pool |     32 MiB |     58 MiB |      4 GiB |      4 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6078 MiB |   7974 MiB |    874 TiB |    874 TiB |
|       from large pool |   6074 MiB |   7969 MiB |    871 TiB |    871 TiB |
|       from small pool |      4 MiB |     16 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Allocations           |     489    |     498    |   28960 K  |   28960 K  |
|       from large pool |     195    |     204    |   14521 K  |   14521 K  |
|       from small pool |     294    |     342    |   14439 K  |   14438 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     489    |     498    |   28960 K  |   28960 K  |
|       from large pool |     195    |     204    |   14521 K  |   14521 K  |
|       from small pool |     294    |     342    |   14439 K  |   14438 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      59    |      75    |    5117    |    5058    |
|       from large pool |      43    |      46    |    2644    |    2601    |
|       from small pool |      16    |      29    |    2473    |    2457    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      70    |      71    |   15813 K  |   15813 K  |
|       from large pool |      48    |      49    |    8974 K  |    8974 K  |
|       from small pool |      22    |      49    |    6839 K  |    6839 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:14:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:14:27] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:14:27]    INFO >> epoch 010:     23 / 1539 loss=3.378, wps=794.5, ups=1.2, wpb=663.3, bsz=663.3, num_updates=13800, lr=0.000193, gnorm=3.672, clip=0, train_wall=12, gb_free=70.8, wall=14365 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:14:41]    INFO >> epoch 010:     73 / 1539 loss=3.38, wps=2630.8, ups=4.15, wpb=634.5, bsz=634.5, num_updates=13850, lr=0.000193, gnorm=3.003, clip=0, train_wall=11, gb_free=63.5, wall=14377 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:14:54]    INFO >> epoch 010:    123 / 1539 loss=3.268, wps=2756.6, ups=3.85, wpb=715.8, bsz=715.8, num_updates=13900, lr=0.000193, gnorm=3.882, clip=0, train_wall=12, gb_free=58.7, wall=14390 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:15:08]    INFO >> epoch 010:    173 / 1539 loss=3.359, wps=2898.7, ups=3.94, wpb=736.4, bsz=736.4, num_updates=13950, lr=0.000193, gnorm=3.874, clip=0, train_wall=12, gb_free=65.9, wall=14403 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:15:11] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.53 GiB. GPU 2 has a total capacity of 79.14 GiB of which 3.46 GiB is free. Including non-PyTorch memory, this process has 75.66 GiB memory in use. Of the allocated memory 71.68 GiB is allocated by PyTorch, and 3.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:15:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:15:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:15:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 75           |        cudaMalloc retries: 112       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72886 MiB |  73395 MiB |    834 TiB |    834 TiB |
|       from large pool |  72859 MiB |  73368 MiB |    832 TiB |    832 TiB |
|       from small pool |     26 MiB |     34 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Active memory         |  72886 MiB |  73395 MiB |    834 TiB |    834 TiB |
|       from large pool |  72859 MiB |  73368 MiB |    832 TiB |    832 TiB |
|       from small pool |     26 MiB |     34 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72874 MiB |  73382 MiB |    834 TiB |    834 TiB |
|       from large pool |  72847 MiB |  73355 MiB |    831 TiB |    831 TiB |
|       from small pool |     26 MiB |     34 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  76962 MiB |  77020 MiB |   1091 GiB |   1016 GiB |
|       from large pool |  76932 MiB |  76932 MiB |   1086 GiB |   1011 GiB |
|       from small pool |     30 MiB |     88 MiB |      4 GiB |      4 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4075 MiB |  11029 MiB |    885 TiB |    885 TiB |
|       from large pool |   4072 MiB |  11026 MiB |    883 TiB |    883 TiB |
|       from small pool |      3 MiB |     21 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Allocations           |     567    |     574    |   29266 K  |   29266 K  |
|       from large pool |     266    |     272    |   14688 K  |   14688 K  |
|       from small pool |     301    |     348    |   14578 K  |   14578 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     567    |     574    |   29266 K  |   29266 K  |
|       from large pool |     266    |     272    |   14688 K  |   14688 K  |
|       from small pool |     301    |     348    |   14578 K  |   14578 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      57    |      86    |    5146    |    5089    |
|       from large pool |      42    |      42    |    2645    |    2603    |
|       from small pool |      15    |      44    |    2501    |    2486    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      88    |      88    |   15973 K  |   15973 K  |
|       from large pool |      64    |      64    |    9072 K  |    9072 K  |
|       from small pool |      24    |      54    |    6900 K  |    6900 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:15:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:15:11] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:15:22]    INFO >> epoch 010:    224 / 1539 loss=3.247, wps=2545.7, ups=3.57, wpb=713.4, bsz=713.4, num_updates=14000, lr=0.000193, gnorm=4.089, clip=0, train_wall=13, gb_free=62.5, wall=14417 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:15:26] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 7.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.71 GiB is allocated by PyTorch, and 926.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:15:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:15:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:15:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 76           |        cudaMalloc retries: 113       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79511 MiB |  79571 MiB |    837 TiB |    837 TiB |
|       from large pool |  79310 MiB |  79370 MiB |    835 TiB |    835 TiB |
|       from small pool |    201 MiB |    202 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Active memory         |  79511 MiB |  79571 MiB |    837 TiB |    837 TiB |
|       from large pool |  79310 MiB |  79370 MiB |    835 TiB |    835 TiB |
|       from small pool |    201 MiB |    202 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79492 MiB |  79552 MiB |    837 TiB |    837 TiB |
|       from large pool |  79291 MiB |  79351 MiB |    834 TiB |    834 TiB |
|       from small pool |    200 MiB |    201 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80498 MiB |  80500 MiB |   1095 GiB |   1016 GiB |
|       from large pool |  80280 MiB |  80280 MiB |   1090 GiB |   1011 GiB |
|       from small pool |    218 MiB |    220 MiB |      5 GiB |      4 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    924 MiB |   6524 MiB |    889 TiB |    889 TiB |
|       from large pool |    907 MiB |   6522 MiB |    886 TiB |    886 TiB |
|       from small pool |     16 MiB |     19 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Allocations           |    3743    |    3746    |   29378 K  |   29375 K  |
|       from large pool |     618    |     619    |   14741 K  |   14741 K  |
|       from small pool |    3125    |    3128    |   14637 K  |   14633 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3743    |    3746    |   29378 K  |   29375 K  |
|       from large pool |     618    |     619    |   14741 K  |   14741 K  |
|       from small pool |    3125    |    3128    |   14637 K  |   14633 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     205    |     206    |    5295    |    5090    |
|       from large pool |      96    |      96    |    2699    |    2603    |
|       from small pool |     109    |     110    |    2596    |    2487    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     281    |     282    |   16034 K  |   16033 K  |
|       from large pool |     105    |     106    |    9104 K  |    9104 K  |
|       from small pool |     176    |     177    |    6929 K  |    6929 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:15:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:15:26] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:15:36]    INFO >> epoch 010:    275 / 1539 loss=3.378, wps=2372, ups=3.46, wpb=684.7, bsz=684.7, num_updates=14050, lr=0.000193, gnorm=3.353, clip=0, train_wall=13, gb_free=63.9, wall=14431 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:15:52] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.33 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.53 GiB is free. Including non-PyTorch memory, this process has 77.58 GiB memory in use. Of the allocated memory 76.06 GiB is allocated by PyTorch, and 1.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:15:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:15:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:15:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 77           |        cudaMalloc retries: 115       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77084 MiB |  78546 MiB |    842 TiB |    842 TiB |
|       from large pool |  77060 MiB |  78522 MiB |    840 TiB |    840 TiB |
|       from small pool |     23 MiB |     27 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Active memory         |  77084 MiB |  78546 MiB |    842 TiB |    842 TiB |
|       from large pool |  77060 MiB |  78522 MiB |    840 TiB |    840 TiB |
|       from small pool |     23 MiB |     27 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77063 MiB |  78526 MiB |    841 TiB |    841 TiB |
|       from large pool |  77040 MiB |  78502 MiB |    839 TiB |    839 TiB |
|       from small pool |     23 MiB |     27 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78934 MiB |  80436 MiB |   1159 GiB |   1082 GiB |
|       from large pool |  78906 MiB |  80218 MiB |   1154 GiB |   1077 GiB |
|       from small pool |     28 MiB |    218 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1849 MiB |   6422 MiB |    895 TiB |    895 TiB |
|       from large pool |   1845 MiB |   6417 MiB |    892 TiB |    892 TiB |
|       from small pool |      4 MiB |     17 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Allocations           |     488    |     496    |   29543 K  |   29543 K  |
|       from large pool |     200    |     208    |   14831 K  |   14831 K  |
|       from small pool |     288    |     342    |   14712 K  |   14711 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     488    |     496    |   29543 K  |   29543 K  |
|       from large pool |     200    |     208    |   14831 K  |   14831 K  |
|       from small pool |     288    |     342    |   14712 K  |   14711 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      69    |     204    |    5326    |    5257    |
|       from large pool |      55    |      95    |    2730    |    2675    |
|       from small pool |      14    |     109    |    2596    |    2582    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      66    |      66    |   16126 K  |   16125 K  |
|       from large pool |      46    |      46    |    9162 K  |    9162 K  |
|       from small pool |      20    |      48    |    6963 K  |    6963 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:15:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:15:52] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:15:52]    INFO >> epoch 010:    326 / 1539 loss=3.372, wps=2394.4, ups=3.39, wpb=706.8, bsz=706.8, num_updates=14100, lr=0.000193, gnorm=3.726, clip=0, train_wall=12, gb_free=65.3, wall=14446 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:16:05]    INFO >> epoch 010:    376 / 1539 loss=3.301, wps=2769.5, ups=3.8, wpb=729.7, bsz=729.7, num_updates=14150, lr=0.000193, gnorm=3.362, clip=0, train_wall=13, gb_free=62.4, wall=14459 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:16:19]    INFO >> epoch 010:    426 / 1539 loss=3.383, wps=2522.6, ups=3.99, wpb=632.4, bsz=632.4, num_updates=14200, lr=0.000193, gnorm=3.468, clip=0, train_wall=12, gb_free=61.7, wall=14472 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:16:32]    INFO >> epoch 010:    476 / 1539 loss=3.284, wps=2659.9, ups=3.97, wpb=670.3, bsz=670.3, num_updates=14250, lr=0.000193, gnorm=3.485, clip=0, train_wall=12, gb_free=35.9, wall=14484 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:16:46]    INFO >> epoch 010:    526 / 1539 loss=3.202, wps=2468.6, ups=3.91, wpb=631, bsz=631, num_updates=14300, lr=0.000193, gnorm=3.703, clip=0, train_wall=12, gb_free=63.9, wall=14497 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:16:59]    INFO >> epoch 010:    576 / 1539 loss=3.345, wps=2619.6, ups=3.95, wpb=662.7, bsz=662.7, num_updates=14350, lr=0.000193, gnorm=3.495, clip=0, train_wall=12, gb_free=60.5, wall=14510 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:17:12]    INFO >> epoch 010:    626 / 1539 loss=3.392, wps=2715.1, ups=3.86, wpb=704.3, bsz=704.3, num_updates=14400, lr=0.000193, gnorm=3.376, clip=0, train_wall=12, gb_free=60.4, wall=14523 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:17:26]    INFO >> epoch 010:    676 / 1539 loss=3.247, wps=3021.1, ups=3.92, wpb=770.2, bsz=770.2, num_updates=14450, lr=0.000193, gnorm=4.189, clip=0, train_wall=12, gb_free=69.6, wall=14535 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:17:34] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 588.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 391.25 MiB is free. Including non-PyTorch memory, this process has 78.73 GiB memory in use. Of the allocated memory 76.35 GiB is allocated by PyTorch, and 1.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:17:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:17:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:17:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 78           |        cudaMalloc retries: 116       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77857 MiB |  78179 MiB |    863 TiB |    863 TiB |
|       from large pool |  77827 MiB |  78149 MiB |    860 TiB |    860 TiB |
|       from small pool |     30 MiB |     30 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Active memory         |  77857 MiB |  78179 MiB |    863 TiB |    863 TiB |
|       from large pool |  77827 MiB |  78149 MiB |    860 TiB |    860 TiB |
|       from small pool |     30 MiB |     30 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77846 MiB |  78168 MiB |    862 TiB |    862 TiB |
|       from large pool |  77816 MiB |  78138 MiB |    860 TiB |    859 TiB |
|       from small pool |     30 MiB |     30 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80114 MiB |  80174 MiB |   1160 GiB |   1082 GiB |
|       from large pool |  80082 MiB |  80082 MiB |   1155 GiB |   1077 GiB |
|       from small pool |     32 MiB |     92 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2256 MiB |   8845 MiB |    919 TiB |    919 TiB |
|       from large pool |   2254 MiB |   8842 MiB |    916 TiB |    916 TiB |
|       from small pool |      1 MiB |     25 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Allocations           |     623    |     626    |   30260 K  |   30260 K  |
|       from large pool |     319    |     322    |   15223 K  |   15223 K  |
|       from small pool |     304    |     356    |   15036 K  |   15036 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     623    |     626    |   30260 K  |   30260 K  |
|       from large pool |     319    |     322    |   15223 K  |   15223 K  |
|       from small pool |     304    |     356    |   15036 K  |   15036 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      73    |     103    |    5360    |    5287    |
|       from large pool |      57    |      57    |    2732    |    2675    |
|       from small pool |      16    |      46    |    2628    |    2612    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      81    |      82    |   16498 K  |   16497 K  |
|       from large pool |      60    |      60    |    9393 K  |    9393 K  |
|       from small pool |      21    |      58    |    7104 K  |    7104 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:17:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:17:34] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:17:38]    INFO >> epoch 010:    727 / 1539 loss=3.366, wps=2548.8, ups=4.14, wpb=616.4, bsz=616.4, num_updates=14500, lr=0.000193, gnorm=3.673, clip=0, train_wall=11, gb_free=68.3, wall=14548 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:17:53]    INFO >> epoch 010:    777 / 1539 loss=3.366, wps=2791.4, ups=3.73, wpb=748.9, bsz=748.9, num_updates=14550, lr=0.000193, gnorm=3.797, clip=0, train_wall=13, gb_free=63.2, wall=14561 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:18:05]    INFO >> epoch 010:    827 / 1539 loss=3.334, wps=2634.6, ups=3.94, wpb=668.7, bsz=668.7, num_updates=14600, lr=0.000193, gnorm=4.387, clip=0, train_wall=12, gb_free=68.6, wall=14574 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:18:20]    INFO >> epoch 010:    877 / 1539 loss=3.277, wps=2492.1, ups=3.52, wpb=708.6, bsz=708.6, num_updates=14650, lr=0.000193, gnorm=3.449, clip=0, train_wall=14, gb_free=64.9, wall=14588 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:18:35]    INFO >> epoch 010:    927 / 1539 loss=3.361, wps=3064.9, ups=3.57, wpb=859.7, bsz=859.7, num_updates=14700, lr=0.000193, gnorm=4.077, clip=0, train_wall=13, gb_free=65.3, wall=14602 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:18:47]    INFO >> epoch 010:    977 / 1539 loss=3.41, wps=2721.1, ups=4.24, wpb=642, bsz=642, num_updates=14750, lr=0.000193, gnorm=3.369, clip=0, train_wall=11, gb_free=62.4, wall=14614 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:19:01]    INFO >> epoch 010:   1027 / 1539 loss=3.37, wps=2797.1, ups=3.96, wpb=706.4, bsz=706.4, num_updates=14800, lr=0.000193, gnorm=3.867, clip=0, train_wall=12, gb_free=69.9, wall=14626 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:19:14]    INFO >> epoch 010:   1077 / 1539 loss=3.339, wps=3005, ups=3.81, wpb=789.5, bsz=789.5, num_updates=14850, lr=0.000193, gnorm=3.882, clip=0, train_wall=13, gb_free=64.4, wall=14639 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:19:26]    INFO >> epoch 010:   1127 / 1539 loss=3.332, wps=2537.3, ups=4.06, wpb=624.4, bsz=624.4, num_updates=14900, lr=0.000193, gnorm=3.585, clip=0, train_wall=12, gb_free=65.2, wall=14652 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:19:39] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 19.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 77.82 GiB is allocated by PyTorch, and 800.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:19:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:19:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:19:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 79           |        cudaMalloc retries: 118       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79626 MiB |  79685 MiB |    888 TiB |    888 TiB |
|       from large pool |  79532 MiB |  79591 MiB |    885 TiB |    885 TiB |
|       from small pool |     94 MiB |     95 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Active memory         |  79626 MiB |  79685 MiB |    888 TiB |    888 TiB |
|       from large pool |  79532 MiB |  79591 MiB |    885 TiB |    885 TiB |
|       from small pool |     94 MiB |     95 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79610 MiB |  79669 MiB |    887 TiB |    887 TiB |
|       from large pool |  79516 MiB |  79575 MiB |    884 TiB |    884 TiB |
|       from small pool |     94 MiB |     95 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80486 MiB |  80488 MiB |   1160 GiB |   1082 GiB |
|       from large pool |  80382 MiB |  80382 MiB |   1155 GiB |   1077 GiB |
|       from small pool |    104 MiB |    134 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    799 MiB |   7621 MiB |    948 TiB |    948 TiB |
|       from large pool |    789 MiB |   7617 MiB |    945 TiB |    945 TiB |
|       from small pool |      9 MiB |     17 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Allocations           |    1851    |    1854    |   31143 K  |   31141 K  |
|       from large pool |     454    |     455    |   15695 K  |   15695 K  |
|       from small pool |    1397    |    1400    |   15447 K  |   15446 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1851    |    1854    |   31143 K  |   31141 K  |
|       from large pool |     454    |     455    |   15695 K  |   15695 K  |
|       from small pool |    1397    |    1400    |   15447 K  |   15446 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     114    |     128    |    5417    |    5303    |
|       from large pool |      62    |      62    |    2737    |    2675    |
|       from small pool |      52    |      67    |    2680    |    2628    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     144    |     145    |   16958 K  |   16958 K  |
|       from large pool |      57    |      58    |    9670 K  |    9670 K  |
|       from small pool |      87    |      88    |    7287 K  |    7287 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:19:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:19:39] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:19:41]    INFO >> epoch 010:   1178 / 1539 loss=3.378, wps=2403.6, ups=3.64, wpb=659.7, bsz=659.7, num_updates=14950, lr=0.000193, gnorm=3.849, clip=0, train_wall=13, gb_free=55.2, wall=14665 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:19:42] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.69 GiB. GPU 2 has a total capacity of 79.14 GiB of which 231.25 MiB is free. Including non-PyTorch memory, this process has 78.89 GiB memory in use. Of the allocated memory 74.78 GiB is allocated by PyTorch, and 3.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:19:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:19:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:19:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 80           |        cudaMalloc retries: 119       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  71056 MiB |  76572 MiB |    888 TiB |    888 TiB |
|       from large pool |  71027 MiB |  76543 MiB |    886 TiB |    886 TiB |
|       from small pool |     28 MiB |     29 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Active memory         |  71056 MiB |  76572 MiB |    888 TiB |    888 TiB |
|       from large pool |  71027 MiB |  76543 MiB |    886 TiB |    886 TiB |
|       from small pool |     28 MiB |     29 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Requested memory      |  71047 MiB |  76562 MiB |    887 TiB |    887 TiB |
|       from large pool |  71018 MiB |  76533 MiB |    885 TiB |    885 TiB |
|       from small pool |     28 MiB |     29 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80274 MiB |  80426 MiB |   1160 GiB |   1082 GiB |
|       from large pool |  80240 MiB |  80322 MiB |   1155 GiB |   1077 GiB |
|       from small pool |     34 MiB |    104 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3763 MiB |  10964 MiB |    948 TiB |    948 TiB |
|       from large pool |   3758 MiB |  10958 MiB |    946 TiB |    945 TiB |
|       from small pool |      5 MiB |     21 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Allocations           |     486    |     495    |   31156 K  |   31156 K  |
|       from large pool |     195    |     204    |   15703 K  |   15702 K  |
|       from small pool |     291    |     342    |   15453 K  |   15453 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     486    |     495    |   31156 K  |   31156 K  |
|       from large pool |     195    |     204    |   15703 K  |   15702 K  |
|       from small pool |     291    |     342    |   15453 K  |   15453 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      77    |     113    |    5417    |    5340    |
|       from large pool |      60    |      61    |    2737    |    2677    |
|       from small pool |      17    |      52    |    2680    |    2663    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      89    |      90    |   16966 K  |   16966 K  |
|       from large pool |      67    |      68    |    9675 K  |    9675 K  |
|       from small pool |      22    |      55    |    7290 K  |    7290 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:19:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:19:42] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:19:56]    INFO >> epoch 010:   1229 / 1539 loss=3.305, wps=2714.1, ups=3.42, wpb=793.6, bsz=793.6, num_updates=15000, lr=0.000193, gnorm=4.472, clip=0, train_wall=13, gb_free=56.8, wall=14680 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:20:11]    INFO >> epoch 010:   1279 / 1539 loss=3.316, wps=2855.1, ups=3.64, wpb=783.6, bsz=783.6, num_updates=15050, lr=0.000193, gnorm=3.617, clip=0, train_wall=13, gb_free=22.8, wall=14694 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:20:24]    INFO >> epoch 010:   1329 / 1539 loss=3.409, wps=2362.6, ups=3.71, wpb=636.3, bsz=636.3, num_updates=15100, lr=0.000193, gnorm=3.778, clip=0, train_wall=13, gb_free=64, wall=14707 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:20:37]    INFO >> epoch 010:   1379 / 1539 loss=3.256, wps=2802.8, ups=3.83, wpb=731.3, bsz=731.3, num_updates=15150, lr=0.000193, gnorm=3.769, clip=0, train_wall=12, gb_free=64.2, wall=14720 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:20:52]    INFO >> epoch 010:   1429 / 1539 loss=3.364, wps=2664.4, ups=3.74, wpb=713.1, bsz=713.1, num_updates=15200, lr=0.000193, gnorm=3.181, clip=0, train_wall=13, gb_free=67.2, wall=14734 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:21:04] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.88 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.61 GiB is free. Including non-PyTorch memory, this process has 77.51 GiB memory in use. Of the allocated memory 72.80 GiB is allocated by PyTorch, and 4.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:21:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:21:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:21:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 81           |        cudaMalloc retries: 122       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  45875 MiB |  74763 MiB |    904 TiB |    904 TiB |
|       from large pool |  45850 MiB |  74738 MiB |    902 TiB |    902 TiB |
|       from small pool |     24 MiB |     38 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Active memory         |  45875 MiB |  74763 MiB |    904 TiB |    904 TiB |
|       from large pool |  45850 MiB |  74738 MiB |    902 TiB |    902 TiB |
|       from small pool |     24 MiB |     38 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Requested memory      |  45867 MiB |  74754 MiB |    904 TiB |    904 TiB |
|       from large pool |  45842 MiB |  74730 MiB |    901 TiB |    901 TiB |
|       from small pool |     24 MiB |     38 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78860 MiB |  79714 MiB |   1182 GiB |   1105 GiB |
|       from large pool |  78830 MiB |  79496 MiB |   1177 GiB |   1100 GiB |
|       from small pool |     30 MiB |    218 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  16860 MiB |  16860 MiB |    966 TiB |    966 TiB |
|       from large pool |  16855 MiB |  16855 MiB |    963 TiB |    963 TiB |
|       from small pool |      5 MiB |     25 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Allocations           |     415    |     466    |   31732 K  |   31732 K  |
|       from large pool |     128    |     178    |   16006 K  |   16006 K  |
|       from small pool |     287    |     356    |   15726 K  |   15725 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     415    |     466    |   31732 K  |   31732 K  |
|       from large pool |     128    |     178    |   16006 K  |   16006 K  |
|       from small pool |     287    |     356    |   15726 K  |   15725 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      69    |     215    |    5562    |    5493    |
|       from large pool |      54    |     106    |    2790    |    2736    |
|       from small pool |      15    |     109    |    2772    |    2757    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      73    |      79    |   17287 K  |   17287 K  |
|       from large pool |      53    |      59    |    9870 K  |    9870 K  |
|       from small pool |      20    |      52    |    7416 K  |    7416 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:21:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:21:04] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:21:05]    INFO >> epoch 010:   1480 / 1539 loss=3.417, wps=2368.5, ups=3.79, wpb=625.4, bsz=625.4, num_updates=15250, lr=0.000193, gnorm=3.395, clip=0, train_wall=11, gb_free=62, wall=14747 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:21:20]    INFO >> epoch 010:   1530 / 1539 loss=3.388, wps=2687, ups=3.61, wpb=744, bsz=744, num_updates=15300, lr=0.000193, gnorm=3.238, clip=0, train_wall=13, gb_free=54, wall=14761 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:21:23]    INFO >> epoch 010 | loss 3.337 | wps 2489 | ups 3.54 | wpb 702.8 | bsz 702.8 | num_updates 15309 | lr 0.000193 | gnorm 3.672 | clip 0 | train_wall 377 | gb_free 61.6 | wall 14763 (progress_bar.py:267, print())[0m
[33m[2025-11-21 06:21:23] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 06:21:53]    INFO >> epoch 010 | valid on 'valid' subset | loss 3.688 | wps 5238.1 | wpb 5412.5 | bsz 5412.5 | num_updates 15309 | best_loss 4.012 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 06:21:54]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 06:21:54]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/checkpoints/checkpoint_last.pt (epoch 10 @ 15309 updates, score 3.688) (writing took 0.033057 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 06:21:54] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 06:22:04]    INFO >> epoch 011:     41 / 1539 loss=3.398, wps=826.8, ups=1.19, wpb=692.4, bsz=692.4, num_updates=15350, lr=0.000161, gnorm=3.644, clip=0, train_wall=12, gb_free=54.6, wall=14803 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:22:17]    INFO >> epoch 011:     91 / 1539 loss=3.265, wps=2860.3, ups=3.78, wpb=756.2, bsz=756.2, num_updates=15400, lr=0.000161, gnorm=4.149, clip=0, train_wall=13, gb_free=61.5, wall=14816 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:22:31]    INFO >> epoch 011:    141 / 1539 loss=3.423, wps=2572.4, ups=3.97, wpb=648.2, bsz=648.2, num_updates=15450, lr=0.000161, gnorm=3.795, clip=0, train_wall=12, gb_free=68, wall=14828 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:22:44]    INFO >> epoch 011:    191 / 1539 loss=3.228, wps=2890.9, ups=3.79, wpb=762.6, bsz=762.6, num_updates=15500, lr=0.000161, gnorm=3.223, clip=0, train_wall=13, gb_free=69.8, wall=14842 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:22:58]    INFO >> epoch 011:    241 / 1539 loss=3.244, wps=2789.7, ups=3.94, wpb=708.8, bsz=708.8, num_updates=15550, lr=0.000161, gnorm=4.391, clip=0, train_wall=12, gb_free=70, wall=14854 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:23:12]    INFO >> epoch 011:    291 / 1539 loss=3.342, wps=2434.4, ups=3.71, wpb=656.6, bsz=656.6, num_updates=15600, lr=0.000161, gnorm=4.248, clip=0, train_wall=13, gb_free=64.6, wall=14868 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:23:25]    INFO >> epoch 011:    341 / 1539 loss=3.235, wps=2743.7, ups=3.72, wpb=736.7, bsz=736.7, num_updates=15650, lr=0.000161, gnorm=3.936, clip=0, train_wall=13, gb_free=65.3, wall=14881 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:23:38]    INFO >> epoch 011:    391 / 1539 loss=3.383, wps=2791.3, ups=4.22, wpb=660.9, bsz=660.9, num_updates=15700, lr=0.000161, gnorm=3.603, clip=0, train_wall=11, gb_free=62.3, wall=14893 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:23:51]    INFO >> epoch 011:    441 / 1539 loss=3.433, wps=2501.5, ups=4.02, wpb=621.7, bsz=621.7, num_updates=15750, lr=0.000161, gnorm=3.409, clip=0, train_wall=12, gb_free=66.7, wall=14906 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:23:58] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.53 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.09 GiB is free. Including non-PyTorch memory, this process has 78.03 GiB memory in use. Of the allocated memory 76.08 GiB is allocated by PyTorch, and 1.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:23:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:23:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:23:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 82           |        cudaMalloc retries: 123       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72888 MiB |  77909 MiB |    941 TiB |    941 TiB |
|       from large pool |  72861 MiB |  77882 MiB |    938 TiB |    938 TiB |
|       from small pool |     26 MiB |     33 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Active memory         |  72888 MiB |  77909 MiB |    941 TiB |    941 TiB |
|       from large pool |  72861 MiB |  77882 MiB |    938 TiB |    938 TiB |
|       from small pool |     26 MiB |     33 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72874 MiB |  77895 MiB |    940 TiB |    940 TiB |
|       from large pool |  72847 MiB |  77867 MiB |    937 TiB |    937 TiB |
|       from small pool |     26 MiB |     33 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79392 MiB |  79454 MiB |   1198 GiB |   1121 GiB |
|       from large pool |  79362 MiB |  79362 MiB |   1193 GiB |   1115 GiB |
|       from small pool |     30 MiB |     92 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1989 MiB |   9949 MiB |   1005 TiB |   1005 TiB |
|       from large pool |   1986 MiB |   9945 MiB |   1002 TiB |   1002 TiB |
|       from small pool |      3 MiB |     21 MiB |      3 TiB |      3 TiB |
|---------------------------------------------------------------------------|
| Allocations           |     567    |     576    |   33022 K  |   33022 K  |
|       from large pool |     266    |     274    |   16591 K  |   16591 K  |
|       from small pool |     301    |     348    |   16430 K  |   16430 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     567    |     576    |   33022 K  |   33022 K  |
|       from large pool |     266    |     274    |   16591 K  |   16591 K  |
|       from small pool |     301    |     348    |   16430 K  |   16430 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      66    |      97    |    5600    |    5534    |
|       from large pool |      51    |      51    |    2797    |    2746    |
|       from small pool |      15    |      46    |    2803    |    2788    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      83    |      83    |   17994 K  |   17994 K  |
|       from large pool |      60    |      60    |   10224 K  |   10224 K  |
|       from small pool |      23    |      51    |    7770 K  |    7770 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:23:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:23:58] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:24:06]    INFO >> epoch 011:    492 / 1539 loss=3.415, wps=2308.1, ups=3.54, wpb=652.6, bsz=652.6, num_updates=15800, lr=0.000161, gnorm=3.202, clip=0, train_wall=13, gb_free=52, wall=14920 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:24:19]    INFO >> epoch 011:    542 / 1539 loss=3.372, wps=2615, ups=4.02, wpb=651.2, bsz=651.2, num_updates=15850, lr=0.000161, gnorm=3.494, clip=0, train_wall=12, gb_free=60.9, wall=14932 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:24:23] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 588.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 339.25 MiB is free. Including non-PyTorch memory, this process has 78.79 GiB memory in use. Of the allocated memory 77.46 GiB is allocated by PyTorch, and 849.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:24:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:24:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:24:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 83           |        cudaMalloc retries: 124       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78810 MiB |  79316 MiB |    946 TiB |    945 TiB |
|       from large pool |  78780 MiB |  79286 MiB |    943 TiB |    943 TiB |
|       from small pool |     30 MiB |     30 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Active memory         |  78810 MiB |  79316 MiB |    946 TiB |    945 TiB |
|       from large pool |  78780 MiB |  79286 MiB |    943 TiB |    943 TiB |
|       from small pool |     30 MiB |     30 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78797 MiB |  79303 MiB |    945 TiB |    945 TiB |
|       from large pool |  78766 MiB |  79273 MiB |    942 TiB |    942 TiB |
|       from small pool |     30 MiB |     30 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80166 MiB |  80200 MiB |   1203 GiB |   1125 GiB |
|       from large pool |  80132 MiB |  80132 MiB |   1198 GiB |   1120 GiB |
|       from small pool |     34 MiB |     68 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1355 MiB |   6561 MiB |   1011 TiB |   1011 TiB |
|       from large pool |   1351 MiB |   6557 MiB |   1008 TiB |   1008 TiB |
|       from small pool |      3 MiB |     15 MiB |      3 TiB |      3 TiB |
|---------------------------------------------------------------------------|
| Allocations           |     628    |     632    |   33173 K  |   33172 K  |
|       from large pool |     324    |     328    |   16676 K  |   16676 K  |
|       from small pool |     304    |     336    |   16496 K  |   16496 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     628    |     632    |   33173 K  |   33172 K  |
|       from large pool |     324    |     328    |   16676 K  |   16676 K  |
|       from small pool |     304    |     336    |   16496 K  |   16496 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      71    |      88    |    5624    |    5553    |
|       from large pool |      54    |      54    |    2802    |    2748    |
|       from small pool |      17    |      34    |    2822    |    2805    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      88    |      89    |   18072 K  |   18072 K  |
|       from large pool |      64    |      65    |   10274 K  |   10274 K  |
|       from small pool |      24    |      48    |    7797 K  |    7797 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:24:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:24:23] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:24:33]    INFO >> epoch 011:    593 / 1539 loss=3.486, wps=2195.1, ups=3.39, wpb=648.2, bsz=648.2, num_updates=15900, lr=0.000161, gnorm=3.291, clip=0, train_wall=13, gb_free=59.2, wall=14947 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:24:39] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 29.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.76 GiB is allocated by PyTorch, and 847.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:24:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:24:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:24:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 84           |        cudaMalloc retries: 125       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79569 MiB |  79628 MiB |    949 TiB |    948 TiB |
|       from large pool |  79475 MiB |  79534 MiB |    946 TiB |    946 TiB |
|       from small pool |     94 MiB |     95 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Active memory         |  79569 MiB |  79628 MiB |    949 TiB |    948 TiB |
|       from large pool |  79475 MiB |  79534 MiB |    946 TiB |    946 TiB |
|       from small pool |     94 MiB |     95 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79551 MiB |  79609 MiB |    948 TiB |    948 TiB |
|       from large pool |  79457 MiB |  79516 MiB |    945 TiB |    945 TiB |
|       from small pool |     93 MiB |     94 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80476 MiB |  80476 MiB |   1204 GiB |   1125 GiB |
|       from large pool |  80372 MiB |  80372 MiB |   1198 GiB |   1120 GiB |
|       from small pool |    104 MiB |    104 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    846 MiB |   6972 MiB |   1014 TiB |   1014 TiB |
|       from large pool |    836 MiB |   6968 MiB |   1011 TiB |   1011 TiB |
|       from small pool |      9 MiB |     21 MiB |      3 TiB |      3 TiB |
|---------------------------------------------------------------------------|
| Allocations           |    1840    |    1843    |   33279 K  |   33277 K  |
|       from large pool |     453    |     454    |   16731 K  |   16731 K  |
|       from small pool |    1387    |    1390    |   16547 K  |   16546 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1840    |    1843    |   33279 K  |   33277 K  |
|       from large pool |     453    |     454    |   16731 K  |   16731 K  |
|       from small pool |    1387    |    1390    |   16547 K  |   16546 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     110    |     110    |    5663    |    5553    |
|       from large pool |      58    |      58    |    2806    |    2748    |
|       from small pool |      52    |      52    |    2857    |    2805    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     138    |     139    |   18128 K  |   18128 K  |
|       from large pool |      57    |      58    |   10307 K  |   10307 K  |
|       from small pool |      81    |      81    |    7821 K  |    7820 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:24:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:24:39] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:24:48]    INFO >> epoch 011:    644 / 1539 loss=3.3, wps=2587.6, ups=3.83, wpb=675.2, bsz=675.2, num_updates=15950, lr=0.000161, gnorm=3.572, clip=0, train_wall=12, gb_free=47.1, wall=14960 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:25:01]    INFO >> epoch 011:    694 / 1539 loss=3.414, wps=2712.8, ups=3.88, wpb=699.9, bsz=699.9, num_updates=16000, lr=0.000161, gnorm=3.481, clip=0, train_wall=12, gb_free=59.6, wall=14973 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:25:17]    INFO >> epoch 011:    744 / 1539 loss=3.215, wps=2457, ups=3.38, wpb=726.6, bsz=726.6, num_updates=16050, lr=0.000161, gnorm=4.111, clip=0, train_wall=14, gb_free=65.6, wall=14988 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:25:28] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.75 GiB. GPU 2 has a total capacity of 79.14 GiB of which 385.25 MiB is free. Including non-PyTorch memory, this process has 78.74 GiB memory in use. Of the allocated memory 67.64 GiB is allocated by PyTorch, and 10.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:25:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:25:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:25:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 85           |        cudaMalloc retries: 127       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  45876 MiB |  74765 MiB |    958 TiB |    958 TiB |
|       from large pool |  45852 MiB |  74740 MiB |    956 TiB |    956 TiB |
|       from small pool |     24 MiB |     35 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Active memory         |  45876 MiB |  74765 MiB |    958 TiB |    958 TiB |
|       from large pool |  45852 MiB |  74740 MiB |    956 TiB |    956 TiB |
|       from small pool |     24 MiB |     35 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Requested memory      |  45867 MiB |  74754 MiB |    957 TiB |    957 TiB |
|       from large pool |  45842 MiB |  74730 MiB |    955 TiB |    955 TiB |
|       from small pool |     24 MiB |     35 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80120 MiB |  80416 MiB |   1208 GiB |   1129 GiB |
|       from large pool |  80090 MiB |  80312 MiB |   1202 GiB |   1124 GiB |
|       from small pool |     30 MiB |    104 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  19063 MiB |  23839 MiB |   1026 TiB |   1026 TiB |
|       from large pool |  19057 MiB |  23834 MiB |   1023 TiB |   1023 TiB |
|       from small pool |      5 MiB |     18 MiB |      3 TiB |      3 TiB |
|---------------------------------------------------------------------------|
| Allocations           |     415    |     465    |   33612 K  |   33611 K  |
|       from large pool |     128    |     177    |   16914 K  |   16914 K  |
|       from small pool |     287    |     356    |   16698 K  |   16697 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     415    |     465    |   33612 K  |   33611 K  |
|       from large pool |     128    |     177    |   16914 K  |   16914 K  |
|       from small pool |     287    |     356    |   16698 K  |   16697 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      66    |     109    |    5664    |    5598    |
|       from large pool |      51    |      57    |    2807    |    2756    |
|       from small pool |      15    |      52    |    2857    |    2842    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      76    |      82    |   18303 K  |   18303 K  |
|       from large pool |      53    |      59    |   10416 K  |   10416 K  |
|       from small pool |      23    |      50    |    7886 K  |    7886 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:25:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:25:28] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:25:30]    INFO >> epoch 011:    795 / 1539 loss=3.392, wps=2257.5, ups=3.83, wpb=589.1, bsz=589.1, num_updates=16100, lr=0.000161, gnorm=3.671, clip=0, train_wall=12, gb_free=57.8, wall=15001 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:25:44]    INFO >> epoch 011:    845 / 1539 loss=3.318, wps=2447.2, ups=3.9, wpb=627.1, bsz=627.1, num_updates=16150, lr=0.000161, gnorm=3.326, clip=0, train_wall=12, gb_free=64.6, wall=15013 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:25:57]    INFO >> epoch 011:    895 / 1539 loss=3.342, wps=3161.6, ups=3.88, wpb=814.7, bsz=814.7, num_updates=16200, lr=0.000161, gnorm=3.591, clip=0, train_wall=12, gb_free=58.2, wall=15026 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:26:10]    INFO >> epoch 011:    945 / 1539 loss=3.259, wps=2784.9, ups=3.76, wpb=739.9, bsz=739.9, num_updates=16250, lr=0.000161, gnorm=3.513, clip=0, train_wall=13, gb_free=64.9, wall=15040 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:26:25]    INFO >> epoch 011:    995 / 1539 loss=3.264, wps=2616.9, ups=3.62, wpb=722, bsz=722, num_updates=16300, lr=0.000161, gnorm=3.723, clip=0, train_wall=13, gb_free=64.5, wall=15053 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:26:39]    INFO >> epoch 011:   1045 / 1539 loss=3.127, wps=2672.7, ups=3.72, wpb=719.3, bsz=719.3, num_updates=16350, lr=0.000161, gnorm=3.705, clip=0, train_wall=13, gb_free=67.1, wall=15067 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:26:49] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.69 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.94 GiB is free. Including non-PyTorch memory, this process has 77.18 GiB memory in use. Of the allocated memory 74.78 GiB is allocated by PyTorch, and 1.90 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:26:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:26:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:26:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 86           |        cudaMalloc retries: 128       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  71056 MiB |  76572 MiB |    974 TiB |    974 TiB |
|       from large pool |  71027 MiB |  76543 MiB |    971 TiB |    971 TiB |
|       from small pool |     28 MiB |     30 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Active memory         |  71056 MiB |  76572 MiB |    974 TiB |    974 TiB |
|       from large pool |  71027 MiB |  76543 MiB |    971 TiB |    971 TiB |
|       from small pool |     28 MiB |     30 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Requested memory      |  71047 MiB |  76562 MiB |    973 TiB |    973 TiB |
|       from large pool |  71018 MiB |  76533 MiB |    970 TiB |    970 TiB |
|       from small pool |     28 MiB |     30 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78520 MiB |  78620 MiB |   1221 GiB |   1144 GiB |
|       from large pool |  78486 MiB |  78486 MiB |   1215 GiB |   1139 GiB |
|       from small pool |     34 MiB |    134 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3683 MiB |  10505 MiB |   1045 TiB |   1045 TiB |
|       from large pool |   3678 MiB |  10500 MiB |   1042 TiB |   1042 TiB |
|       from small pool |      5 MiB |     21 MiB |      3 TiB |      3 TiB |
|---------------------------------------------------------------------------|
| Allocations           |     489    |     498    |   34169 K  |   34169 K  |
|       from large pool |     195    |     204    |   17210 K  |   17210 K  |
|       from small pool |     294    |     342    |   16958 K  |   16958 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     489    |     498    |   34169 K  |   34169 K  |
|       from large pool |     195    |     204    |   17210 K  |   17210 K  |
|       from small pool |     294    |     342    |   16958 K  |   16958 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      69    |     119    |    5722    |    5653    |
|       from large pool |      52    |      52    |    2813    |    2761    |
|       from small pool |      17    |      67    |    2909    |    2892    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      79    |      79    |   18597 K  |   18597 K  |
|       from large pool |      55    |      55    |   10592 K  |   10592 K  |
|       from small pool |      24    |      54    |    8004 K  |    8004 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:26:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:26:49] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:26:55]    INFO >> epoch 011:   1096 / 1539 loss=3.416, wps=2559, ups=3.4, wpb=753.5, bsz=753.5, num_updates=16400, lr=0.000161, gnorm=3.197, clip=0, train_wall=13, gb_free=70.2, wall=15082 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:27:08]    INFO >> epoch 011:   1146 / 1539 loss=3.125, wps=3295.2, ups=3.67, wpb=897.7, bsz=897.7, num_updates=16450, lr=0.000161, gnorm=3.719, clip=0, train_wall=13, gb_free=51.4, wall=15095 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:27:21]    INFO >> epoch 011:   1196 / 1539 loss=3.273, wps=2349, ups=3.83, wpb=612.8, bsz=612.8, num_updates=16500, lr=0.000161, gnorm=3.123, clip=0, train_wall=12, gb_free=68.7, wall=15108 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:27:31] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.14 GiB. GPU 2 has a total capacity of 79.14 GiB of which 555.25 MiB is free. Including non-PyTorch memory, this process has 78.57 GiB memory in use. Of the allocated memory 72.69 GiB is allocated by PyTorch, and 5.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:27:31] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:27:31] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:27:31] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 87           |        cudaMalloc retries: 130       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  67279 MiB |  74435 MiB |    982 TiB |    982 TiB |
|       from large pool |  67252 MiB |  74407 MiB |    980 TiB |    979 TiB |
|       from small pool |     27 MiB |     27 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Active memory         |  67279 MiB |  74435 MiB |    982 TiB |    982 TiB |
|       from large pool |  67252 MiB |  74407 MiB |    980 TiB |    979 TiB |
|       from small pool |     27 MiB |     27 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Requested memory      |  67268 MiB |  74423 MiB |    981 TiB |    981 TiB |
|       from large pool |  67241 MiB |  74396 MiB |    979 TiB |    979 TiB |
|       from small pool |     27 MiB |     27 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79950 MiB |  79950 MiB |   1229 GiB |   1151 GiB |
|       from large pool |  79918 MiB |  79918 MiB |   1223 GiB |   1145 GiB |
|       from small pool |     32 MiB |    218 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   7402 MiB |  13900 MiB |   1054 TiB |   1054 TiB |
|       from large pool |   7397 MiB |  13897 MiB |   1051 TiB |   1051 TiB |
|       from small pool |      4 MiB |     15 MiB |      3 TiB |      3 TiB |
|---------------------------------------------------------------------------|
| Allocations           |     489    |     498    |   34467 K  |   34467 K  |
|       from large pool |     195    |     204    |   17363 K  |   17363 K  |
|       from small pool |     294    |     342    |   17104 K  |   17104 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     489    |     498    |   34467 K  |   34467 K  |
|       from large pool |     195    |     204    |   17363 K  |   17363 K  |
|       from small pool |     294    |     342    |   17104 K  |   17104 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      67    |     208    |    5863    |    5796    |
|       from large pool |      51    |      99    |    2862    |    2811    |
|       from small pool |      16    |     109    |    3001    |    2985    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      76    |      76    |   18765 K  |   18765 K  |
|       from large pool |      51    |      51    |   10691 K  |   10691 K  |
|       from small pool |      25    |      43    |    8074 K  |    8074 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:27:31] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:27:31] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:27:36]    INFO >> epoch 011:   1247 / 1539 loss=3.305, wps=2832, ups=3.72, wpb=761.7, bsz=761.7, num_updates=16550, lr=0.000161, gnorm=3.802, clip=0, train_wall=12, gb_free=50.1, wall=15122 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:27:49]    INFO >> epoch 011:   1297 / 1539 loss=3.271, wps=2738.1, ups=3.81, wpb=718.2, bsz=718.2, num_updates=16600, lr=0.000161, gnorm=3.415, clip=0, train_wall=12, gb_free=64.1, wall=15135 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:27:53] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.33 GiB. GPU 2 has a total capacity of 79.14 GiB of which 491.25 MiB is free. Including non-PyTorch memory, this process has 78.64 GiB memory in use. Of the allocated memory 76.05 GiB is allocated by PyTorch, and 2.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:27:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:27:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:27:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 88           |        cudaMalloc retries: 131       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77073 MiB |  78536 MiB |    987 TiB |    987 TiB |
|       from large pool |  77049 MiB |  78512 MiB |    984 TiB |    984 TiB |
|       from small pool |     23 MiB |     37 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Active memory         |  77073 MiB |  78536 MiB |    987 TiB |    987 TiB |
|       from large pool |  77049 MiB |  78512 MiB |    984 TiB |    984 TiB |
|       from small pool |     23 MiB |     37 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77063 MiB |  78526 MiB |    986 TiB |    986 TiB |
|       from large pool |  77040 MiB |  78502 MiB |    983 TiB |    983 TiB |
|       from small pool |     23 MiB |     37 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80014 MiB |  80190 MiB |   1234 GiB |   1156 GiB |
|       from large pool |  79984 MiB |  80104 MiB |   1229 GiB |   1150 GiB |
|       from small pool |     30 MiB |     86 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2940 MiB |   7418 MiB |   1060 TiB |   1060 TiB |
|       from large pool |   2934 MiB |   7411 MiB |   1057 TiB |   1057 TiB |
|       from small pool |      6 MiB |     23 MiB |      3 TiB |      3 TiB |
|---------------------------------------------------------------------------|
| Allocations           |     488    |     496    |   34627 K  |   34627 K  |
|       from large pool |     200    |     208    |   17448 K  |   17448 K  |
|       from small pool |     288    |     356    |   17179 K  |   17178 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     488    |     496    |   34627 K  |   34627 K  |
|       from large pool |     200    |     208    |   17448 K  |   17448 K  |
|       from small pool |     288    |     356    |   17179 K  |   17178 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      64    |      94    |    5891    |    5827    |
|       from large pool |      49    |      51    |    2863    |    2814    |
|       from small pool |      15    |      43    |    3028    |    3013    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      75    |      75    |   18850 K  |   18849 K  |
|       from large pool |      54    |      54    |   10742 K  |   10742 K  |
|       from small pool |      21    |      57    |    8107 K  |    8107 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:27:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:27:53] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:28:05]    INFO >> epoch 011:   1348 / 1539 loss=3.359, wps=2422.3, ups=3.5, wpb=693, bsz=693, num_updates=16650, lr=0.000161, gnorm=3.421, clip=0, train_wall=13, gb_free=60.1, wall=15149 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:28:19]    INFO >> epoch 011:   1398 / 1539 loss=3.376, wps=2743.5, ups=3.48, wpb=788.5, bsz=788.5, num_updates=16700, lr=0.000161, gnorm=3.85, clip=0, train_wall=14, gb_free=52.5, wall=15164 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:28:33]    INFO >> epoch 011:   1448 / 1539 loss=3.438, wps=2664.9, ups=3.92, wpb=680.2, bsz=680.2, num_updates=16750, lr=0.000161, gnorm=4.242, clip=0, train_wall=12, gb_free=65.1, wall=15176 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:28:39] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 55.25 MiB is free. Including non-PyTorch memory, this process has 79.06 GiB memory in use. Of the allocated memory 77.59 GiB is allocated by PyTorch, and 1001.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:28:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:28:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:28:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 89           |        cudaMalloc retries: 132       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79388 MiB |  79448 MiB |    996 TiB |    996 TiB |
|       from large pool |  79188 MiB |  79248 MiB |    993 TiB |    993 TiB |
|       from small pool |    200 MiB |    201 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Active memory         |  79388 MiB |  79448 MiB |    996 TiB |    996 TiB |
|       from large pool |  79188 MiB |  79248 MiB |    993 TiB |    993 TiB |
|       from small pool |    200 MiB |    201 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79370 MiB |  79431 MiB |    995 TiB |    995 TiB |
|       from large pool |  79171 MiB |  79231 MiB |    992 TiB |    992 TiB |
|       from small pool |    199 MiB |    200 MiB |      2 TiB |      2 TiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80450 MiB |  80450 MiB |   1235 GiB |   1156 GiB |
|       from large pool |  80232 MiB |  80232 MiB |   1229 GiB |   1150 GiB |
|       from small pool |    218 MiB |    218 MiB |      6 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    999 MiB |   6602 MiB |   1071 TiB |   1071 TiB |
|       from large pool |    981 MiB |   6601 MiB |   1068 TiB |   1068 TiB |
|       from small pool |     17 MiB |     18 MiB |      3 TiB |      3 TiB |
|---------------------------------------------------------------------------|
| Allocations           |    3721    |    3724    |   34943 K  |   34940 K  |
|       from large pool |     616    |     617    |   17614 K  |   17613 K  |
|       from small pool |    3105    |    3108    |   17329 K  |   17326 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3721    |    3724    |   34943 K  |   34940 K  |
|       from large pool |     616    |     617    |   17614 K  |   17613 K  |
|       from small pool |    3105    |    3108    |   17329 K  |   17326 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     162    |     162    |    5989    |    5827    |
|       from large pool |      53    |      53    |    2867    |    2814    |
|       from small pool |     109    |     109    |    3122    |    3013    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     235    |     236    |   19016 K  |   19016 K  |
|       from large pool |      60    |      61    |   10840 K  |   10840 K  |
|       from small pool |     175    |     175    |    8176 K  |    8176 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:28:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:28:39] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:28:48]    INFO >> epoch 011:   1499 / 1539 loss=3.387, wps=2514.2, ups=3.45, wpb=729.6, bsz=729.6, num_updates=16800, lr=0.000161, gnorm=3.502, clip=0, train_wall=13, gb_free=65.1, wall=15191 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:28:58]    INFO >> epoch 011 | loss 3.324 | wps 2456.9 | ups 3.5 | wpb 702.8 | bsz 702.8 | num_updates 16840 | lr 0.000161 | gnorm 3.646 | clip 0 | train_wall 383 | gb_free 67.9 | wall 15201 (progress_bar.py:267, print())[0m
[33m[2025-11-21 06:28:58] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 06:29:28]    INFO >> epoch 011 | valid on 'valid' subset | loss 3.683 | wps 5264.8 | wpb 5412.5 | bsz 5412.5 | num_updates 16840 | best_loss 4.012 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 06:29:29]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 06:29:29]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/checkpoints/checkpoint_last.pt (epoch 11 @ 16840 updates, score 3.683) (writing took 0.031498 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[32m[2025-11-21 06:29:29]    INFO >> æ—©åœ: éªŒè¯æ€§èƒ½å·²10è½®æœªæå‡ (train_enhanced.py:616, single_main())[0m
[32m[2025-11-21 06:29:29]    INFO >> è®­ç»ƒå®Œæˆï¼Œç”¨æ—¶ 15155.2 ç§’ (train_enhanced.py:626, single_main())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 06:29:29]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 06:29:29]    INFO >> æ‰€æœ‰æ—¥å¿—å·²ä¿å­˜åˆ°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/logs (train_enhanced.py:631, single_main())[0m
[32m[2025-11-21 06:29:29]    INFO >> 
================================================================================ (train_enhanced.py:634, single_main())[0m
[32m[2025-11-21 06:29:29]    INFO >> å¼€å§‹æµ‹è¯•... (train_enhanced.py:635, single_main())[0m
[32m[2025-11-21 06:29:29]    INFO >> ================================================================================ (train_enhanced.py:636, single_main())[0m
[32m[2025-11-21 06:29:29]    INFO >> åŠ è½½æœ€ä½³checkpoint: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/checkpoints/checkpoint_best.pt (train_enhanced.py:50, run_test_after_training())[0m
[32m[2025-11-21 06:29:29]    INFO >> æµ‹è¯•é›†: test (train_enhanced.py:51, run_test_after_training())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/checkpoint_utils.py:212: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(
[32m[2025-11-21 06:30:44]    INFO >> 
================================================================================ (train_enhanced.py:168, run_test_after_training())[0m
[32m[2025-11-21 06:30:44]    INFO >> æµ‹è¯•ç»“æžœ: (train_enhanced.py:169, run_test_after_training())[0m
[32m[2025-11-21 06:30:44]    INFO >> -------------------------------------------------------------------------------- (train_enhanced.py:170, run_test_after_training())[0m
[32m[2025-11-21 06:30:44]    INFO >> å¹³å‡Loss:      4.1352 (train_enhanced.py:171, run_test_after_training())[0m
[32m[2025-11-21 06:30:44]    INFO >> Acc@1:         16.65% (train_enhanced.py:172, run_test_after_training())[0m
[32m[2025-11-21 06:30:44]    INFO >> Acc@5:         40.32% (train_enhanced.py:173, run_test_after_training())[0m
[32m[2025-11-21 06:30:44]    INFO >> Acc@1 (å«any): 16.65% (train_enhanced.py:174, run_test_after_training())[0m
[32m[2025-11-21 06:30:44]    INFO >> Acc@5 (å«any): 40.32% (train_enhanced.py:175, run_test_after_training())[0m
[32m[2025-11-21 06:30:44]    INFO >> ================================================================================ (train_enhanced.py:176, run_test_after_training())[0m
[32m[2025-11-21 06:30:44]    INFO >> æµ‹è¯•ç»“æžœå·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/checkpoints/res.txt (train_enhanced.py:187, run_test_after_training())[0m
[32m[2025-11-21 06:30:44]    INFO >> è®­ç»ƒæ—¥å¿—å·²æ›´æ–°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/logs (train_enhanced.py:222, run_test_after_training())[0m
[TrainingLogger] æ—¥å¿—ç›®å½•: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/logs
[TrainingLogger] åŽŸå§‹è¾“å‡ºå°†ä¿å­˜åˆ°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/logs/training_output.log
[TrainingLogger] Epoch 1 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/logs/metrics.json
[TrainingLogger] Epoch 2 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/logs/metrics.json
[TrainingLogger] Epoch 3 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/logs/metrics.json
[TrainingLogger] Epoch 4 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/logs/metrics.json
[TrainingLogger] Epoch 5 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/logs/metrics.json
[TrainingLogger] Epoch 6 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/logs/metrics.json
[TrainingLogger] Epoch 7 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/logs/metrics.json
[TrainingLogger] Epoch 8 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/logs/metrics.json
[TrainingLogger] Epoch 9 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/logs/metrics.json
[TrainingLogger] Epoch 10 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/logs/metrics.json
[TrainingLogger] Epoch 11 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_128/logs/metrics.json

âœ“ embed_128 æˆåŠŸ

ç­‰å¾…3ç§’...

è¿›åº¦: 4/16

============================================================
å®žéªŒ: layers_1 - å•å±‚GGNN (å‡å°‘å±‚æ•°ï¼Œæµ‹è¯•æ˜¯å¦è¿‡æ‹Ÿåˆ)
æ—¶é—´: 2025-11-21 06:31:30
============================================================

[32m[2025-11-21 06:31:32]    INFO >> åŠ è½½é…ç½®: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/config.yml (train_enhanced.py:666, cli_main())[0m
[32m[2025-11-21 06:31:32]    INFO >> å•GPUè®­ç»ƒ... (train_enhanced.py:694, cli_main())[0m
[32m[2025-11-21 06:31:32]    INFO >> è®­ç»ƒæ—¥å¿—å°†ä¿å­˜åˆ°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs (train_enhanced.py:561, single_main())[0m
[32m[2025-11-21 06:31:32]    INFO >> [nodes] dictionary: 9999 types (typilus.py:102, setup_task())[0m
[32m[2025-11-21 06:31:32]    INFO >> [edges] dictionary: 0 types (typilus.py:102, setup_task())[0m
[32m[2025-11-21 06:31:32]    INFO >> [supernodes.annotation] dictionary: 99 types (typilus.py:106, setup_task())[0m
[32m[2025-11-21 06:31:42]    INFO >> Typilus(
  (encoder): GGNNEncoder(
    (node_embedding): Embedding(9999, 64, padding_idx=0)
    (node_layer): Sequential(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=64, out_features=64, bias=False)
      (2): Dropout(p=0.1, inplace=False)
    )
    (ggnns): ModuleList(
      (0): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=64, out_features=64, bias=False)
          (OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (NEXT): Linear(in_features=64, out_features=64, bias=False)
          (SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
          (_CHILD): Linear(in_features=64, out_features=64, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT): Linear(in_features=64, out_features=64, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (_COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (_RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
        )
        (rnn_cell): GRUCell(64, 64)
      )
    )
  )
  (decoder): DenseDecoder(
    (cls_layers): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=False)
      (1): Dropout(p=0.1, inplace=False)
      (2): Linear(in_features=64, out_features=99, bias=True)
    )
  )
) (train_enhanced.py:568, single_main())[0m
[32m[2025-11-21 06:31:42]    INFO >> æ¨¡åž‹: typilus, æŸå¤±å‡½æ•°: TypilusCriterion (train_enhanced.py:569, single_main())[0m
[32m[2025-11-21 06:31:42]    INFO >> æ¨¡åž‹å‚æ•°: 745059 (å¯è®­ç»ƒ: 745059) (train_enhanced.py:570, single_main())[0m
[32m[2025-11-21 06:31:42]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 06:31:42]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 80579 MB ; used memory = 1340 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 06:31:42]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 06:31:42]    INFO >> ä½¿ç”¨ 1 ä¸ªGPUè®­ç»ƒ (train_enhanced.py:576, single_main())[0m
[32m[2025-11-21 06:31:42]    INFO >> no existing checkpoint found /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-21 06:31:42]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[32m[2025-11-21 06:32:57]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-21 06:32:57] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-21 06:33:06]    INFO >> epoch 001:     50 / 1539 loss=5.505, wps=5426.6, ups=7.51, wpb=720, bsz=720, num_updates=50, lr=0.0004, gnorm=7.913, clip=0, train_wall=6, gb_free=74.9, wall=79 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:33:13]    INFO >> epoch 001:    100 / 1539 loss=5.602, wps=5648.8, ups=7.07, wpb=798.5, bsz=798.5, num_updates=100, lr=0.0004, gnorm=9.328, clip=0, train_wall=6, gb_free=72.5, wall=86 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:33:20]    INFO >> epoch 001:    150 / 1539 loss=5.969, wps=5757.3, ups=7.03, wpb=818.6, bsz=818.6, num_updates=150, lr=0.0004, gnorm=9.289, clip=0, train_wall=6, gb_free=73.9, wall=93 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:33:26]    INFO >> epoch 001:    200 / 1539 loss=5.953, wps=5065.3, ups=7.97, wpb=635.8, bsz=635.8, num_updates=200, lr=0.0004, gnorm=9.315, clip=0, train_wall=5, gb_free=74, wall=100 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:33:34]    INFO >> epoch 001:    250 / 1539 loss=6.039, wps=5102.8, ups=8, wpb=638, bsz=638, num_updates=250, lr=0.0004, gnorm=7.43, clip=0, train_wall=5, gb_free=74.9, wall=106 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:33:41]    INFO >> epoch 001:    300 / 1539 loss=5.789, wps=5427, ups=6.88, wpb=788.8, bsz=788.8, num_updates=300, lr=0.0004, gnorm=8.666, clip=0, train_wall=6, gb_free=70.2, wall=113 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:33:48]    INFO >> epoch 001:    350 / 1539 loss=5.893, wps=5063.4, ups=7.61, wpb=665.5, bsz=665.5, num_updates=350, lr=0.0004, gnorm=8.31, clip=0, train_wall=6, gb_free=71.3, wall=120 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:33:55]    INFO >> epoch 001:    400 / 1539 loss=5.695, wps=6117.2, ups=7.12, wpb=859.7, bsz=859.7, num_updates=400, lr=0.0004, gnorm=8.374, clip=0, train_wall=6, gb_free=71.4, wall=127 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:34:04] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 3.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.94 GiB is allocated by PyTorch, and 688.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:34:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:34:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:34:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 3         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79753 MiB |  79813 MiB |  11167 GiB |  11089 GiB |
|       from large pool |  79570 MiB |  79630 MiB |  11094 GiB |  11016 GiB |
|       from small pool |    183 MiB |    184 MiB |     73 GiB |     73 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79753 MiB |  79813 MiB |  11167 GiB |  11089 GiB |
|       from large pool |  79570 MiB |  79630 MiB |  11094 GiB |  11016 GiB |
|       from small pool |    183 MiB |    184 MiB |     73 GiB |     73 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79617 MiB |  79676 MiB |  11131 GiB |  11053 GiB |
|       from large pool |  79434 MiB |  79494 MiB |  11058 GiB |  10980 GiB |
|       from small pool |    182 MiB |    183 MiB |     73 GiB |     72 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80502 MiB |  80504 MiB | 165174 MiB |  84672 MiB |
|       from large pool |  80314 MiB |  80314 MiB | 164552 MiB |  84238 MiB |
|       from small pool |    188 MiB |    440 MiB |    622 MiB |    434 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory | 704607 KiB |   3180 MiB |   5558 GiB |   5558 GiB |
|       from large pool | 699620 KiB |   3177 MiB |   5473 GiB |   5472 GiB |
|       from small pool |   4987 KiB |     15 MiB |     85 GiB |     85 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3555    |    3558    |     819 K  |     815 K  |
|       from large pool |     591    |     592    |     378 K  |     377 K  |
|       from small pool |    2964    |    2967    |     440 K  |     437 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3555    |    3558    |     819 K  |     815 K  |
|       from large pool |     591    |     592    |     378 K  |     377 K  |
|       from small pool |    2964    |    2967    |     440 K  |     437 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     432    |     857    |    1459    |    1027    |
|       from large pool |     338    |     637    |    1148    |     810    |
|       from small pool |      94    |     220    |     311    |     217    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     167    |     169    |  506198    |  506031    |
|       from large pool |      83    |      83    |  290453    |  290370    |
|       from small pool |      84    |      86    |  215745    |  215661    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:34:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:34:04] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:34:09]    INFO >> epoch 001:    451 / 1539 loss=5.843, wps=2263.4, ups=3.68, wpb=614.5, bsz=614.5, num_updates=450, lr=0.0004, gnorm=7.904, clip=0, train_wall=6, gb_free=75.4, wall=140 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:34:19]    INFO >> epoch 001:    501 / 1539 loss=5.692, wps=4109.7, ups=5.39, wpb=762.8, bsz=762.8, num_updates=500, lr=0.0004, gnorm=7.906, clip=0, train_wall=8, gb_free=73.3, wall=150 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:34:25]    INFO >> epoch 001:    551 / 1539 loss=5.759, wps=4991.6, ups=7.64, wpb=653.1, bsz=653.1, num_updates=550, lr=0.0004, gnorm=7.101, clip=0, train_wall=6, gb_free=68.1, wall=156 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:34:32]    INFO >> epoch 001:    601 / 1539 loss=5.659, wps=5042.2, ups=7.54, wpb=668.9, bsz=668.9, num_updates=600, lr=0.0004, gnorm=8.174, clip=2, train_wall=6, gb_free=75.7, wall=163 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:34:39]    INFO >> epoch 001:    651 / 1539 loss=5.649, wps=5016.6, ups=7.06, wpb=710.3, bsz=710.3, num_updates=650, lr=0.0004, gnorm=7.427, clip=0, train_wall=6, gb_free=72, wall=170 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:34:48]    INFO >> epoch 001:    701 / 1539 loss=5.589, wps=4408.5, ups=6.55, wpb=673.4, bsz=673.4, num_updates=700, lr=0.0004, gnorm=7.076, clip=0, train_wall=7, gb_free=72.3, wall=178 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:34:55]    INFO >> epoch 001:    751 / 1539 loss=5.504, wps=5434.5, ups=7.14, wpb=760.6, bsz=760.6, num_updates=750, lr=0.0004, gnorm=6.979, clip=0, train_wall=6, gb_free=56, wall=185 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:35:02]    INFO >> epoch 001:    801 / 1539 loss=5.546, wps=5270.3, ups=7.34, wpb=717.9, bsz=717.9, num_updates=800, lr=0.0004, gnorm=7.764, clip=0, train_wall=6, gb_free=73.2, wall=191 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:35:09]    INFO >> epoch 001:    851 / 1539 loss=5.502, wps=4422.3, ups=6.88, wpb=642.6, bsz=642.6, num_updates=850, lr=0.0004, gnorm=7.815, clip=0, train_wall=6, gb_free=75.3, wall=199 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:35:17]    INFO >> epoch 001:    901 / 1539 loss=5.536, wps=4939.2, ups=7.48, wpb=660.3, bsz=660.3, num_updates=900, lr=0.0004, gnorm=6.28, clip=0, train_wall=6, gb_free=75.9, wall=205 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:35:24]    INFO >> epoch 001:    951 / 1539 loss=5.459, wps=5080.7, ups=7.11, wpb=714.2, bsz=714.2, num_updates=950, lr=0.0004, gnorm=7.445, clip=0, train_wall=6, gb_free=71.8, wall=212 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:35:33]    INFO >> epoch 001:   1001 / 1539 loss=5.421, wps=3981.6, ups=5.47, wpb=727.3, bsz=727.3, num_updates=1000, lr=0.0004, gnorm=8.218, clip=2, train_wall=8, gb_free=69.3, wall=221 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:35:41]    INFO >> epoch 001:   1051 / 1539 loss=5.455, wps=5563, ups=6.88, wpb=808.7, bsz=808.7, num_updates=1050, lr=0.0004, gnorm=7.603, clip=2, train_wall=6, gb_free=74.3, wall=229 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:35:50] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.82 GiB is free. Including non-PyTorch memory, this process has 77.29 GiB memory in use. Of the allocated memory 73.80 GiB is allocated by PyTorch, and 2.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:35:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:35:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:35:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 7         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75049 MiB |  75992 MiB |  29281 GiB |  29208 GiB |
|       from large pool |  75038 MiB |  75982 MiB |  29108 GiB |  29035 GiB |
|       from small pool |     10 MiB |     13 MiB |    173 GiB |    173 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75049 MiB |  75992 MiB |  29281 GiB |  29208 GiB |
|       from large pool |  75038 MiB |  75982 MiB |  29108 GiB |  29035 GiB |
|       from small pool |     10 MiB |     13 MiB |    173 GiB |    173 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75032 MiB |  75975 MiB |  29208 GiB |  29135 GiB |
|       from large pool |  75022 MiB |  75964 MiB |  29035 GiB |  28962 GiB |
|       from small pool |     10 MiB |     13 MiB |    173 GiB |    173 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78638 MiB |  80224 MiB | 281698 MiB | 203060 MiB |
|       from large pool |  78620 MiB |  80018 MiB | 280812 MiB | 202192 MiB |
|       from small pool |     18 MiB |    206 MiB |    886 MiB |    868 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3588 MiB |   9196 MiB |  25839 GiB |  25836 GiB |
|       from large pool |   3581 MiB |   9189 MiB |  25638 GiB |  25634 GiB |
|       from small pool |      7 MiB |     17 MiB |    201 GiB |    201 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     499    |     507    |    2001 K  |    2001 K  |
|       from large pool |     314    |     322    |     967 K  |     967 K  |
|       from small pool |     185    |     238    |    1033 K  |    1033 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     499    |     507    |    2001 K  |    2001 K  |
|       from large pool |     314    |     322    |     967 K  |     967 K  |
|       from small pool |     185    |     238    |    1033 K  |    1033 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      81    |     179    |    1657    |    1576    |
|       from large pool |      72    |      76    |    1214    |    1142    |
|       from small pool |       9    |     103    |     443    |     434    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      86    |      86    |    1151 K  |    1151 K  |
|       from large pool |      67    |      67    |     669 K  |     668 K  |
|       from small pool |      19    |      41    |     482 K  |     482 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:35:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:35:50] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:35:51]    INFO >> epoch 001:   1102 / 1539 loss=5.21, wps=5052.6, ups=5.69, wpb=888.7, bsz=888.7, num_updates=1100, lr=0.0004, gnorm=9.783, clip=2, train_wall=7, gb_free=73.2, wall=238 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:35:58]    INFO >> epoch 001:   1152 / 1539 loss=5.439, wps=5214.5, ups=7.07, wpb=737.9, bsz=737.9, num_updates=1150, lr=0.0004, gnorm=7.158, clip=0, train_wall=6, gb_free=74.8, wall=245 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:36:05]    INFO >> epoch 001:   1202 / 1539 loss=5.329, wps=4820.3, ups=7.16, wpb=672.9, bsz=672.9, num_updates=1200, lr=0.0004, gnorm=7.654, clip=2, train_wall=6, gb_free=73.3, wall=252 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:36:12]    INFO >> epoch 001:   1252 / 1539 loss=5.223, wps=5104.4, ups=6.99, wpb=730.1, bsz=730.1, num_updates=1250, lr=0.0004, gnorm=8.117, clip=0, train_wall=6, gb_free=72.2, wall=259 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:36:19]    INFO >> epoch 001:   1302 / 1539 loss=5.211, wps=5251.2, ups=7.03, wpb=746.9, bsz=746.9, num_updates=1300, lr=0.0004, gnorm=6.328, clip=0, train_wall=6, gb_free=70, wall=266 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:36:27]    INFO >> epoch 001:   1352 / 1539 loss=5.171, wps=4542.9, ups=7.29, wpb=622.9, bsz=622.9, num_updates=1350, lr=0.0004, gnorm=6.636, clip=0, train_wall=6, gb_free=76.3, wall=273 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:36:35]    INFO >> epoch 001:   1402 / 1539 loss=5.206, wps=5050.4, ups=6.81, wpb=741.7, bsz=741.7, num_updates=1400, lr=0.0004, gnorm=6.464, clip=0, train_wall=7, gb_free=72.5, wall=280 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:36:42]    INFO >> epoch 001:   1452 / 1539 loss=4.904, wps=4909.8, ups=6.97, wpb=704.9, bsz=704.9, num_updates=1450, lr=0.0004, gnorm=7.81, clip=2, train_wall=6, gb_free=71.5, wall=287 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:36:49]    INFO >> epoch 001:   1502 / 1539 loss=5.012, wps=4945, ups=7.04, wpb=702.3, bsz=702.3, num_updates=1500, lr=0.0004, gnorm=6.604, clip=0, train_wall=6, gb_free=74.2, wall=294 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:36:55]    INFO >> epoch 001 | loss 5.505 | wps 4883.3 | ups 6.77 | wpb 720.8 | bsz 720.8 | num_updates 1537 | lr 0.0004 | gnorm 7.774 | clip 0.5 | train_wall 194 | gb_free 77 | wall 299 (progress_bar.py:267, print())[0m
[33m[2025-11-21 06:36:55] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 06:37:09]    INFO >> epoch 001 | valid on 'valid' subset | loss 4.97 | wps 10998.2 | wpb 5412.5 | bsz 5412.5 | num_updates 1537 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
[32m[2025-11-21 06:37:10]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 06:37:10]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/checkpoints/checkpoint_best.pt (epoch 1 @ 1537 updates, score 4.97) (writing took 0.012757 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 06:37:10] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-21 06:37:12]    INFO >> epoch 002:     13 / 1539 loss=4.894, wps=1671.8, ups=2.34, wpb=714.8, bsz=714.8, num_updates=1550, lr=0.0004, gnorm=7.583, clip=2, train_wall=6, gb_free=74.8, wall=316 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:37:18]    INFO >> epoch 002:     63 / 1539 loss=4.718, wps=5083.8, ups=7.56, wpb=672.6, bsz=672.6, num_updates=1600, lr=0.0004, gnorm=7.624, clip=0, train_wall=6, gb_free=74.3, wall=322 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:37:25]    INFO >> epoch 002:    113 / 1539 loss=4.851, wps=5091.7, ups=7.1, wpb=717.1, bsz=717.1, num_updates=1650, lr=0.0004, gnorm=6.999, clip=0, train_wall=7, gb_free=73.5, wall=329 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:37:34]    INFO >> epoch 002:    163 / 1539 loss=4.613, wps=5058.4, ups=7, wpb=722.3, bsz=722.3, num_updates=1700, lr=0.0004, gnorm=7.212, clip=2, train_wall=7, gb_free=72.8, wall=336 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:37:41]    INFO >> epoch 002:    213 / 1539 loss=4.841, wps=4907.3, ups=6.88, wpb=713.4, bsz=713.4, num_updates=1750, lr=0.0004, gnorm=7.846, clip=2, train_wall=7, gb_free=74.3, wall=344 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:37:49]    INFO >> epoch 002:    263 / 1539 loss=4.623, wps=5599.5, ups=6.5, wpb=861.4, bsz=861.4, num_updates=1800, lr=0.0004, gnorm=7.548, clip=2, train_wall=7, gb_free=74.9, wall=351 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:37:55]    INFO >> epoch 002:    313 / 1539 loss=4.732, wps=4844.2, ups=7.39, wpb=655.5, bsz=655.5, num_updates=1850, lr=0.0004, gnorm=6.446, clip=0, train_wall=6, gb_free=73.6, wall=358 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:38:03]    INFO >> epoch 002:    363 / 1539 loss=4.564, wps=4994.1, ups=7.53, wpb=663.3, bsz=663.3, num_updates=1900, lr=0.0004, gnorm=7.579, clip=0, train_wall=6, gb_free=67, wall=365 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:38:11]    INFO >> epoch 002:    413 / 1539 loss=4.465, wps=4788.5, ups=6.74, wpb=710.4, bsz=710.4, num_updates=1950, lr=0.0004, gnorm=7.344, clip=2, train_wall=7, gb_free=64, wall=372 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:38:12] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.82 GiB is free. Including non-PyTorch memory, this process has 77.29 GiB memory in use. Of the allocated memory 74.09 GiB is allocated by PyTorch, and 2.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:38:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:38:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:38:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 3            |        cudaMalloc retries: 8         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72059 MiB |  75869 MiB |  55097 GiB |  55026 GiB |
|       from large pool |  72044 MiB |  75854 MiB |  54762 GiB |  54691 GiB |
|       from small pool |     15 MiB |     19 MiB |    334 GiB |    334 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72059 MiB |  75869 MiB |  55097 GiB |  55026 GiB |
|       from large pool |  72044 MiB |  75854 MiB |  54762 GiB |  54691 GiB |
|       from small pool |     15 MiB |     19 MiB |    334 GiB |    334 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72042 MiB |  75850 MiB |  54975 GiB |  54904 GiB |
|       from large pool |  72027 MiB |  75835 MiB |  54640 GiB |  54570 GiB |
|       from small pool |     15 MiB |     19 MiB |    334 GiB |    334 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78640 MiB |  78828 MiB | 281888 MiB | 203248 MiB |
|       from large pool |  78620 MiB |  78620 MiB | 280812 MiB | 202192 MiB |
|       from small pool |     20 MiB |    208 MiB |   1076 MiB |   1056 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3852 MiB |   4791 MiB |  53896 GiB |  53892 GiB |
|       from large pool |   3847 MiB |   4781 MiB |  53512 GiB |  53508 GiB |
|       from small pool |      4 MiB |     21 MiB |    383 GiB |    383 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     502    |     511    |    3766 K  |    3766 K  |
|       from large pool |     308    |     317    |    1751 K  |    1751 K  |
|       from small pool |     194    |     240    |    2015 K  |    2014 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     502    |     511    |    3766 K  |    3766 K  |
|       from large pool |     308    |     317    |    1751 K  |    1751 K  |
|       from small pool |     194    |     240    |    2015 K  |    2014 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      82    |     176    |    1752    |    1670    |
|       from large pool |      72    |      72    |    1214    |    1142    |
|       from small pool |      10    |     104    |     538    |     528    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      90    |      90    |    2131 K  |    2131 K  |
|       from large pool |      70    |      70    |    1172 K  |    1172 K  |
|       from small pool |      20    |      47    |     958 K  |     958 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:38:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:38:12] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:38:18]    INFO >> epoch 002:    464 / 1539 loss=4.626, wps=4745, ups=6.6, wpb=719.5, bsz=719.5, num_updates=2000, lr=0.0004, gnorm=6.934, clip=0, train_wall=6, gb_free=69.8, wall=380 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:38:26]    INFO >> epoch 002:    514 / 1539 loss=4.574, wps=4968.3, ups=6.94, wpb=716.1, bsz=716.1, num_updates=2050, lr=0.0004, gnorm=7.876, clip=0, train_wall=7, gb_free=72.9, wall=387 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:38:33]    INFO >> epoch 002:    564 / 1539 loss=4.51, wps=4459.7, ups=7.17, wpb=621.7, bsz=621.7, num_updates=2100, lr=0.0004, gnorm=6.955, clip=2, train_wall=6, gb_free=73.7, wall=394 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:38:41]    INFO >> epoch 002:    614 / 1539 loss=4.431, wps=5970.5, ups=6.89, wpb=866.5, bsz=866.5, num_updates=2150, lr=0.0004, gnorm=7.247, clip=0, train_wall=7, gb_free=74.1, wall=401 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:38:49]    INFO >> epoch 002:    664 / 1539 loss=4.438, wps=5215.3, ups=6.79, wpb=767.7, bsz=767.7, num_updates=2200, lr=0.0004, gnorm=7.827, clip=4, train_wall=7, gb_free=72, wall=409 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:38:55]    INFO >> epoch 002:    714 / 1539 loss=4.571, wps=4926.4, ups=7.25, wpb=679.3, bsz=679.3, num_updates=2250, lr=0.0004, gnorm=6.329, clip=0, train_wall=6, gb_free=75.4, wall=416 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:39:03]    INFO >> epoch 002:    764 / 1539 loss=4.435, wps=4852.5, ups=6.9, wpb=703.3, bsz=703.3, num_updates=2300, lr=0.0004, gnorm=6.334, clip=0, train_wall=7, gb_free=69.8, wall=423 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:39:11]    INFO >> epoch 002:    814 / 1539 loss=4.44, wps=4522.7, ups=7.16, wpb=631.7, bsz=631.7, num_updates=2350, lr=0.0004, gnorm=6.339, clip=0, train_wall=6, gb_free=73.1, wall=430 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:39:18]    INFO >> epoch 002:    864 / 1539 loss=4.308, wps=5040.4, ups=6.74, wpb=747.5, bsz=747.5, num_updates=2400, lr=0.0004, gnorm=7.864, clip=0, train_wall=7, gb_free=76, wall=437 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:39:26]    INFO >> epoch 002:    914 / 1539 loss=4.321, wps=4613, ups=6.89, wpb=669.2, bsz=669.2, num_updates=2450, lr=0.0004, gnorm=6.19, clip=0, train_wall=7, gb_free=75.5, wall=444 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:39:33]    INFO >> epoch 002:    964 / 1539 loss=4.141, wps=4502, ups=7.07, wpb=637.1, bsz=637.1, num_updates=2500, lr=0.0004, gnorm=6.679, clip=0, train_wall=7, gb_free=74.9, wall=452 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:39:39] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 49.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 77.51 GiB is allocated by PyTorch, and 1.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:39:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:39:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:39:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 4            |        cudaMalloc retries: 10        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79305 MiB |  79365 MiB |  70418 GiB |  70341 GiB |
|       from large pool |  79126 MiB |  79186 MiB |  69990 GiB |  69913 GiB |
|       from small pool |    179 MiB |    180 MiB |    427 GiB |    427 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79305 MiB |  79365 MiB |  70418 GiB |  70341 GiB |
|       from large pool |  79126 MiB |  79186 MiB |  69990 GiB |  69913 GiB |
|       from small pool |    179 MiB |    180 MiB |    427 GiB |    427 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79256 MiB |  79315 MiB |  70266 GiB |  70189 GiB |
|       from large pool |  79077 MiB |  79137 MiB |  69839 GiB |  69762 GiB |
|       from small pool |    178 MiB |    179 MiB |    427 GiB |    427 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80456 MiB |  80472 MiB | 286692 MiB | 206236 MiB |
|       from large pool |  80272 MiB |  80272 MiB | 285192 MiB | 204920 MiB |
|       from small pool |    184 MiB |    440 MiB |   1500 MiB |   1316 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1090 MiB |   7837 MiB |  71569 GiB |  71568 GiB |
|       from large pool |   1085 MiB |   7830 MiB |  71077 GiB |  71076 GiB |
|       from small pool |      4 MiB |     21 MiB |    492 GiB |    492 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3491    |    3494    |    4836 K  |    4833 K  |
|       from large pool |     585    |     586    |    2266 K  |    2265 K  |
|       from small pool |    2906    |    2909    |    2570 K  |    2567 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3491    |    3494    |    4836 K  |    4833 K  |
|       from large pool |     585    |     586    |    2266 K  |    2265 K  |
|       from small pool |    2906    |    2909    |    2570 K  |    2567 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     236    |     360    |    2037    |    1801    |
|       from large pool |     144    |     144    |    1287    |    1143    |
|       from small pool |      92    |     220    |     750    |     658    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     164    |     165    |    2719 K  |    2719 K  |
|       from large pool |      80    |      81    |    1497 K  |    1497 K  |
|       from small pool |      84    |      85    |    1221 K  |    1221 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:39:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:39:39] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:39:41]    INFO >> epoch 002:   1015 / 1539 loss=3.619, wps=5170.8, ups=5.92, wpb=873.2, bsz=873.2, num_updates=2550, lr=0.0004, gnorm=8.294, clip=4, train_wall=7, gb_free=73.1, wall=460 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:39:50]    INFO >> epoch 002:   1065 / 1539 loss=3.926, wps=5214.8, ups=6.73, wpb=774.3, bsz=774.3, num_updates=2600, lr=0.0004, gnorm=8.141, clip=2, train_wall=7, gb_free=73.4, wall=467 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:39:57]    INFO >> epoch 002:   1115 / 1539 loss=3.843, wps=5329.1, ups=6.93, wpb=768.6, bsz=768.6, num_updates=2650, lr=0.0004, gnorm=6.974, clip=2, train_wall=7, gb_free=74.5, wall=475 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:40:05]    INFO >> epoch 002:   1165 / 1539 loss=4.215, wps=5390.4, ups=6.79, wpb=794.2, bsz=794.2, num_updates=2700, lr=0.0004, gnorm=7.291, clip=0, train_wall=7, gb_free=72.5, wall=482 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:40:12]    INFO >> epoch 002:   1215 / 1539 loss=4.095, wps=4848.7, ups=6.96, wpb=696.2, bsz=696.2, num_updates=2750, lr=0.0004, gnorm=6.859, clip=0, train_wall=7, gb_free=73.4, wall=489 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:40:17] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.31 GiB is free. Including non-PyTorch memory, this process has 77.80 GiB memory in use. Of the allocated memory 73.80 GiB is allocated by PyTorch, and 3.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:40:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:40:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:40:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 5            |        cudaMalloc retries: 12        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75050 MiB |  75994 MiB |  76835 GiB |  76762 GiB |
|       from large pool |  75040 MiB |  75983 MiB |  76369 GiB |  76296 GiB |
|       from small pool |     10 MiB |     12 MiB |    465 GiB |    465 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75050 MiB |  75994 MiB |  76835 GiB |  76762 GiB |
|       from large pool |  75040 MiB |  75983 MiB |  76369 GiB |  76296 GiB |
|       from small pool |     10 MiB |     12 MiB |    465 GiB |    465 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75032 MiB |  75975 MiB |  76669 GiB |  76596 GiB |
|       from large pool |  75022 MiB |  75964 MiB |  76204 GiB |  76131 GiB |
|       from small pool |     10 MiB |     11 MiB |    465 GiB |    465 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79160 MiB |  80434 MiB | 289140 MiB | 209980 MiB |
|       from large pool |  79142 MiB |  80212 MiB | 287602 MiB | 208460 MiB |
|       from small pool |     18 MiB |    222 MiB |   1538 MiB |   1520 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4109 MiB |   8051 MiB |  77655 GiB |  77651 GiB |
|       from large pool |   4101 MiB |   8043 MiB |  77118 GiB |  77114 GiB |
|       from small pool |      7 MiB |     17 MiB |    536 GiB |    536 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     501    |     509    |    5270 K  |    5270 K  |
|       from large pool |     314    |     322    |    2474 K  |    2474 K  |
|       from small pool |     187    |     240    |    2795 K  |    2795 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     501    |     509    |    5270 K  |    5270 K  |
|       from large pool |     314    |     322    |    2474 K  |    2474 K  |
|       from small pool |     187    |     240    |    2795 K  |    2795 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      99    |     254    |    2061    |    1962    |
|       from large pool |      90    |     143    |    1292    |    1202    |
|       from small pool |       9    |     111    |     769    |     760    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      90    |      90    |    2966 K  |    2966 K  |
|       from large pool |      72    |      72    |    1637 K  |    1637 K  |
|       from small pool |      18    |      43    |    1329 K  |    1329 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:40:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:40:17] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:40:21]    INFO >> epoch 002:   1266 / 1539 loss=3.933, wps=4780.1, ups=6.26, wpb=763.3, bsz=763.3, num_updates=2800, lr=0.0004, gnorm=7.202, clip=2, train_wall=7, gb_free=76.2, wall=497 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:40:28]    INFO >> epoch 002:   1316 / 1539 loss=3.985, wps=4801.6, ups=7.19, wpb=668, bsz=668, num_updates=2850, lr=0.0004, gnorm=7.13, clip=0, train_wall=6, gb_free=73.2, wall=504 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:40:35]    INFO >> epoch 002:   1366 / 1539 loss=4.134, wps=5005.3, ups=7.02, wpb=712.8, bsz=712.8, num_updates=2900, lr=0.0004, gnorm=6.407, clip=0, train_wall=7, gb_free=71, wall=511 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:40:42]    INFO >> epoch 002:   1416 / 1539 loss=3.868, wps=4658.3, ups=6.98, wpb=667, bsz=667, num_updates=2950, lr=0.0004, gnorm=6.735, clip=0, train_wall=7, gb_free=73.8, wall=518 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:40:52]    INFO >> epoch 002:   1466 / 1539 loss=4.082, wps=4262.6, ups=6.04, wpb=705.6, bsz=705.6, num_updates=3000, lr=0.0004, gnorm=6.647, clip=0, train_wall=8, gb_free=71.7, wall=527 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:40:59]    INFO >> epoch 002:   1516 / 1539 loss=4.008, wps=4382.9, ups=6.63, wpb=661.3, bsz=661.3, num_updates=3050, lr=0.0004, gnorm=6.343, clip=0, train_wall=7, gb_free=74.8, wall=534 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:41:03]    INFO >> epoch 002 | loss 4.323 | wps 4630.8 | ups 6.44 | wpb 718.6 | bsz 718.6 | num_updates 3073 | lr 0.0004 | gnorm 7.104 | clip 0.8 | train_wall 206 | gb_free 73.3 | wall 538 (progress_bar.py:267, print())[0m
[33m[2025-11-21 06:41:03] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 06:41:16]    INFO >> epoch 002 | valid on 'valid' subset | loss 3.932 | wps 11700.9 | wpb 5412.5 | bsz 5412.5 | num_updates 3073 | best_loss 4.97 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 06:41:17]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 06:41:17]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/checkpoints/checkpoint_last.pt (epoch 2 @ 3073 updates, score 3.932) (writing took 0.013510 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 06:41:17] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 06:41:20]    INFO >> epoch 003:     27 / 1539 loss=3.959, wps=1660.4, ups=2.41, wpb=689.6, bsz=689.6, num_updates=3100, lr=0.000392, gnorm=7.371, clip=0, train_wall=7, gb_free=72.5, wall=555 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:41:29]    INFO >> epoch 003:     77 / 1539 loss=3.992, wps=5238.6, ups=6.77, wpb=773.7, bsz=773.7, num_updates=3150, lr=0.000392, gnorm=6.398, clip=0, train_wall=7, gb_free=74.2, wall=562 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:41:32] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.31 GiB is free. Including non-PyTorch memory, this process has 77.80 GiB memory in use. Of the allocated memory 74.09 GiB is allocated by PyTorch, and 3.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:41:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:41:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:41:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 6            |        cudaMalloc retries: 13        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72060 MiB |  75870 MiB |  91083 GiB |  91012 GiB |
|       from large pool |  72045 MiB |  75854 MiB |  90524 GiB |  90454 GiB |
|       from small pool |     15 MiB |     17 MiB |    558 GiB |    558 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72060 MiB |  75870 MiB |  91083 GiB |  91012 GiB |
|       from large pool |  72045 MiB |  75854 MiB |  90524 GiB |  90454 GiB |
|       from small pool |     15 MiB |     17 MiB |    558 GiB |    558 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72042 MiB |  75850 MiB |  90890 GiB |  90820 GiB |
|       from large pool |  72027 MiB |  75835 MiB |  90332 GiB |  90262 GiB |
|       from small pool |     15 MiB |     17 MiB |    557 GiB |    557 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79160 MiB |  79262 MiB | 289242 MiB | 210082 MiB |
|       from large pool |  79142 MiB |  79142 MiB | 287602 MiB | 208460 MiB |
|       from small pool |     18 MiB |    120 MiB |   1640 MiB |   1622 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1643 MiB |   5224 MiB |  91127 GiB |  91125 GiB |
|       from large pool |   1640 MiB |   5215 MiB |  90488 GiB |  90487 GiB |
|       from small pool |      2 MiB |     17 MiB |    638 GiB |    638 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     502    |     511    |    6222 K  |    6222 K  |
|       from large pool |     308    |     317    |    2855 K  |    2855 K  |
|       from small pool |     194    |     240    |    3367 K  |    3366 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     502    |     511    |    6222 K  |    6222 K  |
|       from large pool |     308    |     317    |    2855 K  |    2855 K  |
|       from small pool |     194    |     240    |    3367 K  |    3366 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      99    |     150    |    2112    |    2013    |
|       from large pool |      90    |      90    |    1292    |    1202    |
|       from small pool |       9    |      60    |     820    |     811    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      83    |      83    |    3514 K  |    3514 K  |
|       from large pool |      64    |      64    |    1887 K  |    1887 K  |
|       from small pool |      19    |      41    |    1626 K  |    1626 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:41:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:41:32] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:41:37]    INFO >> epoch 003:    128 / 1539 loss=3.926, wps=5144.4, ups=6.23, wpb=825.8, bsz=825.8, num_updates=3200, lr=0.000392, gnorm=6.806, clip=0, train_wall=7, gb_free=64.7, wall=570 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:41:44]    INFO >> epoch 003:    178 / 1539 loss=4.117, wps=4526.2, ups=7.17, wpb=630.9, bsz=630.9, num_updates=3250, lr=0.000392, gnorm=5.963, clip=0, train_wall=6, gb_free=73.3, wall=577 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:41:49] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 591.25 MiB is free. Including non-PyTorch memory, this process has 78.54 GiB memory in use. Of the allocated memory 73.80 GiB is allocated by PyTorch, and 4.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:41:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:41:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:41:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 7            |        cudaMalloc retries: 14        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75051 MiB |  75994 MiB |  94231 GiB |  94158 GiB |
|       from large pool |  75040 MiB |  75983 MiB |  93653 GiB |  93579 GiB |
|       from small pool |     10 MiB |     18 MiB |    578 GiB |    578 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75051 MiB |  75994 MiB |  94231 GiB |  94158 GiB |
|       from large pool |  75040 MiB |  75983 MiB |  93653 GiB |  93579 GiB |
|       from small pool |     10 MiB |     18 MiB |    578 GiB |    578 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75032 MiB |  75975 MiB |  94032 GiB |  93958 GiB |
|       from large pool |  75022 MiB |  75964 MiB |  93454 GiB |  93381 GiB |
|       from small pool |     10 MiB |     18 MiB |    577 GiB |    577 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79914 MiB |  80102 MiB | 295640 MiB | 215726 MiB |
|       from large pool |  79894 MiB |  79894 MiB | 293810 MiB | 213916 MiB |
|       from small pool |     20 MiB |    208 MiB |   1830 MiB |   1810 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4862 MiB |  10522 MiB |  94537 GiB |  94532 GiB |
|       from large pool |   4853 MiB |  10512 MiB |  93876 GiB |  93871 GiB |
|       from small pool |      9 MiB |     25 MiB |    661 GiB |    661 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     501    |     509    |    6441 K  |    6441 K  |
|       from large pool |     314    |     322    |    2957 K  |    2956 K  |
|       from small pool |     187    |     240    |    3484 K  |    3484 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     501    |     509    |    6441 K  |    6441 K  |
|       from large pool |     314    |     322    |    2957 K  |    2956 K  |
|       from small pool |     187    |     240    |    3484 K  |    3484 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     102    |     196    |    2211    |    2109    |
|       from large pool |      92    |      92    |    1296    |    1204    |
|       from small pool |      10    |     104    |     915    |     905    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      93    |      93    |    3638 K  |    3638 K  |
|       from large pool |      73    |      73    |    1953 K  |    1953 K  |
|       from small pool |      20    |      48    |    1684 K  |    1684 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:41:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:41:49] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:41:52]    INFO >> epoch 003:    229 / 1539 loss=3.928, wps=4704.4, ups=5.97, wpb=788.4, bsz=788.4, num_updates=3300, lr=0.000392, gnorm=6.054, clip=0, train_wall=7, gb_free=72, wall=586 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:42:01]    INFO >> epoch 003:    279 / 1539 loss=3.868, wps=5251.8, ups=7.05, wpb=745.4, bsz=745.4, num_updates=3350, lr=0.000392, gnorm=6.108, clip=0, train_wall=7, gb_free=67.8, wall=593 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:42:08]    INFO >> epoch 003:    329 / 1539 loss=3.919, wps=5429, ups=6.64, wpb=817.7, bsz=817.7, num_updates=3400, lr=0.000392, gnorm=6.803, clip=2, train_wall=7, gb_free=73.1, wall=600 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:42:16]    INFO >> epoch 003:    379 / 1539 loss=3.886, wps=4567.1, ups=6.84, wpb=667.7, bsz=667.7, num_updates=3450, lr=0.000392, gnorm=6.143, clip=0, train_wall=7, gb_free=73.9, wall=608 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:42:23]    INFO >> epoch 003:    429 / 1539 loss=4.004, wps=4393.1, ups=6.8, wpb=645.9, bsz=645.9, num_updates=3500, lr=0.000392, gnorm=6.316, clip=0, train_wall=7, gb_free=73.1, wall=615 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:42:32]    INFO >> epoch 003:    479 / 1539 loss=3.908, wps=5266.3, ups=6.8, wpb=774, bsz=774, num_updates=3550, lr=0.000392, gnorm=5.893, clip=0, train_wall=7, gb_free=68.7, wall=622 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:42:39]    INFO >> epoch 003:    529 / 1539 loss=3.89, wps=5106.6, ups=7.1, wpb=718.9, bsz=718.9, num_updates=3600, lr=0.000392, gnorm=7.202, clip=0, train_wall=7, gb_free=73, wall=629 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:42:46]    INFO >> epoch 003:    579 / 1539 loss=3.801, wps=5097.5, ups=6.61, wpb=771.6, bsz=771.6, num_updates=3650, lr=0.000392, gnorm=6.286, clip=2, train_wall=7, gb_free=74.4, wall=637 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:42:53]    INFO >> epoch 003:    629 / 1539 loss=3.944, wps=4879, ups=7.44, wpb=655.6, bsz=655.6, num_updates=3700, lr=0.000392, gnorm=5.825, clip=0, train_wall=6, gb_free=73.5, wall=644 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:43:00]    INFO >> epoch 003:    679 / 1539 loss=3.885, wps=5330.5, ups=6.8, wpb=783.8, bsz=783.8, num_updates=3750, lr=0.000392, gnorm=6.529, clip=0, train_wall=7, gb_free=71.4, wall=651 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:43:10]    INFO >> epoch 003:    729 / 1539 loss=3.821, wps=4880, ups=6.22, wpb=784.1, bsz=784.1, num_updates=3800, lr=0.000392, gnorm=5.382, clip=0, train_wall=7, gb_free=74.3, wall=659 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:43:17]    INFO >> epoch 003:    779 / 1539 loss=3.509, wps=5484.9, ups=6.7, wpb=819.1, bsz=819.1, num_updates=3850, lr=0.000392, gnorm=6.832, clip=0, train_wall=7, gb_free=74, wall=667 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:43:19] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 21.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 77.54 GiB is allocated by PyTorch, and 1.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:43:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:43:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:43:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 8            |        cudaMalloc retries: 16        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79340 MiB |  79400 MiB | 109727 GiB | 109649 GiB |
|       from large pool |  79160 MiB |  79220 MiB | 109056 GiB | 108979 GiB |
|       from small pool |    180 MiB |    181 MiB |    670 GiB |    670 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79340 MiB |  79400 MiB | 109727 GiB | 109649 GiB |
|       from large pool |  79160 MiB |  79220 MiB | 109056 GiB | 108979 GiB |
|       from small pool |    180 MiB |    181 MiB |    670 GiB |    670 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79316 MiB |  79376 MiB | 109495 GiB | 109418 GiB |
|       from large pool |  79137 MiB |  79196 MiB | 108826 GiB | 108748 GiB |
|       from small pool |    179 MiB |    180 MiB |    669 GiB |    669 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80484 MiB |  80496 MiB | 296284 MiB | 215800 MiB |
|       from large pool |  80298 MiB |  80298 MiB | 294230 MiB | 213932 MiB |
|       from small pool |    186 MiB |    242 MiB |   2054 MiB |   1868 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1083 MiB |   5955 MiB | 111232 GiB | 111231 GiB |
|       from large pool |   1077 MiB |   5950 MiB | 110463 GiB | 110462 GiB |
|       from small pool |      5 MiB |     23 MiB |    769 GiB |    769 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3502    |    3505    |    7503 K  |    7499 K  |
|       from large pool |     586    |     587    |    3468 K  |    3468 K  |
|       from small pool |    2916    |    2919    |    4034 K  |    4031 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3502    |    3505    |    7503 K  |    7499 K  |
|       from large pool |     586    |     587    |    3468 K  |    3468 K  |
|       from small pool |    2916    |    2919    |    4034 K  |    4031 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     191    |     219    |    2330    |    2139    |
|       from large pool |      98    |      98    |    1303    |    1205    |
|       from small pool |      93    |     121    |    1027    |     934    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     163    |     163    |    4229 K  |    4229 K  |
|       from large pool |      79    |      79    |    2284 K  |    2284 K  |
|       from small pool |      84    |      84    |    1944 K  |    1944 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:43:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:43:19] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:43:25]    INFO >> epoch 003:    830 / 1539 loss=3.84, wps=4650, ups=6.45, wpb=721, bsz=721, num_updates=3900, lr=0.000392, gnorm=6.826, clip=0, train_wall=7, gb_free=63, wall=674 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:43:32]    INFO >> epoch 003:    880 / 1539 loss=3.931, wps=4993.3, ups=6.75, wpb=740, bsz=740, num_updates=3950, lr=0.000392, gnorm=6.087, clip=0, train_wall=7, gb_free=73.3, wall=682 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:43:41]    INFO >> epoch 003:    930 / 1539 loss=3.765, wps=4723.4, ups=6.75, wpb=699.4, bsz=699.4, num_updates=4000, lr=0.000392, gnorm=6.132, clip=0, train_wall=7, gb_free=60.6, wall=689 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:43:50]    INFO >> epoch 003:    980 / 1539 loss=3.875, wps=4109, ups=5.78, wpb=711.2, bsz=711.2, num_updates=4050, lr=0.000392, gnorm=6.205, clip=0, train_wall=8, gb_free=75, wall=698 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:43:57]    INFO >> epoch 003:   1030 / 1539 loss=3.916, wps=4400.3, ups=7.12, wpb=618.1, bsz=618.1, num_updates=4100, lr=0.000392, gnorm=5.382, clip=0, train_wall=7, gb_free=71.5, wall=705 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:44:04]    INFO >> epoch 003:   1080 / 1539 loss=3.856, wps=4910, ups=7.25, wpb=677.3, bsz=677.3, num_updates=4150, lr=0.000392, gnorm=6.315, clip=0, train_wall=6, gb_free=74.3, wall=712 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:44:13]    INFO >> epoch 003:   1130 / 1539 loss=3.688, wps=5543.2, ups=6.27, wpb=884.7, bsz=884.7, num_updates=4200, lr=0.000392, gnorm=5.95, clip=0, train_wall=7, gb_free=72.8, wall=720 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:44:20]    INFO >> epoch 003:   1180 / 1539 loss=3.769, wps=4325.5, ups=6.68, wpb=647.2, bsz=647.2, num_updates=4250, lr=0.000392, gnorm=5.479, clip=0, train_wall=7, gb_free=68.8, wall=727 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:44:28]    INFO >> epoch 003:   1230 / 1539 loss=3.907, wps=4898.8, ups=6.58, wpb=744.2, bsz=744.2, num_updates=4300, lr=0.000392, gnorm=6.186, clip=0, train_wall=7, gb_free=74, wall=735 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:44:35]    INFO >> epoch 003:   1280 / 1539 loss=3.777, wps=4603.9, ups=7.34, wpb=627.3, bsz=627.3, num_updates=4350, lr=0.000392, gnorm=5.319, clip=0, train_wall=6, gb_free=71.8, wall=742 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:44:42]    INFO >> epoch 003:   1330 / 1539 loss=3.758, wps=4986.6, ups=7.32, wpb=680.9, bsz=680.9, num_updates=4400, lr=0.000392, gnorm=5.502, clip=0, train_wall=6, gb_free=72.6, wall=748 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:44:50]    INFO >> epoch 003:   1380 / 1539 loss=3.74, wps=4919.5, ups=7.14, wpb=689.4, bsz=689.4, num_updates=4450, lr=0.000392, gnorm=6.001, clip=0, train_wall=6, gb_free=75.8, wall=755 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:44:57]    INFO >> epoch 003:   1430 / 1539 loss=3.822, wps=4827, ups=7.17, wpb=673.6, bsz=673.6, num_updates=4500, lr=0.000392, gnorm=5.762, clip=0, train_wall=7, gb_free=74, wall=762 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:45:04]    INFO >> epoch 003:   1480 / 1539 loss=3.834, wps=4580.5, ups=7.1, wpb=645.1, bsz=645.1, num_updates=4550, lr=0.000392, gnorm=5.509, clip=0, train_wall=7, gb_free=71.4, wall=769 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:45:11]    INFO >> epoch 003:   1530 / 1539 loss=3.877, wps=4813.7, ups=7.32, wpb=657.9, bsz=657.9, num_updates=4600, lr=0.000392, gnorm=6.569, clip=0, train_wall=6, gb_free=74.4, wall=776 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:45:12]    INFO >> epoch 003 | loss 3.855 | wps 4605.3 | ups 6.41 | wpb 718.6 | bsz 718.6 | num_updates 4609 | lr 0.000392 | gnorm 6.147 | clip 0.2 | train_wall 208 | gb_free 75.1 | wall 777 (progress_bar.py:267, print())[0m
[33m[2025-11-21 06:45:12] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 06:45:26]    INFO >> epoch 003 | valid on 'valid' subset | loss 3.878 | wps 11739.7 | wpb 5412.5 | bsz 5412.5 | num_updates 4609 | best_loss 4.97 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 06:45:27]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 06:45:27]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/checkpoints/checkpoint_last.pt (epoch 3 @ 4609 updates, score 3.878) (writing took 0.011074 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 06:45:27] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 06:45:33]    INFO >> epoch 004:     41 / 1539 loss=3.574, wps=1880.4, ups=2.41, wpb=780.3, bsz=780.3, num_updates=4650, lr=0.000376, gnorm=7.267, clip=4, train_wall=7, gb_free=75.7, wall=797 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:45:40]    INFO >> epoch 004:     91 / 1539 loss=3.821, wps=4696.4, ups=6.76, wpb=694.9, bsz=694.9, num_updates=4700, lr=0.000376, gnorm=5.121, clip=0, train_wall=7, gb_free=72.4, wall=804 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:45:50]    INFO >> epoch 004:    141 / 1539 loss=3.603, wps=5673, ups=5.47, wpb=1036.7, bsz=1036.7, num_updates=4750, lr=0.000376, gnorm=7.792, clip=2, train_wall=9, gb_free=75.3, wall=814 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:45:58]    INFO >> epoch 004:    191 / 1539 loss=3.737, wps=5317.6, ups=6.91, wpb=769, bsz=769, num_updates=4800, lr=0.000376, gnorm=5.322, clip=0, train_wall=7, gb_free=71.8, wall=821 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:46:05]    INFO >> epoch 004:    241 / 1539 loss=3.669, wps=5135.4, ups=6.97, wpb=737, bsz=737, num_updates=4850, lr=0.000376, gnorm=7.561, clip=2, train_wall=7, gb_free=71.3, wall=828 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:46:12]    INFO >> epoch 004:    291 / 1539 loss=3.85, wps=4681.1, ups=7.17, wpb=653, bsz=653, num_updates=4900, lr=0.000376, gnorm=5.525, clip=0, train_wall=6, gb_free=72.3, wall=835 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:46:19]    INFO >> epoch 004:    341 / 1539 loss=3.823, wps=4908.4, ups=7.29, wpb=673.2, bsz=673.2, num_updates=4950, lr=0.000376, gnorm=5.163, clip=0, train_wall=6, gb_free=73, wall=842 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:46:28]    INFO >> epoch 004:    391 / 1539 loss=3.671, wps=5405, ups=7.05, wpb=766.3, bsz=766.3, num_updates=5000, lr=0.000376, gnorm=5.852, clip=0, train_wall=7, gb_free=73.5, wall=849 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:46:35]    INFO >> epoch 004:    441 / 1539 loss=3.815, wps=4182.2, ups=6.6, wpb=633.4, bsz=633.4, num_updates=5050, lr=0.000376, gnorm=6.127, clip=0, train_wall=7, gb_free=60.6, wall=856 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:46:43]    INFO >> epoch 004:    491 / 1539 loss=3.783, wps=4867.9, ups=6.79, wpb=717, bsz=717, num_updates=5100, lr=0.000376, gnorm=5.184, clip=0, train_wall=7, gb_free=73.9, wall=864 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:46:50]    INFO >> epoch 004:    541 / 1539 loss=3.775, wps=4893.8, ups=6.95, wpb=704.1, bsz=704.1, num_updates=5150, lr=0.000376, gnorm=6.172, clip=0, train_wall=7, gb_free=65.6, wall=871 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:46:57]    INFO >> epoch 004:    591 / 1539 loss=3.76, wps=4334.9, ups=7.18, wpb=603.5, bsz=603.5, num_updates=5200, lr=0.000376, gnorm=5.045, clip=0, train_wall=6, gb_free=75.4, wall=878 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:46:58] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 11.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 76.85 GiB is allocated by PyTorch, and 1.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:46:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:46:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:46:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 9            |        cudaMalloc retries: 19        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78638 MiB |  78698 MiB | 148824 GiB | 148747 GiB |
|       from large pool |  78466 MiB |  78526 MiB | 147913 GiB | 147836 GiB |
|       from small pool |    172 MiB |    173 MiB |    911 GiB |    911 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78638 MiB |  78698 MiB | 148824 GiB | 148747 GiB |
|       from large pool |  78466 MiB |  78526 MiB | 147913 GiB | 147836 GiB |
|       from small pool |    172 MiB |    173 MiB |    911 GiB |    911 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78535 MiB |  78594 MiB | 148509 GiB | 148432 GiB |
|       from large pool |  78363 MiB |  78423 MiB | 147598 GiB | 147522 GiB |
|       from small pool |    171 MiB |    172 MiB |    910 GiB |    909 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80494 MiB |  80494 MiB | 326542 MiB | 246048 MiB |
|       from large pool |  80316 MiB |  80316 MiB | 324074 MiB | 243758 MiB |
|       from small pool |    178 MiB |    440 MiB |   2468 MiB |   2290 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1795 MiB |   6899 MiB | 146586 GiB | 146584 GiB |
|       from large pool |   1789 MiB |   6895 MiB | 145543 GiB | 145541 GiB |
|       from small pool |      5 MiB |     23 MiB |   1043 GiB |   1043 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3359    |    3362    |   10177 K  |   10173 K  |
|       from large pool |     573    |     574    |    4688 K  |    4688 K  |
|       from small pool |    2786    |    2789    |    5488 K  |    5485 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3359    |    3362    |   10177 K  |   10173 K  |
|       from large pool |     573    |     574    |    4688 K  |    4688 K  |
|       from small pool |    2786    |    2789    |    5488 K  |    5485 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     406    |     555    |    2936    |    2530    |
|       from large pool |     317    |     335    |    1702    |    1385    |
|       from small pool |      89    |     220    |    1234    |    1145    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     209    |     209    |    5753 K  |    5753 K  |
|       from large pool |     126    |     132    |    3097 K  |    3097 K  |
|       from small pool |      83    |      83    |    2655 K  |    2655 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:46:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:46:58] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:47:06]    INFO >> epoch 004:    642 / 1539 loss=3.715, wps=4416.2, ups=6.42, wpb=687.7, bsz=687.7, num_updates=5250, lr=0.000376, gnorm=5.724, clip=0, train_wall=6, gb_free=73.1, wall=886 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:47:13] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 329.25 MiB is free. Including non-PyTorch memory, this process has 78.79 GiB memory in use. Of the allocated memory 74.09 GiB is allocated by PyTorch, and 4.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:47:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:47:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:47:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 10           |        cudaMalloc retries: 21        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72059 MiB |  75868 MiB | 151135 GiB | 151064 GiB |
|       from large pool |  72043 MiB |  75852 MiB | 150211 GiB | 150141 GiB |
|       from small pool |     15 MiB |     22 MiB |    923 GiB |    923 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72059 MiB |  75868 MiB | 151135 GiB | 151064 GiB |
|       from large pool |  72043 MiB |  75852 MiB | 150211 GiB | 150141 GiB |
|       from small pool |     15 MiB |     22 MiB |    923 GiB |    923 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72042 MiB |  75850 MiB | 150814 GiB | 150744 GiB |
|       from large pool |  72027 MiB |  75835 MiB | 149892 GiB | 149822 GiB |
|       from small pool |     15 MiB |     22 MiB |    922 GiB |    922 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80176 MiB |  80434 MiB | 337456 MiB | 257280 MiB |
|       from large pool |  80158 MiB |  80256 MiB | 334986 MiB | 254828 MiB |
|       from small pool |     18 MiB |    178 MiB |   2470 MiB |   2452 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5388 MiB |   7349 MiB | 148677 GiB | 148672 GiB |
|       from large pool |   5386 MiB |   7345 MiB | 147620 GiB | 147615 GiB |
|       from small pool |      2 MiB |     19 MiB |   1057 GiB |   1057 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     502    |     511    |   10329 K  |   10329 K  |
|       from large pool |     308    |     317    |    4768 K  |    4767 K  |
|       from small pool |     194    |     240    |    5561 K  |    5561 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     502    |     511    |   10329 K  |   10329 K  |
|       from large pool |     308    |     317    |    4768 K  |    4767 K  |
|       from small pool |     194    |     240    |    5561 K  |    5561 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     149    |     405    |    2941    |    2792    |
|       from large pool |     140    |     316    |    1706    |    1566    |
|       from small pool |       9    |      89    |    1235    |    1226    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     138    |     138    |    5838 K  |    5837 K  |
|       from large pool |     118    |     118    |    3150 K  |    3150 K  |
|       from small pool |      20    |      44    |    2687 K  |    2687 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:47:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:47:13] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:47:14]    INFO >> epoch 004:    693 / 1539 loss=3.731, wps=4086.2, ups=6.34, wpb=644.3, bsz=644.3, num_updates=5300, lr=0.000376, gnorm=5.049, clip=0, train_wall=6, gb_free=73.8, wall=894 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:47:22]    INFO >> epoch 004:    743 / 1539 loss=3.693, wps=4219.9, ups=6.29, wpb=671.3, bsz=671.3, num_updates=5350, lr=0.000376, gnorm=5.453, clip=0, train_wall=7, gb_free=74.1, wall=902 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:47:29]    INFO >> epoch 004:    793 / 1539 loss=3.606, wps=4535.8, ups=6.53, wpb=694.1, bsz=694.1, num_updates=5400, lr=0.000376, gnorm=6.227, clip=0, train_wall=7, gb_free=58.7, wall=909 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:47:37]    INFO >> epoch 004:    843 / 1539 loss=3.607, wps=5535.3, ups=7.34, wpb=754.2, bsz=754.2, num_updates=5450, lr=0.000376, gnorm=5.967, clip=0, train_wall=6, gb_free=65.2, wall=916 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:47:45]    INFO >> epoch 004:    893 / 1539 loss=3.913, wps=5047.9, ups=6.39, wpb=789.4, bsz=789.4, num_updates=5500, lr=0.000376, gnorm=6.018, clip=0, train_wall=7, gb_free=73.2, wall=924 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:47:52]    INFO >> epoch 004:    943 / 1539 loss=3.699, wps=4670, ups=7.28, wpb=641.3, bsz=641.3, num_updates=5550, lr=0.000376, gnorm=5.285, clip=0, train_wall=6, gb_free=74.7, wall=931 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:48:00]    INFO >> epoch 004:    993 / 1539 loss=3.753, wps=5086.7, ups=6.75, wpb=753.8, bsz=753.8, num_updates=5600, lr=0.000376, gnorm=5.306, clip=0, train_wall=7, gb_free=73.3, wall=938 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:48:08]    INFO >> epoch 004:   1043 / 1539 loss=3.874, wps=4473.9, ups=7.25, wpb=617.4, bsz=617.4, num_updates=5650, lr=0.000376, gnorm=5.145, clip=0, train_wall=6, gb_free=73.5, wall=945 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:48:15]    INFO >> epoch 004:   1093 / 1539 loss=3.824, wps=5408, ups=7.1, wpb=762.2, bsz=762.2, num_updates=5700, lr=0.000376, gnorm=5.006, clip=0, train_wall=7, gb_free=74, wall=952 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:48:22]    INFO >> epoch 004:   1143 / 1539 loss=3.633, wps=4552.3, ups=7.28, wpb=625.3, bsz=625.3, num_updates=5750, lr=0.000376, gnorm=5.228, clip=0, train_wall=6, gb_free=72.4, wall=959 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:48:29]    INFO >> epoch 004:   1193 / 1539 loss=3.776, wps=5252.4, ups=6.57, wpb=799.5, bsz=799.5, num_updates=5800, lr=0.000376, gnorm=6.912, clip=2, train_wall=7, gb_free=64.3, wall=967 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:48:36]    INFO >> epoch 004:   1243 / 1539 loss=3.686, wps=4625.7, ups=7.39, wpb=625.9, bsz=625.9, num_updates=5850, lr=0.000376, gnorm=4.809, clip=0, train_wall=6, gb_free=73.7, wall=973 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:48:45]    INFO >> epoch 004:   1293 / 1539 loss=3.725, wps=4241.3, ups=6.66, wpb=637.1, bsz=637.1, num_updates=5900, lr=0.000376, gnorm=5.094, clip=0, train_wall=7, gb_free=73.5, wall=981 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:48:52]    INFO >> epoch 004:   1343 / 1539 loss=3.65, wps=5339.9, ups=6.81, wpb=784, bsz=784, num_updates=5950, lr=0.000376, gnorm=6.01, clip=0, train_wall=7, gb_free=68.1, wall=988 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:49:00] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.41 GiB is free. Including non-PyTorch memory, this process has 77.71 GiB memory in use. Of the allocated memory 73.80 GiB is allocated by PyTorch, and 3.41 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:49:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:49:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:49:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 11           |        cudaMalloc retries: 23        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75050 MiB |  75994 MiB | 169414 GiB | 169340 GiB |
|       from large pool |  75040 MiB |  75983 MiB | 168387 GiB | 168314 GiB |
|       from small pool |     10 MiB |     11 MiB |   1026 GiB |   1026 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75050 MiB |  75994 MiB | 169414 GiB | 169340 GiB |
|       from large pool |  75040 MiB |  75983 MiB | 168387 GiB | 168314 GiB |
|       from small pool |     10 MiB |     11 MiB |   1026 GiB |   1026 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75032 MiB |  75975 MiB | 169052 GiB | 168979 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 168027 GiB | 167954 GiB |
|       from small pool |     10 MiB |     11 MiB |   1025 GiB |   1025 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79062 MiB |  80056 MiB | 342474 MiB | 263412 MiB |
|       from large pool |  79044 MiB |  79814 MiB | 339780 MiB | 260736 MiB |
|       from small pool |     18 MiB |    242 MiB |   2694 MiB |   2676 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4011 MiB |  10661 MiB | 165814 GiB | 165810 GiB |
|       from large pool |   4003 MiB |  10653 MiB | 164636 GiB | 164632 GiB |
|       from small pool |      7 MiB |     15 MiB |   1177 GiB |   1177 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     501    |     509    |   11552 K  |   11551 K  |
|       from large pool |     314    |     322    |    5379 K  |    5379 K  |
|       from small pool |     187    |     220    |    6172 K  |    6172 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     501    |     509    |   11552 K  |   11551 K  |
|       from large pool |     314    |     322    |    5379 K  |    5379 K  |
|       from small pool |     187    |     220    |    6172 K  |    6172 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     102    |     262    |    3060    |    2958    |
|       from large pool |      93    |     141    |    1713    |    1620    |
|       from small pool |       9    |     121    |    1347    |    1338    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      99    |      99    |    6520 K  |    6520 K  |
|       from large pool |      81    |      81    |    3559 K  |    3559 K  |
|       from small pool |      18    |      32    |    2961 K  |    2961 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:49:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:49:00] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:49:01]    INFO >> epoch 004:   1394 / 1539 loss=3.705, wps=4970.5, ups=5.96, wpb=833.5, bsz=833.5, num_updates=6000, lr=0.000376, gnorm=6.314, clip=2, train_wall=7, gb_free=5.8, wall=997 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:49:08]    INFO >> epoch 004:   1444 / 1539 loss=3.758, wps=4956, ups=7.21, wpb=687.8, bsz=687.8, num_updates=6050, lr=0.000376, gnorm=6.018, clip=0, train_wall=6, gb_free=72.1, wall=1004 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:49:16]    INFO >> epoch 004:   1494 / 1539 loss=3.784, wps=5004.4, ups=7.19, wpb=695.7, bsz=695.7, num_updates=6100, lr=0.000376, gnorm=5.959, clip=0, train_wall=6, gb_free=73.8, wall=1010 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:49:23]    INFO >> epoch 004 | loss 3.727 | wps 4600.8 | ups 6.4 | wpb 718.6 | bsz 718.6 | num_updates 6145 | lr 0.000376 | gnorm 5.776 | clip 0.3 | train_wall 208 | gb_free 71.3 | wall 1017 (progress_bar.py:267, print())[0m
[33m[2025-11-21 06:49:23] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 06:49:36]    INFO >> epoch 004 | valid on 'valid' subset | loss 3.819 | wps 11900.4 | wpb 5412.5 | bsz 5412.5 | num_updates 6145 | best_loss 4.97 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 06:49:36]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 06:49:36]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/checkpoints/checkpoint_last.pt (epoch 4 @ 6145 updates, score 3.819) (writing took 0.011094 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 06:49:36] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 06:49:37]    INFO >> epoch 005:      5 / 1539 loss=3.626, wps=1936.9, ups=2.4, wpb=807.4, bsz=807.4, num_updates=6150, lr=0.000354, gnorm=5.523, clip=0, train_wall=7, gb_free=72.2, wall=1031 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:49:45]    INFO >> epoch 005:     55 / 1539 loss=3.814, wps=4565.9, ups=6.39, wpb=714.8, bsz=714.8, num_updates=6200, lr=0.000354, gnorm=5.106, clip=0, train_wall=7, gb_free=71.5, wall=1039 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:49:53]    INFO >> epoch 005:    105 / 1539 loss=3.702, wps=4989.9, ups=7.24, wpb=688.8, bsz=688.8, num_updates=6250, lr=0.000354, gnorm=5.565, clip=0, train_wall=6, gb_free=67.9, wall=1046 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:50:00]    INFO >> epoch 005:    155 / 1539 loss=3.737, wps=4809.3, ups=7.24, wpb=664.2, bsz=664.2, num_updates=6300, lr=0.000354, gnorm=4.629, clip=0, train_wall=6, gb_free=74, wall=1053 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:50:07]    INFO >> epoch 005:    205 / 1539 loss=3.718, wps=4671.3, ups=6.92, wpb=675, bsz=675, num_updates=6350, lr=0.000354, gnorm=5.559, clip=0, train_wall=7, gb_free=71.7, wall=1060 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:50:15]    INFO >> epoch 005:    255 / 1539 loss=3.594, wps=5070.5, ups=6.42, wpb=790, bsz=790, num_updates=6400, lr=0.000354, gnorm=5.449, clip=2, train_wall=7, gb_free=75, wall=1068 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:50:23]    INFO >> epoch 005:    305 / 1539 loss=3.737, wps=5445.6, ups=6.91, wpb=787.6, bsz=787.6, num_updates=6450, lr=0.000354, gnorm=5.482, clip=0, train_wall=7, gb_free=75.1, wall=1075 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:50:31]    INFO >> epoch 005:    355 / 1539 loss=3.515, wps=5270.1, ups=6.92, wpb=761.7, bsz=761.7, num_updates=6500, lr=0.000354, gnorm=5.216, clip=0, train_wall=7, gb_free=73.5, wall=1082 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:50:38]    INFO >> epoch 005:    405 / 1539 loss=3.587, wps=5589.8, ups=6.31, wpb=885.5, bsz=885.5, num_updates=6550, lr=0.000354, gnorm=4.631, clip=0, train_wall=7, gb_free=73.6, wall=1090 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:50:46]    INFO >> epoch 005:    455 / 1539 loss=3.662, wps=4214.7, ups=7.08, wpb=595.1, bsz=595.1, num_updates=6600, lr=0.000354, gnorm=4.925, clip=0, train_wall=7, gb_free=75.5, wall=1097 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:50:53]    INFO >> epoch 005:    505 / 1539 loss=3.632, wps=4742.9, ups=6.61, wpb=718, bsz=718, num_updates=6650, lr=0.000354, gnorm=5.242, clip=0, train_wall=7, gb_free=73.3, wall=1105 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:51:01]    INFO >> epoch 005:    555 / 1539 loss=3.619, wps=4690.9, ups=7.14, wpb=656.7, bsz=656.7, num_updates=6700, lr=0.000354, gnorm=5.698, clip=0, train_wall=7, gb_free=75.1, wall=1112 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:51:11] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 19.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 77.19 GiB is allocated by PyTorch, and 1.41 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:51:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:51:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:51:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 12           |        cudaMalloc retries: 25        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78985 MiB |  79045 MiB | 192495 GiB | 192418 GiB |
|       from large pool |  78809 MiB |  78869 MiB | 191310 GiB | 191233 GiB |
|       from small pool |    176 MiB |    177 MiB |   1184 GiB |   1184 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78985 MiB |  79045 MiB | 192495 GiB | 192418 GiB |
|       from large pool |  78809 MiB |  78869 MiB | 191310 GiB | 191233 GiB |
|       from small pool |    176 MiB |    177 MiB |   1184 GiB |   1184 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78955 MiB |  79015 MiB | 192089 GiB | 192011 GiB |
|       from large pool |  78780 MiB |  78839 MiB | 190905 GiB | 190828 GiB |
|       from small pool |    175 MiB |    176 MiB |   1183 GiB |   1183 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80486 MiB |  80504 MiB | 344160 MiB | 263674 MiB |
|       from large pool |  80304 MiB |  80304 MiB | 341040 MiB | 260736 MiB |
|       from small pool |    182 MiB |    440 MiB |   3120 MiB |   2938 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1440 MiB |   6985 MiB | 189266 GiB | 189264 GiB |
|       from large pool |   1434 MiB |   6979 MiB | 187910 GiB | 187909 GiB |
|       from small pool |      5 MiB |     25 MiB |   1355 GiB |   1355 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3436    |    3439    |   13210 K  |   13206 K  |
|       from large pool |     580    |     581    |    6069 K  |    6068 K  |
|       from small pool |    2856    |    2859    |    7140 K  |    7138 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3436    |    3439    |   13210 K  |   13206 K  |
|       from large pool |     580    |     581    |    6069 K  |    6068 K  |
|       from small pool |    2856    |    2859    |    7140 K  |    7138 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     205    |     330    |    3294    |    3089    |
|       from large pool |     114    |     114    |    1734    |    1620    |
|       from small pool |      91    |     220    |    1560    |    1469    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     165    |     165    |    7469 K  |    7469 K  |
|       from large pool |      82    |      83    |    4004 K  |    4004 K  |
|       from small pool |      83    |      83    |    3465 K  |    3465 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:51:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:51:11] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:51:14]    INFO >> epoch 005:    606 / 1539 loss=3.633, wps=2817.5, ups=3.99, wpb=706.8, bsz=706.8, num_updates=6750, lr=0.000354, gnorm=5.278, clip=0, train_wall=7, gb_free=73.2, wall=1125 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:51:21]    INFO >> epoch 005:    656 / 1539 loss=3.671, wps=5303.4, ups=6.7, wpb=791.8, bsz=791.8, num_updates=6800, lr=0.000354, gnorm=5.502, clip=0, train_wall=7, gb_free=73.1, wall=1132 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:51:30]    INFO >> epoch 005:    706 / 1539 loss=3.651, wps=4972.7, ups=6.96, wpb=714, bsz=714, num_updates=6850, lr=0.000354, gnorm=6.436, clip=0, train_wall=7, gb_free=74.5, wall=1139 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:51:37]    INFO >> epoch 005:    756 / 1539 loss=3.713, wps=5046.3, ups=6.88, wpb=733.7, bsz=733.7, num_updates=6900, lr=0.000354, gnorm=5.134, clip=0, train_wall=7, gb_free=74.1, wall=1146 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:51:45]    INFO >> epoch 005:    806 / 1539 loss=3.653, wps=4345.6, ups=6.57, wpb=661, bsz=661, num_updates=6950, lr=0.000354, gnorm=5.111, clip=0, train_wall=7, gb_free=67.5, wall=1154 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:51:52]    INFO >> epoch 005:    856 / 1539 loss=3.594, wps=4722.6, ups=7.39, wpb=638.6, bsz=638.6, num_updates=7000, lr=0.000354, gnorm=5.208, clip=0, train_wall=6, gb_free=74.9, wall=1161 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:51:59]    INFO >> epoch 005:    906 / 1539 loss=3.597, wps=4851.1, ups=6.81, wpb=712.6, bsz=712.6, num_updates=7050, lr=0.000354, gnorm=4.962, clip=0, train_wall=7, gb_free=73, wall=1168 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:52:07] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 241.25 MiB is free. Including non-PyTorch memory, this process has 78.88 GiB memory in use. Of the allocated memory 74.09 GiB is allocated by PyTorch, and 4.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:52:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:52:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:52:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 13           |        cudaMalloc retries: 26        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72057 MiB |  75867 MiB | 201896 GiB | 201826 GiB |
|       from large pool |  72042 MiB |  75852 MiB | 200660 GiB | 200589 GiB |
|       from small pool |     15 MiB |     21 MiB |   1236 GiB |   1236 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72057 MiB |  75867 MiB | 201896 GiB | 201826 GiB |
|       from large pool |  72042 MiB |  75852 MiB | 200660 GiB | 200589 GiB |
|       from small pool |     15 MiB |     21 MiB |   1236 GiB |   1236 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72042 MiB |  75850 MiB | 201469 GiB | 201398 GiB |
|       from large pool |  72027 MiB |  75835 MiB | 200234 GiB | 200164 GiB |
|       from small pool |     15 MiB |     21 MiB |   1234 GiB |   1234 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80264 MiB |  80426 MiB | 344160 MiB | 263896 MiB |
|       from large pool |  80244 MiB |  80244 MiB | 341040 MiB | 260796 MiB |
|       from small pool |     20 MiB |    182 MiB |   3120 MiB |   3100 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2750 MiB |   7203 MiB | 198828 GiB | 198825 GiB |
|       from large pool |   2745 MiB |   7193 MiB | 197413 GiB | 197411 GiB |
|       from small pool |      4 MiB |     17 MiB |   1414 GiB |   1414 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     502    |     511    |   13826 K  |   13826 K  |
|       from large pool |     308    |     317    |    6382 K  |    6382 K  |
|       from small pool |     194    |     240    |    7444 K  |    7443 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     502    |     511    |   13826 K  |   13826 K  |
|       from large pool |     308    |     317    |    6382 K  |    6382 K  |
|       from small pool |     194    |     240    |    7444 K  |    7443 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     123    |     204    |    3294    |    3171    |
|       from large pool |     113    |     113    |    1734    |    1621    |
|       from small pool |      10    |      91    |    1560    |    1550    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     111    |     111    |    7806 K  |    7806 K  |
|       from large pool |      91    |      91    |    4208 K  |    4208 K  |
|       from small pool |      20    |      42    |    3598 K  |    3598 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:52:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:52:07] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:52:08]    INFO >> epoch 005:    957 / 1539 loss=3.705, wps=4377, ups=6.55, wpb=668.1, bsz=668.1, num_updates=7100, lr=0.000354, gnorm=5.25, clip=0, train_wall=6, gb_free=71.4, wall=1176 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:52:16]    INFO >> epoch 005:   1007 / 1539 loss=3.343, wps=5732.3, ups=6.08, wpb=942.7, bsz=942.7, num_updates=7150, lr=0.000354, gnorm=5.867, clip=2, train_wall=8, gb_free=70.5, wall=1184 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:52:24]    INFO >> epoch 005:   1057 / 1539 loss=3.57, wps=5003, ups=6.51, wpb=768, bsz=768, num_updates=7200, lr=0.000354, gnorm=5.079, clip=0, train_wall=7, gb_free=76.9, wall=1192 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:52:31]    INFO >> epoch 005:   1107 / 1539 loss=3.652, wps=4749.6, ups=7.17, wpb=662.3, bsz=662.3, num_updates=7250, lr=0.000354, gnorm=4.869, clip=0, train_wall=6, gb_free=72.1, wall=1199 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:52:34] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 139.25 MiB is free. Including non-PyTorch memory, this process has 78.98 GiB memory in use. Of the allocated memory 75.66 GiB is allocated by PyTorch, and 2.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:52:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:52:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:52:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 14           |        cudaMalloc retries: 28        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76954 MiB |  77477 MiB | 206802 GiB | 206727 GiB |
|       from large pool |  76944 MiB |  77466 MiB | 205536 GiB | 205461 GiB |
|       from small pool |     10 MiB |     18 MiB |   1265 GiB |   1265 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76954 MiB |  77477 MiB | 206802 GiB | 206727 GiB |
|       from large pool |  76944 MiB |  77466 MiB | 205536 GiB | 205461 GiB |
|       from small pool |     10 MiB |     18 MiB |   1265 GiB |   1265 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76934 MiB |  77455 MiB | 206365 GiB | 206290 GiB |
|       from large pool |  76923 MiB |  77445 MiB | 205101 GiB | 205026 GiB |
|       from small pool |     10 MiB |     18 MiB |   1263 GiB |   1263 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80366 MiB |  80366 MiB | 351058 MiB | 270692 MiB |
|       from large pool |  80344 MiB |  80344 MiB | 347736 MiB | 267392 MiB |
|       from small pool |     22 MiB |    222 MiB |   3322 MiB |   3300 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3411 MiB |   7797 MiB | 203895 GiB | 203892 GiB |
|       from large pool |   3399 MiB |   7786 MiB | 202446 GiB | 202443 GiB |
|       from small pool |     11 MiB |     23 MiB |   1448 GiB |   1448 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     502    |     509    |   14157 K  |   14157 K  |
|       from large pool |     315    |     322    |    6538 K  |    6538 K  |
|       from small pool |     187    |     240    |    7619 K  |    7618 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     502    |     509    |   14157 K  |   14157 K  |
|       from large pool |     315    |     322    |    6538 K  |    6538 K  |
|       from small pool |     187    |     240    |    7619 K  |    7618 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     111    |     229    |    3403    |    3292    |
|       from large pool |     100    |     118    |    1742    |    1642    |
|       from small pool |      11    |     111    |    1661    |    1650    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     100    |     100    |    7992 K  |    7992 K  |
|       from large pool |      79    |      79    |    4310 K  |    4310 K  |
|       from small pool |      21    |      46    |    3682 K  |    3682 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:52:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:52:34] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:52:40]    INFO >> epoch 005:   1158 / 1539 loss=3.517, wps=4756.2, ups=6.12, wpb=777.1, bsz=777.1, num_updates=7300, lr=0.000354, gnorm=5.382, clip=0, train_wall=7, gb_free=73.5, wall=1207 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:52:47]    INFO >> epoch 005:   1208 / 1539 loss=3.826, wps=4324.3, ups=7.25, wpb=596.4, bsz=596.4, num_updates=7350, lr=0.000354, gnorm=4.441, clip=0, train_wall=6, gb_free=71.4, wall=1214 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:52:54]    INFO >> epoch 005:   1258 / 1539 loss=3.508, wps=5068.9, ups=7.36, wpb=688.6, bsz=688.6, num_updates=7400, lr=0.000354, gnorm=5.27, clip=2, train_wall=6, gb_free=75.8, wall=1221 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:53:01]    INFO >> epoch 005:   1308 / 1539 loss=3.682, wps=4739.2, ups=7.27, wpb=651.5, bsz=651.5, num_updates=7450, lr=0.000354, gnorm=4.656, clip=0, train_wall=6, gb_free=73.6, wall=1227 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:53:08]    INFO >> epoch 005:   1358 / 1539 loss=3.572, wps=4969, ups=6.88, wpb=722.6, bsz=722.6, num_updates=7500, lr=0.000354, gnorm=5.101, clip=0, train_wall=7, gb_free=72.1, wall=1235 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:53:17]    INFO >> epoch 005:   1408 / 1539 loss=3.769, wps=4569.5, ups=7.07, wpb=646.4, bsz=646.4, num_updates=7550, lr=0.000354, gnorm=4.923, clip=0, train_wall=7, gb_free=71.9, wall=1242 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:53:24]    INFO >> epoch 005:   1458 / 1539 loss=3.497, wps=5355.1, ups=7.12, wpb=752.5, bsz=752.5, num_updates=7600, lr=0.000354, gnorm=6.311, clip=2, train_wall=7, gb_free=72.4, wall=1249 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:53:31]    INFO >> epoch 005:   1508 / 1539 loss=3.63, wps=5026, ups=6.7, wpb=750.5, bsz=750.5, num_updates=7650, lr=0.000354, gnorm=4.977, clip=0, train_wall=7, gb_free=75.9, wall=1256 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:53:36]    INFO >> epoch 005 | loss 3.63 | wps 4529.5 | ups 6.3 | wpb 718.6 | bsz 718.6 | num_updates 7681 | lr 0.000354 | gnorm 5.278 | clip 0.3 | train_wall 207 | gb_free 65.4 | wall 1261 (progress_bar.py:267, print())[0m
[33m[2025-11-21 06:53:36] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 06:53:50]    INFO >> epoch 005 | valid on 'valid' subset | loss 3.76 | wps 11906.5 | wpb 5412.5 | bsz 5412.5 | num_updates 7681 | best_loss 4.97 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 06:53:50]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 06:53:50]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/checkpoints/checkpoint_last.pt (epoch 5 @ 7681 updates, score 3.76) (writing took 0.011221 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 06:53:50] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 06:53:53]    INFO >> epoch 006:     19 / 1539 loss=3.668, wps=1868.9, ups=2.42, wpb=773.3, bsz=773.3, num_updates=7700, lr=0.000327, gnorm=5.972, clip=0, train_wall=7, gb_free=68.3, wall=1277 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:54:01]    INFO >> epoch 006:     69 / 1539 loss=3.618, wps=5430.3, ups=6.46, wpb=841.2, bsz=841.2, num_updates=7750, lr=0.000327, gnorm=4.978, clip=0, train_wall=7, gb_free=75.1, wall=1285 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:54:08]    INFO >> epoch 006:    119 / 1539 loss=3.557, wps=4939.7, ups=7.19, wpb=687.2, bsz=687.2, num_updates=7800, lr=0.000327, gnorm=4.724, clip=0, train_wall=6, gb_free=73.7, wall=1292 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:54:15]    INFO >> epoch 006:    169 / 1539 loss=3.642, wps=4817.3, ups=6.87, wpb=701.3, bsz=701.3, num_updates=7850, lr=0.000327, gnorm=5.053, clip=0, train_wall=7, gb_free=73, wall=1299 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:54:19] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 139.25 MiB is free. Including non-PyTorch memory, this process has 78.98 GiB memory in use. Of the allocated memory 73.80 GiB is allocated by PyTorch, and 4.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:54:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:54:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:54:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 15           |        cudaMalloc retries: 29        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75051 MiB |  75994 MiB | 226272 GiB | 226199 GiB |
|       from large pool |  75040 MiB |  75983 MiB | 224883 GiB | 224809 GiB |
|       from small pool |     10 MiB |     12 MiB |   1389 GiB |   1389 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75051 MiB |  75994 MiB | 226272 GiB | 226199 GiB |
|       from large pool |  75040 MiB |  75983 MiB | 224883 GiB | 224809 GiB |
|       from small pool |     10 MiB |     12 MiB |   1389 GiB |   1389 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75032 MiB |  75975 MiB | 225797 GiB | 225724 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 224409 GiB | 224336 GiB |
|       from small pool |     10 MiB |     12 MiB |   1387 GiB |   1387 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80366 MiB |  80440 MiB | 351132 MiB | 270766 MiB |
|       from large pool |  80344 MiB |  80344 MiB | 347736 MiB | 267392 MiB |
|       from small pool |     22 MiB |     96 MiB |   3396 MiB |   3374 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5314 MiB |   9037 MiB | 223036 GiB | 223030 GiB |
|       from large pool |   5303 MiB |   9025 MiB | 221450 GiB | 221444 GiB |
|       from small pool |     11 MiB |     21 MiB |   1585 GiB |   1585 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     501    |     509    |   15474 K  |   15473 K  |
|       from large pool |     314    |     322    |    7099 K  |    7098 K  |
|       from small pool |     187    |     240    |    8375 K  |    8375 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     501    |     509    |   15474 K  |   15473 K  |
|       from large pool |     314    |     322    |    7099 K  |    7098 K  |
|       from small pool |     187    |     240    |    8375 K  |    8375 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     111    |     148    |    3440    |    3329    |
|       from large pool |     100    |     100    |    1742    |    1642    |
|       from small pool |      11    |      48    |    1698    |    1687    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     105    |     107    |    8744 K  |    8743 K  |
|       from large pool |      86    |      88    |    4673 K  |    4673 K  |
|       from small pool |      19    |      42    |    4070 K  |    4070 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:54:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:54:19] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:54:24]    INFO >> epoch 006:    220 / 1539 loss=3.565, wps=4621, ups=6.59, wpb=701.4, bsz=701.4, num_updates=7900, lr=0.000327, gnorm=4.632, clip=0, train_wall=6, gb_free=75.3, wall=1307 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:54:25] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 1.25 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 75.07 GiB is allocated by PyTorch, and 3.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:54:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:54:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:54:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 16           |        cudaMalloc retries: 30        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76872 MiB |  76932 MiB | 227324 GiB | 227249 GiB |
|       from large pool |  76717 MiB |  76776 MiB | 225928 GiB | 225854 GiB |
|       from small pool |    155 MiB |    156 MiB |   1395 GiB |   1395 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76872 MiB |  76932 MiB | 227324 GiB | 227249 GiB |
|       from large pool |  76717 MiB |  76776 MiB | 225928 GiB | 225854 GiB |
|       from small pool |    155 MiB |    156 MiB |   1395 GiB |   1395 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76851 MiB |  76911 MiB | 226847 GiB | 226772 GiB |
|       from large pool |  76697 MiB |  76756 MiB | 225453 GiB | 225378 GiB |
|       from small pool |    154 MiB |    155 MiB |   1393 GiB |   1393 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80504 MiB |  80504 MiB | 351270 MiB | 270766 MiB |
|       from large pool |  80344 MiB |  80344 MiB | 347736 MiB | 267392 MiB |
|       from small pool |    160 MiB |    160 MiB |   3534 MiB |   3374 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3631 MiB |   6898 MiB | 224155 GiB | 224152 GiB |
|       from large pool |   3626 MiB |   6894 MiB | 222562 GiB | 222558 GiB |
|       from small pool |      4 MiB |     21 MiB |   1593 GiB |   1593 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3044    |    3045    |   15542 K  |   15539 K  |
|       from large pool |     545    |     546    |    7129 K  |    7128 K  |
|       from small pool |    2499    |    2500    |    8413 K  |    8410 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3044    |    3045    |   15542 K  |   15539 K  |
|       from large pool |     545    |     546    |    7129 K  |    7128 K  |
|       from small pool |    2499    |    2500    |    8413 K  |    8410 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     180    |     180    |    3509    |    3329    |
|       from large pool |     100    |     100    |    1742    |    1642    |
|       from small pool |      80    |      80    |    1767    |    1687    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     154    |     157    |    8782 K  |    8782 K  |
|       from large pool |      80    |      83    |    4692 K  |    4692 K  |
|       from small pool |      74    |      77    |    4089 K  |    4089 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:54:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:54:25] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:54:32]    INFO >> epoch 006:    271 / 1539 loss=3.64, wps=4173.2, ups=6.5, wpb=641.8, bsz=641.8, num_updates=7950, lr=0.000327, gnorm=4.725, clip=0, train_wall=6, gb_free=75, wall=1314 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:54:39]    INFO >> epoch 006:    321 / 1539 loss=3.543, wps=5435.4, ups=6.52, wpb=834, bsz=834, num_updates=8000, lr=0.000327, gnorm=5.408, clip=0, train_wall=7, gb_free=72.9, wall=1322 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:54:47]    INFO >> epoch 006:    371 / 1539 loss=3.676, wps=4426.5, ups=6.79, wpb=651.5, bsz=651.5, num_updates=8050, lr=0.000327, gnorm=4.397, clip=0, train_wall=7, gb_free=74.4, wall=1329 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:54:55]    INFO >> epoch 006:    421 / 1539 loss=3.595, wps=5049.2, ups=7.33, wpb=688.9, bsz=688.9, num_updates=8100, lr=0.000327, gnorm=5.425, clip=0, train_wall=6, gb_free=73.9, wall=1336 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:54:58] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 2.65 GiB is free. Including non-PyTorch memory, this process has 76.47 GiB memory in use. Of the allocated memory 70.76 GiB is allocated by PyTorch, and 5.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:54:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:54:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:54:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 17           |        cudaMalloc retries: 32        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72054 MiB |  72454 MiB | 233017 GiB | 232946 GiB |
|       from large pool |  72039 MiB |  72439 MiB | 231588 GiB | 231517 GiB |
|       from small pool |     15 MiB |     18 MiB |   1429 GiB |   1429 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72054 MiB |  72454 MiB | 233017 GiB | 232946 GiB |
|       from large pool |  72039 MiB |  72439 MiB | 231588 GiB | 231517 GiB |
|       from small pool |     15 MiB |     18 MiB |   1429 GiB |   1429 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72042 MiB |  72442 MiB | 232526 GiB | 232456 GiB |
|       from large pool |  72027 MiB |  72426 MiB | 231099 GiB | 231029 GiB |
|       from small pool |     15 MiB |     18 MiB |   1427 GiB |   1427 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77794 MiB |  77984 MiB | 386472 MiB | 308678 MiB |
|       from large pool |  77776 MiB |  77776 MiB | 382890 MiB | 305114 MiB |
|       from small pool |     18 MiB |    208 MiB |   3582 MiB |   3564 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5739 MiB |   7725 MiB | 229792 GiB | 229786 GiB |
|       from large pool |   5736 MiB |   7720 MiB | 228159 GiB | 228154 GiB |
|       from small pool |      2 MiB |     23 MiB |   1632 GiB |   1632 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     502    |     509    |   15935 K  |   15934 K  |
|       from large pool |     308    |     315    |    7321 K  |    7321 K  |
|       from small pool |     194    |     240    |    8613 K  |    8613 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     502    |     509    |   15935 K  |   15934 K  |
|       from large pool |     308    |     315    |    7321 K  |    7321 K  |
|       from small pool |     194    |     240    |    8613 K  |    8613 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     146    |     241    |    3584    |    3438    |
|       from large pool |     137    |     137    |    1793    |    1656    |
|       from small pool |       9    |     104    |    1791    |    1782    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     137    |     137    |    9004 K  |    9004 K  |
|       from large pool |     119    |     119    |    4820 K  |    4820 K  |
|       from small pool |      18    |      46    |    4184 K  |    4184 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:54:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:54:58] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:55:03]    INFO >> epoch 006:    472 / 1539 loss=3.775, wps=4958.4, ups=6.53, wpb=759.1, bsz=759.1, num_updates=8150, lr=0.000327, gnorm=5.099, clip=0, train_wall=7, gb_free=74, wall=1344 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:55:10]    INFO >> epoch 006:    522 / 1539 loss=3.581, wps=4867.4, ups=6.64, wpb=733.1, bsz=733.1, num_updates=8200, lr=0.000327, gnorm=4.801, clip=0, train_wall=7, gb_free=71.8, wall=1351 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:55:18]    INFO >> epoch 006:    572 / 1539 loss=3.673, wps=5371, ups=6.76, wpb=794, bsz=794, num_updates=8250, lr=0.000327, gnorm=4.612, clip=0, train_wall=7, gb_free=73.3, wall=1359 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:55:26]    INFO >> epoch 006:    622 / 1539 loss=3.561, wps=4750, ups=7.43, wpb=639.3, bsz=639.3, num_updates=8300, lr=0.000327, gnorm=4.814, clip=0, train_wall=6, gb_free=75.1, wall=1365 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:55:33]    INFO >> epoch 006:    672 / 1539 loss=3.668, wps=5029.2, ups=6.95, wpb=723.8, bsz=723.8, num_updates=8350, lr=0.000327, gnorm=4.617, clip=0, train_wall=7, gb_free=73, wall=1373 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:55:39]    INFO >> epoch 006:    722 / 1539 loss=3.552, wps=4844.3, ups=7.64, wpb=634.1, bsz=634.1, num_updates=8400, lr=0.000327, gnorm=4.3, clip=0, train_wall=6, gb_free=75.4, wall=1379 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:55:47]    INFO >> epoch 006:    772 / 1539 loss=3.401, wps=5307.9, ups=6.44, wpb=823.8, bsz=823.8, num_updates=8450, lr=0.000327, gnorm=4.857, clip=0, train_wall=7, gb_free=72.3, wall=1387 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:55:54]    INFO >> epoch 006:    822 / 1539 loss=3.681, wps=4455.6, ups=7.13, wpb=624.8, bsz=624.8, num_updates=8500, lr=0.000327, gnorm=4.408, clip=0, train_wall=7, gb_free=72.4, wall=1394 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:56:02]    INFO >> epoch 006:    872 / 1539 loss=3.531, wps=5155.4, ups=7.11, wpb=725.2, bsz=725.2, num_updates=8550, lr=0.000327, gnorm=4.561, clip=0, train_wall=7, gb_free=73.1, wall=1401 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:56:10]    INFO >> epoch 006:    922 / 1539 loss=3.711, wps=5061, ups=6.82, wpb=741.9, bsz=741.9, num_updates=8600, lr=0.000327, gnorm=5.292, clip=0, train_wall=7, gb_free=70.1, wall=1408 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:56:18]    INFO >> epoch 006:    972 / 1539 loss=3.221, wps=5126.3, ups=6.23, wpb=822.9, bsz=822.9, num_updates=8650, lr=0.000327, gnorm=5.61, clip=0, train_wall=7, gb_free=72.2, wall=1416 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:56:25]    INFO >> epoch 006:   1022 / 1539 loss=3.501, wps=4709.9, ups=6.64, wpb=709.4, bsz=709.4, num_updates=8700, lr=0.000327, gnorm=4.843, clip=0, train_wall=7, gb_free=74.9, wall=1424 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:56:35]    INFO >> epoch 006:   1072 / 1539 loss=3.744, wps=4636.3, ups=5.84, wpb=794.2, bsz=794.2, num_updates=8750, lr=0.000327, gnorm=5.198, clip=0, train_wall=8, gb_free=68, wall=1432 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:56:43]    INFO >> epoch 006:   1122 / 1539 loss=3.56, wps=4701.9, ups=6.25, wpb=752.3, bsz=752.3, num_updates=8800, lr=0.000327, gnorm=5.068, clip=0, train_wall=7, gb_free=74, wall=1440 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:56:50]    INFO >> epoch 006:   1172 / 1539 loss=3.676, wps=5065.4, ups=6.98, wpb=725.2, bsz=725.2, num_updates=8850, lr=0.000327, gnorm=5.563, clip=0, train_wall=7, gb_free=67, wall=1448 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:56:58]    INFO >> epoch 006:   1222 / 1539 loss=3.688, wps=4397.3, ups=6.43, wpb=683.4, bsz=683.4, num_updates=8900, lr=0.000327, gnorm=4.943, clip=0, train_wall=7, gb_free=74.2, wall=1455 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:57:06]    INFO >> epoch 006:   1272 / 1539 loss=3.646, wps=4625.4, ups=7.26, wpb=637.1, bsz=637.1, num_updates=8950, lr=0.000327, gnorm=4.389, clip=0, train_wall=6, gb_free=74.6, wall=1462 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:57:14]    INFO >> epoch 006:   1322 / 1539 loss=3.446, wps=5059.8, ups=6.72, wpb=752.8, bsz=752.8, num_updates=9000, lr=0.000327, gnorm=5.44, clip=0, train_wall=7, gb_free=74.1, wall=1470 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:57:21]    INFO >> epoch 006:   1372 / 1539 loss=3.622, wps=4190, ups=7.18, wpb=583.3, bsz=583.3, num_updates=9050, lr=0.000327, gnorm=4.347, clip=0, train_wall=6, gb_free=68.4, wall=1477 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:57:28]    INFO >> epoch 006:   1422 / 1539 loss=3.592, wps=5194.9, ups=6.87, wpb=756.3, bsz=756.3, num_updates=9100, lr=0.000327, gnorm=5.048, clip=0, train_wall=7, gb_free=68, wall=1484 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:57:36]    INFO >> epoch 006:   1472 / 1539 loss=3.291, wps=4653.1, ups=6.42, wpb=725.1, bsz=725.1, num_updates=9150, lr=0.000327, gnorm=5.968, clip=0, train_wall=7, gb_free=67.8, wall=1492 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:57:45]    INFO >> epoch 006:   1522 / 1539 loss=3.659, wps=4571.7, ups=6.83, wpb=669.4, bsz=669.4, num_updates=9200, lr=0.000327, gnorm=4.339, clip=0, train_wall=7, gb_free=68.4, wall=1499 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:57:47]    INFO >> epoch 006 | loss 3.587 | wps 4589 | ups 6.39 | wpb 718.6 | bsz 718.6 | num_updates 9217 | lr 0.000327 | gnorm 4.891 | clip 0 | train_wall 209 | gb_free 73.2 | wall 1502 (progress_bar.py:267, print())[0m
[33m[2025-11-21 06:57:47] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 06:58:00]    INFO >> epoch 006 | valid on 'valid' subset | loss 3.765 | wps 11373 | wpb 5412.5 | bsz 5412.5 | num_updates 9217 | best_loss 4.97 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 06:58:01]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 06:58:01]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/checkpoints/checkpoint_last.pt (epoch 6 @ 9217 updates, score 3.765) (writing took 0.013810 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 06:58:01] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 06:58:07]    INFO >> epoch 007:     33 / 1539 loss=3.622, wps=1633.7, ups=2.26, wpb=723.6, bsz=723.6, num_updates=9250, lr=0.000295, gnorm=4.728, clip=0, train_wall=8, gb_free=64.8, wall=1521 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:58:09] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 2.64 GiB is free. Including non-PyTorch memory, this process has 76.47 GiB memory in use. Of the allocated memory 70.76 GiB is allocated by PyTorch, and 5.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:58:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:58:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:58:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 18           |        cudaMalloc retries: 33        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72054 MiB |  72454 MiB | 267074 GiB | 267004 GiB |
|       from large pool |  72039 MiB |  72439 MiB | 265434 GiB | 265363 GiB |
|       from small pool |     15 MiB |     22 MiB |   1640 GiB |   1640 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72054 MiB |  72454 MiB | 267074 GiB | 267004 GiB |
|       from large pool |  72039 MiB |  72439 MiB | 265434 GiB | 265363 GiB |
|       from small pool |     15 MiB |     22 MiB |   1640 GiB |   1640 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72042 MiB |  72442 MiB | 266512 GiB | 266441 GiB |
|       from large pool |  72027 MiB |  72426 MiB | 264873 GiB | 264803 GiB |
|       from small pool |     15 MiB |     22 MiB |   1638 GiB |   1638 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77798 MiB |  78216 MiB | 386894 MiB | 309096 MiB |
|       from large pool |  77776 MiB |  77776 MiB | 382890 MiB | 305114 MiB |
|       from small pool |     22 MiB |    440 MiB |   4004 MiB |   3982 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5743 MiB |   7729 MiB | 259888 GiB | 259883 GiB |
|       from large pool |   5736 MiB |   7720 MiB | 258015 GiB | 258009 GiB |
|       from small pool |      6 MiB |     31 MiB |   1873 GiB |   1873 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     502    |     509    |   18259 K  |   18259 K  |
|       from large pool |     308    |     315    |    8365 K  |    8364 K  |
|       from small pool |     194    |     240    |    9894 K  |    9894 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     502    |     509    |   18259 K  |   18259 K  |
|       from large pool |     308    |     315    |    8365 K  |    8364 K  |
|       from small pool |     194    |     240    |    9894 K  |    9894 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     148    |     357    |    3795    |    3647    |
|       from large pool |     137    |     137    |    1793    |    1656    |
|       from small pool |      11    |     220    |    2002    |    1991    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     139    |     139    |   10348 K  |   10348 K  |
|       from large pool |     119    |     119    |    5524 K  |    5524 K  |
|       from small pool |      20    |      52    |    4823 K  |    4823 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:58:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:58:09] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:58:16]    INFO >> epoch 007:     84 / 1539 loss=3.563, wps=4257.8, ups=6.58, wpb=647.3, bsz=647.3, num_updates=9300, lr=0.000295, gnorm=4.727, clip=0, train_wall=6, gb_free=72.7, wall=1529 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:58:23]    INFO >> epoch 007:    134 / 1539 loss=3.603, wps=5012.9, ups=7.27, wpb=689.3, bsz=689.3, num_updates=9350, lr=0.000295, gnorm=5.027, clip=0, train_wall=6, gb_free=72.5, wall=1536 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:58:30]    INFO >> epoch 007:    184 / 1539 loss=3.531, wps=5677.5, ups=6.66, wpb=852.2, bsz=852.2, num_updates=9400, lr=0.000295, gnorm=4.467, clip=0, train_wall=7, gb_free=75.4, wall=1543 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:58:37]    INFO >> epoch 007:    234 / 1539 loss=3.53, wps=5430.3, ups=6.74, wpb=805.3, bsz=805.3, num_updates=9450, lr=0.000295, gnorm=5.322, clip=0, train_wall=7, gb_free=71.8, wall=1551 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:58:46]    INFO >> epoch 007:    284 / 1539 loss=3.587, wps=4835.3, ups=6.66, wpb=726.5, bsz=726.5, num_updates=9500, lr=0.000295, gnorm=4.514, clip=0, train_wall=7, gb_free=73.9, wall=1558 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:58:53] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 25.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.38 GiB is allocated by PyTorch, and 1.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:58:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:58:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:58:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 19           |        cudaMalloc retries: 35        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79180 MiB |  79240 MiB | 274354 GiB | 274276 GiB |
|       from large pool |  79002 MiB |  79062 MiB | 272669 GiB | 272592 GiB |
|       from small pool |    178 MiB |    179 MiB |   1684 GiB |   1684 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79180 MiB |  79240 MiB | 274354 GiB | 274276 GiB |
|       from large pool |  79002 MiB |  79062 MiB | 272669 GiB | 272592 GiB |
|       from small pool |    178 MiB |    179 MiB |   1684 GiB |   1684 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79136 MiB |  79195 MiB | 273774 GiB | 273697 GiB |
|       from large pool |  78958 MiB |  79018 MiB | 272092 GiB | 272015 GiB |
|       from small pool |    177 MiB |    178 MiB |   1682 GiB |   1682 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80480 MiB |  80480 MiB | 389616 MiB | 309136 MiB |
|       from large pool |  80296 MiB |  80296 MiB | 385410 MiB | 305114 MiB |
|       from small pool |    184 MiB |    222 MiB |   4206 MiB |   4022 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1239 MiB |   9307 MiB | 266716 GiB | 266715 GiB |
|       from large pool |   1233 MiB |   9301 MiB | 264791 GiB | 264790 GiB |
|       from small pool |      5 MiB |     19 MiB |   1924 GiB |   1924 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3469    |    3472    |   18762 K  |   18759 K  |
|       from large pool |     583    |     584    |    8607 K  |    8607 K  |
|       from small pool |    2886    |    2889    |   10154 K  |   10152 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3469    |    3472    |   18762 K  |   18759 K  |
|       from large pool |     583    |     584    |    8607 K  |    8607 K  |
|       from small pool |    2886    |    2889    |   10154 K  |   10152 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     271    |     289    |    3938    |    3667    |
|       from large pool |     179    |     179    |    1835    |    1656    |
|       from small pool |      92    |     111    |    2103    |    2011    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     190    |     191    |   10633 K  |   10633 K  |
|       from large pool |     105    |     109    |    5686 K  |    5686 K  |
|       from small pool |      85    |      86    |    4946 K  |    4946 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:58:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:58:53] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:58:55]    INFO >> epoch 007:    335 / 1539 loss=3.65, wps=3888.8, ups=5.87, wpb=662, bsz=662, num_updates=9550, lr=0.000295, gnorm=4.082, clip=0, train_wall=7, gb_free=75.9, wall=1567 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:59:02]    INFO >> epoch 007:    385 / 1539 loss=3.582, wps=4314.9, ups=7.15, wpb=603.8, bsz=603.8, num_updates=9600, lr=0.000295, gnorm=4.848, clip=0, train_wall=6, gb_free=74, wall=1574 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:59:09]    INFO >> epoch 007:    435 / 1539 loss=3.543, wps=4278.5, ups=6.8, wpb=629.3, bsz=629.3, num_updates=9650, lr=0.000295, gnorm=4.421, clip=0, train_wall=7, gb_free=72.5, wall=1581 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:59:15] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.07 GiB is free. Including non-PyTorch memory, this process has 78.05 GiB memory in use. Of the allocated memory 73.80 GiB is allocated by PyTorch, and 3.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:59:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:59:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:59:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 20           |        cudaMalloc retries: 37        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75050 MiB |  75993 MiB | 278269 GiB | 278196 GiB |
|       from large pool |  75039 MiB |  75982 MiB | 276565 GiB | 276492 GiB |
|       from small pool |     10 MiB |     15 MiB |   1704 GiB |   1704 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75050 MiB |  75993 MiB | 278269 GiB | 278196 GiB |
|       from large pool |  75039 MiB |  75982 MiB | 276565 GiB | 276492 GiB |
|       from small pool |     10 MiB |     15 MiB |   1704 GiB |   1704 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75032 MiB |  75975 MiB | 277681 GiB | 277608 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 275979 GiB | 275906 GiB |
|       from small pool |     10 MiB |     15 MiB |   1701 GiB |   1701 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79414 MiB |  80420 MiB | 392026 MiB | 312612 MiB |
|       from large pool |  79394 MiB |  80236 MiB | 387820 MiB | 308426 MiB |
|       from small pool |     20 MiB |    184 MiB |   4206 MiB |   4186 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4363 MiB |   8679 MiB | 270101 GiB | 270097 GiB |
|       from large pool |   4354 MiB |   8670 MiB | 268153 GiB | 268149 GiB |
|       from small pool |      9 MiB |     21 MiB |   1947 GiB |   1947 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     501    |     509    |   19005 K  |   19005 K  |
|       from large pool |     314    |     322    |    8733 K  |    8733 K  |
|       from small pool |     187    |     240    |   10272 K  |   10272 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     501    |     509    |   19005 K  |   19005 K  |
|       from large pool |     314    |     322    |    8733 K  |    8733 K  |
|       from small pool |     187    |     240    |   10272 K  |   10272 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     152    |     270    |    3943    |    3791    |
|       from large pool |     142    |     178    |    1840    |    1698    |
|       from small pool |      10    |      92    |    2103    |    2093    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     142    |     142    |   10770 K  |   10770 K  |
|       from large pool |     123    |     123    |    5773 K  |    5773 K  |
|       from small pool |      19    |      42    |    4996 K  |    4996 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:59:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:59:15] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:59:17]    INFO >> epoch 007:    486 / 1539 loss=3.614, wps=4766.9, ups=6.09, wpb=782.3, bsz=782.3, num_updates=9700, lr=0.000295, gnorm=4.824, clip=0, train_wall=7, gb_free=72.3, wall=1589 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:59:26]    INFO >> epoch 007:    536 / 1539 loss=3.458, wps=4830, ups=7.17, wpb=673.4, bsz=673.4, num_updates=9750, lr=0.000295, gnorm=4.526, clip=0, train_wall=6, gb_free=75, wall=1596 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:59:33]    INFO >> epoch 007:    586 / 1539 loss=3.718, wps=4733.9, ups=7.12, wpb=665.3, bsz=665.3, num_updates=9800, lr=0.000295, gnorm=4.732, clip=0, train_wall=7, gb_free=73.7, wall=1603 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:59:40]    INFO >> epoch 007:    636 / 1539 loss=3.513, wps=4532.2, ups=6.76, wpb=670.8, bsz=670.8, num_updates=9850, lr=0.000295, gnorm=5.532, clip=2, train_wall=7, gb_free=75.1, wall=1611 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:59:48]    INFO >> epoch 007:    686 / 1539 loss=3.401, wps=5544.4, ups=6.56, wpb=845.3, bsz=845.3, num_updates=9900, lr=0.000295, gnorm=5.024, clip=0, train_wall=7, gb_free=72.3, wall=1618 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:59:56]    INFO >> epoch 007:    736 / 1539 loss=3.618, wps=5020.1, ups=6.91, wpb=726.6, bsz=726.6, num_updates=9950, lr=0.000295, gnorm=4.188, clip=0, train_wall=7, gb_free=75.1, wall=1625 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:00:04]    INFO >> epoch 007:    786 / 1539 loss=3.637, wps=4482.4, ups=6.93, wpb=646.6, bsz=646.6, num_updates=10000, lr=0.000295, gnorm=4.139, clip=0, train_wall=7, gb_free=72.6, wall=1633 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:00:11]    INFO >> epoch 007:    836 / 1539 loss=3.568, wps=4995.7, ups=6.92, wpb=722, bsz=722, num_updates=10050, lr=0.000295, gnorm=4.232, clip=0, train_wall=7, gb_free=73.1, wall=1640 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:00:18]    INFO >> epoch 007:    886 / 1539 loss=3.623, wps=4684.8, ups=6.84, wpb=684.6, bsz=684.6, num_updates=10100, lr=0.000295, gnorm=4.551, clip=0, train_wall=7, gb_free=64.9, wall=1647 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:00:25]    INFO >> epoch 007:    936 / 1539 loss=3.469, wps=4842.6, ups=7.39, wpb=655.4, bsz=655.4, num_updates=10150, lr=0.000295, gnorm=4.948, clip=0, train_wall=6, gb_free=74.6, wall=1654 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:00:34]    INFO >> epoch 007:    986 / 1539 loss=3.514, wps=5179.7, ups=6.72, wpb=770.6, bsz=770.6, num_updates=10200, lr=0.000295, gnorm=4.915, clip=0, train_wall=7, gb_free=73.7, wall=1661 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:00:41]    INFO >> epoch 007:   1036 / 1539 loss=3.451, wps=4666.8, ups=6.67, wpb=699.9, bsz=699.9, num_updates=10250, lr=0.000295, gnorm=5.163, clip=0, train_wall=7, gb_free=72.6, wall=1669 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:00:49]    INFO >> epoch 007:   1086 / 1539 loss=3.585, wps=4667.5, ups=6.57, wpb=710.2, bsz=710.2, num_updates=10300, lr=0.000295, gnorm=4.573, clip=0, train_wall=7, gb_free=75.3, wall=1676 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:00:57]    INFO >> epoch 007:   1136 / 1539 loss=3.484, wps=5025.8, ups=6.37, wpb=788.4, bsz=788.4, num_updates=10350, lr=0.000295, gnorm=4.103, clip=0, train_wall=7, gb_free=72.1, wall=1684 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:01:06]    INFO >> epoch 007:   1186 / 1539 loss=3.516, wps=5220.4, ups=6.43, wpb=812, bsz=812, num_updates=10400, lr=0.000295, gnorm=4.696, clip=0, train_wall=7, gb_free=74.8, wall=1692 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:01:13]    INFO >> epoch 007:   1236 / 1539 loss=3.472, wps=4772.9, ups=7.09, wpb=673.4, bsz=673.4, num_updates=10450, lr=0.000295, gnorm=4.663, clip=0, train_wall=7, gb_free=75.3, wall=1699 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:01:20]    INFO >> epoch 007:   1286 / 1539 loss=3.512, wps=4561.6, ups=6.88, wpb=663.1, bsz=663.1, num_updates=10500, lr=0.000295, gnorm=4.452, clip=0, train_wall=7, gb_free=75.7, wall=1706 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:01:27]    INFO >> epoch 007:   1336 / 1539 loss=3.611, wps=4759.3, ups=7.02, wpb=678.4, bsz=678.4, num_updates=10550, lr=0.000295, gnorm=4.345, clip=0, train_wall=7, gb_free=74.3, wall=1714 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:01:36]    INFO >> epoch 007:   1386 / 1539 loss=3.473, wps=4478.6, ups=6.8, wpb=658.5, bsz=658.5, num_updates=10600, lr=0.000295, gnorm=4.728, clip=0, train_wall=7, gb_free=75.2, wall=1721 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:01:45]    INFO >> epoch 007:   1436 / 1539 loss=3.428, wps=5437.8, ups=5.78, wpb=941.4, bsz=941.4, num_updates=10650, lr=0.000295, gnorm=4.964, clip=0, train_wall=8, gb_free=71.9, wall=1730 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:01:52]    INFO >> epoch 007:   1486 / 1539 loss=3.518, wps=5140.3, ups=7.06, wpb=727.7, bsz=727.7, num_updates=10700, lr=0.000295, gnorm=4.83, clip=0, train_wall=7, gb_free=72.7, wall=1737 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:01:59]    INFO >> epoch 007:   1536 / 1539 loss=3.263, wps=4799.5, ups=6.54, wpb=734.2, bsz=734.2, num_updates=10750, lr=0.000295, gnorm=4.655, clip=0, train_wall=7, gb_free=72.6, wall=1744 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:02:00]    INFO >> epoch 007 | loss 3.533 | wps 4540.1 | ups 6.32 | wpb 718.6 | bsz 718.6 | num_updates 10753 | lr 0.000295 | gnorm 4.683 | clip 0.1 | train_wall 211 | gb_free 71.4 | wall 1745 (progress_bar.py:267, print())[0m
[33m[2025-11-21 07:02:00] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 07:02:15]    INFO >> epoch 007 | valid on 'valid' subset | loss 3.696 | wps 11086.3 | wpb 5412.5 | bsz 5412.5 | num_updates 10753 | best_loss 4.97 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 07:02:15]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 07:02:15]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/checkpoints/checkpoint_last.pt (epoch 7 @ 10753 updates, score 3.696) (writing took 0.013364 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 07:02:15] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 07:02:22]    INFO >> epoch 008:     47 / 1539 loss=3.401, wps=1687.6, ups=2.31, wpb=730.5, bsz=730.5, num_updates=10800, lr=0.000262, gnorm=4.932, clip=0, train_wall=7, gb_free=73.5, wall=1766 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:02:30]    INFO >> epoch 008:     97 / 1539 loss=3.51, wps=4983.1, ups=6.55, wpb=760.7, bsz=760.7, num_updates=10850, lr=0.000262, gnorm=4.734, clip=0, train_wall=7, gb_free=74.7, wall=1774 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:02:37]    INFO >> epoch 008:    147 / 1539 loss=3.597, wps=4391.3, ups=7.23, wpb=607.6, bsz=607.6, num_updates=10900, lr=0.000262, gnorm=3.901, clip=0, train_wall=6, gb_free=75.5, wall=1780 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:02:46]    INFO >> epoch 008:    197 / 1539 loss=3.437, wps=5141.9, ups=6.11, wpb=841.4, bsz=841.4, num_updates=10950, lr=0.000262, gnorm=5.395, clip=0, train_wall=8, gb_free=59.3, wall=1789 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:02:54]    INFO >> epoch 008:    247 / 1539 loss=3.556, wps=5241.7, ups=6.95, wpb=753.8, bsz=753.8, num_updates=11000, lr=0.000262, gnorm=4.403, clip=0, train_wall=7, gb_free=72.2, wall=1796 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:03:01]    INFO >> epoch 008:    297 / 1539 loss=3.617, wps=4781.8, ups=6.74, wpb=709.8, bsz=709.8, num_updates=11050, lr=0.000262, gnorm=4.612, clip=0, train_wall=7, gb_free=70.8, wall=1803 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:03:08]    INFO >> epoch 008:    347 / 1539 loss=3.518, wps=4419.9, ups=6.77, wpb=653.3, bsz=653.3, num_updates=11100, lr=0.000262, gnorm=4.786, clip=0, train_wall=7, gb_free=74.2, wall=1811 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:03:18]    INFO >> epoch 008:    397 / 1539 loss=3.55, wps=4736.7, ups=6.37, wpb=743.5, bsz=743.5, num_updates=11150, lr=0.000262, gnorm=4.486, clip=0, train_wall=7, gb_free=72.6, wall=1819 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:03:25]    INFO >> epoch 008:    447 / 1539 loss=3.48, wps=5639.1, ups=6.57, wpb=857.8, bsz=857.8, num_updates=11200, lr=0.000262, gnorm=4.786, clip=0, train_wall=7, gb_free=72.4, wall=1826 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:03:32]    INFO >> epoch 008:    497 / 1539 loss=3.558, wps=5230.5, ups=7.1, wpb=736.7, bsz=736.7, num_updates=11250, lr=0.000262, gnorm=4.548, clip=0, train_wall=7, gb_free=74, wall=1833 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:03:40]    INFO >> epoch 008:    547 / 1539 loss=3.428, wps=4426.1, ups=6.73, wpb=657.9, bsz=657.9, num_updates=11300, lr=0.000262, gnorm=4.609, clip=0, train_wall=7, gb_free=75.1, wall=1841 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:03:47]    INFO >> epoch 008:    597 / 1539 loss=3.637, wps=4586.5, ups=6.67, wpb=688.1, bsz=688.1, num_updates=11350, lr=0.000262, gnorm=4.603, clip=0, train_wall=7, gb_free=73.5, wall=1848 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:03:55]    INFO >> epoch 008:    647 / 1539 loss=3.596, wps=4713.2, ups=7.35, wpb=641.2, bsz=641.2, num_updates=11400, lr=0.000262, gnorm=4.062, clip=0, train_wall=6, gb_free=71.5, wall=1855 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:04:02]    INFO >> epoch 008:    697 / 1539 loss=3.398, wps=4894.6, ups=7.09, wpb=690, bsz=690, num_updates=11450, lr=0.000262, gnorm=4.887, clip=0, train_wall=7, gb_free=73.7, wall=1862 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:04:09]    INFO >> epoch 008:    747 / 1539 loss=3.651, wps=4709.7, ups=7.16, wpb=657.8, bsz=657.8, num_updates=11500, lr=0.000262, gnorm=4.216, clip=0, train_wall=6, gb_free=74.4, wall=1869 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:04:17]    INFO >> epoch 008:    797 / 1539 loss=3.39, wps=5130.1, ups=6.57, wpb=780.3, bsz=780.3, num_updates=11550, lr=0.000262, gnorm=4.837, clip=0, train_wall=7, gb_free=60.1, wall=1877 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:04:25]    INFO >> epoch 008:    847 / 1539 loss=3.567, wps=4421.6, ups=7.39, wpb=598.3, bsz=598.3, num_updates=11600, lr=0.000262, gnorm=4.202, clip=0, train_wall=6, gb_free=70.3, wall=1883 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:04:33]    INFO >> epoch 008:    897 / 1539 loss=3.431, wps=5272.2, ups=6.46, wpb=815.6, bsz=815.6, num_updates=11650, lr=0.000262, gnorm=4.284, clip=0, train_wall=7, gb_free=70.9, wall=1891 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:04:40]    INFO >> epoch 008:    947 / 1539 loss=3.462, wps=4676.5, ups=7.16, wpb=653.5, bsz=653.5, num_updates=11700, lr=0.000262, gnorm=5.081, clip=0, train_wall=6, gb_free=72.5, wall=1898 (progress_bar.py:258, log())[0m
[33m[2025-11-21 07:04:46] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 31.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.14 GiB is allocated by PyTorch, and 1.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 07:04:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:04:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:04:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 21           |        cudaMalloc retries: 39        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78928 MiB |  78988 MiB | 335153 GiB | 335076 GiB |
|       from large pool |  78753 MiB |  78813 MiB | 333097 GiB | 333020 GiB |
|       from small pool |    175 MiB |    177 MiB |   2055 GiB |   2055 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78928 MiB |  78988 MiB | 335153 GiB | 335076 GiB |
|       from large pool |  78753 MiB |  78813 MiB | 333097 GiB | 333020 GiB |
|       from small pool |    175 MiB |    177 MiB |   2055 GiB |   2055 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78895 MiB |  78955 MiB | 334438 GiB | 334361 GiB |
|       from large pool |  78720 MiB |  78780 MiB | 332385 GiB | 332308 GiB |
|       from small pool |    175 MiB |    176 MiB |   2052 GiB |   2052 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80474 MiB |  80494 MiB | 393350 MiB | 312876 MiB |
|       from large pool |  80294 MiB |  80294 MiB | 388720 MiB | 308426 MiB |
|       from small pool |    180 MiB |    440 MiB |   4630 MiB |   4450 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1485 MiB |   9187 MiB | 322867 GiB | 322865 GiB |
|       from large pool |   1480 MiB |   9181 MiB | 320518 GiB | 320516 GiB |
|       from small pool |      4 MiB |     25 MiB |   2348 GiB |   2348 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3425    |    3428    |   22966 K  |   22962 K  |
|       from large pool |     579    |     580    |   10575 K  |   10575 K  |
|       from small pool |    2846    |    2849    |   12390 K  |   12387 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3425    |    3428    |   22966 K  |   22962 K  |
|       from large pool |     579    |     580    |   10575 K  |   10575 K  |
|       from small pool |    2846    |    2849    |   12390 K  |   12387 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     247    |     373    |    4170    |    3923    |
|       from large pool |     157    |     157    |    1855    |    1698    |
|       from small pool |      90    |     220    |    2315    |    2225    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     188    |     189    |   13035 K  |   13034 K  |
|       from large pool |     106    |     108    |    7014 K  |    7014 K  |
|       from small pool |      82    |      83    |    6020 K  |    6020 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:04:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:04:46] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 07:04:47]    INFO >> epoch 008:    998 / 1539 loss=3.579, wps=3854.3, ups=6.55, wpb=588.2, bsz=588.2, num_updates=11750, lr=0.000262, gnorm=4.03, clip=0, train_wall=6, gb_free=74.8, wall=1906 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:04:57]    INFO >> epoch 008:   1048 / 1539 loss=3.379, wps=4932.8, ups=5.87, wpb=840.2, bsz=840.2, num_updates=11800, lr=0.000262, gnorm=4.427, clip=0, train_wall=8, gb_free=74.8, wall=1914 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:05:04]    INFO >> epoch 008:   1098 / 1539 loss=3.577, wps=4467.6, ups=6.93, wpb=645.1, bsz=645.1, num_updates=11850, lr=0.000262, gnorm=4.362, clip=0, train_wall=7, gb_free=70.9, wall=1921 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:05:11]    INFO >> epoch 008:   1148 / 1539 loss=3.614, wps=4360.5, ups=7.22, wpb=603.6, bsz=603.6, num_updates=11900, lr=0.000262, gnorm=4.192, clip=0, train_wall=6, gb_free=73.9, wall=1928 (progress_bar.py:258, log())[0m
[33m[2025-11-21 07:05:15] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 563.25 MiB is free. Including non-PyTorch memory, this process has 78.57 GiB memory in use. Of the allocated memory 73.80 GiB is allocated by PyTorch, and 4.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 07:05:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:05:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:05:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 22           |        cudaMalloc retries: 41        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75049 MiB |  75992 MiB | 339973 GiB | 339900 GiB |
|       from large pool |  75039 MiB |  75981 MiB | 337892 GiB | 337819 GiB |
|       from small pool |     10 MiB |     16 MiB |   2081 GiB |   2081 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75049 MiB |  75992 MiB | 339973 GiB | 339900 GiB |
|       from large pool |  75039 MiB |  75981 MiB | 337892 GiB | 337819 GiB |
|       from small pool |     10 MiB |     16 MiB |   2081 GiB |   2081 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75032 MiB |  75975 MiB | 339248 GiB | 339175 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 337169 GiB | 337096 GiB |
|       from small pool |     10 MiB |     16 MiB |   2078 GiB |   2078 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79942 MiB |  80414 MiB | 394314 MiB | 314372 MiB |
|       from large pool |  79922 MiB |  80234 MiB | 389684 MiB | 309762 MiB |
|       from small pool |     20 MiB |    180 MiB |   4630 MiB |   4610 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4892 MiB |  10361 MiB | 327190 GiB | 327185 GiB |
|       from large pool |   4882 MiB |  10352 MiB | 324811 GiB | 324807 GiB |
|       from small pool |      9 MiB |     25 MiB |   2378 GiB |   2378 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     501    |     509    |   23277 K  |   23276 K  |
|       from large pool |     314    |     322    |   10734 K  |   10734 K  |
|       from small pool |     187    |     232    |   12542 K  |   12542 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     501    |     509    |   23277 K  |   23276 K  |
|       from large pool |     314    |     322    |   10734 K  |   10734 K  |
|       from small pool |     187    |     232    |   12542 K  |   12542 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     152    |     246    |    4172    |    4020    |
|       from large pool |     142    |     156    |    1857    |    1715    |
|       from small pool |      10    |      90    |    2315    |    2305    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     145    |     145    |   13210 K  |   13210 K  |
|       from large pool |     127    |     127    |    7122 K  |    7122 K  |
|       from small pool |      18    |      48    |    6087 K  |    6087 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:05:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:05:15] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 07:05:19]    INFO >> epoch 008:   1199 / 1539 loss=3.578, wps=4454.4, ups=6.28, wpb=708.8, bsz=708.8, num_updates=11950, lr=0.000262, gnorm=3.937, clip=0, train_wall=7, gb_free=63, wall=1936 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:05:29]    INFO >> epoch 008:   1249 / 1539 loss=3.345, wps=5611.9, ups=5.71, wpb=983.4, bsz=983.4, num_updates=12000, lr=0.000262, gnorm=5.22, clip=0, train_wall=8, gb_free=70.5, wall=1945 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:05:37]    INFO >> epoch 008:   1299 / 1539 loss=3.585, wps=5007.9, ups=6.98, wpb=717.5, bsz=717.5, num_updates=12050, lr=0.000262, gnorm=4.114, clip=0, train_wall=7, gb_free=75.3, wall=1952 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:05:44]    INFO >> epoch 008:   1349 / 1539 loss=3.522, wps=4738.9, ups=6.79, wpb=697.5, bsz=697.5, num_updates=12100, lr=0.000262, gnorm=4.576, clip=0, train_wall=7, gb_free=73.7, wall=1960 (progress_bar.py:258, log())[0m
[33m[2025-11-21 07:05:46] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 682.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 565.25 MiB is free. Including non-PyTorch memory, this process has 78.56 GiB memory in use. Of the allocated memory 73.42 GiB is allocated by PyTorch, and 4.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 07:05:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:05:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:05:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 23           |        cudaMalloc retries: 42        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72055 MiB |  75182 MiB | 345383 GiB | 345313 GiB |
|       from large pool |  72039 MiB |  75166 MiB | 343268 GiB | 343198 GiB |
|       from small pool |     15 MiB |     16 MiB |   2114 GiB |   2114 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72055 MiB |  75182 MiB | 345383 GiB | 345313 GiB |
|       from large pool |  72039 MiB |  75166 MiB | 343268 GiB | 343198 GiB |
|       from small pool |     15 MiB |     16 MiB |   2114 GiB |   2114 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72042 MiB |  75169 MiB | 344646 GiB | 344576 GiB |
|       from large pool |  72027 MiB |  75153 MiB | 342534 GiB | 342464 GiB |
|       from small pool |     15 MiB |     16 MiB |   2111 GiB |   2111 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79940 MiB |  80362 MiB | 394734 MiB | 314794 MiB |
|       from large pool |  79922 MiB |  79922 MiB | 389684 MiB | 309762 MiB |
|       from small pool |     18 MiB |    440 MiB |   5050 MiB |   5032 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5156 MiB |   9383 MiB | 332449 GiB | 332444 GiB |
|       from large pool |   5154 MiB |   9380 MiB | 330031 GiB | 330026 GiB |
|       from small pool |      2 MiB |     27 MiB |   2417 GiB |   2417 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     502    |     510    |   23643 K  |   23642 K  |
|       from large pool |     308    |     316    |   10903 K  |   10903 K  |
|       from small pool |     194    |     232    |   12739 K  |   12739 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     502    |     510    |   23643 K  |   23642 K  |
|       from large pool |     308    |     316    |   10903 K  |   10903 K  |
|       from small pool |     194    |     232    |   12739 K  |   12739 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     151    |     362    |    4382    |    4231    |
|       from large pool |     142    |     142    |    1857    |    1715    |
|       from small pool |       9    |     220    |    2525    |    2516    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     149    |     149    |   13423 K  |   13423 K  |
|       from large pool |     130    |     130    |    7236 K  |    7236 K  |
|       from small pool |      19    |      43    |    6187 K  |    6187 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:05:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:05:46] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 07:05:52]    INFO >> epoch 008:   1400 / 1539 loss=3.496, wps=4855.6, ups=6.27, wpb=774.5, bsz=774.5, num_updates=12150, lr=0.000262, gnorm=4.134, clip=0, train_wall=7, gb_free=72.8, wall=1968 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:06:00]    INFO >> epoch 008:   1450 / 1539 loss=3.637, wps=4728.8, ups=6.31, wpb=749.5, bsz=749.5, num_updates=12200, lr=0.000262, gnorm=4.838, clip=0, train_wall=7, gb_free=34.1, wall=1975 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:06:09]    INFO >> epoch 008:   1500 / 1539 loss=3.512, wps=4502.9, ups=6.79, wpb=663.2, bsz=663.2, num_updates=12250, lr=0.000262, gnorm=4.666, clip=0, train_wall=7, gb_free=74.1, wall=1983 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:06:14]    INFO >> epoch 008 | loss 3.512 | wps 4531 | ups 6.31 | wpb 718.6 | bsz 718.6 | num_updates 12289 | lr 0.000262 | gnorm 4.524 | clip 0 | train_wall 211 | gb_free 75.4 | wall 1988 (progress_bar.py:267, print())[0m
[33m[2025-11-21 07:06:14] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 07:06:27]    INFO >> epoch 008 | valid on 'valid' subset | loss 3.792 | wps 12079.3 | wpb 5412.5 | bsz 5412.5 | num_updates 12289 | best_loss 4.97 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 07:06:27]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 07:06:27]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/checkpoints/checkpoint_last.pt (epoch 8 @ 12289 updates, score 3.792) (writing took 0.014593 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 07:06:27] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 07:06:28]    INFO >> epoch 009:     11 / 1539 loss=3.495, wps=1685.7, ups=2.51, wpb=671.4, bsz=671.4, num_updates=12300, lr=0.000227, gnorm=4.152, clip=0, train_wall=6, gb_free=74.3, wall=2003 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:06:37]    INFO >> epoch 009:     61 / 1539 loss=3.591, wps=4378.9, ups=6.92, wpb=632.8, bsz=632.8, num_updates=12350, lr=0.000227, gnorm=3.957, clip=0, train_wall=7, gb_free=74.6, wall=2010 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:06:45]    INFO >> epoch 009:    111 / 1539 loss=3.425, wps=4895.6, ups=6.7, wpb=730.3, bsz=730.3, num_updates=12400, lr=0.000227, gnorm=4.792, clip=0, train_wall=7, gb_free=70.8, wall=2017 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:06:52]    INFO >> epoch 009:    161 / 1539 loss=3.377, wps=5494, ups=6.53, wpb=840.8, bsz=840.8, num_updates=12450, lr=0.000227, gnorm=5.019, clip=0, train_wall=7, gb_free=74, wall=2025 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:06:59]    INFO >> epoch 009:    211 / 1539 loss=3.561, wps=4627.3, ups=7.21, wpb=642.1, bsz=642.1, num_updates=12500, lr=0.000227, gnorm=4.053, clip=0, train_wall=6, gb_free=72.9, wall=2032 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:07:06]    INFO >> epoch 009:    261 / 1539 loss=3.539, wps=5366.9, ups=7.25, wpb=739.9, bsz=739.9, num_updates=12550, lr=0.000227, gnorm=4.428, clip=0, train_wall=6, gb_free=74.3, wall=2039 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:07:14]    INFO >> epoch 009:    311 / 1539 loss=3.528, wps=4823.6, ups=7.21, wpb=668.9, bsz=668.9, num_updates=12600, lr=0.000227, gnorm=4.616, clip=0, train_wall=6, gb_free=75, wall=2046 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:07:23]    INFO >> epoch 009:    361 / 1539 loss=3.464, wps=4876.1, ups=5.79, wpb=842.7, bsz=842.7, num_updates=12650, lr=0.000227, gnorm=4.991, clip=0, train_wall=8, gb_free=71.8, wall=2055 (progress_bar.py:258, log())[0m
[33m[2025-11-21 07:07:25] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 883.25 MiB is free. Including non-PyTorch memory, this process has 78.25 GiB memory in use. Of the allocated memory 73.80 GiB is allocated by PyTorch, and 3.95 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 07:07:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:07:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:07:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 24           |        cudaMalloc retries: 43        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75052 MiB |  75995 MiB | 363822 GiB | 363749 GiB |
|       from large pool |  75041 MiB |  75984 MiB | 361590 GiB | 361516 GiB |
|       from small pool |     10 MiB |     14 MiB |   2232 GiB |   2232 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75052 MiB |  75995 MiB | 363822 GiB | 363749 GiB |
|       from large pool |  75041 MiB |  75984 MiB | 361590 GiB | 361516 GiB |
|       from small pool |     10 MiB |     14 MiB |   2232 GiB |   2232 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75032 MiB |  75975 MiB | 363049 GiB | 362976 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 360819 GiB | 360746 GiB |
|       from small pool |     10 MiB |     14 MiB |   2229 GiB |   2229 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79622 MiB |  79724 MiB | 397246 MiB | 317624 MiB |
|       from large pool |  79604 MiB |  79604 MiB | 392094 MiB | 312490 MiB |
|       from small pool |     18 MiB |    120 MiB |   5152 MiB |   5134 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4569 MiB |   8470 MiB | 348188 GiB | 348184 GiB |
|       from large pool |   4562 MiB |   8463 MiB | 345640 GiB | 345635 GiB |
|       from small pool |      7 MiB |     17 MiB |   2548 GiB |   2548 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     501    |     509    |   24879 K  |   24879 K  |
|       from large pool |     314    |     322    |   11421 K  |   11421 K  |
|       from small pool |     187    |     226    |   13458 K  |   13457 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     501    |     509    |   24879 K  |   24879 K  |
|       from large pool |     314    |     322    |   11421 K  |   11421 K  |
|       from small pool |     187    |     226    |   13458 K  |   13457 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     155    |     206    |    4438    |    4283    |
|       from large pool |     146    |     146    |    1862    |    1716    |
|       from small pool |       9    |      60    |    2576    |    2567    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     145    |     145    |   14145 K  |   14144 K  |
|       from large pool |     126    |     126    |    7588 K  |    7588 K  |
|       from small pool |      19    |      39    |    6556 K  |    6556 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:07:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:07:25] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 07:07:31]    INFO >> epoch 009:    412 / 1539 loss=3.538, wps=4376.9, ups=6.35, wpb=689.6, bsz=689.6, num_updates=12700, lr=0.000227, gnorm=4.2, clip=0, train_wall=7, gb_free=75.1, wall=2062 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:07:38]    INFO >> epoch 009:    462 / 1539 loss=3.43, wps=5048.9, ups=7.1, wpb=711.3, bsz=711.3, num_updates=12750, lr=0.000227, gnorm=4.323, clip=0, train_wall=7, gb_free=71.4, wall=2069 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:07:46]    INFO >> epoch 009:    512 / 1539 loss=3.467, wps=5176.2, ups=6.95, wpb=745.2, bsz=745.2, num_updates=12800, lr=0.000227, gnorm=4.532, clip=0, train_wall=7, gb_free=75.9, wall=2077 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:07:54]    INFO >> epoch 009:    562 / 1539 loss=3.488, wps=4915.5, ups=6.33, wpb=776.5, bsz=776.5, num_updates=12850, lr=0.000227, gnorm=4.926, clip=0, train_wall=7, gb_free=74.3, wall=2085 (progress_bar.py:258, log())[0m
[33m[2025-11-21 07:07:57] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 1.25 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 77.14 GiB is allocated by PyTorch, and 1.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 07:07:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:07:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:07:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 25           |        cudaMalloc retries: 45        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78866 MiB |  78986 MiB | 369215 GiB | 369138 GiB |
|       from large pool |  78691 MiB |  78811 MiB | 366952 GiB | 366875 GiB |
|       from small pool |    175 MiB |    176 MiB |   2263 GiB |   2263 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78866 MiB |  78986 MiB | 369215 GiB | 369138 GiB |
|       from large pool |  78691 MiB |  78811 MiB | 366952 GiB | 366875 GiB |
|       from small pool |    175 MiB |    176 MiB |   2263 GiB |   2263 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78835 MiB |  78954 MiB | 368430 GiB | 368353 GiB |
|       from large pool |  78661 MiB |  78780 MiB | 366170 GiB | 366093 GiB |
|       from small pool |    174 MiB |    175 MiB |   2260 GiB |   2260 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80504 MiB |  80504 MiB | 398130 MiB | 317626 MiB |
|       from large pool |  80324 MiB |  80324 MiB | 392814 MiB | 312490 MiB |
|       from small pool |    180 MiB |    182 MiB |   5316 MiB |   5136 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1517 MiB |   9395 MiB | 353355 GiB | 353354 GiB |
|       from large pool |   1512 MiB |   9391 MiB | 350770 GiB | 350769 GiB |
|       from small pool |      4 MiB |     23 MiB |   2584 GiB |   2584 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3414    |    3417    |   25239 K  |   25236 K  |
|       from large pool |     578    |     580    |   11598 K  |   11598 K  |
|       from small pool |    2836    |    2839    |   13641 K  |   13638 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3414    |    3417    |   25239 K  |   25236 K  |
|       from large pool |     578    |     580    |   11598 K  |   11598 K  |
|       from small pool |    2836    |    2839    |   13641 K  |   13638 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     248    |     248    |    4532    |    4284    |
|       from large pool |     158    |     158    |    1874    |    1716    |
|       from small pool |      90    |      91    |    2658    |    2568    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     193    |     194    |   14347 K  |   14347 K  |
|       from large pool |     109    |     111    |    7708 K  |    7708 K  |
|       from small pool |      84    |      85    |    6639 K  |    6639 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:07:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:07:57] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 07:08:02]    INFO >> epoch 009:    613 / 1539 loss=3.502, wps=4052.7, ups=6.31, wpb=642.6, bsz=642.6, num_updates=12900, lr=0.000227, gnorm=4.53, clip=0, train_wall=7, gb_free=72.2, wall=2092 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:08:10]    INFO >> epoch 009:    663 / 1539 loss=3.544, wps=4555.4, ups=6.8, wpb=669.5, bsz=669.5, num_updates=12950, lr=0.000227, gnorm=4.407, clip=0, train_wall=7, gb_free=61.1, wall=2100 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:08:18]    INFO >> epoch 009:    713 / 1539 loss=3.588, wps=4944.5, ups=7.19, wpb=687.7, bsz=687.7, num_updates=13000, lr=0.000227, gnorm=4.404, clip=0, train_wall=6, gb_free=76.7, wall=2107 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:08:26]    INFO >> epoch 009:    763 / 1539 loss=3.513, wps=4497.9, ups=6.55, wpb=687, bsz=687, num_updates=13050, lr=0.000227, gnorm=4.083, clip=0, train_wall=7, gb_free=72.7, wall=2114 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:08:33]    INFO >> epoch 009:    813 / 1539 loss=3.511, wps=5272.7, ups=6.98, wpb=755.6, bsz=755.6, num_updates=13100, lr=0.000227, gnorm=5.037, clip=0, train_wall=7, gb_free=74.5, wall=2122 (progress_bar.py:258, log())[0m
[33m[2025-11-21 07:08:34] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 283.25 MiB is free. Including non-PyTorch memory, this process has 78.84 GiB memory in use. Of the allocated memory 70.76 GiB is allocated by PyTorch, and 7.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 07:08:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:08:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:08:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 26           |        cudaMalloc retries: 46        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72057 MiB |  72457 MiB | 375528 GiB | 375457 GiB |
|       from large pool |  72042 MiB |  72441 MiB | 373230 GiB | 373159 GiB |
|       from small pool |     15 MiB |     16 MiB |   2297 GiB |   2297 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72057 MiB |  72457 MiB | 375528 GiB | 375457 GiB |
|       from large pool |  72042 MiB |  72441 MiB | 373230 GiB | 373159 GiB |
|       from small pool |     15 MiB |     16 MiB |   2297 GiB |   2297 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72042 MiB |  72442 MiB | 374728 GiB | 374658 GiB |
|       from large pool |  72027 MiB |  72426 MiB | 372434 GiB | 372363 GiB |
|       from small pool |     15 MiB |     16 MiB |   2294 GiB |   2294 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80222 MiB |  80384 MiB | 398130 MiB | 317908 MiB |
|       from large pool |  80204 MiB |  80204 MiB | 392814 MiB | 312610 MiB |
|       from small pool |     18 MiB |    180 MiB |   5316 MiB |   5298 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   8164 MiB |   9199 MiB | 359178 GiB | 359170 GiB |
|       from large pool |   8161 MiB |   9196 MiB | 356554 GiB | 356546 GiB |
|       from small pool |      2 MiB |     21 MiB |   2624 GiB |   2624 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     502    |     509    |   25655 K  |   25654 K  |
|       from large pool |     308    |     315    |   11810 K  |   11810 K  |
|       from small pool |     194    |     232    |   13844 K  |   13844 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     502    |     509    |   25655 K  |   25654 K  |
|       from large pool |     308    |     315    |   11810 K  |   11810 K  |
|       from small pool |     194    |     232    |   13844 K  |   13844 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     165    |     246    |    4532    |    4367    |
|       from large pool |     156    |     156    |    1874    |    1718    |
|       from small pool |       9    |      90    |    2658    |    2649    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     159    |     159    |   14580 K  |   14580 K  |
|       from large pool |     141    |     141    |    7851 K  |    7851 K  |
|       from small pool |      18    |      41    |    6728 K  |    6728 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:08:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:08:34] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 07:08:40]    INFO >> epoch 009:    864 / 1539 loss=3.562, wps=3989.4, ups=6.71, wpb=594.8, bsz=594.8, num_updates=13150, lr=0.000227, gnorm=4.119, clip=0, train_wall=6, gb_free=73.4, wall=2129 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:08:47]    INFO >> epoch 009:    914 / 1539 loss=3.571, wps=4778.6, ups=7.22, wpb=662, bsz=662, num_updates=13200, lr=0.000227, gnorm=4.234, clip=0, train_wall=6, gb_free=71.1, wall=2136 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:08:57]    INFO >> epoch 009:    964 / 1539 loss=3.443, wps=4661, ups=5.57, wpb=837.1, bsz=837.1, num_updates=13250, lr=0.000227, gnorm=4.849, clip=0, train_wall=8, gb_free=74.8, wall=2145 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:09:05]    INFO >> epoch 009:   1014 / 1539 loss=3.501, wps=4738.8, ups=6.88, wpb=688.6, bsz=688.6, num_updates=13300, lr=0.000227, gnorm=4.31, clip=0, train_wall=7, gb_free=73.1, wall=2152 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:09:12]    INFO >> epoch 009:   1064 / 1539 loss=3.39, wps=5219.2, ups=6.72, wpb=777.1, bsz=777.1, num_updates=13350, lr=0.000227, gnorm=4.667, clip=0, train_wall=7, gb_free=72, wall=2160 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:09:19]    INFO >> epoch 009:   1114 / 1539 loss=3.62, wps=4779.3, ups=6.78, wpb=704.4, bsz=704.4, num_updates=13400, lr=0.000227, gnorm=4.436, clip=0, train_wall=7, gb_free=68.8, wall=2167 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:09:28]    INFO >> epoch 009:   1164 / 1539 loss=3.473, wps=5141.7, ups=7.18, wpb=716.5, bsz=716.5, num_updates=13450, lr=0.000227, gnorm=4.651, clip=0, train_wall=6, gb_free=69.7, wall=2174 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:09:36]    INFO >> epoch 009:   1214 / 1539 loss=3.272, wps=5400.1, ups=6.15, wpb=878.3, bsz=878.3, num_updates=13500, lr=0.000227, gnorm=5.2, clip=0, train_wall=8, gb_free=35.4, wall=2182 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:09:44]    INFO >> epoch 009:   1264 / 1539 loss=3.531, wps=4993.6, ups=6.59, wpb=758.3, bsz=758.3, num_updates=13550, lr=0.000227, gnorm=4.511, clip=0, train_wall=7, gb_free=70.2, wall=2190 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:09:51]    INFO >> epoch 009:   1314 / 1539 loss=3.597, wps=4596, ups=6.78, wpb=677.9, bsz=677.9, num_updates=13600, lr=0.000227, gnorm=4.614, clip=0, train_wall=7, gb_free=74.9, wall=2197 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:09:59]    INFO >> epoch 009:   1364 / 1539 loss=3.494, wps=4503.3, ups=6.89, wpb=653.4, bsz=653.4, num_updates=13650, lr=0.000227, gnorm=4.527, clip=0, train_wall=7, gb_free=72.8, wall=2204 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:10:07]    INFO >> epoch 009:   1414 / 1539 loss=3.392, wps=5410.3, ups=6.78, wpb=798.3, bsz=798.3, num_updates=13700, lr=0.000227, gnorm=4.355, clip=0, train_wall=7, gb_free=73.6, wall=2212 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:10:15]    INFO >> epoch 009:   1464 / 1539 loss=3.479, wps=4874.4, ups=5.83, wpb=835.4, bsz=835.4, num_updates=13750, lr=0.000227, gnorm=4.178, clip=0, train_wall=8, gb_free=74.6, wall=2220 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:10:22]    INFO >> epoch 009:   1514 / 1539 loss=3.479, wps=4415.3, ups=7.36, wpb=599.8, bsz=599.8, num_updates=13800, lr=0.000227, gnorm=4.552, clip=0, train_wall=6, gb_free=70.6, wall=2227 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:10:26]    INFO >> epoch 009 | loss 3.493 | wps 4550.7 | ups 6.33 | wpb 718.6 | bsz 718.6 | num_updates 13825 | lr 0.000227 | gnorm 4.511 | clip 0 | train_wall 211 | gb_free 74.9 | wall 2231 (progress_bar.py:267, print())[0m
[33m[2025-11-21 07:10:26] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 07:10:40]    INFO >> epoch 009 | valid on 'valid' subset | loss 3.741 | wps 11888.2 | wpb 5412.5 | bsz 5412.5 | num_updates 13825 | best_loss 4.97 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 07:10:41]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 07:10:41]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/checkpoints/checkpoint_last.pt (epoch 9 @ 13825 updates, score 3.741) (writing took 0.010898 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 07:10:41] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 07:10:46]    INFO >> epoch 010:     25 / 1539 loss=3.535, wps=1660.4, ups=2.26, wpb=733.7, bsz=733.7, num_updates=13850, lr=0.000193, gnorm=4.296, clip=0, train_wall=8, gb_free=69.4, wall=2249 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:10:53]    INFO >> epoch 010:     75 / 1539 loss=3.504, wps=4427.2, ups=7.19, wpb=615.8, bsz=615.8, num_updates=13900, lr=0.000193, gnorm=4.703, clip=0, train_wall=6, gb_free=75, wall=2256 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:11:00]    INFO >> epoch 010:    125 / 1539 loss=3.443, wps=4898.7, ups=6.82, wpb=718.7, bsz=718.7, num_updates=13950, lr=0.000193, gnorm=4.036, clip=0, train_wall=7, gb_free=72.9, wall=2263 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:11:08]    INFO >> epoch 010:    175 / 1539 loss=3.45, wps=5130.3, ups=6.96, wpb=736.8, bsz=736.8, num_updates=14000, lr=0.000193, gnorm=5.043, clip=0, train_wall=7, gb_free=73, wall=2271 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:11:17]    INFO >> epoch 010:    225 / 1539 loss=3.463, wps=4500, ups=6.23, wpb=722.5, bsz=722.5, num_updates=14050, lr=0.000193, gnorm=4.289, clip=0, train_wall=7, gb_free=68.4, wall=2279 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:11:25]    INFO >> epoch 010:    275 / 1539 loss=3.482, wps=4907.7, ups=6.06, wpb=810, bsz=810, num_updates=14100, lr=0.000193, gnorm=4.099, clip=0, train_wall=8, gb_free=72.9, wall=2287 (progress_bar.py:258, log())[0m
[33m[2025-11-21 07:11:32] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 277.25 MiB is free. Including non-PyTorch memory, this process has 78.85 GiB memory in use. Of the allocated memory 74.09 GiB is allocated by PyTorch, and 4.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 07:11:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:11:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:11:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 27           |        cudaMalloc retries: 50        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72059 MiB |  75868 MiB | 407061 GiB | 406991 GiB |
|       from large pool |  72044 MiB |  75853 MiB | 404562 GiB | 404492 GiB |
|       from small pool |     15 MiB |     16 MiB |   2499 GiB |   2499 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72059 MiB |  75868 MiB | 407061 GiB | 406991 GiB |
|       from large pool |  72044 MiB |  75853 MiB | 404562 GiB | 404492 GiB |
|       from small pool |     15 MiB |     16 MiB |   2499 GiB |   2499 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72042 MiB |  75850 MiB | 406194 GiB | 406123 GiB |
|       from large pool |  72027 MiB |  75835 MiB | 403698 GiB | 403628 GiB |
|       from small pool |     15 MiB |     16 MiB |   2495 GiB |   2495 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80228 MiB |  80228 MiB | 450050 MiB | 369822 MiB |
|       from large pool |  80208 MiB |  80208 MiB | 444108 MiB | 363900 MiB |
|       from small pool |     20 MiB |    222 MiB |   5942 MiB |   5922 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4758 MiB |   8350 MiB | 387150 GiB | 387145 GiB |
|       from large pool |   4753 MiB |   8342 MiB | 384297 GiB | 384292 GiB |
|       from small pool |      4 MiB |     19 MiB |   2852 GiB |   2852 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     502    |     511    |   27837 K  |   27836 K  |
|       from large pool |     308    |     317    |   12768 K  |   12768 K  |
|       from small pool |     194    |     226    |   15068 K  |   15068 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     502    |     511    |   27837 K  |   27836 K  |
|       from large pool |     308    |     317    |   12768 K  |   12768 K  |
|       from small pool |     194    |     226    |   15068 K  |   15068 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     147    |     245    |    5069    |    4922    |
|       from large pool |     137    |     137    |    2098    |    1961    |
|       from small pool |      10    |     111    |    2971    |    2961    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     138    |     138    |   15850 K  |   15850 K  |
|       from large pool |     117    |     117    |    8496 K  |    8496 K  |
|       from small pool |      21    |      41    |    7353 K  |    7353 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:11:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:11:32] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 07:11:32]    INFO >> epoch 010:    326 / 1539 loss=3.521, wps=4600.6, ups=6.51, wpb=706.8, bsz=706.8, num_updates=14150, lr=0.000193, gnorm=4.538, clip=0, train_wall=6, gb_free=73.5, wall=2295 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:11:41]    INFO >> epoch 010:    376 / 1539 loss=3.452, wps=4913.7, ups=6.73, wpb=729.7, bsz=729.7, num_updates=14200, lr=0.000193, gnorm=4.31, clip=0, train_wall=7, gb_free=72.2, wall=2302 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:11:49]    INFO >> epoch 010:    426 / 1539 loss=3.524, wps=4318.2, ups=6.83, wpb=632.4, bsz=632.4, num_updates=14250, lr=0.000193, gnorm=4.263, clip=0, train_wall=7, gb_free=71.9, wall=2309 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:11:56]    INFO >> epoch 010:    476 / 1539 loss=3.417, wps=4486.8, ups=6.69, wpb=670.3, bsz=670.3, num_updates=14300, lr=0.000193, gnorm=4.006, clip=0, train_wall=7, gb_free=60.6, wall=2317 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:12:03]    INFO >> epoch 010:    526 / 1539 loss=3.411, wps=4484.2, ups=7.11, wpb=631, bsz=631, num_updates=14350, lr=0.000193, gnorm=4.538, clip=0, train_wall=7, gb_free=72.9, wall=2324 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:12:10]    INFO >> epoch 010:    576 / 1539 loss=3.551, wps=4565.5, ups=6.89, wpb=662.7, bsz=662.7, num_updates=14400, lr=0.000193, gnorm=3.92, clip=0, train_wall=7, gb_free=71.4, wall=2331 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:12:19]    INFO >> epoch 010:    626 / 1539 loss=3.503, wps=4934.2, ups=7.01, wpb=704.3, bsz=704.3, num_updates=14450, lr=0.000193, gnorm=4.579, clip=0, train_wall=7, gb_free=71.4, wall=2338 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:12:26]    INFO >> epoch 010:    676 / 1539 loss=3.439, wps=5288.9, ups=6.87, wpb=770.2, bsz=770.2, num_updates=14500, lr=0.000193, gnorm=4.888, clip=0, train_wall=7, gb_free=75.2, wall=2346 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:12:34]    INFO >> epoch 010:    726 / 1539 loss=3.212, wps=5333, ups=6.62, wpb=806.1, bsz=806.1, num_updates=14550, lr=0.000193, gnorm=3.968, clip=0, train_wall=7, gb_free=73.9, wall=2353 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:12:41]    INFO >> epoch 010:    776 / 1539 loss=3.525, wps=4859.3, ups=6.5, wpb=747.4, bsz=747.4, num_updates=14600, lr=0.000193, gnorm=4.494, clip=0, train_wall=7, gb_free=72, wall=2361 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:12:50]    INFO >> epoch 010:    826 / 1539 loss=3.459, wps=4514.6, ups=6.76, wpb=668.1, bsz=668.1, num_updates=14650, lr=0.000193, gnorm=4.562, clip=0, train_wall=7, gb_free=71.9, wall=2368 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:12:58]    INFO >> epoch 010:    876 / 1539 loss=3.479, wps=4581.3, ups=6.54, wpb=700.3, bsz=700.3, num_updates=14700, lr=0.000193, gnorm=4.282, clip=2, train_wall=7, gb_free=71.8, wall=2376 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:13:06]    INFO >> epoch 010:    926 / 1539 loss=3.405, wps=5480.1, ups=6.35, wpb=862.5, bsz=862.5, num_updates=14750, lr=0.000193, gnorm=5.216, clip=0, train_wall=7, gb_free=73.9, wall=2384 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:13:12]    INFO >> epoch 010:    976 / 1539 loss=3.52, wps=4706.1, ups=7.3, wpb=645.1, bsz=645.1, num_updates=14800, lr=0.000193, gnorm=3.968, clip=0, train_wall=6, gb_free=72.2, wall=2391 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:13:21]    INFO >> epoch 010:   1026 / 1539 loss=3.528, wps=4801.3, ups=6.77, wpb=709, bsz=709, num_updates=14850, lr=0.000193, gnorm=4.439, clip=0, train_wall=7, gb_free=74.7, wall=2398 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:13:29]    INFO >> epoch 010:   1076 / 1539 loss=3.504, wps=5292.7, ups=6.73, wpb=786.3, bsz=786.3, num_updates=14900, lr=0.000193, gnorm=4.697, clip=0, train_wall=7, gb_free=71.9, wall=2405 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:13:36]    INFO >> epoch 010:   1126 / 1539 loss=3.46, wps=4323.3, ups=6.93, wpb=623.8, bsz=623.8, num_updates=14950, lr=0.000193, gnorm=4.058, clip=0, train_wall=7, gb_free=74.4, wall=2413 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:13:44]    INFO >> epoch 010:   1176 / 1539 loss=3.406, wps=4979.9, ups=6.37, wpb=781.8, bsz=781.8, num_updates=15000, lr=0.000193, gnorm=4.503, clip=0, train_wall=7, gb_free=76, wall=2420 (progress_bar.py:258, log())[0m
[33m[2025-11-21 07:13:45] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 45.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 77.33 GiB is allocated by PyTorch, and 1.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 07:13:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:13:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:13:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 28           |        cudaMalloc retries: 52        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79130 MiB |  79190 MiB | 429215 GiB | 429137 GiB |
|       from large pool |  78952 MiB |  79012 MiB | 426584 GiB | 426506 GiB |
|       from small pool |    177 MiB |    178 MiB |   2631 GiB |   2630 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79130 MiB |  79190 MiB | 429215 GiB | 429137 GiB |
|       from large pool |  78952 MiB |  79012 MiB | 426584 GiB | 426506 GiB |
|       from small pool |    177 MiB |    178 MiB |   2631 GiB |   2630 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79076 MiB |  79135 MiB | 428298 GiB | 428221 GiB |
|       from large pool |  78899 MiB |  78958 MiB | 425671 GiB | 425594 GiB |
|       from small pool |    176 MiB |    178 MiB |   2627 GiB |   2627 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80460 MiB |  80478 MiB | 453954 MiB | 373494 MiB |
|       from large pool |  80278 MiB |  80278 MiB | 447588 MiB | 367310 MiB |
|       from small pool |    182 MiB |    440 MiB |   6366 MiB |   6184 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1269 MiB |  10941 MiB | 409546 GiB | 409545 GiB |
|       from large pool |   1265 MiB |  10933 MiB | 406540 GiB | 406539 GiB |
|       from small pool |      4 MiB |     21 MiB |   3005 GiB |   3005 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3458    |    3461    |   29373 K  |   29369 K  |
|       from large pool |     582    |     583    |   13518 K  |   13518 K  |
|       from small pool |    2876    |    2879    |   15854 K  |   15851 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3458    |    3461    |   29373 K  |   29369 K  |
|       from large pool |     582    |     583    |   13518 K  |   13518 K  |
|       from small pool |    2876    |    2879    |   15854 K  |   15851 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     284    |     409    |    5339    |    5055    |
|       from large pool |     193    |     193    |    2156    |    1963    |
|       from small pool |      91    |     220    |    3183    |    3092    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     183    |     184    |   16705 K  |   16704 K  |
|       from large pool |     102    |     108    |    8988 K  |    8988 K  |
|       from small pool |      81    |      82    |    7716 K  |    7716 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:13:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:13:45] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 07:13:52]    INFO >> epoch 010:   1227 / 1539 loss=3.474, wps=4720.7, ups=5.95, wpb=793.8, bsz=793.8, num_updates=15050, lr=0.000193, gnorm=5.319, clip=0, train_wall=7, gb_free=74.9, wall=2429 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:14:01]    INFO >> epoch 010:   1277 / 1539 loss=3.312, wps=5160.5, ups=6.57, wpb=785.2, bsz=785.2, num_updates=15100, lr=0.000193, gnorm=4.784, clip=0, train_wall=7, gb_free=72.5, wall=2436 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:14:08]    INFO >> epoch 010:   1327 / 1539 loss=3.549, wps=4198.1, ups=6.7, wpb=626.3, bsz=626.3, num_updates=15150, lr=0.000193, gnorm=4.205, clip=0, train_wall=7, gb_free=72.8, wall=2444 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:14:16]    INFO >> epoch 010:   1377 / 1539 loss=3.475, wps=4645.3, ups=6.32, wpb=734.9, bsz=734.9, num_updates=15200, lr=0.000193, gnorm=4.968, clip=0, train_wall=7, gb_free=70.4, wall=2452 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:14:24]    INFO >> epoch 010:   1427 / 1539 loss=3.483, wps=4782.9, ups=6.53, wpb=732.6, bsz=732.6, num_updates=15250, lr=0.000193, gnorm=3.805, clip=0, train_wall=7, gb_free=76.7, wall=2459 (progress_bar.py:258, log())[0m
[33m[2025-11-21 07:14:32] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 419.25 MiB is free. Including non-PyTorch memory, this process has 78.71 GiB memory in use. Of the allocated memory 75.66 GiB is allocated by PyTorch, and 2.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 07:14:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:14:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:14:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 29           |        cudaMalloc retries: 54        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76955 MiB |  77478 MiB | 436949 GiB | 436873 GiB |
|       from large pool |  76944 MiB |  77467 MiB | 434273 GiB | 434198 GiB |
|       from small pool |     10 MiB |     21 MiB |   2675 GiB |   2675 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76955 MiB |  77478 MiB | 436949 GiB | 436873 GiB |
|       from large pool |  76944 MiB |  77467 MiB | 434273 GiB | 434198 GiB |
|       from small pool |     10 MiB |     21 MiB |   2675 GiB |   2675 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76934 MiB |  77455 MiB | 436015 GiB | 435940 GiB |
|       from large pool |  76923 MiB |  77445 MiB | 433344 GiB | 433269 GiB |
|       from small pool |     10 MiB |     21 MiB |   2671 GiB |   2671 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80086 MiB |  80424 MiB | 459228 MiB | 379142 MiB |
|       from large pool |  80068 MiB |  80218 MiB | 452838 MiB | 372770 MiB |
|       from small pool |     18 MiB |    206 MiB |   6390 MiB |   6372 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3130 MiB |   8864 MiB | 416901 GiB | 416898 GiB |
|       from large pool |   3123 MiB |   8856 MiB | 413844 GiB | 413841 GiB |
|       from small pool |      7 MiB |     31 MiB |   3057 GiB |   3057 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     502    |     509    |   29895 K  |   29895 K  |
|       from large pool |     315    |     322    |   13776 K  |   13776 K  |
|       from small pool |     187    |     240    |   16118 K  |   16118 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     502    |     509    |   29895 K  |   29895 K  |
|       from large pool |     315    |     322    |   13776 K  |   13776 K  |
|       from small pool |     187    |     240    |   16118 K  |   16118 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     116    |     295    |    5356    |    5240    |
|       from large pool |     107    |     192    |    2161    |    2054    |
|       from small pool |       9    |     103    |    3195    |    3186    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     111    |     111    |   16998 K  |   16997 K  |
|       from large pool |      93    |      93    |    9160 K  |    9160 K  |
|       from small pool |      18    |      49    |    7837 K  |    7837 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:14:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:14:32] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 07:14:33]    INFO >> epoch 010:   1478 / 1539 loss=3.595, wps=4027.4, ups=6.58, wpb=611.8, bsz=611.8, num_updates=15300, lr=0.000193, gnorm=4.348, clip=0, train_wall=6, gb_free=72.9, wall=2467 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:14:40]    INFO >> epoch 010:   1528 / 1539 loss=3.484, wps=5011.1, ups=6.74, wpb=743.6, bsz=743.6, num_updates=15350, lr=0.000193, gnorm=3.896, clip=0, train_wall=7, gb_free=72.6, wall=2475 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:14:42]    INFO >> epoch 010 | loss 3.465 | wps 4498.9 | ups 6.26 | wpb 718.6 | bsz 718.6 | num_updates 15361 | lr 0.000193 | gnorm 4.419 | clip 0.1 | train_wall 213 | gb_free 71.9 | wall 2476 (progress_bar.py:267, print())[0m
[33m[2025-11-21 07:14:42] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 07:14:55]    INFO >> epoch 010 | valid on 'valid' subset | loss 3.74 | wps 11836.5 | wpb 5412.5 | bsz 5412.5 | num_updates 15361 | best_loss 4.97 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 07:14:55]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 07:14:55]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/checkpoints/checkpoint_last.pt (epoch 10 @ 15361 updates, score 3.74) (writing took 0.015333 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 07:14:55] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 07:15:02]    INFO >> epoch 011:     39 / 1539 loss=3.518, wps=1682, ups=2.45, wpb=686.5, bsz=686.5, num_updates=15400, lr=0.000161, gnorm=4.491, clip=0, train_wall=7, gb_free=75, wall=2495 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:15:10]    INFO >> epoch 011:     89 / 1539 loss=3.456, wps=5218.5, ups=6.8, wpb=767.8, bsz=767.8, num_updates=15450, lr=0.000161, gnorm=4.753, clip=0, train_wall=7, gb_free=66.7, wall=2502 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:15:16]    INFO >> epoch 011:    139 / 1539 loss=3.541, wps=4751.5, ups=7.24, wpb=656.7, bsz=656.7, num_updates=15500, lr=0.000161, gnorm=4.5, clip=0, train_wall=6, gb_free=72.2, wall=2509 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:15:24]    INFO >> epoch 011:    189 / 1539 loss=3.269, wps=5057.9, ups=6.74, wpb=750.9, bsz=750.9, num_updates=15550, lr=0.000161, gnorm=4.345, clip=0, train_wall=7, gb_free=74.9, wall=2517 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:15:31]    INFO >> epoch 011:    239 / 1539 loss=3.462, wps=4986.1, ups=7.04, wpb=708.2, bsz=708.2, num_updates=15600, lr=0.000161, gnorm=4.682, clip=0, train_wall=7, gb_free=72.9, wall=2524 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:15:39]    INFO >> epoch 011:    289 / 1539 loss=3.419, wps=4766.6, ups=7.15, wpb=667, bsz=667, num_updates=15650, lr=0.000161, gnorm=4.586, clip=0, train_wall=7, gb_free=74.9, wall=2531 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:15:47]    INFO >> epoch 011:    339 / 1539 loss=3.312, wps=4830.4, ups=6.57, wpb=735.6, bsz=735.6, num_updates=15700, lr=0.000161, gnorm=4.753, clip=0, train_wall=7, gb_free=65.3, wall=2538 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:15:54]    INFO >> epoch 011:    389 / 1539 loss=3.5, wps=4844.2, ups=7.35, wpb=659.4, bsz=659.4, num_updates=15750, lr=0.000161, gnorm=4.255, clip=0, train_wall=6, gb_free=75.1, wall=2545 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:16:01]    INFO >> epoch 011:    439 / 1539 loss=3.492, wps=4535.7, ups=7.22, wpb=628.2, bsz=628.2, num_updates=15800, lr=0.000161, gnorm=4.748, clip=0, train_wall=6, gb_free=73.4, wall=2552 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:16:10]    INFO >> epoch 011:    489 / 1539 loss=3.535, wps=4152.8, ups=6.45, wpb=643.9, bsz=643.9, num_updates=15850, lr=0.000161, gnorm=4.552, clip=0, train_wall=7, gb_free=76.3, wall=2560 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:16:17]    INFO >> epoch 011:    539 / 1539 loss=3.604, wps=4546.7, ups=6.93, wpb=655.8, bsz=655.8, num_updates=15900, lr=0.000161, gnorm=4.091, clip=0, train_wall=7, gb_free=71.7, wall=2567 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:16:25]    INFO >> epoch 011:    589 / 1539 loss=3.235, wps=4937.6, ups=5.98, wpb=826.1, bsz=826.1, num_updates=15950, lr=0.000161, gnorm=4.736, clip=0, train_wall=8, gb_free=72.7, wall=2575 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:16:33]    INFO >> epoch 011:    639 / 1539 loss=3.571, wps=5170.2, ups=6.54, wpb=790.9, bsz=790.9, num_updates=16000, lr=0.000161, gnorm=3.941, clip=0, train_wall=7, gb_free=73.4, wall=2583 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:16:40]    INFO >> epoch 011:    689 / 1539 loss=3.528, wps=4797.1, ups=6.73, wpb=713.1, bsz=713.1, num_updates=16050, lr=0.000161, gnorm=4.573, clip=0, train_wall=7, gb_free=75.7, wall=2590 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:16:50]    INFO >> epoch 011:    739 / 1539 loss=3.47, wps=4294.4, ups=6.2, wpb=692.4, bsz=692.4, num_updates=16100, lr=0.000161, gnorm=4.178, clip=0, train_wall=7, gb_free=72.1, wall=2599 (progress_bar.py:258, log())[0m
[33m[2025-11-21 07:16:57] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 281.25 MiB is free. Including non-PyTorch memory, this process has 78.84 GiB memory in use. Of the allocated memory 75.66 GiB is allocated by PyTorch, and 2.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 07:16:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:16:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:16:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 30           |        cudaMalloc retries: 57        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76953 MiB |  77476 MiB | 462854 GiB | 462779 GiB |
|       from large pool |  76943 MiB |  77465 MiB | 460015 GiB | 459940 GiB |
|       from small pool |     10 MiB |     18 MiB |   2839 GiB |   2839 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76953 MiB |  77476 MiB | 462854 GiB | 462779 GiB |
|       from large pool |  76943 MiB |  77465 MiB | 460015 GiB | 459940 GiB |
|       from small pool |     10 MiB |     18 MiB |   2839 GiB |   2839 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76934 MiB |  77455 MiB | 461869 GiB | 461794 GiB |
|       from large pool |  76923 MiB |  77445 MiB | 459034 GiB | 458959 GiB |
|       from small pool |     10 MiB |     18 MiB |   2835 GiB |   2835 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80224 MiB |  80224 MiB | 462516 MiB | 382292 MiB |
|       from large pool |  80206 MiB |  80206 MiB | 455704 MiB | 375498 MiB |
|       from small pool |     18 MiB |    440 MiB |   6812 MiB |   6794 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3270 MiB |   8717 MiB | 443520 GiB | 443517 GiB |
|       from large pool |   3262 MiB |   8709 MiB | 440277 GiB | 440273 GiB |
|       from small pool |      7 MiB |     23 MiB |   3243 GiB |   3243 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     502    |     509    |   31683 K  |   31683 K  |
|       from large pool |     315    |     322    |   14561 K  |   14561 K  |
|       from small pool |     187    |     240    |   17122 K  |   17122 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     502    |     509    |   31683 K  |   31683 K  |
|       from large pool |     315    |     322    |   14561 K  |   14561 K  |
|       from small pool |     187    |     240    |   17122 K  |   17122 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     118    |     328    |    5570    |    5452    |
|       from large pool |     109    |     109    |    2164    |    2055    |
|       from small pool |       9    |     220    |    3406    |    3397    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     119    |     119    |   18011 K  |   18011 K  |
|       from large pool |      99    |      99    |    9665 K  |    9665 K  |
|       from small pool |      20    |      48    |    8346 K  |    8346 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:16:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:16:57] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 07:16:58]    INFO >> epoch 011:    790 / 1539 loss=3.437, wps=4043.7, ups=6.4, wpb=631.5, bsz=631.5, num_updates=16150, lr=0.000161, gnorm=4.037, clip=0, train_wall=7, gb_free=4.3, wall=2606 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:17:05]    INFO >> epoch 011:    840 / 1539 loss=3.393, wps=4320.9, ups=7.09, wpb=609.7, bsz=609.7, num_updates=16200, lr=0.000161, gnorm=4.256, clip=0, train_wall=7, gb_free=72.5, wall=2613 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:17:12]    INFO >> epoch 011:    890 / 1539 loss=3.517, wps=5609.7, ups=6.96, wpb=806.2, bsz=806.2, num_updates=16250, lr=0.000161, gnorm=4.336, clip=0, train_wall=7, gb_free=65.9, wall=2621 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:17:21]    INFO >> epoch 011:    940 / 1539 loss=3.433, wps=5220.1, ups=6.72, wpb=776.8, bsz=776.8, num_updates=16300, lr=0.000161, gnorm=4.721, clip=2, train_wall=7, gb_free=72.6, wall=2628 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:17:28]    INFO >> epoch 011:    990 / 1539 loss=3.361, wps=4872.3, ups=6.92, wpb=704.4, bsz=704.4, num_updates=16350, lr=0.000161, gnorm=5.147, clip=0, train_wall=7, gb_free=71.2, wall=2635 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:17:35]    INFO >> epoch 011:   1040 / 1539 loss=3.408, wps=4626.7, ups=6.79, wpb=681.9, bsz=681.9, num_updates=16400, lr=0.000161, gnorm=4.09, clip=0, train_wall=7, gb_free=65.2, wall=2643 (progress_bar.py:258, log())[0m
[33m[2025-11-21 07:17:42] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 57.25 MiB is free. Including non-PyTorch memory, this process has 79.06 GiB memory in use. Of the allocated memory 77.24 GiB is allocated by PyTorch, and 1.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 07:17:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:17:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:17:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 31           |        cudaMalloc retries: 58        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79036 MiB |  79096 MiB | 470446 GiB | 470369 GiB |
|       from large pool |  78859 MiB |  78919 MiB | 467561 GiB | 467484 GiB |
|       from small pool |    177 MiB |    178 MiB |   2884 GiB |   2884 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79036 MiB |  79096 MiB | 470446 GiB | 470369 GiB |
|       from large pool |  78859 MiB |  78919 MiB | 467561 GiB | 467484 GiB |
|       from small pool |    177 MiB |    178 MiB |   2884 GiB |   2884 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79016 MiB |  79075 MiB | 469445 GiB | 469368 GiB |
|       from large pool |  78839 MiB |  78899 MiB | 466565 GiB | 466488 GiB |
|       from small pool |    176 MiB |    177 MiB |   2880 GiB |   2880 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80448 MiB |  80450 MiB | 462742 MiB | 382294 MiB |
|       from large pool |  80266 MiB |  80266 MiB | 455764 MiB | 375498 MiB |
|       from small pool |    182 MiB |    184 MiB |   6978 MiB |   6796 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1351 MiB |   7746 MiB | 451936 GiB | 451934 GiB |
|       from large pool |   1346 MiB |   7741 MiB | 448640 GiB | 448639 GiB |
|       from small pool |      4 MiB |     18 MiB |   3295 GiB |   3295 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3447    |    3450    |   32201 K  |   32197 K  |
|       from large pool |     581    |     582    |   14814 K  |   14813 K  |
|       from small pool |    2866    |    2869    |   17387 K  |   17384 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3447    |    3450    |   32201 K  |   32197 K  |
|       from large pool |     581    |     582    |   14814 K  |   14813 K  |
|       from small pool |    2866    |    2869    |   17387 K  |   17384 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     201    |     202    |    5654    |    5453    |
|       from large pool |     110    |     110    |    2165    |    2055    |
|       from small pool |      91    |      92    |    3489    |    3398    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     176    |     178    |   18293 K  |   18293 K  |
|       from large pool |      92    |      94    |    9826 K  |    9826 K  |
|       from small pool |      84    |      86    |    8467 K  |    8467 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:17:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:17:42] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 07:17:44]    INFO >> epoch 011:   1091 / 1539 loss=3.411, wps=4669.9, ups=5.8, wpb=804.9, bsz=804.9, num_updates=16450, lr=0.000161, gnorm=4.651, clip=0, train_wall=7, gb_free=76.1, wall=2651 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:17:53]    INFO >> epoch 011:   1141 / 1539 loss=3.338, wps=5759.3, ups=6.53, wpb=881.7, bsz=881.7, num_updates=16500, lr=0.000161, gnorm=4.806, clip=0, train_wall=7, gb_free=52.7, wall=2659 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:18:00]    INFO >> epoch 011:   1191 / 1539 loss=3.361, wps=4342.7, ups=6.84, wpb=635, bsz=635, num_updates=16550, lr=0.000161, gnorm=4.623, clip=0, train_wall=7, gb_free=76.9, wall=2666 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:18:08]    INFO >> epoch 011:   1241 / 1539 loss=3.368, wps=4671.4, ups=6.33, wpb=738.4, bsz=738.4, num_updates=16600, lr=0.000161, gnorm=4.423, clip=0, train_wall=7, gb_free=74.7, wall=2674 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:18:15]    INFO >> epoch 011:   1291 / 1539 loss=3.547, wps=5270.4, ups=6.85, wpb=769.1, bsz=769.1, num_updates=16650, lr=0.000161, gnorm=3.747, clip=0, train_wall=7, gb_free=74, wall=2681 (progress_bar.py:258, log())[0m
[33m[2025-11-21 07:18:19] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 281.25 MiB is free. Including non-PyTorch memory, this process has 78.84 GiB memory in use. Of the allocated memory 74.09 GiB is allocated by PyTorch, and 4.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 07:18:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:18:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:18:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 32           |        cudaMalloc retries: 59        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72060 MiB |  75869 MiB | 476750 GiB | 476680 GiB |
|       from large pool |  72044 MiB |  75853 MiB | 473829 GiB | 473758 GiB |
|       from small pool |     15 MiB |     20 MiB |   2921 GiB |   2921 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72060 MiB |  75869 MiB | 476750 GiB | 476680 GiB |
|       from large pool |  72044 MiB |  75853 MiB | 473829 GiB | 473758 GiB |
|       from small pool |     15 MiB |     20 MiB |   2921 GiB |   2921 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72042 MiB |  75850 MiB | 475737 GiB | 475666 GiB |
|       from large pool |  72027 MiB |  75835 MiB | 472819 GiB | 472749 GiB |
|       from small pool |     15 MiB |     20 MiB |   2917 GiB |   2917 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80224 MiB |  80412 MiB | 462766 MiB | 382542 MiB |
|       from large pool |  80206 MiB |  80206 MiB | 455764 MiB | 375558 MiB |
|       from small pool |     18 MiB |    206 MiB |   7002 MiB |   6984 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5435 MiB |   8212 MiB | 458796 GiB | 458791 GiB |
|       from large pool |   5433 MiB |   8208 MiB | 455458 GiB | 455453 GiB |
|       from small pool |      2 MiB |     25 MiB |   3338 GiB |   3338 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     502    |     511    |   32623 K  |   32622 K  |
|       from large pool |     308    |     317    |   15017 K  |   15017 K  |
|       from small pool |     194    |     240    |   17605 K  |   17605 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     502    |     511    |   32623 K  |   32622 K  |
|       from large pool |     308    |     317    |   15017 K  |   15017 K  |
|       from small pool |     194    |     240    |   17605 K  |   17605 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     118    |     212    |    5666    |    5548    |
|       from large pool |     109    |     109    |    2165    |    2056    |
|       from small pool |       9    |     103    |    3501    |    3492    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     109    |     111    |   18527 K  |   18527 K  |
|       from large pool |      90    |      93    |    9956 K  |    9955 K  |
|       from small pool |      19    |      45    |    8571 K  |    8571 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:18:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:18:19] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 07:18:25]    INFO >> epoch 011:   1342 / 1539 loss=3.498, wps=4142.4, ups=6.09, wpb=680.6, bsz=680.6, num_updates=16700, lr=0.000161, gnorm=4.177, clip=0, train_wall=7, gb_free=73.5, wall=2690 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:18:32]    INFO >> epoch 011:   1392 / 1539 loss=3.555, wps=5168.7, ups=6.69, wpb=772.6, bsz=772.6, num_updates=16750, lr=0.000161, gnorm=4.14, clip=0, train_wall=7, gb_free=72.4, wall=2697 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:18:40]    INFO >> epoch 011:   1442 / 1539 loss=3.511, wps=4846.7, ups=6.84, wpb=708.6, bsz=708.6, num_updates=16800, lr=0.000161, gnorm=4.02, clip=0, train_wall=7, gb_free=62.2, wall=2704 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:18:47]    INFO >> epoch 011:   1492 / 1539 loss=3.319, wps=5154.3, ups=6.45, wpb=799.4, bsz=799.4, num_updates=16850, lr=0.000161, gnorm=4.19, clip=0, train_wall=7, gb_free=72.3, wall=2712 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:18:54]    INFO >> epoch 011 | loss 3.44 | wps 4544.1 | ups 6.32 | wpb 718.6 | bsz 718.6 | num_updates 16897 | lr 0.000161 | gnorm 4.422 | clip 0.1 | train_wall 211 | gb_free 74.7 | wall 2719 (progress_bar.py:267, print())[0m
[33m[2025-11-21 07:18:54] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 07:19:09]    INFO >> epoch 011 | valid on 'valid' subset | loss 3.613 | wps 11688 | wpb 5412.5 | bsz 5412.5 | num_updates 16897 | best_loss 4.97 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 07:19:09]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 07:19:09]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/checkpoints/checkpoint_last.pt (epoch 11 @ 16897 updates, score 3.613) (writing took 0.011081 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[32m[2025-11-21 07:19:09]    INFO >> æ—©åœ: éªŒè¯æ€§èƒ½å·²10è½®æœªæå‡ (train_enhanced.py:616, single_main())[0m
[32m[2025-11-21 07:19:09]    INFO >> è®­ç»ƒå®Œæˆï¼Œç”¨æ—¶ 2660.4 ç§’ (train_enhanced.py:626, single_main())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 07:19:10]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 07:19:10]    INFO >> æ‰€æœ‰æ—¥å¿—å·²ä¿å­˜åˆ°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs (train_enhanced.py:631, single_main())[0m
[32m[2025-11-21 07:19:10]    INFO >> 
================================================================================ (train_enhanced.py:634, single_main())[0m
[32m[2025-11-21 07:19:10]    INFO >> å¼€å§‹æµ‹è¯•... (train_enhanced.py:635, single_main())[0m
[32m[2025-11-21 07:19:10]    INFO >> ================================================================================ (train_enhanced.py:636, single_main())[0m
[32m[2025-11-21 07:19:10]    INFO >> åŠ è½½æœ€ä½³checkpoint: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/checkpoints/checkpoint_best.pt (train_enhanced.py:50, run_test_after_training())[0m
[32m[2025-11-21 07:19:10]    INFO >> æµ‹è¯•é›†: test (train_enhanced.py:51, run_test_after_training())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/checkpoint_utils.py:212: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(
[32m[2025-11-21 07:20:04]    INFO >> 
================================================================================ (train_enhanced.py:168, run_test_after_training())[0m
[32m[2025-11-21 07:20:04]    INFO >> æµ‹è¯•ç»“æžœ: (train_enhanced.py:169, run_test_after_training())[0m
[32m[2025-11-21 07:20:04]    INFO >> -------------------------------------------------------------------------------- (train_enhanced.py:170, run_test_after_training())[0m
[32m[2025-11-21 07:20:04]    INFO >> å¹³å‡Loss:      3.9465 (train_enhanced.py:171, run_test_after_training())[0m
[32m[2025-11-21 07:20:04]    INFO >> Acc@1:         18.62% (train_enhanced.py:172, run_test_after_training())[0m
[32m[2025-11-21 07:20:04]    INFO >> Acc@5:         59.31% (train_enhanced.py:173, run_test_after_training())[0m
[32m[2025-11-21 07:20:04]    INFO >> Acc@1 (å«any): 18.62% (train_enhanced.py:174, run_test_after_training())[0m
[32m[2025-11-21 07:20:04]    INFO >> Acc@5 (å«any): 59.31% (train_enhanced.py:175, run_test_after_training())[0m
[32m[2025-11-21 07:20:04]    INFO >> ================================================================================ (train_enhanced.py:176, run_test_after_training())[0m
[32m[2025-11-21 07:20:04]    INFO >> æµ‹è¯•ç»“æžœå·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/checkpoints/res.txt (train_enhanced.py:187, run_test_after_training())[0m
[32m[2025-11-21 07:20:04]    INFO >> è®­ç»ƒæ—¥å¿—å·²æ›´æ–°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs (train_enhanced.py:222, run_test_after_training())[0m
[TrainingLogger] æ—¥å¿—ç›®å½•: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs
[TrainingLogger] åŽŸå§‹è¾“å‡ºå°†ä¿å­˜åˆ°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/training_output.log
[TrainingLogger] Epoch 1 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/metrics.json
[TrainingLogger] Epoch 2 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/metrics.json
[TrainingLogger] Epoch 3 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/metrics.json
[TrainingLogger] Epoch 4 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/metrics.json
[TrainingLogger] Epoch 5 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/metrics.json
[TrainingLogger] Epoch 6 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/metrics.json
[TrainingLogger] Epoch 7 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/metrics.json
[TrainingLogger] Epoch 8 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/metrics.json
[TrainingLogger] Epoch 9 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/metrics.json
[TrainingLogger] Epoch 10 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/metrics.json
[TrainingLogger] Epoch 11 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/metrics.json

âœ“ layers_1 æˆåŠŸ

ç­‰å¾…3ç§’...

è¿›åº¦: 5/16

============================================================
å®žéªŒ: layers_3 - ä¸‰å±‚GGNN (å¢žåŠ æ·±åº¦)
æ—¶é—´: 2025-11-21 07:20:50
============================================================

[32m[2025-11-21 07:20:52]    INFO >> åŠ è½½é…ç½®: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_3/config.yml (train_enhanced.py:666, cli_main())[0m
[32m[2025-11-21 07:20:52]    INFO >> å•GPUè®­ç»ƒ... (train_enhanced.py:694, cli_main())[0m
[32m[2025-11-21 07:20:52]    INFO >> è®­ç»ƒæ—¥å¿—å°†ä¿å­˜åˆ°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_3/logs (train_enhanced.py:561, single_main())[0m
[32m[2025-11-21 07:20:52]    INFO >> [nodes] dictionary: 9999 types (typilus.py:102, setup_task())[0m
[32m[2025-11-21 07:20:52]    INFO >> [edges] dictionary: 0 types (typilus.py:102, setup_task())[0m
[32m[2025-11-21 07:20:52]    INFO >> [supernodes.annotation] dictionary: 99 types (typilus.py:106, setup_task())[0m
[TrainingLogger] æ—¥å¿—ç›®å½•: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_3/logs
[TrainingLogger] åŽŸå§‹è¾“å‡ºå°†ä¿å­˜åˆ°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_3/logs/training_output.log
Traceback (most recent call last):
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py", line 699, in <module>
    cli_main()
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py", line 695, in cli_main
    single_main(args)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py", line 566, in single_main
    model = task.build_model(args)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py", line 259, in build_model
    return models.build_model(args, config, self)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/models/__init__.py", line 37, in build_model
    return MODEL_REGISTRY[args['model']['arch']].build_model(args, config, task)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/models/type_prediction/typilus.py", line 144, in build_model
    encoder = GGNNEncoder(
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/models/type_prediction/typilus.py", line 85, in __init__
    self.ggnns = nn.ModuleList([
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/models/type_prediction/typilus.py", line 88, in <listcomp>
    backward=edge_backward, timesteps=timesteps[i], dropout=dropout)
IndexError: list index out of range

âœ— layers_3 å¤±è´¥(1)

å®žéªŒå¤±è´¥ï¼Œç»§ç»­? (y/n): 